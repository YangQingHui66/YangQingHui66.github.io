<!DOCTYPE html><html><head>
  <meta charset="utf-8">
  <title>【笔记】动手学深度学习v2（全73讲）李沐 | YJT's Blog</title>
  <meta name="keywords" content=" 深度学习 ">
  <meta name="description" content="【笔记】动手学深度学习v2（全73讲）李沐 | YJT's Blog">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="Oops～，我崩溃了！找不到你想要的页面 :(">
<meta property="og:type" content="website">
<meta property="og:title" content="404">
<meta property="og:url" content="http://example.com/404/index.html">
<meta property="og:site_name" content="YJT's Blog">
<meta property="og:description" content="Oops～，我崩溃了！找不到你想要的页面 :(">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-09-23T12:50:43.000Z">
<meta property="article:modified_time" content="2023-09-23T12:51:04.478Z">
<meta property="article:author" content="YJT">
<meta name="twitter:card" content="summary">


<link rel="icon" href="/img/avatar.jpg">

<link href="/css/style.css?v=1.1.0" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.1.0" rel="stylesheet">

<link href="//cdn.jsdelivr.net/npm/animate.css@4.1.0/animate.min.css" rel="stylesheet">

<script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
<script src="/js/titleTip.js?v=1.1.0"></script>

<script src="//cdn.jsdelivr.net/npm/highlightjs@9.16.2/highlight.pack.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script>



<script src="//cdn.jsdelivr.net/npm/jquery.cookie@1.4.1/jquery.cookie.min.js"></script>

<script src="/js/iconfont.js?v=1.1.0"></script>

<meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>
<body><div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="">
  <input class="theme_blog_path" value="">
  <input id="theme_shortcut" value="true">
  <input id="theme_highlight_on" value="true">
  <input id="theme_code_copy" value="true">
</div>




<aside class="nav">
    <div class="nav-left">
        <a href="/" class="avatar_target">
    <img class="avatar" src="/img/avatar.jpg">
</a>
<div class="author">
    <span>YJT</span>
</div>

<div class="icon">
    
</div>





<ul>
    <li>
        <div class="all active" data-rel="全部文章">全部文章
            
                <small>(47)</small>
            
        </div>
    </li>
    
        
            
                
    <li>
        <div data-rel="python">
            
            python
            <small>(2)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="sciencewriting">
            
            sciencewriting
            <small>(4)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="刷题">
            
            刷题
            <small>(2)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="动手学深度学习">
            
            动手学深度学习
            <small>(2)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="数学">
            
            数学
            <small>(1)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="论文写作">
            
            论文写作
            <small>(1)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="论文精读">
            
            论文精读
            <small>(12)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="论文翻译">
            
            论文翻译
            <small>(17)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="课程笔记">
            
            课程笔记
            <small>(2)</small>
        </div>
        
    </li>

            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
        
            
            
            
    </div>
    <div>
        
        
    </div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="47">

<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="iconfont icon-left"></i>
    </div>
    <div class="friends-content">
        <ul>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <div class="right-top">
        <div id="default-panel">
            <i class="iconfont icon-search" data-title="搜索 快捷键 i"></i>
            <div class="right-title">全部文章</div>
            <i class="iconfont icon-file-tree" data-title="切换到大纲视图 快捷键 w"></i>
        </div>
        <div id="search-panel">
            <i class="iconfont icon-left" data-title="返回"></i>
            <input id="local-search-input" autocomplete="off">
            <label class="border-line" for="input"></label>
            <i class="iconfont icon-case-sensitive" data-title="大小写敏感"></i>
            <i class="iconfont icon-tag" data-title="标签"></i>
        </div>
        <div id="outline-panel" style="display: none">
            <div class="right-title">大纲</div>
            <i class="iconfont icon-list" data-title="切换到文章列表"></i>
        </div>
    </div>

    <div class="tags-list">
    <input id="tag-search">
    <div class="tag-wrapper">
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>hexo</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>leetcode</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>markdown</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>python</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>YOLO</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>水声传感器网络</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>水声信道</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>水声通信</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>深度学习</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>目标检测</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>论文写作</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>论文精读</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>论文翻译</a>
            </li>
        
    </div>

</div>

    
    <nav id="title-list-nav">
        
        
        <a id="top" class="全部文章 " href="/2023/11/11/xue-xi-lu-xian-tu-shen-du-xue-xi-cong-ru-men-dao-ru-tu/" data-tag="深度学习" data-author="">
            <span class="post-title" title="【学习路线图】深度学习从入门到入土">【学习路线图】深度学习从入门到入土</span>
            <span class="post-date" title="2023-11-11 12:25:49">2023/11/11</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2024/03/12/spd-conv-no-more-strided-convolutions-or-pooling-a-new-cnn-building-block-for-low-resolution-images-and-small-objects/" data-tag="YOLO" data-author="">
            <span class="post-title" title="【SPD-conv】No More Strided Convolutions or Pooling: A New CNN Building Block for Low-Resolution Images and Small Objects">【SPD-conv】No More Strided Convolutions or Pooling: A New CNN Building Block for Low-Resolution Images and Small Objects</span>
            <span class="post-date" title="2024-03-12 12:58:01">2024/03/12</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2024/02/29/mu-biao-jian-ce-centralized-feature-pyramid-for-object-detection/" data-tag="目标检测" data-author="">
            <span class="post-title" title="【目标检测】Centralized Feature Pyramid for Object Detection">【目标检测】Centralized Feature Pyramid for Object Detection</span>
            <span class="post-date" title="2024-02-29 18:37:17">2024/02/29</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2024/02/27/mu-biao-jian-ce-you-only-look-once-unified-real-time-object-detection/" data-tag="YOLO" data-author="">
            <span class="post-title" title="【目标检测】You Only Look Once Unified Real Time Object Detection">【目标检测】You Only Look Once Unified Real Time Object Detection</span>
            <span class="post-date" title="2024-02-27 19:51:22">2024/02/27</span>
        </a>
        
        
        <a class="全部文章 sciencewriting " href="/2024/02/25/unit-3-writing-about-results/" data-tag="论文写作" data-author="">
            <span class="post-title" title="Unit 3 Writing about Results">Unit 3 Writing about Results</span>
            <span class="post-date" title="2024-02-25 23:50:30">2024/02/25</span>
        </a>
        
        
        <a class="全部文章 sciencewriting " href="/2024/02/25/unit-2-writing-about-methodology/" data-tag="论文写作" data-author="">
            <span class="post-title" title="Unit 2 Writing about Methodology">Unit 2 Writing about Methodology</span>
            <span class="post-date" title="2024-02-25 23:48:04">2024/02/25</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2024/02/25/mu-biao-jian-ce-object-detection-in-20-years-a-survey/" data-tag="目标检测" data-author="">
            <span class="post-title" title="【目标检测】Object Detection in 20 Years A Survey">【目标检测】Object Detection in 20 Years A Survey</span>
            <span class="post-date" title="2024-02-25 20:00:13">2024/02/25</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2024/02/25/mu-biao-jian-ce-underwater-target-detection-based-on-improved-yolov7/" data-tag="YOLO" data-author="">
            <span class="post-title" title="【目标检测】Underwater target detection based on improved YOLOv7">【目标检测】Underwater target detection based on improved YOLOv7</span>
            <span class="post-date" title="2024-02-25 19:18:01">2024/02/25</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2024/02/06/mu-biao-jian-ce-underwater-target-detection-based-on-faster-r-cnn-and-adversarial-occlusion-network/" data-tag="目标检测" data-author="">
            <span class="post-title" title="【目标检测】Underwater target detection based on Faster R-CNN and adversarial occlusion network">【目标检测】Underwater target detection based on Faster R-CNN and adversarial occlusion network</span>
            <span class="post-date" title="2024-02-06 15:05:16">2024/02/06</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2024/02/03/mu-biao-jian-ce-underwater-object-detection-algorithm-based-on-feature-enhancement-and-progressive-dynamic-aggregation-strategy/" data-tag="YOLO" data-author="">
            <span class="post-title" title="【目标检测】Underwater object detection algorithm based on feature enhancement and progressive dynamic aggregation strategy">【目标检测】Underwater object detection algorithm based on feature enhancement and progressive dynamic aggregation strategy</span>
            <span class="post-date" title="2024-02-03 14:38:42">2024/02/03</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2024/01/29/shui-sheng-xin-dao-channel-state-information-prediction-for-adaptive-underwater-acoustic-downlink-ofdma-system-deep-neural-networks-based-approach/" data-tag="水声信道" data-author="">
            <span class="post-title" title="【水声信道】Channel State Information Prediction for Adaptive Underwater Acoustic Downlink OFDMA System: Deep Neural Networks Based Approach">【水声信道】Channel State Information Prediction for Adaptive Underwater Acoustic Downlink OFDMA System: Deep Neural Networks Based Approach</span>
            <span class="post-date" title="2024-01-29 13:07:47">2024/01/29</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2024/01/28/xin-dao-gu-ji-sparse-channel-estimation-for-ofdm-based-underwater-acoustic-systems-in-rician-fading-with-a-new-omp-map-algorithm/" data-tag="水声信道" data-author="">
            <span class="post-title" title="【信道估计】Sparse Channel Estimation for OFDM-Based Underwater Acoustic Systems in Rician Fading With a New OMP-MAP Algorithm">【信道估计】Sparse Channel Estimation for OFDM-Based Underwater Acoustic Systems in Rician Fading With a New OMP-MAP Algorithm</span>
            <span class="post-date" title="2024-01-28 22:25:42">2024/01/28</span>
        </a>
        
        
        <a class="全部文章 sciencewriting " href="/2024/01/26/unit-1-how-to-write-an-introduction/" data-tag="论文写作" data-author="">
            <span class="post-title" title="Unit 1 How to Write an Introduction">Unit 1 How to Write an Introduction</span>
            <span class="post-date" title="2024-01-26 14:20:20">2024/01/26</span>
        </a>
        
        
        <a class="全部文章 sciencewriting " href="/2024/01/26/introduction-how-to-use-this-book/" data-tag="论文写作" data-author="">
            <span class="post-title" title="Introduction: How to Use This Book">Introduction: How to Use This Book</span>
            <span class="post-date" title="2024-01-26 10:44:16">2024/01/26</span>
        </a>
        
        
        <a class="全部文章 动手学深度学习 " href="/2024/01/25/04-shu-ju-cao-zuo-shu-ju-yu-chu-li/" data-tag="深度学习" data-author="">
            <span class="post-title" title="04数据操作+数据预处理">04数据操作+数据预处理</span>
            <span class="post-date" title="2024-01-25 14:25:01">2024/01/25</span>
        </a>
        
        
        <a class="全部文章 动手学深度学习 " href="/2024/01/25/00-03/" data-tag="深度学习" data-author="">
            <span class="post-title" title="00-03">00-03</span>
            <span class="post-date" title="2024-01-25 14:22:41">2024/01/25</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2024/01/24/lun-wen-fan-yi-channel-state-information-based-ranging-for-underwater-acoustic-sensor-networks/" data-tag="水声传感器网络" data-author="">
            <span class="post-title" title="【论文翻译】Channel State Information-Based Ranging for Underwater Acoustic Sensor Networks">【论文翻译】Channel State Information-Based Ranging for Underwater Acoustic Sensor Networks</span>
            <span class="post-date" title="2024-01-24 09:13:38">2024/01/24</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2023/12/12/lun-wen-fan-yi-retentive-network-a-successor-to-transformer-for-large-language-models/" data-tag="论文翻译" data-author="">
            <span class="post-title" title="【论文翻译】Retentive Network A Successor to Transformer for Large Language Models">【论文翻译】Retentive Network A Successor to Transformer for Large Language Models</span>
            <span class="post-date" title="2023-12-12 10:58:30">2023/12/12</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2023/12/12/lun-wen-fan-yi-a-novel-noise-aware-deep-learning-model-for-underwater-acoustic-denoising/" data-tag="论文翻译" data-author="">
            <span class="post-title" title="【论文翻译】A Novel Noise-Aware Deep Learning Model for Underwater Acoustic Denoising">【论文翻译】A Novel Noise-Aware Deep Learning Model for Underwater Acoustic Denoising</span>
            <span class="post-date" title="2023-12-12 10:47:35">2023/12/12</span>
        </a>
        
        
        <a class="全部文章 论文精读 " href="/2023/12/06/lun-wen-11-swin-transformer-lun-wen-zhu-duan-jing-du/" data-tag="论文精读" data-author="">
            <span class="post-title" title="【论文11】Swin Transformer论文逐段精读">【论文11】Swin Transformer论文逐段精读</span>
            <span class="post-date" title="2023-12-06 08:30:35">2023/12/06</span>
        </a>
        
        
        <a class="全部文章 论文精读 " href="/2023/12/04/lun-wen-10-deepmind-yong-ji-qi-xue-xi-zhi-dao-shu-xue-zhi-jue-lun-wen-zhu-duan-jing-du/" data-tag="论文精读" data-author="">
            <span class="post-title" title="【论文10】Deepmind用机器学习指导数学直觉论文逐段精读">【论文10】Deepmind用机器学习指导数学直觉论文逐段精读</span>
            <span class="post-date" title="2023-12-04 14:49:30">2023/12/04</span>
        </a>
        
        
        <a class="全部文章 论文精读 " href="/2023/12/04/lun-wen-9-moco-lun-wen-zhu-duan-jing-du/" data-tag="论文精读" data-author="">
            <span class="post-title" title="【论文9】MoCo论文逐段精读">【论文9】MoCo论文逐段精读</span>
            <span class="post-date" title="2023-12-04 14:49:15">2023/12/04</span>
        </a>
        
        
        <a class="全部文章 论文精读 " href="/2023/12/04/lun-wen-8-mae-lun-wen-zhu-duan-jing-du/" data-tag="论文精读" data-author="">
            <span class="post-title" title="【论文8】MAE论文逐段精读">【论文8】MAE论文逐段精读</span>
            <span class="post-date" title="2023-12-04 14:49:02">2023/12/04</span>
        </a>
        
        
        <a class="全部文章 论文精读 " href="/2023/12/04/lun-wen-7-vit-lun-wen-zhu-duan-jing-du/" data-tag="论文精读" data-author="">
            <span class="post-title" title="【论文7】ViT论文逐段精读">【论文7】ViT论文逐段精读</span>
            <span class="post-date" title="2023-12-04 14:46:34">2023/12/04</span>
        </a>
        
        
        <a class="全部文章 论文精读 " href="/2023/12/04/lun-wen-6-bert-lun-wen-zhu-duan-jing-du/" data-tag="论文精读" data-author="">
            <span class="post-title" title="【论文6】BERT论文逐段精读">【论文6】BERT论文逐段精读</span>
            <span class="post-date" title="2023-12-04 14:44:02">2023/12/04</span>
        </a>
        
        
        <a class="全部文章 论文精读 " href="/2023/12/04/lun-wen-5-gan-lun-wen-zhu-duan-jing-du/" data-tag="论文精读" data-author="">
            <span class="post-title" title="【论文5】GAN论文逐段精读">【论文5】GAN论文逐段精读</span>
            <span class="post-date" title="2023-12-04 14:43:45">2023/12/04</span>
        </a>
        
        
        <a class="全部文章 论文精读 " href="/2023/12/04/lun-wen-4-ling-ji-chu-duo-tu-xiang-jie-tu-shen-jing-wang-luo-gnn-gcn/" data-tag="论文精读" data-author="">
            <span class="post-title" title="【论文4】零基础多图详解图神经网络（GNN-GCN）">【论文4】零基础多图详解图神经网络（GNN-GCN）</span>
            <span class="post-date" title="2023-12-04 14:43:23">2023/12/04</span>
        </a>
        
        
        <a class="全部文章 论文精读 " href="/2023/12/04/lun-wen-3-transformer-lun-wen-zhu-duan-jing-du/" data-tag="论文精读" data-author="">
            <span class="post-title" title="【论文3】Transformer论文逐段精读">【论文3】Transformer论文逐段精读</span>
            <span class="post-date" title="2023-12-04 14:39:57">2023/12/04</span>
        </a>
        
        
        <a class="全部文章 论文精读 " href="/2023/12/04/lun-wen-2-cheng-qi-ji-suan-ji-shi-jue-ban-bian-tian-de-resnet/" data-tag="论文精读" data-author="">
            <span class="post-title" title="【论文2】撑起计算机视觉半边天的ResNet">【论文2】撑起计算机视觉半边天的ResNet</span>
            <span class="post-date" title="2023-12-04 14:38:53">2023/12/04</span>
        </a>
        
        
        <a class="全部文章 论文精读 " href="/2023/12/04/lun-wen-1-shen-du-xue-xi-dian-ji-zuo-zhi-yi-alexnet/" data-tag="论文精读" data-author="">
            <span class="post-title" title="【论文1】深度学习奠基作之一：AlexNet">【论文1】深度学习奠基作之一：AlexNet</span>
            <span class="post-date" title="2023-12-04 14:36:53">2023/12/04</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2023/12/02/lun-wen-fan-yi-swin-transformer-hierarchical-vit-using-shifted-windows/" data-tag="论文翻译" data-author="">
            <span class="post-title" title="【论文翻译】Swin Transformer: Hierarchical ViT using Shifted Windows">【论文翻译】Swin Transformer: Hierarchical ViT using Shifted Windows</span>
            <span class="post-date" title="2023-12-02 10:07:14">2023/12/02</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2023/11/29/lun-wen-fan-yi-spectrum-sensing-for-underwater-cognitive-radio-with-limited-sensing-time/" data-tag="论文翻译" data-author="">
            <span class="post-title" title="【论文翻译】Spectrum Sensing for Underwater Cognitive Radio With Limited Sensing Time">【论文翻译】Spectrum Sensing for Underwater Cognitive Radio With Limited Sensing Time</span>
            <span class="post-date" title="2023-11-29 10:05:04">2023/11/29</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2023/11/23/lun-wen-fan-yi-underwater-acoustic-communication-receiver-using-deep-belief-network/" data-tag="论文翻译" data-author="">
            <span class="post-title" title="【论文翻译】Underwater Acoustic Communication Receiver Using Deep Belief Network">【论文翻译】Underwater Acoustic Communication Receiver Using Deep Belief Network</span>
            <span class="post-date" title="2023-11-23 19:39:00">2023/11/23</span>
        </a>
        
        
        <a class="全部文章 python " href="/2023/11/13/bi-ji-li-yong-python-jin-xing-shu-ju-fen-xi/" data-tag="python" data-author="">
            <span class="post-title" title="【笔记】利用Python进行数据分析">【笔记】利用Python进行数据分析</span>
            <span class="post-date" title="2023-11-13 15:39:51">2023/11/13</span>
        </a>
        
        
        <a class="全部文章 论文写作 " href="/2023/11/09/bi-ji-the-craft-of-research/" data-tag="论文写作" data-author="">
            <span class="post-title" title="【笔记】The Craft of Research">【笔记】The Craft of Research</span>
            <span class="post-date" title="2023-11-09 22:38:28">2023/11/09</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2023/11/04/lun-wen-fan-yi-a-transformer-based-deep-learning-network-for-underwater-acoustic-target-recognition/" data-tag="论文翻译" data-author="">
            <span class="post-title" title="【论文翻译】A Transformer-Based Deep Learning Network for Underwater Acoustic Target Recognition">【论文翻译】A Transformer-Based Deep Learning Network for Underwater Acoustic Target Recognition</span>
            <span class="post-date" title="2023-11-04 08:55:31">2023/11/04</span>
        </a>
        
        
        <a class="全部文章 " href="/2023/11/03/bi-ji-shui-sheng-tong-xin-zhou-sheng-li/" data-tag="水声通信" data-author="">
            <span class="post-title" title="【笔记】水声通信（周胜利）">【笔记】水声通信（周胜利）</span>
            <span class="post-date" title="2023-11-03 16:40:05">2023/11/03</span>
        </a>
        
        
        <a class="全部文章 python " href="/2023/11/02/bi-ji-ling-ji-chu-ru-men-xue-xi-python-xiao-jia-yu/" data-tag="python" data-author="">
            <span class="post-title" title="【笔记】零基础入门学习Python（小甲鱼）">【笔记】零基础入门学习Python（小甲鱼）</span>
            <span class="post-date" title="2023-11-02 15:58:57">2023/11/02</span>
        </a>
        
        
        <a class="全部文章 论文翻译 " href="/2023/11/01/lun-wen-fan-yi-attention-is-all-you-need/" data-tag="论文翻译" data-author="">
            <span class="post-title" title="【论文翻译】Attention is all you need">【论文翻译】Attention is all you need</span>
            <span class="post-date" title="2023-11-01 22:21:09">2023/11/01</span>
        </a>
        
        
        <a class="全部文章 论文精读 " href="/2023/10/31/bi-ji-ai-lun-wen-jing-du-li-mu/" data-tag="论文精读" data-author="">
            <span class="post-title" title="【笔记】AI论文精读（李沐）">【笔记】AI论文精读（李沐）</span>
            <span class="post-date" title="2023-10-31 13:32:42">2023/10/31</span>
        </a>
        
        
        <a class="全部文章 数学 " href="/2023/10/28/bi-ji-shi-fen-zhong-ji-qi-xue-xi-xi-lie-shi-pin-tong-ji-xue-xi-fang-fa-jian-bo-shi/" data-tag="深度学习" data-author="">
            <span class="post-title" title="【笔记】十分钟 机器学习 系列视频 《统计学习方法》（简博士）">【笔记】十分钟 机器学习 系列视频 《统计学习方法》（简博士）</span>
            <span class="post-date" title="2023-10-28 08:16:18">2023/10/28</span>
        </a>
        
        
        <a class="全部文章 课程笔记 " href="/2023/10/11/bi-ji-pytorch-shen-du-xue-xi-kuai-su-ru-men-jiao-cheng-xiao-tu-dui/" data-tag="深度学习" data-author="">
            <span class="post-title" title="【笔记】PyTorch深度学习快速入门教程（小土堆）">【笔记】PyTorch深度学习快速入门教程（小土堆）</span>
            <span class="post-date" title="2023-10-11 08:34:42">2023/10/11</span>
        </a>
        
        
        <a class="全部文章 课程笔记 " href="/2023/09/30/bi-ji-dong-shou-xue-shen-du-xue-xi-v2-quan-73-jiang-li-mu/" data-tag="深度学习" data-author="">
            <span class="post-title" title="【笔记】动手学深度学习v2（全73讲）李沐">【笔记】动手学深度学习v2（全73讲）李沐</span>
            <span class="post-date" title="2023-09-30 13:11:50">2023/09/30</span>
        </a>
        
        
        <a class="全部文章 刷题 " href="/2023/09/26/leetcode-704-er-fen-cha-zhao/" data-tag="leetcode" data-author="">
            <span class="post-title" title="【leetcode】704、二分查找">【leetcode】704、二分查找</span>
            <span class="post-date" title="2023-09-26 17:22:01">2023/09/26</span>
        </a>
        
        
        <a class="全部文章 刷题 " href="/2023/09/26/leetcode-1-liang-shu-zhi-he/" data-tag="leetcode" data-author="">
            <span class="post-title" title="【leetcode】1、两数之和">【leetcode】1、两数之和</span>
            <span class="post-date" title="2023-09-26 17:21:41">2023/09/26</span>
        </a>
        
        
        <a class="全部文章 " href="/2023/09/23/markdown-yu-fa/" data-tag="markdown" data-author="">
            <span class="post-title" title="markdown语法">markdown语法</span>
            <span class="post-date" title="2023-09-23 21:57:53">2023/09/23</span>
        </a>
        
        
        <a class="全部文章 " href="/2023/09/23/da-jian-ge-ren-bo-ke/" data-tag="hexo" data-author="">
            <span class="post-title" title="搭建个人博客">搭建个人博客</span>
            <span class="post-date" title="2023-09-23 21:57:26">2023/09/23</span>
        </a>
        
        <div id="no-item-tips">

        </div>
    </nav>
    <div id="outline-list">
    </div>
</div>

    </div>
    <div class="hide-list">
        <div class="semicircle" data-title="切换全屏 快捷键 s">
            <div class="brackets first">&lt;</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div id="post">
    <div class="pjax">
        <article id="post-【笔记】动手学深度学习v2（全73讲）李沐" class="article article-type-post" itemscope="" itemprop="blogPost">
    
        <h1 class="article-title">【笔记】动手学深度学习v2（全73讲）李沐</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            <i class="iconfont icon-category"></i>
            
            
            <a data-rel="课程笔记">课程笔记</a>
            
        </span>
        
        
        <span class="tag">
            <i class="iconfont icon-tag"></i>
            
            <a class="color5">深度学习</a>
            
        </span>
        
    </div>
    <div class="article-meta">
        
            发布时间 : <time class="date" title="最后更新: 2024-01-25 14:25:48">2023-09-30 13:11</time>
        
    </div>
    <div class="article-meta">
        
        
        <span id="busuanzi_container_page_pv">
            阅读 :<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#00%E9%A2%84%E5%91%8A"><span class="toc-text">00预告</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#01%E8%AF%BE%E7%A8%8B%E5%AE%89%E6%8E%92"><span class="toc-text">01课程安排</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87"><span class="toc-text">目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%AE%B9"><span class="toc-text">内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%A0%E5%B0%86%E5%AD%A6%E5%88%B0%E4%BB%80%E4%B9%88"><span class="toc-text">你将学到什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B5%84%E6%BA%90"><span class="toc-text">资源</span></a></li></ol></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#02%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D"><span class="toc-text">02深度学习介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#03%E5%AE%89%E8%A3%85"><span class="toc-text">03安装</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#05%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-text">05线性代数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#06%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97"><span class="toc-text">06矩阵计算</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#07-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="toc-text">07 自动求导</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#08%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-text">08线性回归 + 基础优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="toc-text">一个简化模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-text">线性模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%81%9A%E6%98%AF%E5%8D%95%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">线性模型可以看做是单层神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%BA%90%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%A7%91%E5%AD%A6"><span class="toc-text">神经网络源于神经科学</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A1%E9%87%8F%E9%A2%84%E4%BC%B0%E8%B4%A8%E9%87%8F"><span class="toc-text">衡量预估质量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-text">训练数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="toc-text">参数学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%98%BE%E7%A4%BA%E8%A7%A3"><span class="toc-text">显示解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-text">选择学习率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">小批量随机梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E6%89%B9%E9%87%8F%E5%A4%A7%E5%B0%8F"><span class="toc-text">选择批量大小</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text">线性回归的从零开始实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#09-Softmax-%E5%9B%9E%E5%BD%92-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">09 Softmax 回归 + 损失函数 + 图片分类数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92-vs-%E5%88%86%E7%B1%BB"><span class="toc-text">回归 vs 分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kaggle%E4%B8%8A%E7%9A%84%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-text">Kaggle上的分类问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E5%9B%9E%E5%BD%92%E5%88%B0%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB"><span class="toc-text">从回归到多类分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E5%9B%9E%E5%BD%92%E5%88%B0%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB%E2%80%94%E2%80%94%E5%9D%87%E6%96%B9%E6%8D%9F%E5%A4%B1"><span class="toc-text">从回归到多类分类——均方损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Softmax%E5%92%8C%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1"><span class="toc-text">Softmax和交叉熵损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-2"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text">softmax回归的从零开始实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-text">10多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-text">感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-text">训练感知机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%B6%E6%95%9B%E5%AE%9A%E7%90%86"><span class="toc-text">收敛定理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#XOR%E9%97%AE%E9%A2%98%EF%BC%88Minsky-Papert%EF%BC%8C1969%EF%BC%89"><span class="toc-text">XOR问题（Minsky &amp; Papert，1969）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-3"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-text">多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0XOR"><span class="toc-text">学习XOR</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82"><span class="toc-text">单隐藏层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82%E2%80%94%E2%80%94%E5%8D%95%E5%88%86%E7%B1%BB"><span class="toc-text">单隐藏层——单分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sigmoid%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">Sigmoid激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tanh%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">Tanh激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">ReLU激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB"><span class="toc-text">多类分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E9%9A%90%E8%97%8F%E5%B1%82"><span class="toc-text">多隐藏层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-4"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-1"><span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-text">11模型选择 + 过拟合和欠拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E5%92%8C%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="toc-text">训练误差和泛化误差</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">验证数据集和测试数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-%E5%88%99%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-text">K-则交叉验证</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-text">过拟合和欠拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F"><span class="toc-text">模型容量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">模型容量的影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%B0%E8%AE%A1%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F"><span class="toc-text">估计模型容量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VC%E7%BB%B4"><span class="toc-text">VC维</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84VC%E7%BB%B4"><span class="toc-text">线性分类器的VC维</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VC%E7%BB%B4%E7%9A%84%E7%94%A8%E5%A4%84"><span class="toc-text">VC维的用处</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-text">数据复杂度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-5"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-2"><span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80"><span class="toc-text">12权重衰退</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%9D%87%E6%96%B9%E8%8C%83%E6%95%B0%E4%BD%9C%E4%B8%BA%E7%A1%AC%E6%80%A7%E9%99%90%E5%88%B6"><span class="toc-text">使用均方范数作为硬性限制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%9D%87%E6%96%B9%E8%8C%83%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%9F%94%E6%80%A7%E9%99%90%E5%88%B6"><span class="toc-text">使用均方范数作为柔性限制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BC%94%E7%A4%BA%E5%AF%B9%E6%9C%80%E4%BC%98%E8%A7%A3%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">演示对最优解的影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E6%B3%95%E5%88%99"><span class="toc-text">参数更新法则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-6"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-3"><span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13%E4%B8%A2%E5%BC%83%E6%B3%95"><span class="toc-text">13丢弃法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E6%9C%BA"><span class="toc-text">动机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E5%81%8F%E5%B7%AE%E7%9A%84%E5%8A%A0%E5%85%A5%E5%99%AA%E9%9F%B3"><span class="toc-text">无偏差的加入噪音</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E4%B8%A2%E5%BC%83%E6%B3%95"><span class="toc-text">使用丢弃法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E4%B8%AD%E7%9A%84%E4%B8%A2%E5%BC%83%E6%B3%95"><span class="toc-text">推理中的丢弃法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-7"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-4"><span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#14%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7-%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">14数值稳定性 + 模型初始化和激活函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#19%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-text">19卷积层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%A4%E4%B8%AA%E5%8E%9F%E5%88%99"><span class="toc-text">两个原则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E6%96%B0%E8%80%83%E5%AF%9F%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="toc-text">重新考察全连接层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%88%991%E2%80%94%E5%B9%B3%E7%A7%BB%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="toc-text">原则1—平移不变性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%88%992%E2%80%94%E5%B1%80%E9%83%A8%E6%80%A75"><span class="toc-text">原则2—局部性5</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-text">卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E7%BB%B4%E4%BA%A4%E5%8F%89%E7%9B%B8%E5%85%B3"><span class="toc-text">二维交叉相关</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E7%BB%B4%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-text">二维卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%9B%B8%E5%85%B3vs%E5%8D%B7%E7%A7%AF"><span class="toc-text">交叉相关vs卷积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E7%BB%B4%E5%92%8C%E4%B8%89%E7%BB%B4%E4%BA%A4%E5%8F%89%E7%9B%B8%E5%85%B3"><span class="toc-text">一维和三维交叉相关</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#68-Transformer"><span class="toc-text">68 Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E6%9E%B6%E6%9E%84"><span class="toc-text">Transformer架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">多头注意力</span></a></li></ol></li>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="00预告"><a href="#00预告" class="headerlink" title="00预告"></a>00预告</h1><p>学习深度学习关键是动手</p>
<ul>
<li>深度学习是人工智能最热的领域</li>
<li>核心是神经网络</li>
<li>神经网络是一门语言</li>
<li>应该像学习Python/C++一样学习深度学习<br><a target="_blank" rel="noopener" href="https://zh.d2l.ai/">《动手深度学习》书籍链接</a><br><a target="_blank" rel="noopener" href="https://courses.d2l.ai/zh-v2/">课程网站</a></li>
</ul>
<h1 id="01课程安排"><a href="#01课程安排" class="headerlink" title="01课程安排"></a>01课程安排</h1><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ul>
<li>介绍深度学习经典和最新模型<ul>
<li>LeNet, ResNet, LSTM, BERT, …</li>
</ul>
</li>
<li>机器学习<ul>
<li>损失函数、目标函数、 过拟合、 优化</li>
</ul>
</li>
<li>实践<ul>
<li>使用Pytorch实现介绍的知识点</li>
<li>在真实数据上体验算法效果</li>
</ul>
</li>
</ul>
<h3 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h3><ul>
<li>深度学习基础——线性神经网络，多层感知机</li>
<li>卷积神经网络——LeNet, AlexNet, VGG, Inception, ResNet</li>
<li>循环神经网络——RNN, GRU, LSTM, seq2seq</li>
<li>注意力机制——Attention, Transformer</li>
<li>优化算法——SGD, Momentum, Adam</li>
<li>高性能计算——并行，多GPU，分布式</li>
<li>计算机视觉——目标检测，语义分割</li>
<li>自然语言处理——词嵌入，BERT</li>
</ul>
<h3 id="你将学到什么"><a href="#你将学到什么" class="headerlink" title="你将学到什么"></a>你将学到什么</h3><ul>
<li>What<ul>
<li>深度学习里有那些技术</li>
</ul>
</li>
<li>How<ul>
<li>如何实现和调参</li>
</ul>
</li>
<li>Why<ul>
<li>背后的原因（直觉、数学）</li>
</ul>
</li>
</ul>
<h3 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h3><ul>
<li><a target="_blank" rel="noopener" href="https://courses.d2l.ai/zh-v2/">课程主页</a></li>
<li><a target="_blank" rel="noopener" href="https://zh.d2l.ai/">教材</a></li>
<li><a target="_blank" rel="noopener" href="https://discuss.d2l.ai/c/16">课程论坛讨论</a></li>
<li><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/">Pytorch论坛</a></li>
</ul>
<h1 id="02深度学习介绍"><a href="#02深度学习介绍" class="headerlink" title="02深度学习介绍"></a>02深度学习介绍</h1><h1 id="03安装"><a href="#03安装" class="headerlink" title="03安装"></a>03安装</h1><p>pip install d2l -i <a target="_blank" rel="noopener" href="http://pypi.douban.com/simple">http://pypi.douban.com/simple</a> –trusted-host pypi.douban.com –user</p>
<h1 id="05线性代数"><a href="#05线性代数" class="headerlink" title="05线性代数"></a>05线性代数</h1><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>标量由只有一个元素的张量表示</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([3.0])</span><br><span class="line">y = torch.tensor([2.0])</span><br><span class="line"></span><br><span class="line">x + y, x * y, x / y, x**y</span><br></pre></td></tr></tbody></table></figure>
<p>你可以将向量视为标量值组成的列表</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(4)</span><br><span class="line">x</span><br></pre></td></tr></tbody></table></figure>
<p>通过张量的索引来访问任一元素</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[3]</span><br></pre></td></tr></tbody></table></figure>



<h1 id="06矩阵计算"><a href="#06矩阵计算" class="headerlink" title="06矩阵计算"></a>06矩阵计算</h1><h1 id="07-自动求导"><a href="#07-自动求导" class="headerlink" title="07 自动求导"></a>07 自动求导</h1><h1 id="08线性回归-基础优化算法"><a href="#08线性回归-基础优化算法" class="headerlink" title="08线性回归 + 基础优化算法"></a>08线性回归 + 基础优化算法</h1><p><em><strong>线性回归</strong></em></p>
<h3 id="一个简化模型"><a href="#一个简化模型" class="headerlink" title="一个简化模型"></a>一个简化模型</h3><p>![](<a target="_blank" rel="noopener" href="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-1">https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-1</a> .png)</p>
<h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-2.png"></p>
<h3 id="线性模型可以看做是单层神经网络"><a href="#线性模型可以看做是单层神经网络" class="headerlink" title="线性模型可以看做是单层神经网络"></a>线性模型可以看做是单层神经网络</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-3.png"></p>
<h3 id="神经网络源于神经科学"><a href="#神经网络源于神经科学" class="headerlink" title="神经网络源于神经科学"></a>神经网络源于神经科学</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-4.png"></p>
<h3 id="衡量预估质量"><a href="#衡量预估质量" class="headerlink" title="衡量预估质量"></a>衡量预估质量</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-5.png"></p>
<h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-6.png"></p>
<h3 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-7.png"></p>
<h3 id="显示解"><a href="#显示解" class="headerlink" title="显示解"></a>显示解</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-8.png"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-9.png"><br><em><strong>基础优化方法</strong></em></p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-10.png"></p>
<h3 id="选择学习率"><a href="#选择学习率" class="headerlink" title="选择学习率"></a>选择学习率</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-11.png"></p>
<h3 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-12.png"></p>
<h3 id="选择批量大小"><a href="#选择批量大小" class="headerlink" title="选择批量大小"></a>选择批量大小</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-13.png"></p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-14.png"></p>
<h3 id="线性回归的从零开始实现"><a href="#线性回归的从零开始实现" class="headerlink" title="线性回归的从零开始实现"></a>线性回归的从零开始实现</h3><p>我们将从零开始实现整个方法，包括数据流水线、模型、损失函数和小批量随机梯度下降优化器</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import math</span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">from d2l import torch as d2l</span><br></pre></td></tr></tbody></table></figure>


<h1 id="09-Softmax-回归-损失函数-图片分类数据集"><a href="#09-Softmax-回归-损失函数-图片分类数据集" class="headerlink" title="09 Softmax 回归 + 损失函数 + 图片分类数据集"></a>09 Softmax 回归 + 损失函数 + 图片分类数据集</h1><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-1.png"><br>Softmax 回归是机器学习中非常经典、重要的模型，虽然它名字里带着回归，但它其实是个分类问题</p>
<h3 id="回归-vs-分类"><a href="#回归-vs-分类" class="headerlink" title="回归 vs 分类"></a>回归 vs 分类</h3><ul>
<li>回归估计一个连续值</li>
<li>分类预测一个离散类别<br>MNIST：手写数字识别（10类）<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-2.png"><br>ImgeNet：自然物体分类（1000类）<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-3.png"></li>
</ul>
<h3 id="Kaggle上的分类问题"><a href="#Kaggle上的分类问题" class="headerlink" title="Kaggle上的分类问题"></a>Kaggle上的分类问题</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-4.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-5.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-6.png"></p>
<h3 id="从回归到多类分类"><a href="#从回归到多类分类" class="headerlink" title="从回归到多类分类"></a>从回归到多类分类</h3><p><code>回归</code></p>
<ul>
<li>单连续数值输出</li>
<li>自然区间R</li>
<li>跟真实值的区别作为损失<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-7.png"><br><code>分类</code></li>
<li>通常多个输出</li>
<li>输出i是预测为第i类的置信度<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-8.png"></li>
</ul>
<h3 id="从回归到多类分类——均方损失"><a href="#从回归到多类分类——均方损失" class="headerlink" title="从回归到多类分类——均方损失"></a>从回归到多类分类——均方损失</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-9.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-10.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-11.png"></p>
<h3 id="Softmax和交叉熵损失"><a href="#Softmax和交叉熵损失" class="headerlink" title="Softmax和交叉熵损失"></a>Softmax和交叉熵损失</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-12.png"></p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><ul>
<li>Softmax回归是一个多类分类模型</li>
<li>使用Softmax操作得到每个类的预测置信度</li>
<li>使用交叉熵来衡量预测和标号的区别</li>
</ul>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>用来衡量预测值和真实值之间的区别<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-13.png"></p>
<ol>
<li><p>均方损失函数<br> <img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-14.png"><br> 蓝色的线——当y=0时，变换预测值后y’的函数<br> 绿色的线——似然函数<br> 橙色的线——损失函数的梯度</p>
</li>
<li><p>绝对值损失函数</p>
</li>
</ol>
<h3 id="softmax回归的从零开始实现"><a href="#softmax回归的从零开始实现" class="headerlink" title="softmax回归的从零开始实现"></a>softmax回归的从零开始实现</h3><p>就像我们从零开始实现线性回归一样， 你应该知道实现softmax的细节</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from IPython import display</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">batch_size = 256 #每次随机读取256张图片</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">#返回训练集和测试集的迭代器</span><br></pre></td></tr></tbody></table></figure>
<p>将展平每个图像，把它们看作长度为784的向量。 因为我们的数据集有10个类别，所以网络输出维度为 10</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#图片的长和宽都为28，28*28=784</span><br><span class="line">#对于softmax函数而言，输入必须是一个向量，需要把整个图片拉长，拉成一个向量</span><br><span class="line">num_inputs = 784</span><br><span class="line">num_outputs = 10</span><br><span class="line"></span><br><span class="line">#定义权重w，初始成一个高斯随机分布的值，均值为0，方差为0.01，形状是行数等于输入的个数、列数等于输出的个数，计算梯度requires_grad=True</span><br><span class="line">W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=True) #对于每一个输出都需要一个偏移</span><br></pre></td></tr></tbody></table></figure>

<p>给定一个矩阵<code>X</code>，我们可以对所有元素求和</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#定义一个2*3的矩阵</span><br><span class="line">X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])</span><br><span class="line">#如果按照维度=0来求和的话</span><br><span class="line">X.sum(0, keepdim=True), X.sum(1, keepdim=True)</span><br></pre></td></tr></tbody></table></figure>
<p>实现softmax<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-15.png"></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def softmax(X):</span><br><span class="line">    X_exp = torch.exp(X) #对每个元素做指数计算</span><br><span class="line">    partition = X_exp.sum(1, keepdim=True) #按照维度为1来求和</span><br><span class="line">    return X_exp / partition #广播机制：X_exp的每一行除以partition对应行的那个数</span><br></pre></td></tr></tbody></table></figure>
<p>我们将每个元素变成一个非负数。此外，依据概率原理，每行总和为1</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.normal(0, 1, (2, 5)) #创建一个随机的均值为0，方差为1的两行五列的矩阵X</span><br><span class="line">X_prob = softmax(X) #将X放入softmax函数</span><br><span class="line">X_prob, X_prob.sum(1)</span><br></pre></td></tr></tbody></table></figure>
<p>实现softmax回归模型</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def net(X):</span><br><span class="line">    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)</span><br></pre></td></tr></tbody></table></figure>
<p>创建一个数据y_hat，其中包含2个样本在3个类别的预测概率， 使用y作为y_hat中概率的索引</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = torch.tensor([0, 2])</span><br><span class="line">y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])</span><br><span class="line">y_hat[[0, 1], y]</span><br></pre></td></tr></tbody></table></figure>
<h1 id="10多层感知机"><a href="#10多层感知机" class="headerlink" title="10多层感知机"></a>10多层感知机</h1><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-1.png"></p>
<h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><ul>
<li><p>给定输入x，权重w，和偏移b，感知机输出：<br>（其中x，w为向量，b为标量）<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-2.png"><br>输入w和权重x做内积，加上偏移b<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-3.png"></p>
</li>
<li><p>二分类：-1或1</p>
<ul>
<li>VS 回归输出实数</li>
<li>VS Softmax回归输出概率</li>
</ul>
</li>
</ul>
<h3 id="训练感知机"><a href="#训练感知机" class="headerlink" title="训练感知机"></a>训练感知机</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-4.png"><br><code>预测和真实值如果准确就是同号，预测错误就是异号</code><br>等价于使用批量大小为1的梯度下降，并使用如下的损失函数：<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-5.png"></p>
<h3 id="收敛定理"><a href="#收敛定理" class="headerlink" title="收敛定理"></a>收敛定理</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-6.png"></p>
<h3 id="XOR问题（Minsky-Papert，1969）"><a href="#XOR问题（Minsky-Papert，1969）" class="headerlink" title="XOR问题（Minsky &amp; Papert，1969）"></a>XOR问题（Minsky &amp; Papert，1969）</h3><p>感知机不能拟合XOR函数，它智能产生线性分割面<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-7.png"></p>
<h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><ul>
<li>感知机是一个二分类模型，是最早的AI模型之一</li>
<li>它的求解算法等价于使用批量大小为1的梯度下降</li>
<li>它不能拟合XOR函数，导致的第一次AI寒冬</li>
</ul>
<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-8.png"></p>
<h3 id="学习XOR"><a href="#学习XOR" class="headerlink" title="学习XOR"></a>学习XOR</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-9.png"></p>
<h3 id="单隐藏层"><a href="#单隐藏层" class="headerlink" title="单隐藏层"></a>单隐藏层</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-10.png"></p>
<h3 id="单隐藏层——单分类"><a href="#单隐藏层——单分类" class="headerlink" title="单隐藏层——单分类"></a>单隐藏层——单分类</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-11.png"></p>
<h3 id="Sigmoid激活函数"><a href="#Sigmoid激活函数" class="headerlink" title="Sigmoid激活函数"></a>Sigmoid激活函数</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-12.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-13.png"></p>
<h3 id="Tanh激活函数"><a href="#Tanh激活函数" class="headerlink" title="Tanh激活函数"></a>Tanh激活函数</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-14.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-15.png"></p>
<h3 id="ReLU激活函数"><a href="#ReLU激活函数" class="headerlink" title="ReLU激活函数"></a>ReLU激活函数</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-16.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-17.png"></p>
<h3 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-18.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-19.png"></p>
<h3 id="多隐藏层"><a href="#多隐藏层" class="headerlink" title="多隐藏层"></a>多隐藏层</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-20.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-21.png"></p>
<h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h3><ul>
<li>多层感知机使用隐藏层和激活函数来得到非线性模型</li>
<li>常用激活函数是Sigmoid，Tanh，ReLU</li>
<li>使用Softmax来处理多类分类</li>
<li>超参数为隐藏层数，和各个隐藏层大小</li>
</ul>
<h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><p>多层感知机的从零开始实现</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">batch_size = 256</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></tbody></table></figure>

<p>实现一个具有单隐藏层的多层感知机，它包含256个隐藏单元</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = 784, 10, 256</span><br><span class="line"></span><br><span class="line">W1 = nn.Parameter(</span><br><span class="line">    torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))</span><br><span class="line">W2 = nn.Parameter(</span><br><span class="line">    torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br></pre></td></tr></tbody></table></figure>
<p>实现ReLU激活函数</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def relu(X):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    return torch.max(X, a)</span><br></pre></td></tr></tbody></table></figure>

<p>实现我们的模型</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def net(X):</span><br><span class="line">    X = X.reshape((-1, num_inputs))</span><br><span class="line">    H = relu(X @ W1 + b1)</span><br><span class="line">    return (H @ W2 + b2)</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></tbody></table></figure>

<p>多层感知机的训练过程与softmax回归的训练过程完全相同</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = 10, 0.1</span><br><span class="line">updater = torch.optim.SGD(params, lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br></pre></td></tr></tbody></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-22.png"></p>
<p>在一些测试数据上应用这个模型</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.predict_ch3(net, test_iter)</span><br></pre></td></tr></tbody></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-23.png"></p>
<h1 id="11模型选择-过拟合和欠拟合"><a href="#11模型选择-过拟合和欠拟合" class="headerlink" title="11模型选择 + 过拟合和欠拟合"></a>11模型选择 + 过拟合和欠拟合</h1><h2 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a>训练误差和泛化误差</h2><ul>
<li>训练误差：模型在训练数据上的误差</li>
<li>泛化误差：模型在新数据上的误差</li>
<li>例子：根据摸考成绩来预测未来考试分数<ul>
<li>在过去的考试中表现很好(训练误差)不代表未来考试一定会好(泛化误差)</li>
<li>学生A通过背书在摸考中拿到很好成绩</li>
<li>学生B知道答案后面的原因</li>
</ul>
</li>
</ul>
<h3 id="验证数据集和测试数据集"><a href="#验证数据集和测试数据集" class="headerlink" title="验证数据集和测试数据集"></a>验证数据集和测试数据集</h3><ul>
<li>验证数据集：一个用来评估模型好坏的数据集<ul>
<li>例如拿出50%的训练数据</li>
<li>不要跟训练数据混在一起（常犯错误)</li>
</ul>
</li>
<li>测试数据集：只用一次的数据集。例如<ul>
<li>未来的考试</li>
<li>我出价的房子的实际成交价</li>
<li>用在Kaggle私有排行榜中的数据集</li>
</ul>
</li>
</ul>
<h3 id="K-则交叉验证"><a href="#K-则交叉验证" class="headerlink" title="K-则交叉验证"></a>K-则交叉验证</h3><ul>
<li>在没有足够多数据时使用（(这是常态)</li>
<li>算法：<ul>
<li>将训练数据分割成K块-</li>
<li>For i= 1,…,K<ul>
<li>使用第i块作为验证数据集，其余的作为训练数据集</li>
</ul>
</li>
<li>报告K个验证集误差的平均</li>
</ul>
</li>
<li>常用：K=5或10</li>
</ul>
<h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h2><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-1.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-2.png"></p>
<h3 id="模型容量"><a href="#模型容量" class="headerlink" title="模型容量"></a>模型容量</h3><ul>
<li>拟合各种函数的能力</li>
<li>低容量的模型难以拟合训练数据</li>
<li>高容量的模型可以记住所有的训练数据</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-3.png"></p>
<h3 id="模型容量的影响"><a href="#模型容量的影响" class="headerlink" title="模型容量的影响"></a>模型容量的影响</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-4.png"></p>
<h3 id="估计模型容量"><a href="#估计模型容量" class="headerlink" title="估计模型容量"></a>估计模型容量</h3><ul>
<li>难以在不同的种类算法之间比较<ul>
<li>例如树模型和神经网络</li>
</ul>
</li>
<li>给定一个模型种类,将有两个主要因素<ul>
<li>参数的个数</li>
<li>参数值的选择范围<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-5.png"></li>
</ul>
</li>
</ul>
<h3 id="VC维"><a href="#VC维" class="headerlink" title="VC维"></a>VC维</h3><ul>
<li>统计学习理论的一个核心思想</li>
<li>对于一个分类模型，VC等于一个最大的数据集的大小，不管如何给定标号，都存在一个模型来对它进行完美分类<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-6.png"></li>
</ul>
<h3 id="线性分类器的VC维"><a href="#线性分类器的VC维" class="headerlink" title="线性分类器的VC维"></a>线性分类器的VC维</h3><ul>
<li>2维输入的感知机，VC维=3<ul>
<li>能够分类任何三个点，但不是4个(xor)</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-7.png"></p>
<ul>
<li>支持N维输入的感知机的VC维是N+1</li>
<li>—些多层感知机的VC维O(N $log_2$ N)</li>
</ul>
<h3 id="VC维的用处"><a href="#VC维的用处" class="headerlink" title="VC维的用处"></a>VC维的用处</h3><ul>
<li>提供为什么一个模型好的理论依据<ul>
<li>它可以衡量训练误差和泛化误差之间的间隔</li>
</ul>
</li>
<li>但深度学习中很少使用<ul>
<li>衡量不是很准确</li>
<li>计算深度学习模型的VC维很困难</li>
</ul>
</li>
</ul>
<h3 id="数据复杂度"><a href="#数据复杂度" class="headerlink" title="数据复杂度"></a>数据复杂度</h3><ul>
<li>多个重要因素<ul>
<li>样本个数</li>
<li>个样本的元素个数</li>
<li>时间、空间结构多样性</li>
</ul>
</li>
</ul>
<h3 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a>总结</h3><ul>
<li>模型容量需要匹配数据复杂度，否则可能导致欠拟合和过拟合</li>
<li>统计机器学习提供数学工具来衡量模型复杂度</li>
<li>实际中一般靠观察训练误差和验证误差</li>
</ul>
<h2 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h2><p>模型选择、欠拟合和过拟合<br>通过多项式拟合来交互地探索这些概念</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br></pre></td></tr></tbody></table></figure>

<p>使用以下三阶多项式来生成训练和测试数据的标签：<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-8.png"></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">max_degree = 20</span><br><span class="line">n_train, n_test = 100, 100</span><br><span class="line">true_w = np.zeros(max_degree)</span><br><span class="line">true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])</span><br><span class="line"></span><br><span class="line">features = np.random.normal(size=(n_train + n_test, 1))</span><br><span class="line">np.random.shuffle(features)</span><br><span class="line">poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))</span><br><span class="line">for i in range(max_degree):</span><br><span class="line">    poly_features[:, i] /= math.gamma(i + 1)</span><br><span class="line">labels = np.dot(poly_features, true_w)</span><br><span class="line">labels += np.random.normal(scale=0.1, size=labels.shape)</span><br></pre></td></tr></tbody></table></figure>
<p>看一下前2个样本</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">true_w, features, poly_features, labels = [</span><br><span class="line">    torch.tensor(x, dtype=torch.float32)</span><br><span class="line">    for x in [true_w, features, poly_features, labels]]</span><br><span class="line"></span><br><span class="line">features[:2], poly_features[:2, :], labels[:2]</span><br></pre></td></tr></tbody></table></figure>
<p>实现一个函数来评估模型在给定数据集上的损失</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def evaluate_loss(net, data_iter, loss):  </span><br><span class="line">    """评估给定数据集上模型的损失。"""</span><br><span class="line">    metric = d2l.Accumulator(2)</span><br><span class="line">    for X, y in data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.sum(), l.numel())</span><br><span class="line">    return metric[0] / metric[1]</span><br></pre></td></tr></tbody></table></figure>
<p>定义训练函数</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def train(train_features, test_features, train_labels, test_labels,</span><br><span class="line">          num_epochs=400):</span><br><span class="line">    loss = nn.MSELoss()</span><br><span class="line">    input_shape = train_features.shape[-1]</span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))</span><br><span class="line">    batch_size = min(10, train_labels.shape[0])</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels.reshape(-1, 1)),</span><br><span class="line">                                batch_size)</span><br><span class="line">    test_iter = d2l.load_array((test_features, test_labels.reshape(-1, 1)),</span><br><span class="line">                               batch_size, is_train=False)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=0.01)</span><br><span class="line">    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',</span><br><span class="line">                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],</span><br><span class="line">                            legend=['train', 'test'])</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class="line">        if epoch == 0 or (epoch + 1) % 20 == 0:</span><br><span class="line">            animator.add(epoch + 1, (evaluate_loss(</span><br><span class="line">                net, train_iter, loss), evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    print('weight:', net[0].weight.data.numpy())</span><br></pre></td></tr></tbody></table></figure>
<p>三阶多项式函数拟合(正态)</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train(poly_features[:n_train, :4], poly_features[n_train:, :4],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-9.png"><br>线性函数拟合(欠拟合)</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train(poly_features[:n_train, :2], poly_features[n_train:, :2],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-10.png"><br>高阶多项式函数拟合(过拟合)</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class="line">      labels[:n_train], labels[n_train:], num_epochs=1500)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-11.png"></p>
<h1 id="12权重衰退"><a href="#12权重衰退" class="headerlink" title="12权重衰退"></a>12权重衰退</h1><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-1.png"></p>
<h3 id="使用均方范数作为硬性限制"><a href="#使用均方范数作为硬性限制" class="headerlink" title="使用均方范数作为硬性限制"></a>使用均方范数作为硬性限制</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-2.png"></p>
<h3 id="使用均方范数作为柔性限制"><a href="#使用均方范数作为柔性限制" class="headerlink" title="使用均方范数作为柔性限制"></a>使用均方范数作为柔性限制</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-3.png"></p>
<h3 id="演示对最优解的影响"><a href="#演示对最优解的影响" class="headerlink" title="演示对最优解的影响"></a>演示对最优解的影响</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-4.png"></p>
<h3 id="参数更新法则"><a href="#参数更新法则" class="headerlink" title="参数更新法则"></a>参数更新法则</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-5.png"></p>
<h3 id="总结-6"><a href="#总结-6" class="headerlink" title="总结"></a>总结</h3><ul>
<li>权重衰退通过L2正则项使得模型参数不会过大，从而控制模型复杂度</li>
<li>正则项权重是控制模型复杂度的超参数</li>
</ul>
<h3 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h3><p>权重衰减是最广泛使用的正则化的技术之一</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br></pre></td></tr></tbody></table></figure>
<p>像以前一样生成一些数据<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-6.png"></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5</span><br><span class="line">true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05</span><br><span class="line">train_data = d2l.synthetic_data(true_w, true_b, n_train)</span><br><span class="line">train_iter = d2l.load_array(train_data, batch_size)</span><br><span class="line">test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class="line">test_iter = d2l.load_array(test_data, batch_size, is_train=False)</span><br></pre></td></tr></tbody></table></figure>
<p>初始化模型参数</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def init_params():</span><br><span class="line">    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)</span><br><span class="line">    b = torch.zeros(1, requires_grad=True)</span><br><span class="line">    return [w, b]</span><br></pre></td></tr></tbody></table></figure>
<p>定义L2范数惩罚</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def l2_penalty(w):</span><br><span class="line">    return torch.sum(w.pow(2)) / 2</span><br></pre></td></tr></tbody></table></figure>
<p>定义训练代码实现</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def train(lambd):</span><br><span class="line">    w, b = init_params()</span><br><span class="line">    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss</span><br><span class="line">    num_epochs, lr = 100, 0.003</span><br><span class="line">    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',</span><br><span class="line">                            xlim=[5, num_epochs], legend=['train', 'test'])</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        for X, y in train_iter:</span><br><span class="line">            with torch.enable_grad():</span><br><span class="line">                l = loss(net(X), y) + lambd * l2_penalty(w)</span><br><span class="line">            l.sum().backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        if (epoch + 1) % 5 == 0:</span><br><span class="line">            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    print('w的L2范数是：', torch.norm(w).item())</span><br></pre></td></tr></tbody></table></figure>
<p>忽略正则化直接训练</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(lambd=0)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-7.png"><br>使用权重衰减</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(lambd=3)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-8.png"><br>简洁实现</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def train_concise(wd):</span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, 1))</span><br><span class="line">    for param in net.parameters():</span><br><span class="line">        param.data.normal_()</span><br><span class="line">    loss = nn.MSELoss()</span><br><span class="line">    num_epochs, lr = 100, 0.003</span><br><span class="line">    trainer = torch.optim.SGD([{</span><br><span class="line">        "params": net[0].weight,</span><br><span class="line">        'weight_decay': wd}, {</span><br><span class="line">            "params": net[0].bias}], lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',</span><br><span class="line">                            xlim=[5, num_epochs], legend=['train', 'test'])</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        for X, y in train_iter:</span><br><span class="line">            with torch.enable_grad():</span><br><span class="line">                trainer.zero_grad()</span><br><span class="line">                l = loss(net(X), y)</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        if (epoch + 1) % 5 == 0:</span><br><span class="line">            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    print('w的L2范数：', net[0].weight.norm().item())</span><br></pre></td></tr></tbody></table></figure>
<p>这些图看起来和我们从零开始实现权重衰减时的图相同</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_concise(0)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-9.png"></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_concise(3)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-10.png"></p>
<h1 id="13丢弃法"><a href="#13丢弃法" class="headerlink" title="13丢弃法"></a>13丢弃法</h1><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><ul>
<li>一个好的模型需要对输入数据的扰动鲁棒<ul>
<li>使用有噪音的数据等价于Tikhonov正则</li>
<li>丢弃法：在层之间加入噪音</li>
</ul>
</li>
</ul>
<h3 id="无偏差的加入噪音"><a href="#无偏差的加入噪音" class="headerlink" title="无偏差的加入噪音"></a>无偏差的加入噪音</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/13-1.png"></p>
<h3 id="使用丢弃法"><a href="#使用丢弃法" class="headerlink" title="使用丢弃法"></a>使用丢弃法</h3><p>通常将丢弃法作用在隐藏全连接层的输出上<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/13-2.png"></p>
<h3 id="推理中的丢弃法"><a href="#推理中的丢弃法" class="headerlink" title="推理中的丢弃法"></a>推理中的丢弃法</h3><ul>
<li>正则项只在训练中使用：他们影响模型参数的更新</li>
<li>在推理过程中，丢弃法直接返回输入h = dropout(h)<ul>
<li>这样也能保证确定性的输出</li>
</ul>
</li>
</ul>
<h3 id="总结-7"><a href="#总结-7" class="headerlink" title="总结"></a>总结</h3><ul>
<li>丢弃法将一些输出项随机置0来控制模型复杂度</li>
<li>常作用在多层感知机的隐藏层输出上</li>
<li>丢弃概率是控制模型复杂度的超参数</li>
</ul>
<h3 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h3><p>我们实现<code>dropout_layer</code>函数，该函数以<code>dropout</code>的概率丢弃张量输入<code>X</code>中的元素</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">def dropout_layer(X, dropout):</span><br><span class="line">    assert 0 &lt;= dropout &lt;= 1</span><br><span class="line">    if dropout == 1:</span><br><span class="line">        return torch.zeros_like(X)</span><br><span class="line">    if dropout == 0:</span><br><span class="line">        return X</span><br><span class="line">    mask = (torch.Tensor(X.shape).uniform_(0, 1) &gt; dropout).float()</span><br><span class="line">    return mask * X / (1.0 - dropout)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>测试<code>dropout_layer</code>函数</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(16, dtype=torch.float32).reshape((2, 8))</span><br><span class="line">print(X)</span><br><span class="line">print(dropout_layer(X, 0.))</span><br><span class="line">print(dropout_layer(X, 0.5))</span><br><span class="line">print(dropout_layer(X, 1.))</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/13-3.png"><br>定义具有两个隐藏层的多层感知机，每个隐藏层包含256个单元</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256</span><br><span class="line"></span><br><span class="line">dropout1, dropout2 = 0.2, 0.5</span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span><br><span class="line">                 is_training=True):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.num_inputs = num_inputs</span><br><span class="line">        self.training = is_training</span><br><span class="line">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class="line">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class="line">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    def forward(self, X):</span><br><span class="line">        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))</span><br><span class="line">        if self.training == True:</span><br><span class="line">            H1 = dropout_layer(H1, dropout1)</span><br><span class="line">        H2 = self.relu(self.lin2(H1))</span><br><span class="line">        if self.training == True:</span><br><span class="line">            H2 = dropout_layer(H2, dropout2)</span><br><span class="line">        out = self.lin3(H2)</span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br></pre></td></tr></tbody></table></figure>
<p>训练和测试</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr, batch_size = 10, 0.5, 256</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/13-4.png"><br>简洁实现</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),</span><br><span class="line">                    nn.Dropout(dropout1), nn.Linear(256, 256), nn.ReLU(),</span><br><span class="line">                    nn.Dropout(dropout2), nn.Linear(256, 10))</span><br><span class="line"></span><br><span class="line">def init_weights(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=0.01)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights);</span><br></pre></td></tr></tbody></table></figure>
<p>对模型进行训练和测试</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/13-5.png"></p>
<h1 id="14数值稳定性-模型初始化和激活函数"><a href="#14数值稳定性-模型初始化和激活函数" class="headerlink" title="14数值稳定性 + 模型初始化和激活函数"></a>14数值稳定性 + 模型初始化和激活函数</h1><h1 id="19卷积层"><a href="#19卷积层" class="headerlink" title="19卷积层"></a>19卷积层</h1><p><em><strong>分类猫和狗</strong></em></p>
<ul>
<li>使用一个还不错的相机采集图片（12M像素）</li>
<li>RGB图片有36M元素</li>
<li>使用100大小的单隐藏层MLP，模型有3.6B元素<ul>
<li>远多于世界上所有猫和狗总算（900M狗，600M猫）</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-1.png"><br><em><strong>回顾：单隐藏层MLP</strong></em><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-2.png"></p>
<h3 id="两个原则"><a href="#两个原则" class="headerlink" title="两个原则"></a>两个原则</h3><ul>
<li>平移不变性</li>
<li>局部性</li>
</ul>
<h3 id="重新考察全连接层"><a href="#重新考察全连接层" class="headerlink" title="重新考察全连接层"></a>重新考察全连接层</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-3.png"></p>
<h3 id="原则1—平移不变性"><a href="#原则1—平移不变性" class="headerlink" title="原则1—平移不变性"></a>原则1—平移不变性</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-4.png"></p>
<h3 id="原则2—局部性5"><a href="#原则2—局部性5" class="headerlink" title="原则2—局部性5"></a>原则2—局部性5</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-5.png"><br><em><strong>总结</strong></em><br>对全连接层使用平移不变性和局部性得到卷积层<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-6.png"></p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-7.png"></p>
<h3 id="二维交叉相关"><a href="#二维交叉相关" class="headerlink" title="二维交叉相关"></a>二维交叉相关</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-8.png"></p>
<h3 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-9.png"><br>例子<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-10.png"></p>
<h3 id="交叉相关vs卷积"><a href="#交叉相关vs卷积" class="headerlink" title="交叉相关vs卷积"></a>交叉相关vs卷积</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-11.png"></p>
<h3 id="一维和三维交叉相关"><a href="#一维和三维交叉相关" class="headerlink" title="一维和三维交叉相关"></a>一维和三维交叉相关</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-12.png"><br><em><strong>总结</strong></em></p>
<ul>
<li>卷积层将输入和核矩阵进行交叉相关，加上偏移后得到输出</li>
<li>核矩阵和偏移是可学习的参数</li>
<li>核矩阵的大小是超参数</li>
</ul>
<h1 id="68-Transformer"><a href="#68-Transformer" class="headerlink" title="68 Transformer"></a>68 Transformer</h1><h3 id="Transformer架构"><a href="#Transformer架构" class="headerlink" title="Transformer架构"></a>Transformer架构</h3><ul>
<li>基于编码器-解码器架构来处理序列对</li>
<li>跟使用注意力的seq2seq不同，Transformer是<em><strong>纯基于注意力</strong></em></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/68-1.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/68-2.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/68-3.png"></p>
<h3 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h3><ul>
<li>对同一Key，value，query，希望抽取不同的信息<ul>
<li>例如短距离关系和长距离关系</li>
</ul>
</li>
<li>多头注意力使用h个独立的注意力池化<ul>
<li>合并各个头（head）输出得到最终输出</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/68-4.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/68-5.png"></p>

      
       
    </div>
</article>







    




    </div>
    <div class="copyright">
        <p class="footer-entry">
    ©2017 YJT
</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full" data-title="切换全屏 快捷键 s"><span class="min "></span></button>
<a class="" id="rocket"></a>

    </div>
</div>


<script src="/js/jquery.pjax.js?v=1.1.0"></script>

<script src="/js/script.js?v=1.1.0"></script>
<script>
    var img_resize = 'default';
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $("#post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        

        /*高亮代码块行号*/
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
    }

    /*打赏页面隐藏与展示*/
    

</script>

<!--加入行号的高亮代码块样式-->

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 582px;
    }
    .nav.fullscreen {
        margin-left: -582px;
    }
    .nav-left {
        width: 160px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 532px;
        }
        .nav.fullscreen {
            margin-left: -532px;
        }
        .nav-left {
            width: 140px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 532px;
            margin-left: -532px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    

    
</style>








<script type="text/javascript" charset="utf-8" src="/js/lazyload-plugin/lazyload.intersectionObserver.min.js"></script></body></html>