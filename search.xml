<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【目标检测】You Only Look Once Unified Real Time Object Detection</title>
      <link href="/2024/02/27/mu-biao-jian-ce-you-only-look-once-unified-real-time-object-detection/"/>
      <url>/2024/02/27/mu-biao-jian-ce-you-only-look-once-unified-real-time-object-detection/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><em><strong>You Only Look Once: Unified, Real-Time Object Detection</strong></em><br><em><strong>您只需查看一次：统一的实时目标检测</strong></em></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.<br><code>本文提出YOLO，一种新的目标检测方法。之前关于目标检测的工作重新利用分类器来进行检测。相反，我们将目标检测框架为空间分离的边界框和相关的类概率的回归问题。在一次评估中，单个神经网络直接从完整图像中预测边界框和类概率。由于整个检测流水线是一个单一的网络，因此可以直接对检测性能进行端到端的优化。</code><br>Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations ofobjects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.<br><code>我们的统一架构非常快。我们的基本YOLO模型以每秒45帧的速度实时处理图像。该网络的一个较小版本Fast YOLO，处理速度达到惊人的155帧/秒，同时仍然实现了其他实时检测器的mAP的两倍。与最先进的检测系统相比，YOLO有更多的定位误差，但不太可能预测背景的误报。最后，YOLO可以学习物体的一般表示。当从自然图像泛化到艺术作品等其他领域时，它优于其他检测方法，包括DPM和R-CNN。</code></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>Humans glance at an image and instantly know what objects are in the image, where they are, and how they interact. The human visual system is fast and accurate, allowing us to perform complex tasks like driving with little conscious thought. Fast, accurate algorithms for object detection would allow computers to drive cars without specialized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems.<br><code>人类瞥一眼图像，就立刻知道图像中有什么物体，它们在哪里，以及它们如何相互作用。人类的视觉系统是快速和准确的，使我们能够执行复杂的任务，如驾驶时很少有意识的思考。快速、准确的目标检测算法将允许计算机在没有专门传感器的情况下驾驶汽车，使辅助设备能够向人类用户传递实时场景信息，并释放用于通用、响应式机器人系统的潜力。</code><br>Current detection systems repurpose classifiers to perform detection. To detect an object, these systems take a classifier for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image [10].<br><code>当前的检测系统重新使用分类器来执行检测。为了检测对象，这些系统采用该对象的分类器，并在测试图像中的各种位置和尺度处评估该对象。像可变形零件模型（DPM）这样的系统使用滑动窗口方法，其中分类器在整个图像上的均匀间隔位置处运行[10]。</code><br>More recent approaches like R-CNN use region proposal methods to first generate potential bounding boxes in an image and then run a classifier on these proposed boxes. After classification, post-processing is used to refine the bounding boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene [13]. These complex pipelines are slow and hard to optimize because each individual component must be trained separately.<br><code>最近的方法，如R-CNN，使用区域建议方法首先在图像中生成潜在的边界框，然后在这些建议的框上运行分类器。在分类之后，使用后处理来细化边界框、消除重复检测，并基于场景中的其他对象对框进行重新取芯[13]。这些复杂的流水线速度慢且难以优化，因为每个单独的组件都必须单独训练。</code><br>We reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are.<br><code>我们将目标检测重构为一个单一的回归问题，直接从图像像素到边界框坐标和类概率。使用我们的系统，您只需看一次（YOLO）图像，即可预测存在哪些对象以及它们在哪里。</code><br>YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection.<br><code>YOLO非常简单:参见图1。一个卷积网络同时预测多个边界框和这些框的类别概率。YOLO在全图像上进行训练，并直接优化检测性能。这种统一的模型比传统的目标检测方法有几个优点。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/7-1.png"></p><blockquote><p>Figure 1: The YOLO Detection System. Processing images with YOLO is simple and straightforward. Our system (1) resizes the input image to 448 × 448, (2) runs a single convolutional network on the image, and (3) thresholds the resulting detections by the model’s confidence.<br><code>图1：YOLO检测系统。使用YOLO处理图像简单明了。我们的系统（1）将输入图像的大小调整为448 × 448，（2）在图像上运行单个卷积网络，（3）通过模型的置信度对结果检测进行阈值化。</code></p></blockquote><p>First, YOLO is extremely fast. Since we frame detection as a regression problem we don’t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems.<br><code>首先，YOLO的速度非常快。由于我们将检测视为回归问题，因此不需要复杂的管道。我们只是在测试时对新图像运行神经网络来预测检测结果。我们的基础网络以每秒45帧的速度运行，在Titan X GPU上没有批处理，快速版本的运行速度超过150 fps。这意味着我们可以实时处理流视频，延迟时间不到25毫秒。此外，YOLO的平均精度是其他实时系统的两倍多。</code><br>For a demo of our system running in real-time on a webcam please see our project webpage: <a href="http://pjreddie.com/yolo/">http://pjreddie.com/yolo/</a>.<br><code>有关我们的系统在网络摄像头上实时运行的演示，请参阅我们的项目网页:http://pjreddie.com/yolo/。</code><br>Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.<br><code>第二，YOLO在做出预测时，会对形象进行全局性的推理。与基于滑动窗口和区域提议的技术不同，YOLO在训练和测试期间看到整个图像，因此它隐式地编码了关于类及其外观的上下文信息。快速R-CNN是一种顶部检测方法[14]，它会将图像中的背景块误认为是对象，因为它无法看到更大的上下文。与Fast R-CNN相比，YOLO的背景错误数量不到其一半。</code><br>Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.<br><code>第三，YOLO学习对象的可概括表示。当在自然图像上进行训练并在艺术品上进行测试时，YOLO的性能远远优于DPM和R-CNN等顶级检测方法。由于YOLO具有高度的可推广性，因此在应用于新领域或意外输入时，它不太可能崩溃。</code><br>YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.<br><code>YOLO在准确性上仍然落后于最先进的检测系统。虽然它可以快速识别图像中的物体，但它很难精确定位一些物体，尤其是小物体。我们将在实验中进一步考察这些权衡。</code><br>All of our training and testing code is open source. A variety of pretrained models are also available to download.<br><code>我们所有的培训和测试代码都是开源的。还可以下载各种预先训练的模型。</code></p><h1 id="2-Unified-Detection"><a href="#2-Unified-Detection" class="headerlink" title="2. Unified Detection"></a>2. Unified Detection</h1><p><code>2.统一检测</code><br>We unify the separate components of object detection into a single neural network. Our network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means our network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and realtime speeds while maintaining high average precision.<br><code>我们将目标检测的各个组件统一到一个神经网络中。我们的网络使用整个图像的特征来预测每个边界框。它还可以同时预测图像的所有类中的所有边界框。这意味着我们的网络会对整个图像和图像中的所有对象进行全局推理。YOLO设计可实现端到端的训练和实时速度，同时保持较高的平均精度。</code><br>Our system divides the input image into an S × S grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.<br><code>该系统将输入图像划分为S × S网格。如果一个物体的中心落在一个网格单元中，该网格单元负责检测该物体。</code><br>Each grid cell predicts B bounding boxes and confidence scores for those boxes. These confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts. Formally we define confidence as $\operatorname{Pr}($ Object $) * \mathrm{IOU}<em>{\text {pred }}^{\text {truth }}$. If no object exists in that cell, the confidence scores should be zero. Otherwise we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth.<br><code>每个网格单元预测B个边界框和这些框的置信度得分。这些置信度分数反映了模型对该框包含对象的置信度，以及模型认为该框的预测准确度。形式上，我们将置信定义为</code>$\operatorname{Pr}($ Object $) * \mathrm{IOU}</em>{\text {pred }}^{\text {truth }}$<code>。如果该单元格中不存在对象，则置信度分数应为零。否则，我们希望置信度得分等于预测框和地面真实值之间的并集交集（IOU）。</code><br>Each bounding box consists of 5 predictions: x, y, w, h, and confidence. The (x, y) coordinates represent the center of the box relative to the bounds of the grid cell. The width and height are predicted relative to the whole image. Finally the confidence prediction represents the IOU between the predicted box and any ground truth box.<br><code>每个边界框由5个预测组成：x、y、w、h和置信度。（x，y）坐标表示相对于网格单元格边界的框的中心。宽度和高度是相对于整个图像预测的。最后，置信度预测表示预测框和任何地面实况框之间的IOU。</code><br>Each grid cell also predicts C conditional class probabilities, $\operatorname{Pr}\left(\right.$ Class $_i \mid$ Object $)$. These probabilities are conditioned on the grid cell containing an object. We only predict one set of class probabilities per grid cell, regardless of the number of boxes B.<br><code>每个网格单元还预测C个条件类概率</code>$\operatorname{Pr}\left(\right.$ Class $_i \mid$ Object $)$<code>。这些概率取决于包含对象的网格单元。我们只预测每个网格单元格的一组类概率，而不考虑框B的数量。</code><br>At test time we multiply the conditional class probabilities and the individual box confidence predictions,<br><code>在测试时，我们将条件类概率与单个框置信度预测相乘，</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/7-2.png"><br>which gives us class-specific confidence scores for each box. These scores encode both the probability of that class appearing in the box and how well the predicted box fits the object.<br><code>这给了我们每个盒子的类特定的置信度分数。这些分数编码该类出现在框中的概率以及预测的框与对象的匹配程度。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/7-3.png"></p><blockquote><p>Figure 2: The Model. Our system models detection as a regression problem. It divides the image into an S×S grid and for each grid cell predicts B bounding boxes, confidence for those boxes, and C class probabilities. These predictions are encoded as an S × S × (B ∗ 5 +C) tensor.<br><code>图2：模型。我们的系统将检测建模为回归问题。它将图像划分为S×S网格，并为每个网格单元预测B边界框，这些框的置信度和C类概率。这些预测被编码为S × S ×（B * 5 +C）张量。</code></p></blockquote><p>For evaluating YOLO on PASCAL VOC, we use S = 7, B = 2. PASCAL VOC has 20 labelled classes so C = 20. Our final prediction is a 7 × 7 × 30 tensor.<br><code>为了在PASCAL VOC上评估YOLO，我们使用S = 7，B = 2。PASCAL VOC有20个标记类，因此C = 20。我们的最终预测是一个7 × 7 × 30的张量。</code></p><h2 id="2-1-Network-Design"><a href="#2-1-Network-Design" class="headerlink" title="2.1. Network Design"></a>2.1. Network Design</h2><p><code>2.1.网络设计</code><br>We implement this model as a convolutional neural network and evaluate it on the PASCAL VOC detection dataset [9]. The initial convolutional layers of the network extract features from the image while the fully connected layers predict the output probabilities and coordinates.<br><code>我们将此模型实现为卷积神经网络，并在PASCAL VOC检测数据集上对其进行评估[9]。网络的初始卷积层从图像中提取特征，而全连通层预测输出概率和坐标。</code><br>Our network architecture is inspired by the GoogLeNet model for image classification [34]. Our network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we simply use 1×1 reduction layers followed by 3×3 convolutional layers, similar to Lin et al [22]. The full network is shown in Figure 3.<br><code>我们的网络架构受到GoogLeNet图像分类模型的启发[34]。我们的网络有24个卷积层，后面是2个完全连接的层。我们不使用GoogleNet使用的初始模块，而是简单地使用1×1约简层，然后是3×3卷积层，类似于Lin等人[22]的方法。完整的网络如图3所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/7-4.png"></p><blockquote><p>Figure 3: The Architecture. Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating 1 × 1 convolutional layers reduce the features space from preceding layers. We pretrain the convolutional layers on the ImageNet classification task at half the resolution (224 × 224 input image) and then double the resolution for detection.<br><code>图3：架构。我们的检测网络有24个卷积层，后面是2个完全连接层。交替的1 × 1卷积层减少了前一层的特征空间。我们在ImageNet分类任务中以一半的分辨率（224 × 224输入图像）预训练卷积层，然后将分辨率加倍以进行检测。</code></p></blockquote><p>We also train a fast version of YOLO designed to push the boundaries of fast object detection. Fast YOLO uses a neural network with fewer convolutional layers (9 instead of 24) and fewer filters in those layers. Other than the size of the network, all training and testing parameters are the same between YOLO and Fast YOLO.<br><code>我们还训练了YOLO的快速版本，旨在推动快速目标检测的边界。Fast YOLO使用的神经网络具有较少的卷积层（9层而不是24层）和较少的滤波器。除了网络规模之外，YOLO和Fast YOLO的所有培训和测试参数都相同。</code><br>The final output of our network is the 7 × 7 × 30 tensor of predictions.<br><code>我们的网络的最终输出是7 × 7 × 30的预测张量。</code></p><h2 id="2-2-Training"><a href="#2-2-Training" class="headerlink" title="2.2. Training"></a>2.2. Training</h2><p><code>2.2.训练</code><br>We pretrain our convolutional layers on the ImageNet 1000-class competition dataset [30]. For pretraining we use the first 20 convolutional layers from Figure 3 followed by a average-pooling layer and a fully connected layer. We train this network for approximately a week and achieve a single crop top-5 accuracy of 88% on the ImageNet 2012 validation set, comparable to the GoogLeNet models in Caffe’s Model Zoo [24]. We use the Darknet framework for all training and inference [26].<br><code>我们在ImageNet 1000级竞赛数据集上预训练卷积层[30]。对于预训练，我们使用图3中的前20个卷积层，然后是平均池化层和全连接层。我们对这个网络进行了大约一周的训练，并在ImageNet 2012验证集上实现了88%的单次crop top-5准确率，与Caffe的Model Zoo中的GoogLeNet模型相当。我们使用Darknet框架进行所有训练和推理[26]。</code><br>We then convert the model to perform detection. Ren et al. show that adding both convolutional and connected layers to pretrained networks can improve performance [29]. Following their example, we add four convolutional layers and two fully connected layers with randomly initialized weights. Detection often requires fine-grained visual information so we increase the input resolution of the network from 224 × 224 to 448 × 448.<br><code>然后我们转换模型以执行检测。Ren等人表明，将卷积层和连接层添加到预训练的网络中可以提高性能[29]。按照他们的例子，我们添加了四个卷积层和两个具有随机初始化权重的全连接层。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从224 × 224提高到448 × 448。</code><br>Our final layer predicts both class probabilities and bounding box coordinates. We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1. We parametrize the bounding box x and y coordinates to be offsets of a particular grid cell location so they are also bounded between 0 and 1.<br><code>我们的最后一层预测类概率和边界框坐标。我们通过图像的宽度和高度来规范化边界框的宽度和高度，使它们落在0和1之间。我们将边界框x和y坐标参数化为特定网格单元位置的偏移量，因此它们也被限制在0和1之间。</code><br>We use a linear activation function for the final layer and all other layers use the following leaky rectified linear activation:<br><code>我们对最后一层使用线性激活函数，所有其他层使用以下泄漏整流线性激活：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/7-5.png"><br>We optimize for sum-squared error in the output of our model. We use sum-squared error because it is easy to optimize, however it does not perfectly align with our goal of maximizing average precision. It weights localization error equally with classification error which may not be ideal. Also, in every image many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, causing training to diverge early on.<br><code>我们优化了模型输出的平方和误差。我们使用平方和误差，因为它很容易优化，但它并不完全符合我们最大化平均精度的目标。它将定位误差与分类误差相等地加权，这可能是不理想的。此外，在每个图像中，许多网格单元不包含任何对象。这会将这些单元格的“置信度”分数推向零，通常会压倒包含对象的单元格的梯度。这可能导致模型不稳定，导致训练在早期出现分歧。</code><br>To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don’t contain objects. We use two parameters, $λ_{coord}$ and $λ_{noobj}$ to accomplish this. We set $λ_{coord}$ = 5 and $λ_{noobj}$ = .5.<br><code>为了解决这个问题，我们增加了边界框坐标预测的损失，并减少了不包含对象的框的置信度预测的损失。我们使用两个参数</code>$λ_{coord}$<code>和</code>$λ_{noobj}$<code>来实现这一点。我们设置</code>$λ_{coord}$ = 5<code>和</code>$λ_{noobj}$ = .5。<br>Sum-squared error also equally weights errors in large boxes and small boxes. Our error metric should reflect that small deviations in large boxes matter less than in small boxes. To partially address this we predict the square root of the bounding box width and height instead of the width and height directly.<br><code>平方和误差也同样加权大框和小框中的误差。我们的误差度量应该反映出大盒子中的小偏差比小盒子中的小偏差更重要。为了部分解决这个问题，我们预测边界框宽度和高度的平方根，而不是直接预测宽度和高度。</code><br>YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors. Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall.<br><code>YOLO预测每个网格单元的多个边界框。在训练时，我们只希望一个边界框预测器负责每个对象。我们分配一个预测器来“负责”预测对象，基于该预测具有最高的当前IOU与地面真相。这导致边界框预测器之间的专门化。每个预测器都能更好地预测物体的特定大小、长宽比或类别，从而提高整体召回率。</code><br>During training we optimize the following, multi-part loss function:<br><code>在训练过程中，我们优化了以下多部分损失函数：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/7-6.png"><br>where $\mathbb{1}<em>i^{\mathrm{obj}}$ denotes if object appears in cell i and $\mathbb{1}</em>{ij}^{\mathrm{obj}}$ denotes that the jth bounding box predictor in cell i is “responsible” for that prediction.<br><code>其中</code>$\mathbb{1}<em>i^{\mathrm{obj}}$<code>表示对象是否出现在单元i中，</code>$\mathbb{1}</em>{ij}^{\mathrm{obj}}$<code>表示单元i中的第j个边界框预测器“负责”该预测。</code><br>Note that the loss function only penalizes classification error if an object is present in that grid cell (hence the conditional class probability discussed earlier). It also only penalizes bounding box coordinate error if that predictor is “responsible” for the ground truth box (i.e. has the highest IOU of any predictor in that grid cell).<br><code>请注意，损失函数仅在对象存在于该网格单元中时才惩罚分类错误（因此前面讨论了条件类概率）。如果预测器“负责”地面实况框（即，具有该网格单元中的任何预测器的最高IOU），则它也仅惩罚边界框坐标误差。</code></p><h1 id="GPT解读"><a href="#GPT解读" class="headerlink" title="GPT解读"></a>GPT解读</h1><p> 这篇文章介绍了一种新的实时目标检测方法，称为YOLO（You Only Look Once）。以下是文章的核心内容概述：</p><ol><li><p><strong>YOLO方法介绍</strong>：</p><ul><li>YOLO将目标检测问题视为一个回归问题，直接从图像像素到边界框坐标和类别概率的预测。</li><li>使用单一神经网络在一次评估中预测边界框和类别概率。</li><li>YOLO的统一架构非常快速，基础模型能够实时处理图像，速度达到每秒45帧。</li><li>一个更小的版本，Fast YOLO，能够达到每秒155帧的处理速度，同时在平均精度（mAP）上是其他实时检测器的两倍。</li></ul></li><li><p><strong>YOLO的优势</strong>：</p><ul><li>快速：YOLO的检测流程简单，不需要复杂的管道处理。</li><li>全局推理：YOLO在训练和测试时看到整个图像，能够隐式编码类别和外观的上下文信息。</li><li>泛化能力：YOLO学习到的物体表示具有很好的泛化能力，能够在自然图像和其他领域（如艺术作品）之间进行泛化。</li></ul></li><li><p><strong>YOLO的网络设计</strong>：</p><ul><li>YOLO网络由24个卷积层和2个全连接层组成，灵感来源于GoogLeNet模型。</li><li>网络在PASCAL VOC数据集上进行预训练，然后转换为执行检测任务。</li><li>使用了特定的损失函数来优化网络，包括边界框坐标预测的加权和非极大值抑制。</li></ul></li><li><p><strong>YOLO的局限性</strong>：</p><ul><li>空间约束：每个网格单元只预测两个边界框，限制了模型预测附近多个物体的能力。</li><li>对小物体的检测存在挑战，尤其是在它们以群体出现时。</li><li>使用相对粗糙的特征进行边界框预测，因为网络架构包含多个下采样层。</li></ul></li><li><p><strong>与其他检测系统的比较</strong>：</p><ul><li>YOLO与DPM（可变形部件模型）、R-CNN及其变体等传统目标检测方法相比，具有更快的速度和更高的平均精度。</li><li>YOLO在实时检测系统上的表现优于其他方法，并且在新领域（如艺术作品）的泛化能力更强。</li></ul></li><li><p><strong>实验结果</strong>：</p><ul><li>在PASCAL VOC 2007和2012数据集上，YOLO展示了与其他实时检测系统的比较。</li><li>YOLO在艺术作品数据集上的表现表明其良好的泛化能力。</li><li>结合Fast R-CNN和YOLO的方法能够显著提高性能。</li></ul></li><li><p><strong>结论</strong>：</p><ul><li>YOLO是一个简单、快速且泛化能力强的目标检测模型，适用于需要快速、稳健目标检测的应用场景。</li></ul></li></ol><p>文章还提到了YOLO的开源代码和预训练模型的可用性，以及对YOLO进行实时检测的演示。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> YOLO </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unit 3 Writing about Results</title>
      <link href="/2024/02/25/unit-3-writing-about-results/"/>
      <url>/2024/02/25/unit-3-writing-about-results/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><code>第三单元 关于结果的写作</code></p><h3 id="3-1-Structure"><a href="#3-1-Structure" class="headerlink" title="3.1 Structure"></a>3.1 Structure</h3><p><code>3.1结构</code><br>The title of this section varies in different disciplines, and also in different journals. Instead of Results, it is sometimes called ‘Analysis’ or ‘Data Analysis’. Th e table below shows four options for the subtitles from this point until the end of the research paper.<br><code>这一部分的标题在不同的学科和不同的期刊中有所不同。它有时被称为“分析”或“数据分析”。下表显示了从这一点到研究论文结束的字幕的四个选项。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/3-1.png"><br>In all cases this section reports your comments on what you found or observed, and if the subtitle contains the word Discussion (i.e. Results and Discussion), it includes some or all of the Discussion. As with the Methodology section, the best way to choose an appropriate subtitle is to look at the Guide for Authors of the journals you read regularly.<br><code>在所有情况下，本节报告您对发现或观察到的内容的评论，如果副标题包含“讨论”一词（即结果和讨论），则包括部分或全部讨论。与方法论部分一样，选择合适副标题的最佳方法是查看您经常阅读的期刊的作者指南。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/3-2.png"><br>In most cases, the results of your work can be given in graphs, tables, equations or images. Why, then, should you bother to write a Results section? Why not simply provide good, clear graphs or tables with good, clear titles and perhaps a few notes underneath each? Thinking about these questions is a good way to begin to understand what you should be writing in this section. Almost everyone writes a Results section, so it is clear that some things cannot be achieved by just using tables, graphs or other images of your results. They can be achieved only by using words.<br><code>在大多数情况下，您的工作结果可以以图形，表格，方程或图像的形式给出。那么，为什么还要费心编写结果部分呢？为什么不简单地提供好的，清晰的图表或表格，以及好的，清晰的标题，也许在每个下面有一些注释？思考这些问题是开始理解你应该在这一部分写什么的好方法。几乎每个人都会写一个结果部分，所以很明显，有些事情不能仅仅通过使用表格，图表或其他结果图像来实现。它们只能通过文字来实现。</code><br>There are many reasons for writing a Results section. In the first place, some of your results may be more interesting or significant than others, and it is difficult to communicate this in a table or graph. Also, it is essential to relate your results to the aim(s) of the research. Thirdly, in some cases you may want to off er background information to explain why a particular result occurred, or to compare your results with those of other researchers. In addition, your results may be problematic; perhaps some experiments were not fully successful and you want to suggest possible reasons for this.<br><code>有很多理由写一个结果部分。首先，你的一些结果可能比其他结果更有趣或更重要，很难用表格或图表来传达这一点。此外，将您的结果与研究目标联系起来也很重要。第三，在某些情况下，你可能想提供背景信息来解释为什么会出现特定的结果，或者将你的结果与其他研究人员的结果进行比较。此外，你的结果可能是有问题的;也许有些实验没有完全成功，你想提出可能的原因。</code><br>However, one of the most important reasons for writing a Results section rather than relying on graphs, tables and other images is that you must communicate your own understanding and interpretation of the results to your readers. Results do not speak for themselves; if they did, the tables or graphs of your results would be enough. Your readers do not have to agree with you but they need to know your opinion and understanding of your results.<br><code>然而，撰写结果部分而不是依赖于图表，表格和其他图像的最重要原因之一是，您必须将自己对结果的理解和解释传达给读者。结果本身并不能说明问题;如果它们能说明问题，那么你的结果的图表就足够了。你的读者不必同意你的观点，但他们需要知道你的观点和对你的结果的理解。</code><br>So when we come to ask our three questions:<br><code>所以当我们来问我们的三个问题：</code></p><ul><li>How do I start the Results section? What type of sentence should I begin with?<br><code>如何启动结果部分？我开始应该用什么类型的句子？</code></li><li>What type of information should be in this section and in what order?<br><code>本节中应包含哪些类型的信息，以及顺序如何？</code></li><li>How do I end this section?<br><code>我如何结束这一节？</code></li></ul><p>You already know that this section contains some comments on what you found or observed rather than just a description of your findings and observations, and this helps to answer the second question.<br><code>你已经知道，这一部分包含了一些关于你发现或观察到的东西的评论，而不仅仅是对你的发现和观察的描述，这有助于回答第二个问题。</code><br>Read the Results section below. The title of the paper is: A modelling approach to traffic management and CO exposure during peak hours. Don’t worry if the subject matter is not familiar to you or if you have difficulty understanding certain words, especially technical terms such as median exposure. Just try to get a general understanding at this stage and familiarise yourself with the type of language used.<br><code>阅读下面的结果部分。该文件的标题是：交通管理和CO暴露在高峰时段的建模方法。如果您不熟悉该主题，或者您难以理解某些单词，尤其是技术术语（如中位数暴露），请不要担心。在这一阶段，试着获得一个大致的理解，并熟悉所使用的语言类型。</code></p><blockquote><p>Results<br>1 Data obtained in previous studies using a fixed on-site monitor indicated that travel by car resulted in lower CO exposure than travel on foot. 2 According to Figo et al. (1999), the median exposure of car passengers was 11% lower than for those walking. 3 In our study, modelled emission rates were obtained using the Traffic Emission Model (TEM), a CO-exposure modelling framework developed by Ka. 4 Modelled results were compared with actual roadside CO concentrations measured hourly at a fixed monitor. 5 Figure 1 shows the results obtained using TEM.<br><code>1以前的研究使用固定的现场监测器获得的数据表明，驾车旅行比步行旅行导致更低的CO暴露。2根据Figo等人（1999年）的研究，汽车乘客的暴露中位数比步行者低11%。3在我们的研究中，模拟排放率是使用交通排放模型（TEM）获得的，交通排放模型是Ka开发的一种CO暴露建模框架。4模拟结果与固定监测器每小时测量的路边CO浓度进行了比较。图1显示了使用TEM获得的结果。</code><br>6 As can be seen, during morning peak-time journeys the CO concentrations for car passengers were significantly lower than for pedestrians, which is consistent with results obtained in previous studies. 7 However, the modelled data were not consistent with these results for afternoon journeys. 8 Although the mean CO concentrations modelled by TEM for aft ernoon journeys on foot were in line with those of Figo et al., a striking difference was noted when each of the three peak hours was considered singly (Fig. 2). 9 It can be observed that during the first hour (H1) of the peak period, journeys on foot resulted in a considerably lower level of CO exposure. 10 Although levels for journeys on foot generally exceeded those modelled for car journeys during H2, during the last hour (H3) the levels for journeys on foot were again frequently far lower than for car journeys.<br><code>6可以看出，在早上高峰时间的行程中，汽车乘客的CO浓度明显低于行人，这与以前的研究结果一致。7然而，模型数据与下午旅行的结果不一致。[8]尽管TEM模拟的午后徒步旅行的平均CO浓度与Figo等人的结果一致，当单独考虑三个高峰时间时，注意到一个显著的差异（图2）。9可以观察到，在高峰期的第一个小时（H1），步行旅行导致的一氧化碳暴露水平相当低。10虽然在H2期间徒步旅行的水平一般超过为汽车旅行模拟的水平，但在最后一小时（H3）期间，徒步旅行的水平又经常远远低于汽车旅行。</code><br>11 A quantitative analysis to determine modelling uncertainties was applied, based on the maximum deviation of the measured and calculated levels within the considered period. 12 Using this approach, the uncertainty of the model prediction for this study slightly exceeds the 50% acceptability limit defined by Jiang. 13 Nevertheless, these results suggest that data obtained using TEM to simulate CO exposures may provide more sensitive information for assessing the impact of traffic management strategies than traditional on-site measurement.<br><code>11.根据所涉时期内测量和计算水平的最大偏差，进行了定量分析，以确定模型的不确定性。[12]使用这种方法，本研究的模型预测的不确定性略超过Jiang定义的50%可接受限度。13然而，这些结果表明，使用TEM模拟CO暴露所获得的数据可能比传统的现场测量提供更敏感的信息，用于评估交通管理策略的影响。</code></p></blockquote><p>Before you begin to build a model, read the following section on grammar and writing skills.<br><code>在你开始建立一个模型之前，阅读下面关于语法和写作技巧的部分。</code></p><h3 id="3-2-Grammar-and-Writing-Skills"><a href="#3-2-Grammar-and-Writing-Skills" class="headerlink" title="3.2 Grammar and Writing Skills"></a>3.2 Grammar and Writing Skills</h3><p><code>3.2语法和写作技巧</code><br>This section deals with four language areas which are important in the Results section:<br><code>本节涉及在结果部分中非常重要的四个语言领域：</code><br>SEQUENCE<br>FREQUENCY<br>QUANTITY<br>CAUSALITY<br><code>顺序</code><br><code>频率</code><br><code>数量</code><br><code>因果关系</code></p><h5 id="3-2-1-Sequence"><a href="#3-2-1-Sequence" class="headerlink" title="3.2.1 Sequence"></a>3.2.1 Sequence</h5><p><code>3.2.1顺序</code><br>In order for other researchers to be able to repeat your work accurately and compare their results with yours, you need to be able to describe the order and time sequence of what you did and found in a very precise way. Time sequence means how long each step took and where it occurred in the sequence. You cannot use only then or next; these words tell your reader the order in which events occurred but they don’t provide information about how long each event took, how soon the next event occurred or where it occurred in the sequence. A clear understanding of the time sequence will help your reader to picture it and repeat it for themselves.<br><code>为了让其他研究人员能够准确地重复你的工作，并将他们的结果与你的结果进行比较，你需要能够以非常精确的方式描述你所做和发现的顺序和时间顺序。时间顺序是指每个步骤花费的时间以及它在序列中发生的位置。你不能使用only then或next;这些词告诉你的读者事件发生的顺序，但它们没有提供关于每个事件花了多长时间，下一个事件发生的时间或它在序列中发生的位置的信息。对时间顺序的清晰理解将有助于读者描绘它并为自己重复它。</code><br>The words and phrases that communicate sequence can be divided into eight groups.<br><code>表达顺序的单词和短语可以分为八组。</code></p><ol><li><p>The first group contains words or phrases which refer to events that occurred before you began your experiment/simulation or before you began observing your results:<br><code>1)第一组包含的单词或短语指的是在你开始实验/模拟之前或在你开始观察结果之前发生的事件：</code><br><em><strong>It was apparent beforehand that a reduction in temperature would be a desirable outcome.</strong></em><br><code>事先很明显，温度降低将是一个理想的结果。</code></p></li><li><p>The second group marks the beginning of the experiment/simulation or the first result you are describing:<br><code>2)第二组标志着实验/模拟的开始或您描述的第一个结果：</code><br><em><strong>At the beginning the temperature was stable, as predicted.</strong></em><br><code>正如预测的那样，开始时温度稳定。</code></p></li><li><p>The third group contains words/phrases which tell you the order in which events occurred but do not give any information about the time sequence:<br><code>3)第三组包含的单词/短语告诉你事件发生的顺序，但不给予任何关于时间顺序的信息：</code><br><em><strong>The temperature increased to 49°C and then dropped to 30°C.</strong></em><br><code>温度上升到49 ℃，然后下降到30 ℃。</code><br>In this case, the drop in temperature may have occurred quite soon aft er the temperature reached 49°C or it may have taken a long time; the word then only tells the reader the order in which these events occurred.<br><code>在这种情况下，温度下降可能在温度达到49°C后很快就发生了，或者可能花了很长时间;这个词只告诉读者这些事件发生的顺序。</code></p></li><li><p>The fourth group is used to communicate that there was (only) a short period of time between two events:<br><code>4)第四组用于传达两个事件之间（只有）很短的时间：</code><br><em><strong>The temperature increased to 49°C but soon dropped to 30°C.</strong></em></p></li><li><p>The fifth group communicates that the period of time between events was long, or that the event occurred near the end of the sequence:<br><code>5)第五组传达事件之间的时间间隔很长，或者事件发生在序列的末尾：</code><br><em><strong>The temperature increased to 49°C and later dropped to 30°C.</strong></em><br><code>气温上升到49 ℃，后来又降到30 ℃</code></p></li><li><p>The sixth group is extremely useful and important. It contains words and phrases that are used to communicate that events occurred at the same time or almost at the same time, or during the same period, and therefore the items in this group are sometimes used to communicate a possible causal relationship between events:<br><code>6)第六组是非常有用和重要的。它包含用于传达事件发生在同一时间或几乎同一时间或同一时期的单词和短语，因此该组中的项目有时用于传达事件之间可能的因果关系：</code><br><em><strong>The temperature dropped sharply when we reduced the pressure.</strong></em><br><code>当我们降低压力时，温度急剧下降。</code></p></li><li><p>The seventh group marks the end of the sequence of events:<br><code>7)第七组标志着事件序列的结束：</code><br><em><strong>At the end there was a noticeable drop in temperature.</strong></em><br><code>最后，温度明显下降。</code></p></li><li><p>The last group refers to events that occurred aft er you finished your experiment/simulation or aft er you finished observing the results:<br><code>8)最后一组是指在完成实验/模拟或完成观察结果后发生的事件：</code><br><em><strong>At the end there was a noticeable drop in temperature but it was decided afterwards to omit it from the input data.</strong></em><br><code>最后，温度明显下降，但后来决定从输入数据中忽略它。</code><br>Here is a list of the words and phrases that communicate sequence:<br><code>以下是传达序列的单词和短语列表：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/3-3.png"><br>Now put them into one (or more) of the appropriate groups. One example in each group has been entered in the box as a guide and some of the words or phrases can appear in more than one group.<br><code>现在把它们放入一个（或多个）适当的组中。每组中的一个示例已输入框中作为指导，并且某些单词或短语可以出现在多个组中。</code></p></li></ol><ol><li>before the beginning<br><code>开始前</code><br><em><strong>beforehand</strong></em></li><li>the beginning or first step<br><code>开始或第一步</code><br><em><strong>at the beginning</strong></em></li><li>steps/order<br><em><strong>then</strong></em></li><li>after a short while<br><code>过了一会儿</code><br><em><strong>soon</strong></em></li><li>at a late/later stage; aft er a while/longer period<br><code>在后期;在一段时间后</code><br><em><strong>later</strong></em></li><li>one point/period occurring almost or exactly at the same time as anothe<br><code>与另一点几乎或恰好同时发生的一点/时期</code><br><em><strong>when</strong></em></li><li>the end or last step<br><code>结束或最后一步</code><br><em><strong>at the end</strong></em></li><li>after the end<br><code>在最后</code><br><em><strong>afterwards</strong></em></li></ol></body></html>]]></content>
      
      
      <categories>
          
          <category> sciencewriting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unit 2 Writing about Methodology</title>
      <link href="/2024/02/25/unit-2-writing-about-methodology/"/>
      <url>/2024/02/25/unit-2-writing-about-methodology/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><code>第二单元 写作方法论</code></p><h3 id="2-1-Structure"><a href="#2-1-Structure" class="headerlink" title="2.1 Structure"></a>2.1 Structure</h3><p><code>结构</code><br>The title of this section varies in different disciplines and in different journals. It is sometimes called Materials and Methods, or it can be called Procedure, Experiments, Experimental, Simulation, Methodology or Model. Th is section is the first part of the central ‘report’ section of the research article (the second part is the Results section), and it reports what you did and/or what you used.<br><code>这一部分的标题在不同的学科和不同的期刊中有所不同。它有时被称为材料和方法，或者它可以被称为程序，实验，实验，模拟，方法或模型。这部分是研究文章的中心“报告”部分的第一部分（第二部分是结果部分），它报告了你做了什么和/或你使用了什么。</code><br>Most journals publish (usually on the Internet) a Guide for Authors. Before you begin to read this unit, access the guide for a journal you read regularly — if you’re lucky, it will include a short description of what the editors expect in each section in addition to technical information relating to the figures. Here is a typical sentence from such a guide:<br><code>大多数期刊都出版（通常在互联网上）作者指南。在你开始阅读本单元之前，请查阅你经常阅读的期刊指南--如果你幸运的话，除了与数字有关的技术信息外，它还包括编辑对每一部分的期望的简短描述。下面是这样一个指南中的典型句子：</code></p><blockquote><p>The Methodology should contain sufficient detail for readers to replicate the work done and obtain similar results.<br><code>该方法应包含足够的细节，使读者能够重复所做的工作并获得类似的结果。</code></p></blockquote><p>It is true that your work must contain sufficient detail to be repeatable, but the type of writing you will need to do is not just a record of what you did and/or used. One of the most interesting and important changes you need to make in the way you write is that until now, you have probably been writing for people (perhaps your teachers) who know more about your research topic than you do. You have been displaying to them that you understand the tasks they have set and have performed them correctly. However, when you write a research article, people will be learning from you. Therefore you now need to be able to communicate information about a new procedure, a new method, or a new approach so that everyone reading it can not only carry it out and obtain similar results, but also understand and accept your procedure.<br><code>的确，你的作品必须包含足够的细节才能被重复，但你需要做的写作类型不仅仅是记录你做了什么和/或使用了什么。你需要在写作方式上做出的最有趣和最重要的改变之一是，到目前为止，你可能一直在为比你更了解你的研究主题的人（也许是你的老师）写作。你一直在向他们展示你理解他们设定的任务，并正确地执行了这些任务。然而，当你写一篇研究文章时，人们会向你学习。因此，您现在需要能够传达有关新程序、新方法或新途径的信息，以便每个阅读它的人不仅可以执行它并获得类似的结果，而且还可以理解和接受您的程序。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-38.png"><br>When we come to ask our three questions:<br><code>当我们来问我们的三个问题：</code></p><ul><li>How do I start the Methodology/Experiments section? What type of sentence should I begin with?<br><code>如何开始方法/实验部分？我开始应该用什么类型的句子？</code></li><li>What type of information should be in this section, and in what order?<br><code>本节应包含哪些类型的信息，顺序如何？</code></li><li>How do I end this section?<br><code>我如何结束这一节？</code></li></ul><p>you already know that the Methodology should contain a detailed description of what you did and/or used, and this helps to answer the second of the three questions. As we will see, however, it is not a full answer; to be effective and conform to what is normally done in a research paper, this section must contain other important information as well.<br><code>你已经知道方法论应该包含你所做和/或使用的详细描述，这有助于回答三个问题中的第二个。然而，正如我们将看到的，这并不是一个完整的答案;为了有效并符合研究论文中通常所做的，这一部分还必须包含其他重要信息。</code><br>Read the example below. The title of the paper is Changes in the chemistry of groundwater in the chalk of the London Basin. Don’t worry if the subject matter is not familiar to you or if you have difficulty understanding individual words, especially technical terms like groundwater. Just try to get a general understanding at this stage and familiarise yourself with the type of language used.<br><code>阅读下面的例子。这篇论文的题目是《伦敦盆地白垩层地下水化学性质的变化》。如果你不熟悉这个主题，或者你很难理解个别单词，尤其是地下水这样的专业术语，不要担心。在这一阶段，试着获得一个大致的理解，并熟悉所使用的语言类型。</code></p><blockquote><p>Methodology<br><code>方法论</code></p><ol><li>The current investigation involved sampling and analysing six sites to measure changes in groundwater chemistry.<br>  <code>目前的调查包括对六个地点进行取样和分析，以测量地下水化学的变化。</code></li><li>The sites were selected from the London Basin area, which is located in the south-east of England and has been frequently used to interpret groundwater evolution.<br>  <code>这些地点选自伦敦盆地地区，该地区位于英格兰东南部，经常用于解释地下水演化。</code></li><li>A total of 18 samples was collected and then analysed for the isotopes mentioned earlier.<br>  <code>总共收集了18个样品，然后分析了前面提到的同位素。</code></li><li>Samples 1–9 were collected in thoroughly-rinsed 25 ml brown glass bottles which were filled to the top and then sealed tightly to prevent contamination.<br>  <code>将样品1-9收集在充分冲洗的25 ml棕色玻璃瓶中，将其填充至顶部，然后紧密密封以防止污染。</code></li><li>The filled bottles were shipped directly to two separate laboratories at Reading University, where they were analysed using standard methods suitably miniaturised to handle small quantities of water.<br>  <code>灌装后的瓶子被直接运送到阅读大学的两个独立实验室，在那里使用标准方法对它们进行分析，这些方法经过适当的改进以处理少量的水。</code></li><li>Samples 10–18 were prepared in our laboratory using a revised version of the precipitation method established by the ISF Institute in Germany.<br>  <code>样品10-18在我们的实验室中使用由德国的ISF研究所建立的沉淀法的修订版本制备。</code></li><li>This method obtains a precipitate through the addition of BaCl2.2H2O; the resulting precipitate can be washed and stored easily.<br>  <code>该方法通过加入BaCl 2·2 H2O获得沉淀物;所得沉淀物可以容易地洗涤和储存。</code></li><li>The samples were subsequently shipped to ISF for analysis by accelerator mass spectrometry (AMS).<br>  <code>随后将样品运送到ISF进行加速器质谱分析（AMS）。</code></li><li>All tubing used was stainless steel, and although two samples were at risk of CFC contamination as a result of brief contact with plastic, variation among samples was negligible.<br>  <code>所有使用的管道都是不锈钢的，虽然有两个样品由于与塑料短暂接触而有CFC污染的风险，但样品之间的差异可以忽略不计。</code></li></ol></blockquote><h3 id="2-2-Grammar-and-Writing-Skills"><a href="#2-2-Grammar-and-Writing-Skills" class="headerlink" title="2.2 Grammar and Writing Skills"></a>2.2 Grammar and Writing Skills</h3><p><code>语法和写作技巧</code><br>This section deals with three language areas which are important in the Methodology:<br><code>本节涉及方法中的三个重要语言领域：</code><br>PASSIVES AND TENSE PAIRS<br><code>被动语态和时态对</code><br>USE OF ‘A’ AND ‘THE’<br><code>“A”和“THE”的用法</code><br>ADVERBS AND ADVERB LOCATION<br><code>副词和副词位置</code></p><h5 id="2-2-1-Passives-and-tense-pairs"><a href="#2-2-1-Passives-and-tense-pairs" class="headerlink" title="2.2.1 Passives and tense pairs"></a>2.2.1 Passives and tense pairs</h5><p><code>被动语态和时态对</code><br>When a sentence changes from active to passive, it looks like this:<br><code>当一个句子从主动变为被动时，它看起来像这样：</code></p><blockquote><p>The dog bit the policeman.<br><code>狗咬了警察。</code>active<br>Th e policeman was bitten by the dog.<br><code>警察被狗咬了。</code>passive</p></blockquote><p>But in formal academic writing, when you report what you did, you don’t write ‘by us’ or ‘by me’ when changing the sentence from active to passive. You simply leave the agent out, creating an agentless passive:<br><code>但在正式的学术写作中，当你报告你所做的事情时，当句子从主动转为被动时，你不会写“by us”或“by me”。您只需将代理排除在外，创建无代理被动：</code></p><blockquote><p>We/I collected the samples.<br><code>我们/我收集了样本。</code>active<br>The samples were collected.<br><code>采集了样本。</code>passive</p></blockquote><p>Before you begin to write the description of what you did and used, you need to check with the Guide for Authors in your target journal (if you are writing a doctoral thesis in an English-speaking country, check with your supervisor) to find out whether this part of the paper or thesis should be written in the passive or in the active. You can use the active (we collected) if you worked as part of a research team. Using the active is not usually appropriate when you write your PhD thesis because you worked alone, and research is not normally written up in the first person singular (I collected). In most cases, you will find that in papers and theses, the procedure you used in your research is described in the passive, either in the Present Simple passive (is collected) or in the Past Simple passive (was collected). To make that choice, it is useful to explore the advantages and disadvantages of each.<br><code>在你开始描述你所做的和使用的东西之前，你需要检查你的目标期刊的作者指南（如果你是在一个讲英语的国家写博士论文，请与你的导师核实），以确定这部分论文或论文是否应该以被动或主动的方式撰写。如果你是研究团队的一员，你可以使用我们收集的活动。当你写博士论文时，使用主动语态通常是不合适的，因为你是独自工作的，研究通常不会用第一人称单数（我收集的）。在大多数情况下，你会发现在论文和论文中，你在研究中使用的程序是用被动语态描述的，要么是现在式被动语态（被收集），要么是过去式被动语态（被收集）。要做出这种选择，探索每种方法的优点和缺点是有用的。</code><br>There are two common errors in the way passives are used in this section. First, look at these two sentences:<br><code>本节中使用被动语态的方式有两个常见的错误。首先来看这两句话：</code></p><blockquote><p>(a)A flexible section is inserted in the pipe.<br><code>将在管道中插入一个柔性截面。</code>Present Simple passive <code>现在时简单被动语态</code><br>A flexible section was inserted in the pipe.<br><code>在管道中插入了一个柔性节段。</code>Past Simple passive <code>过去式简单被动语态</code></p></blockquote><p>When you write about what you did and what you used, you need to be able to distinguish between standard procedures, i.e. what is normally done or how a piece of equipment is normally constructed, and what you did yourself. In the examples above, (a) uses the Present Simple tense to describe what is normally done or to describe a standard piece of equipment used in the research and (b) uses the Past Simple tense to describe what you did yourself. It is conventional in this section to use the passive for both, and the agent of the action is not mentioned in the sentence — we don’t add ‘by the researcher’ or ‘by me’ at the end.<br><code>当你写你做了什么和你使用了什么，你需要能够区分标准程序，即通常做什么或一件设备通常是如何构造的，以及你自己做了什么。在上面的例子中，（a）使用一般现在时来描述通常所做的事情或描述研究中使用的标准设备;（b）使用一般过去时来描述你自己所做的事情。在这一节中，习惯性地使用被动语态来表示两者，并且在句子中没有提到动作的施事--我们不会在末尾加上“by the researcher”或“by me”。</code><br>Passives used in formal writing are normally of this type, i.e. agentless passives. However, because the agent is not given, the only way that the reader can separate what is normally done (Sentence (a)) from what you did yourself (Sentence (b)) is if you use the correct tense. Check your target journal, but wherever possible it is clearer to use the Present Simple passive for what is normally done and the Past Simple passive to indicate what you did yourself.<br><code>在正式写作中使用的被动句通常是这种类型，即无施事被动句。然而，由于施事没有给出，读者要想区分正常情况下做的事（句子（a））和你自己做的事（句子（b）），唯一的办法就是使用正确的时态。检查你的目标日记，但在可能的情况下，用现在一般被动语态来表示通常做的事情，用过去时一般被动语态来表示你自己做的事情会更清楚。</code><br>You can see that if you don’t pay careful attention to the tense of these sentences, your own work may become confused with the standard procedures you are describing. This is a very common error, even among native speakers, and has serious consequences. If the reader cannot identify your contribution, that is a disaster! Look at this example:<br><code>你可以看到，如果你不仔细注意这些句子的时态，你自己的工作可能会与你所描述的标准程序混淆。这是一个非常常见的错误，即使在母语人士中也是如此，并且会产生严重的后果。如果读者不能识别你的贡献，那是一场灾难！看看这个例子：</code></p><blockquote><p>Two dye jets are placed in the laser cavity. A gain jet is then excited by an argon ion laser and the pulses are spatially filtered in order to obtain a Gaussian beam. Polarisation is confirmed using a polarising cube. Th e pulses were split into reference pulses and probe pulses and the reference pulses were carefully aligned into the detector to minimise noise levels.<br><code>两个染料喷嘴被放置在激光腔中。增益射流，然后激发氩离子激光器和脉冲进行空间滤波，以获得高斯光束。使用偏振立方体确认偏振。脉冲被分成参考脉冲和探测脉冲，并且参考脉冲被仔细地对准到检测器中以使噪声水平最小化。</code></p></blockquote><p>In this case, splitting the pulses into two groups for testing was the significant innovation of the writer’s research team but the only way the reader knows this is because of the change in tense from Present Simple passive to Past Simple passive (were split). Here is another example:<br><code>在这种情况下，将脉冲分为两组进行测试是作者研究团队的重大创新，但读者知道这一点的唯一方法是因为时态从现在一般被动式到过去一般被动式的变化（被分裂）。下面是另一个例子：</code></p><blockquote><p>Samples for gas analysis were collected using the method described by Brown (1999), which uses a pneumatic air sampling pump.<br><code>使用Brown（1999）描述的方法收集用于气体分析的样品，该方法使用气动空气取样泵。</code></p></blockquote><p>Another difficulty arises with the passive when you write about the procedure you used and compare it with the work of other researchers. You can use the Past Simple agentless passive to describe the procedure you used (the samples were collected using a suction tube) but you may also need to use exactly the same Past Simple agentless passive to describe the procedure used by the other researcher whose work you are citing (the samples were collected using a suction tube). Th is means that unless you are very careful, the reader has no way of separating your work from that of the other researcher. The fact that you are so familiar with what you did means that your own contribution is obvious to you — but it may not be obvious to your reader.<br><code>当你写下你所使用的程序并将其与其他研究人员的工作进行比较时，被动语态会出现另一个困难。你可以使用一般过去时无施事被动语态来描述你所使用的过程（样本是用吸管收集的），但你也可能需要使用完全相同的一般过去时无施事被动语态来描述你所引用的其他研究者所使用的过程（样本是用吸管收集的）。这意味着，除非你非常小心，否则读者无法将你的工作与其他研究人员的工作分开。你对自己所做的事情如此熟悉，这意味着你自己的贡献对你来说是显而易见的，但对你的读者来说可能并不明显。</code><br>One way to make sure that your own contribution is clear and easy to identify is by marking it with words — perhaps by adding phrases like In this study, the samples were collected using a suction tube or In our experiments the samples were collected using a suction tube, and by identifying the procedure used by other researchers with careful references at the appropriate place in the sentence (In Brown (1999) the samples were collected using a suction tube).<br><code>一种确保你自己的贡献是清晰和容易识别的方法是用文字标记它-也许通过添加短语，如在这项研究中，样本是使用吸管收集的，或者在我们的实验中，样本是使用吸管收集的，并通过在句子中适当的地方仔细引用其他研究人员使用的程序来识别（在Brown（1999）中，使用吸管收集样本）。</code><br>There are five possible uses that you may need. Note the different tenses.<br><code>有五种可能的用途，你可能需要。注意不同的时态。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-1.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-2.png"></p><h5 id="2-2-2-Use-of-‘a’-and-‘the’"><a href="#2-2-2-Use-of-‘a’-and-‘the’" class="headerlink" title="2.2.2 Use of ‘a’ and ‘the’"></a>2.2.2 Use of ‘a’ and ‘the’</h5><p><code>“a”和“the”的用法</code><br>This is one of the most problematic areas of English grammar and usage. Many languages do not have separate words for a and the, and even if they do, these words may not correspond exactly to the way in which they are used in English. Students studying English as a second language are oft en given the following useful, but sometimes confusing, rule:<br><code>这是英语语法和用法中最有问题的领域之一。许多语言没有单独的单词表示a和the，即使有，这些单词也可能与英语中的用法不完全对应。学习英语作为第二语言的学生经常被给予以下有用的，但有时令人困惑的规则：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-3.png"><br><code>单数可数名词需要限定词</code><br>A determiner is a word like the, a, my, this, one, some. It’s a difficult rule to operate successfully because two problems need to be solved before you can use it. Firstly, it’s hard to know exactly which nouns are countable and, secondly, even when you know, how do you decide whether to use a or the?<br><code>限定词是像the，a，my，this，one，some这样的词。这是一个很难成功运用的规则，因为在你使用它之前需要解决两个问题：首先，很难确切地知道哪些名词是可数名词;其次，即使你知道，你如何决定是用a还是the？</code><br>Let’s look at the first problem. Deciding which nouns are countable nouns and which aren’t isn’t as easy as it looks. Many nouns which are oft en considered uncountable can actually be used ‘countably’. Nouns like death or childhood, for example, can occur in the plural:<br><code>我们来看第一个问题。判断哪些名词是可数名词，哪些不是，并不像看起来那么容易。许多常被认为是不可数的名词实际上也可以用countably。例如，像死亡或童年这样的名词可以以复数形式出现：</code></p><blockquote><p>There have been three deaths this year from pneumonia. Our childhoods were very different; I grew up in France and she grew up in China.<br><code>今年已有三人死于肺炎。我们的童年非常不同;我在法国长大，她在中国长大。</code></p></blockquote><p>and so can nouns like industry:<br><code>像工业这样的名词也可以：</code></p><blockquote><p>Many industries rely on fossil fuels.<br><code>许多工业依赖化石燃料。</code></p></blockquote><p>Even names of materials like steel can occur in the plural:<br><code>甚至像钢这样的材料的名称也可以以复数形式出现：</code></p><blockquote><p>Some steels are used in the manufacture of medical instruments.<br><code>一些钢用于制造医疗器械。</code></p></blockquote><p>In the following list of uncountable nouns, mark those which can also be used in the plural, i.e. countably. Th e way you use a noun determines whether it is used in its countable or uncountable form. So when you use a noun like industry, stop and think — do you mean industry in general (uncountable) or a particular industry (countable)? Check your answers in the Key.<br><code>在下面的不可数名词表中，标出那些也可以用复数形式的名词，即countably。你使用名词的方式决定了它是可数形式还是不可数形式。所以，当你使用像industry这样的名词时，停下来想一想，你是指一般的industry（不可数的）还是特定的industry（可数的）？检查你的答案在关键。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-4.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-5.png"></p><p>KEY<br>The nouns which can also have a countable meaning appear in italics.<br><code>也可以有可数意义的名词用斜体字表示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-6.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-7.png"><br>Now look at the second problem: how do you decide whether to use a or the? You may have been told that a is used for general reference and the is used for specific reference, but in the following sentence:<br><code>现在来看第二个问题：如何决定使用a还是the？你可能已经被告知，a用于一般指代，the用于具体指代，但在下面的句子中：</code><br>There is a book on the shelf above my desk; can you bring it here?<br><code>我书桌上方的书架上有一本书，你能把它拿过来吗？</code><br>a book clearly refers to a specific book; in fact, that part of the sentence specifies which book the speaker wants. So if the specific/general criterion doesn’t help you to select a or the, what does?<br><code>一本书显然是指一本特定的书;事实上，句子的这一部分指定了说话者想要哪本书。那么，如果特定/一般标准不能帮助您选择a或the，那么什么可以呢？</code><br>Start by asking yourself this simple question: Why do you use a the first time you talk about something, but when you refer to it again you use the? Aft er all, it’s the same specific item on both occasions. For example, in the sentence below, why does the first reference to the cheese sandwich use a and the second reference use the if both refer to the same specific sandwich?<br><code>首先问自己一个简单的问题：为什么你第一次谈论某事时使用a，但当你再次提到它时，你会使用the？毕竟，这是相同的具体项目在两个场合。例如，在下面的句子中，为什么第一次提到奶酪三明治时使用a，第二次提到奶酪三明治时使用if，这两次都是指同一个三明治？</code></p><blockquote><p>I had a cheese sandwich and an apple for lunch. Th e sandwich was fine but the apple had a worm in it.<br><code>我午饭吃了一个奶酪三明治和一个苹果。三明治很好，但苹果里有虫。</code></p></blockquote><p>The difference is that the first time the speaker mentions the cheese sandwich or the apple, only the speaker knows about them — but the second time, both the speaker and listener know. Th e worm, however, is ‘new’ to the listener, and so is referred to using a. Now we can add a new rule:<br><code>不同的是，第一次说话者提到奶酪三明治或苹果时，只有说话者知道它们，但第二次，说话者和听者都知道。然而，蠕虫对侦听器来说是“新的”，因此使用。现在我们可以添加一个新规则：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-8.png"><br><code>如果或当你和你的读者都知道你的意思是什么东西/人时使用the</code><br>This is true even if the thing or person has not been mentioned before, for example, in the following sentences:<br><code>这是真的，即使该事物或人之前没有被提到过，例如，在下面的句子中：</code></p><blockquote><p>I arrived at Heathrow Airport but the check-in was closed.<br><code>我到达了希思罗机场，但是登记手续已经结束了。</code><br>I bought a new computer but the keyboard was faulty.<br><code>我买了一台新电脑，但键盘有毛病。</code></p></blockquote><p>check-in and keyboard need the because as soon as Heathrow Airport is mentioned, the speaker and listener know about and therefore share checkin; as soon as a computer is mentioned, they share keyboard. Similarly, in the sentence:<br><code>登记和键盘需要，因为一旦提到希思罗机场，说话者和听者就知道并因此共享登记;一旦提到计算机，他们就共享键盘。同样，在这句话中：</code></p><blockquote><p>He lit a match but the flame went out.<br><code>他点燃了一根火柴，但火焰熄灭了。</code></p></blockquote><p>mentioning a match automatically creates the concept of flame in the reader’s mind — and this shared understanding is marked by the use of the. Similarly, if we were in the same room and I told you to look up at the ceiling, you wouldn’t ask me ‘Which ceiling are you talking about?’ because it would be obvious; we would share it.<br><code>提及火柴会自动在读者的脑海中创造出火焰的概念--这种共同的理解是以使用。同样，如果我们在同一个房间里，我告诉你抬头看天花板，你不会问我“你说的是哪一个天花板？”因为这是显而易见的，我们可以分享</code></p><blockquote><p>Did she get the job? (the job we both know she wanted)<br><code>她得到工作了吗？(the我们都知道她想要的工作）</code><br>I’ll meet you in the library later. (the library we normally use)<br><code>一会儿图书馆见。(the我们通常使用的库）</code></p></blockquote><p>Here are some more useful rules:<br><code>以下是一些更有用的规则：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-9.png"><br><code>如果只有一个可能的引用，请使用the</code></p><blockquote><p>We removed the softest layer of membrane.<br><code>我们去掉了最软的一层膜。</code><br>Cairo is the capital of Egypt.<br><code>开罗是埃及的首都。</code><br>The opening was located in the centre of each mesh.<br><code>开口位于每个网格的中心。</code><br>Government policy is committed to protecting the environment.<br><code>政府的政策致力于保护环境。</code><br>The sun’s altitude is used to determine latitude.<br><code>太阳的高度被用来确定纬度。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-10.png"><br><code>如果它不重要，或者你不知道，或者你的读者不知道你指的是什么东西/什么人，就用A。</code><br>A 35 ml brown glass bottle was used to store the liquid. (It doesn’t matter which 35 ml brown glass bottle was used.)<br><code>使用35 ml棕色玻璃瓶储存液体。(It不管使用的是哪种35毫升棕色玻璃瓶。</code><br>The subject then spoke to an interviewer. (It doesn’t matter which interviewer/I know which one but you don’t.)<br><code>然后，这个人接受了采访。(It不管是哪个面试官/我知道是哪个，但你不知道。</code><br>It works on the same principle as a combustion engine. (It doesn’t matter which combustion engine.)<br><code>它的工作原理与内燃机相同。(It不在乎是哪种内燃机）。</code></p></blockquote><p>Sometimes the choice of a or the changes the meaning of the sentence completely:<br><code>有时，选择a或the会完全改变句子的意思：</code></p><blockquote><p>(a)This effect may hide a connection between the two. (There may possibly be a connection between the two but if there is, we cannot see it.)<br><code>这种影响可能隐藏了两者之间的联系。（两者之间可能有联系，但如果有，我们看不到。）</code><br>(b)This effect may hide the connection between the two. (There is definitely a connection between the two but we may not be able to see it because of this effect.)<br><code>这种影响可能隐藏了两者之间的联系。（两者之间肯定有联系，但我们可能因为这种影响而无法看到它。）</code><br>这种影响可能隐藏了两者之间的联系。（两者之间肯定有联系，但我们可能因为这种影响而无法看到它。</p></blockquote><p>Here’s another pair in which the choice of a or the has a significant effect on the meaning (∅ is used here to indicate the plural of a):<br><code>下面是另一对句子，其中a或the的选择对意义有很大影响（这里用∅表示a的复数）：</code></p><blockquote><p>(a)The nodes should be attached to ∅ two adjacent receptor sites. (There are many receptor sites and any two adjacent ones will do.)<br><code>淋巴结应连接到两个相邻的受体位点。（有许多受体位点，任何两个相邻的都可以。）</code><br>(b)The nodes should be attached to the two adjacent receptor sites. (There are only two receptor sites.)<br><code>淋巴结应连接到两个相邻的受体位点。（只有两个受体位点。）</code></p></blockquote><p>The best way to use the information you have just learned is to take a paragraph from a research article that you are reading and use the information in this grammar section to work out why the writer has chosen each instance of the or a, or why the writer has not used any determiner before a particular noun.<br><code>使用你刚刚学到的信息的最好方法是从你正在阅读的一篇研究文章中选取一段，并使用语法部分的信息来弄清楚作者为什么选择了the或a的每个实例，或者为什么作者在特定的名词前没有使用任何限定词。</code><br>Another important point to note about the use of a, the and ∅ is that they can all be used generically, i.e. when expressing a general truth:<br><code>另一个关于a、the和∅的用法需要注意的要点是，它们都可以一般地使用，即当表达一个普遍真理时：</code></p><blockquote><p>The electroencephalograph is a machine for measuring brain waves.<br><code>脑电图仪是一种测量脑电波的机器。</code><br>An electroencephalograph is a machine for measuring brain waves.<br><code>脑电图仪是一种测量脑电波的机器。</code><br>Electroencephalographs are machines for measuring brain waves.<br><code>脑电图仪是测量脑电波的机器。</code></p></blockquote><p>One last note: a is used before consonant sounds, while an is used before vowel sounds. Sound, not spelling, is important here, so we write an MRI scan because the letter ‘M’ is pronounced ‘em’, but a UV light because the letter ‘U’ is pronounced ‘yoo’.<br><code>最后一个音符：a用在辅音之前，an用在元音之前。声音，而不是拼写，在这里是重要的，所以我们写一个MRI扫描，因为字母'M'是发音'em'，但紫外线灯，因为字母'U'是发音'yoo'。</code></p><h5 id="2-2-3-Adverbs-and-adverb-location"><a href="#2-2-3-Adverbs-and-adverb-location" class="headerlink" title="2.2.3 Adverbs and adverb location"></a>2.2.3 Adverbs and adverb location</h5><p><code>2.2.3副词和副词位置</code><br>When you are communicating complex ideas in another language, an obvious grammatical error is not as bad as an error which is invisible. A proofreader or editor will notice an obvious grammatical error and correct it, but if the sentence is written in grammatically correct English the error is not visible to proofreaders and editors. An example of an invisible error is where the sentence is grammatically correct but the choice of which verb tense to use is inappropriate or does not represent the intention of the writer. Th ese hidden errors are worrying because neither the writer nor the editor/proofreader knows they have occurred and yet the sentence does not mean what the writer intended.<br><code>当你用另一种语言交流复杂的思想时，一个明显的语法错误并不像一个看不见的错误那么糟糕。校对员或编辑会注意到一个明显的语法错误并纠正它，但如果句子是用语法正确的英语写的，校对员和编辑就看不到这个错误。看不见的错误的一个例子是，句子在语法上是正确的，但使用的动词时态的选择是不合适的，或者不代表作者的意图。这些隐藏的错误令人担忧，因为无论是作者还是编辑/校对员都不知道它们已经发生，但句子并不意味着作者的意图。</code><br>Common hidden errors include mistakes in the use of a and the (see Section 2.2.2 above), whether or not to use a comma before the word which in relative clauses and adverb location errors. Adverb location errors are easy to make and hard to detect.<br><code>常见的隐藏错误包括a和the的使用错误（见上文第2.2.2节），关系从句中which一词前是否使用逗号以及副词位置错误。定位误差很容易产生，但很难检测到。</code><br>Adverbs don’t always do what you want or expect them to do. In the fi rst place, adverbs needing prepositions can be ambiguous (Look at that dog with one eye can either mean USING one eye or HAVING one eye) and in the second place, adverbs may attach themselves to unexpected parts of a sentence. Be careful where you put your adverb, and be especially careful if you are using more than one adverb in a sentence. Here is an example of the kind of problem you may encounter:<br><code>他们并不总是做你想做或期望他们做的事。首先，需要介词的副词可能是模棱两可的（用一只眼睛看那只狗可以意味着用一只眼睛或有一只眼睛），其次，副词可能会附着在句子中意想不到的部分。注意副词的位置，尤其是在一个句子中使用多个副词时。以下是您可能遇到的问题类型的示例：</code></p><blockquote><p>The patient was discharged from hospital aft er being shot in the back with a 9 mm gun.<br><code>患者背部被9毫米口径手枪击中后出院。</code></p></blockquote><p>Did the doctors shoot her?<br><code>医生开枪打她了吗？</code></p><blockquote><p>He gave a lecture about liver cancer at the hospital last January.<br><code>去年一月他在医院做了一个关于肝癌的讲座。</code></p></blockquote><p>Was the lecture in the hospital — or the cancer? Did the lecture refer to cancer cases occurring in January or did the lecture itself occur in January?<br><code>是在医院演讲还是癌症？讲座是指发生在一月的癌症病例还是讲座本身发生在一月？</code><br>Although there are rules for adverb location, they are complex and hard to apply when you are writing. Since your aim is to stay safe and write clearly, it is better to avoid adverb clusters like these, and rewrite the information in a different order. If your adverb relates to the whole sentence (i.e. clearly, last January, as sentence (i.e. clearly, last January, as a result) then consider putting the adverb at the front of the sentence sentence (i.e. clearly, last January, as a result) then consider putting the adverb at the front of the sentence:<br><code>虽然副词的位置是有规则的，但它们很复杂，很难在写作中应用。既然你的目标是保持安全和清晰，最好避免像这样的副词集群，并以不同的顺序重写信息。如果你的副词与整个句子有关（即显然，去年一月，作为句子（即显然，去年一月，作为结果）然后考虑将副词放在句子的前面（即显然，去年一月，作为结果）然后考虑将副词放在句子的前面：</code></p><blockquote><p>Last January he gave a lecture about liver cancer at the hospital<br><code>去年一月他在医院做了一个关于肝癌的讲座</code></p></blockquote><p>If you are still left with ambiguous adverb clusters, consider breaking the sentence down into units, each with its own adverb:<br><code>如果你仍然有歧义的副词群，考虑把句子分成几个单元，每个单元都有自己的副词：</code></p><blockquote><p>Last January he gave a lecture at the hospital; his subject was liver cancer<br><code>去年一月他在医院做了一个讲座;他的主题是肝癌</code></p></blockquote><h3 id="2-3-Writing-Task-Build-a-Model"><a href="#2-3-Writing-Task-Build-a-Model" class="headerlink" title="2.3 Writing Task: Build a Model"></a>2.3 Writing Task: Build a Model</h3><p><code>2.3写作任务：构建模型</code></p><h5 id="2-3-1-Building-a-model"><a href="#2-3-1-Building-a-model" class="headerlink" title="2.3.1 Building a model"></a>2.3.1 Building a model</h5><p><code>2.3.1建立模型</code><br>You are now ready to begin to build a model of the Methodology by writing a short description of what the writer is doing in each sentence in the space provided below. The Key is on the next page. Once you have tried to produce your own model, you can use the Key to help you write this section of a research article when you eventually do it on your own.<br><code>你现在已经准备好开始建立一个方法论的模型，在下面的空白处写一个简短的描述，描述作者在每个句子中做了什么。钥匙在下一页。一旦你尝试创建自己的模型，当你最终独立完成时，你可以使用Key来帮助你撰写研究文章的这一部分。</code><br><em><strong>GUIDELINES</strong></em><br>You should spend 30–45 minutes on this task.If you can’t think of a good description of the first sentence, choose an easier one, for example Sentence 4, and start with that. Remember that your model is only useful if it can be transferred to other Methodology sections, so don’t include content words such as groundwater or you won’t be able to use your model to generate Methodology sections in your field.<br><code>你应该花30-45分钟来完成这个任务。如果你想不出一个好的描述第一句话，选择一个更容易的，比如第4句，然后从那里开始。请记住，只有当模型可以转移到其他方法学部分时，模型才有用，因此不要包含地下水等内容词，否则您将无法使用模型在字段中生成方法学部分。</code><br>One way to find out what the writer is doing in a sentence — rather than what s/he is saying — is to imagine that your computer has accidentally deleted it. What is different for you (as a reader) when it disappears? If you press another key on the computer and the sentence comes back, how does that affect the way you respond to the information?<br><code>要想知道作者在一个句子里做了什么，而不是他/她在说什么，一个方法是想象你的电脑不小心删除了它。当它消失时，你（作为读者）有什么不同？如果你按下电脑上的另一个键，句子又回来了，这对你对信息的反应有什么影响？</code><br>Another way to figure out what the writer is doing in a sentence — rather than what s/he is saying — is to look at the grammar and vocabulary clues. What is the tense of the main verb? What is that tense normally used for? Is it the same tense as in the previous sentence? If not, why has the writer changed the tense? What words has the writer chosen to use?<br><code>另一种弄清楚作者在句子中做了什么的方法--而不是他/她在说什么--是看语法和词汇线索。主要动词的时态是什么？这个时态通常用来做什么？这句话的时态和前一句是一样的吗？如果不是，作者为什么改变了时态？作者选用了哪些词语？</code><br>Don’t expect to produce a perfect model. You will modify your model when you look at the Key, and perhaps again when you compare it to the way Methodology sections in your target articles work.<br><code>不要指望创造一个完美的模型。当您查看Key时，您将修改您的模型，当您将其与目标文章中的方法论部分的工作方式进行比较时，可能会再次修改。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-11.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-12.png"></p><h5 id="2-3-2-Key"><a href="#2-3-2-Key" class="headerlink" title="2.3.2 Key"></a>2.3.2 Key</h5><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-13.png"><br><code>在第1句中，“目前的调查包括对六个地点进行取样和分析，以测量地下水化学的变化。”作者提供了整个分节的总体概述，包括调查的目的。</code><br>If you wrote ‘introduction’ or ‘introduces the Methodology’ here, that won’t help you when you come to write your own thesis or research article because it doesn’t tell you what exactly to write in that sentence.<br><code>如果你在这里写了“介绍”或“介绍方法论”，当你写自己的论文或研究文章时，这对你没有帮助，因为它没有告诉你在这句话中到底要写什么。</code><br><em><strong>Why do I need to introduce the Methodology?</strong></em><br><code>为什么需要介绍方法论？</code><br>In some cases, writers begin immediately with a description of the procedure or the materials. This is appropriate where the research focus is very narrow and all those who are likely to read it are carrying out similar research. If this is not the case, it is more reader-friendly to start with some introductory material. The aim of providing a short introduction is to make the entry to that section smooth for the reader. There are many ways to introduce the Methodology. Here are three of the most common ways:<br><code>在某些情况下，写作者开始时会立即描述程序或材料。这是适当的研究重点是非常狭窄的，所有那些谁可能阅读它正在进行类似的研究。如果不是这种情况，那么从一些介绍性材料开始会更方便读者。提供简短介绍的目的是使读者顺利进入该部分。有许多方法可以介绍该方法。以下是三种最常见的方式：</code></p><ul><li>Offer a general overview by outlining the parameters of the work, for example the number of tests, the equipment /material/soft ware used and perhaps also the purpose of the investigation. This helps the reader to get a general idea of this section.<br><code>通过概述工作参数提供总体概述，例如测试次数、使用的设备/材料/软件以及可能的调查目的。这有助于读者对本节有一个大致的了解。</code></li><li>Provide background information about the materials or about the source of the materials/equipment.<br><code>提供有关材料或材料/设备来源的背景信息。</code></li></ul><p>Refer back to something in the previous section. Common options are restating the aim of the project or the problem you are hoping to address.<br><code>回到上一节的内容。常见的选择是重申项目的目标或你希望解决的问题。</code><br>If you start with a general overview or even a general paragraph about what was done and used, it can then be broken down to produce the details. However, if you begin with the details, you force the reader to put those details together to create a general picture of what you did and used. This is quite difficult for the reader to do and it is not his/her job; it is your job as a writer to arrange the information in an appropriate order so that it is easy for the reader to process it.<br><code>如果你从一个概述开始，甚至是一个关于做了什么和使用了什么的一般段落，那么它可以被分解以产生细节。然而，如果你开始的细节，你迫使读者把这些细节放在一起，创造一个你做了什么和使用的一般图片。这对读者来说是相当困难的，这不是他/她的工作;作为一个作家，你的工作是把信息安排在一个适当的顺序，以便读者容易处理它。</code><br>Furthermore, asking your reader to put details together to create a picture of what you did is risky, because each reader may create a slightly different picture of the process if they begin ‘bottom-up’ with the details, rather than ‘top-down’ with a general overview. When you write using ‘topdown’ strategies you are in control. If you begin with general statements about what was done/used (In all cases, Most sites), you and your reader share the same framework, so when you fill in the details you are creating the same picture of what was done/used in the mind of each individual reader. Remember: show your reader the wall before you begin to examine the bricks.<br><code>此外，要求读者把细节放在一起来描绘你所做的事情是有风险的，因为如果每个读者开始“自下而上”地描述细节，而不是“自上而下”地概述，他们可能会描绘出一个稍微不同的过程。当你使用“自上而下”的策略写作时，你就处于控制之中。如果你开始一般性的陈述做了什么/使用（在所有情况下，大多数网站），你和你的读者共享相同的框架，所以当你填写的细节，你是创建相同的图片做了什么/使用在每个读者的脑海中。记住：在你开始检查砖块之前，先让你的读者看到墙。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-14.png"><br><code>在第2句中，“地点选自伦敦盆地地区，该地区位于英格兰东南部，经常用于解释地下水演变。2 -4作者提供了背景信息，并通过参考以前的研究证明了地点选择的合理性。</code><br><em><strong>Why do I need to justify or give reasons for what I did? Isn’t it obvious?</strong></em><br><code>为什么我需要为我所做的事情辩护或给予理由？不是很明显吗？</code><br>Your reasons may be obvious to you, but they are not always obvious to your readers. If you fail to provide justification for what you did, then the reader may not accept the validity of your choices. They may wonder why you did things in a particular way, or why you used a particular procedure. This has a negative effect: if you don’t explain why you did things then readers cannot be expected to accept your methodology, and this will eventually affect the way they evaluate your whole paper.<br><code>你的理由对你来说可能很明显，但对你的读者来说并不总是很明显。如果你不能为你所做的事情提供理由，那么读者可能不会接受你的选择的有效性。他们可能想知道你为什么以一种特定的方式做事，或者为什么你使用一种特定的程序。这有一个负面影响：如果你不解释你为什么这么做，读者就不能接受你的方法论，这最终会影响他们对你整篇论文的评价。</code><br>Many writers believe that this section is just an impersonal description of what was done or used; in fact there is a strong persuasive and communicative element. We see this not only in language such as thoroughly or with care but also in the frequency of justification. In this description of your materials and methods, you need to communicate not only This is exactly what I did/used but also I had good reasons for those decisions. Justification enables the reader to trust the choices you made.<br><code>许多作者认为，这一部分只是对所做或使用的内容进行客观的描述;事实上，这一部分具有很强的说服力和沟通性。我们不仅在诸如彻底或谨慎的语言中看到这一点，而且在辩护的频率中也看到这一点。在对你的材料和方法的描述中，你不仅需要传达这正是我所做/使用的，而且我有很好的理由做出这些决定。合理性使读者相信你所做的选择。</code><br>Sometimes background information is given in the Present Simple to justify choices made. For example, you may have chosen a particular material because of its properties; if so, say what those properties are (Th is material is able to…). You may have chosen specific equipment or soft ware because of what it can do; if so, say what that is. In Sentence 2, we understand that the writer chose this geographical area because it had been previously validated as an appropriate location by other researchers.<br><code>有时候，一般现在时会给出背景信息来证明所做的选择。例如，你可能选择了一种特定的材料，因为它的属性;如果是这样，说这些属性是什么（这是材料能够...）。你可能已经选择了特定的设备或软件，因为它可以做什么;如果是这样，说这是什么。在句子2中，我们理解作者选择这个地理区域是因为它以前被其他研究人员证实为合适的位置。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-15.png"><br><em><strong>If I gave a general overview at the start of this subsection, why should I also give an overview of the procedure itself?</strong></em><br><code>如果我在本小节的开头给出了一个总体概述，那么我为什么还要给予程序本身的概述呢？</code><br>As you saw in Section 1.2.4, the beginning of a paragraph oft en signals the beginning of a new topic, and providing an introductory sentence is a reader-friendly technique. In addition, the overview in Sentence 3, like the one at the start of the subsection, enables the writer to move in a ‘topdown’ direction by creating a general framework into which the details can be easily slotted. Because the reader knows from the start how many samples were tested and what was done with them, both reader and writer share the same clear picture. These sentences oft en start with phrases like Most of the tests or In all cases (see the vocabulary list in Section 2.4.2).<br><code>正如你在1.2.4节中看到的，一个段落的开头往往意味着一个新主题的开始，提供一个介绍性的句子是一个读者友好的技巧。此外，第三句中的概述，就像小节开头的概述一样，通过创建一个可以轻松插入细节的总体框架，使作者能够沿着“自上而下”的方向前进。因为读者从一开始就知道测试了多少个样本以及对它们做了什么，所以读者和作者都有同样清晰的画面。这些句子通常以Most of the tests或In all cases（参见2.4.2节中的词汇表）这样的短语开头。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-16.png"><br><code>在第4句中，“将样品1-9收集在充分冲洗的25 ml棕色玻璃瓶中，将其填充至顶部，然后紧密密封以防止污染”。作者提供了所做和使用的细节，并表明采取了谨慎措施。</code><br><em><strong>How much detail do I need to provide?</strong></em><br><code>我需要提供多少细节？</code><br>If you’re not certain that all readers are familiar with the precise details of your methodology, it is better to give slightly too much information than too little. By the time you write up your research you will probably have repeated your experiments or simulations many times and so you are very familiar with the materials, quantities, equipment, soft ware, the sequence or steps in the procedure and the time taken for each step. Because of this familiarity, specific details (the size of the bottles in Sentence 5, for example) may seem obvious to you, but those details may not be obvious to every reader. If you want another researcher to be able to reproduce your work and obtain similar results, you should include every specification and detail.<br><code>如果你不能确定所有的读者都熟悉你的方法的精确细节，那么最好是给予稍微多一点的信息。当你写你的研究时，你可能已经重复了很多次你的实验或模拟，所以你非常熟悉材料，数量，设备，软件，程序中的顺序或步骤以及每个步骤所花费的时间。由于这种熟悉，具体的细节（例如句子5中瓶子的大小）对你来说可能是显而易见的，但这些细节对每个读者来说可能并不明显。如果你想让其他研究人员能够复制你的工作并获得类似的结果，你应该包括每一个规格和细节。</code><br>Note that in this sentence, the writer uses thoroughly, filled to the top and tightly to communicate to the reader that the work was carried out with care. Remember that your aim in writing the paper is not only to say what you did and found, but also to make sure that your reader accepts the conclusions at the end of your paper. In order to do this, the reader has to accept your results — but to accept your results s/he must first accept your methodology. For this reason, it is important to present yourself as a competent researcher who carries out procedures accurately and with care.<br><code>请注意，在这句话中，作者使用彻底，充满顶部和紧密地向读者传达工作是精心进行的。记住，你写论文的目的不仅仅是说出你做了什么和发现了什么，还要确保你的读者接受你论文结尾的结论。为了做到这一点，读者必须接受你的结果-但要接受你的结果，他/她必须首先接受你的方法。出于这个原因，重要的是要把自己作为一个称职的研究人员，准确和谨慎地执行程序。</code><br>Notice the use of 25 ml in Sentence 4. ml is the SI (Système International d’Unités) symbol for millilitre. Check the SI to make sure that you are using the correct symbol. Th ere is oft en a space between the quantity/number and the SI symbol; in addition, although SI symbols look like abbreviations they are not, and therefore should not be followed by a period.<br><code>注意句子4中使用的25 ml。ml是SI（国际单位制）的毫升符号。检查SI以确保您使用正确的符号。在数量/数字和SI符号之间通常有一个空格;此外，尽管SI符号看起来像缩写，但它们不是，因此后面不应该有句号。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-17.png"><br><code>在第5句中，"装满的瓶子被直接运到阅读大学的两个独立的实验室，在那里，它们被用标准的方法进行分析，这些方法被适当地简化以处理少量的水。“作者继续详细描述了所做的事情，使用的语言传达了谨慎的意思。</code><br>Can you see which words in Sentence 5 communicate to the reader that care was taken? Th e writer could just have written Th e filled bottles were shipped to two laboratories and analysed using standard methods miniaturised to handle small quantities of water, but including words like directly, separate and suitably communicates reliability.<br><code>你能看出句子5中的哪些词向读者传达了注意的意思吗？作者本可以这样写：装满水的瓶子被运到两个实验室，用标准的方法进行分析，这些方法被用来处理少量的水，但包括直接、分开和适当地传达可靠性等词语。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-18.png"><br>In Sentence 6 ‘Samples 10–18 were prepared in our laboratory using a revised version of the precipitation method established by the ISF Institute in Germany.6’ the writer describes what was done by referring to existing methods in the literature.<br><code>在第6句中，“样品10-18是在我们的实验室中使用德国ISF研究所建立的沉淀法的修订版制备的。6 "作者通过参考文献中的现有方法描述了所做的工作。</code><br><em><strong>Why should I refer to other research; why not just describe the method I used?</strong></em><br><code>为什么我应该参考其他研究;为什么不只是描述我使用的方法？</code><br>One reason is that it is unlikely that you created the entire method you used all by yourself. In many cases part of it will be taken from a method used or discovered by someone else and their method may be very well known, so if you give the research reference you do not need to give every detail. Giving the research reference, therefore, provides you with a shortcut. You will find vocabulary for this in Option 1 in Section 2.4.<br><code>一个原因是，您不太可能独自创建您使用的整个方法。在许多情况下，它的一部分将采取从一个方法使用或发现的其他人和他们的方法可能是非常有名的，所以如果你给予的研究参考，你不需要给予每一个细节。因此，提供研究参考为您提供了一条捷径。你可以在2.4节的选项1中找到相关词汇。</code><br>But if the reference is available in the literature, why does the writer need to give any details? Why can’t readers just go to the library, find the reference and read it themselves?<br><code>但是，如果参考文献中有，为什么作者需要给予任何细节？为什么读者不能去图书馆，找到参考资料，自己读呢？</code><br>In this case, the writer provides basic details of the method because some readers may not be familiar with it and it is not always appropriate to send readers to the library or Internet to look up a reference. It’s a matter of professional courtesy for writers to describe the procedures, tests, equipment or materials they used even when they are used in a way that is identical to the reference. Remember to use the Present Simple for this kind of background information (This method obtains) and to switch back to the Past Simple when you return to describing what you did.<br><code>在这种情况下，作者提供了该方法的基本细节，因为一些读者可能不熟悉它，而且送读者去图书馆或互联网查阅参考文献并不总是合适的。对于作家来说，描述他们使用的程序、测试、设备或材料是专业礼貌的问题，即使它们的使用方式与参考文献相同。记住对这类背景信息使用现在简式(此方法获得)，并在返回到描述您所做的事情时切换回过去简式。</code><br>Comparisons between your materials and methods and those of other researchers in the same field are a legitimate topic for the Methodology section. It is common to keep previous or current research procedures clearly in your readers’ view so that they can see how your work is different from other work in the area. Either your method is identical to others you mention (Option 1 in the vocabulary list in Section 2.4), or it is similar (Option 2 in the vocabulary list), or it is significantly different, in which case the differences between your materials/method and those of other researchers in the same field may even represent the actual contribution of your paper/thesis itself (Option 3).<br><code>将你的材料和方法与同一领域的其他研究人员的材料和方法进行比较是方法学部分的一个合理主题。通常，在读者的视野中清晰地保留以前或当前的研究程序，以便他们可以看到您的工作与该领域的其他工作有何不同。要么你的方法与你提到的其他方法相同（第2.4节词汇表中的选项1），要么相似（词汇表中的选项2），要么显著不同，在这种情况下，你的材料/方法与同一领域其他研究人员的材料/方法之间的差异甚至可能代表你的论文/论文本身的实际贡献（选项3）。</code><br>When you refer to the work of other researchers, be careful about the location of your reference notation in the sentence; you may accidentally credit someone with work they have not done — perhaps even with your own work! Remember that reference notations do not automatically go at the end of a sentence.<br><code>当你提到其他研究者的工作时，要注意你的参考符号在句子中的位置;你可能会意外地把别人没有做过的工作归功于他们-甚至可能是你自己的工作！请记住，引用符号不会自动放在句子的末尾。</code><br>It is sometimes appropriate or necessary to mention the effects of the procedures you used. However, it is not a good idea to discuss them or comment at this stage. If you go into too much detail you may leave yourself with nothing to write about in the Results section. Interestingly, it is common to provide further details about the methodology in the Results section. Sometimes the Methodology section just provides basic parameters and the method itself is detailed in the Results section in relation to the results obtained.<br><code>有时，提及您所使用的程序的效果是适当的或必要的。但是，在这个阶段讨论或评论它们并不是一个好主意。如果你进入太多的细节，你可能会让自己在结果部分没有什么可写的。有趣的是，通常在结果部分提供有关方法的更多细节。有时，方法部分仅提供基本参数，方法本身在结果部分中与所获得的结果相关。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-19.png"><br><code>在第7句中，“该方法通过加入BaCl 2·2 H2O获得沉淀;所得沉淀可以容易地洗涤和储存。”作者提供了有关该方法的更详细的信息，并表明它是一个很好的选择。</code><br>Justification is common throughout this section; as before, the aims are to answer possible criticisms or doubts about your choices, to assure the reader that your choices were made on the basis of good reasons and to give those reasons. We oft en see justification of significant choices and the reason for rejecting alternative options given in full. As mentioned earlier, this is because it is essential that your reader accepts the decisions you made about your methodology.<br><code>在这一节中，论证是常见的;和以前一样，目的是回答可能对你的选择的批评或怀疑，向读者保证你的选择是基于好的理由，并给予这些理由。我们经常看到重大选择的理由和拒绝替代选择的理由都是充分的。如前所述，这是因为读者必须接受你对方法论所做的决定。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-20.png"><br><code>在第8句中，“随后将样品运送至ISF，通过加速器质谱法（AMS）进行分析”。作者提供了该方法的更多细节。</code><br>It is interesting to note that, as mentioned earlier, you need to do more than just provide details of what you did and used; this is the only sentence in this section that gives details and nothing more — every other sentence has an additional function.<br><code>值得注意的是，正如前面提到的，你需要做的不仅仅是提供你所做和使用的细节;这是本节中唯一一个给出细节的句子，没有更多的-每一个其他的句子都有一个额外的功能。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-21.png"><br><code>在第9句中，“使用的所有管道都是不锈钢的，尽管有两个样品由于与塑料的短暂接触而面临氟氯化碳污染的风险，但样品之间的差异可以忽略不计。”作者提到了方法上可能存在的一个困难。</code><br><em><strong>Doesn’t this discuss a result of what was done?</strong></em><br><code>这不是讨论了所做的事情的结果吗？</code><br>No, it’s actually saying that the problems in the methodology didn’t affect the results. Sometimes you do need to mention results in this section, but only if the preliminary results were used to modify or develop the design of the main experiments/simulations.<br><code>不，它实际上是说，方法中的问题并没有影响结果。有时，您确实需要在本节中提及结果，但前提是初步结果用于修改或开发主要实验/模拟的设计。</code><br><em><strong>Why should I mention problems in the methodology? Won’t it make me look bad?</strong></em><br><code>为什么要说方法论上的问题？不会让我难堪吗？</code><br>In fact the opposite is true. In the first place, if you don’t mention the imperfections in your work, it may look as though you are not aware of them, which gives a very poor impression. So you look far more professional if you do mention them. If you ignore or try to hide imperfections (such as a data set which was too small, equipment or soft ware that was not ideal) and your readers notice them, they will begin to doubt your legitimacy as a researcher, which affects their acceptance of your results and conclusions.<br><code>事实上，情况正好相反。首先，如果你不提你工作中的不完美之处，它可能看起来好像你没有意识到它们，这给人一个很差的印象。如果你提到他们，你看起来会更专业。如果你忽略或试图隐藏不完美之处（例如数据集太小，设备或软件不理想），并且你的读者注意到了它们，他们将开始怀疑你作为研究人员的合法性，这会影响他们对你的结果和结论的接受。</code><br>Second, whenever you finish a piece of research, there is a good chance that you have learned enough from the problems encountered during the project to do it better next time. Should you delay writing it up while you repeat the work and improve your technique? What if you learn more this time too; should you delay again while you do it again? And again? If you do, you may never actually write it up. An acceptable option is to write up the research and acknowledge the problems or difficulties you encountered. In fact, it’s not only considered acceptable to mention them in this section, it’s much better to do it here rather than wait until the end. It isn’t considered appropriate to mention limitations or imperfections for the first time when you are discussing suggestions for future work in the Discussion/Conclusion.<br><code>其次，无论何时你完成了一项研究，你都很有可能从项目中遇到的问题中学到了足够的东西，以便下次做得更好。当你重复这项工作并提高你的技术时，你是否应该推迟写它？如果你这次也学到了更多的东西呢？你应该再次拖延吗？又一次？如果你这样做，你可能永远不会真正写出来。一个可以接受的选择是写研究报告，并承认你遇到的问题或困难。事实上，在这一节中提到它们不仅被认为是可以接受的，而且在这里提到它们比等到最后更好。在讨论/结论中讨论对未来工作的建议时，首次提及局限性或不完善之处被认为是不合适的。</code><br><em><strong>But how can I talk about problems in my work without looking like a failure?</strong></em><br>Use vocabulary that minimises the problem, minimises your responsibility, maximises the good aspects and suggests a solution. In the example above, the writer has acknowledged that there was a problem and then minimised its effects (variation among samples was negligible). This is a standard way of dealing with the need to talk about problems. You can find examples of the language needed to refer to problems and difficulties in a conventional, professional way in the vocabulary list in Section 2.4.<br><code>使用词汇，尽量减少问题，尽量减少你的责任，最大限度地发挥好的方面，并提出解决方案。在上面的例子中，作者承认存在问题，然后将其影响降至最低（样本之间的差异可以忽略不计）。这是一个标准的方式来处理需要谈论的问题。你可以在2.4节的词汇表中找到一些例子，这些例子是用常规的、专业的方式来说明问题和困难所需要的语言。</code></p><h5 id="2-3-3-The-model"><a href="#2-3-3-The-model" class="headerlink" title="2.3.3 The model"></a>2.3.3 The model</h5><p><code>2.3.3模型</code><br>Here are the sentence descriptions we have collected:<br><code>以下是我们收集的句子描述：</code><br>In Sentence 1 the writer offers a general overview of the subsection.<br><code>在第1句中，作者对该小节作了一个概述。</code><br>In Sentence 2 the writer provides background information and justification.<br><code>在第2句中，作者提供了背景信息和理由。</code><br>In Sentence 3 the writer provides an overview of the procedure/ method itself.<br><code>在句子3中，作者提供了程序/方法本身的概述。</code><br>In Sentence 4 the writer provides details about what was done and used and shows that care was taken.<br><code>在第4句中，作者提供了所做和使用的细节，并表明采取了谨慎措施。</code><br>In Sentence 5 the writer continues to describe what was done in detail, using language which communicates that care was taken.<br><code>在第5句中，作者继续详细描述了所做的事情，使用的语言传达了谨慎的意思。</code><br>In Sentence 6 the writer describes what was done by referring to existing methods in the literature.<br><code>在第6句中，作者通过参考文献中现有的方法描述了所做的工作。· In Sentence 7 the writer provides more detailed information about the method and shows it to have been a good choice. </code>在第7句中，作者提供了关于该方法的更详细的信息，并表明它是一个很好的选择。<code>In Sentence 8 the writer provides more details of the method.</code>在第8句中，作者提供了该方法的更多细节。<code>In Sentence 9 the writer mentions a possible difficulty in the methodology.</code>在第9句中，作者提到了方法论上可能存在的困难。<code>We can streamline these so that our model has FOUR basic components. Unlike the Introduction model, in which all the items of each component are likely to be used, this is a ‘menu’ from which you select items appropriate to your research topic and the journal you are submitting to. If you constructed the equipment yourself you won’t need to ‘give the source of’ the equipment used in component 1. If there were no problems, you won’t need the fourth component at all.</code>我们可以简化这些，使我们的模型有四个基本组成部分。与介绍模型不同，在介绍模型中，每个组件的所有项目都可能被使用，这是一个“菜单”，您可以从中选择适合您的研究主题和您提交的期刊的项目。如果你自己建造了设备，你就不需要“给予”组件1中使用的设备的来源。如果没有问题，你根本不需要第四个组件。<code>![](https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-22.png) 2.3.4 Testing the model</code>2.3.4测试模型<code>The next step is to look at the way this model works in a real Materials/ Methods section (remember it may not be called Materials and Methods) and in the target articles you have selected. Here are some full-length Methodology sections from real research articles. Read them through, and mark the model components (1, 2, 3 or 4) wherever you think you see them. For example, if you think the first sentence corresponds to number 1 in the model, write 1 next to it, etc.</code>下一步是在真实的材料/方法部分（记住它可能不叫材料和方法）和你选择的目标文章中看看这个模型是如何工作的。这里有一些来自真实的研究文章的完整的方法论部分。通读一遍，并在您认为看到的任何地方标记模型组件（1、2、3或4）。例如，如果你认为第一个句子对应于模型中的数字1，那么在它旁边写上1，等等。`</p><blockquote><p>Effects of H2O on structure of acid-catalysed SiO2 sol-gel films<br><code>水对酸催化SiO2溶胶-凝胶薄膜结构的影响</code><br>Experimental procedure<br><code>实验程序</code><br>Equal volumes of tetraethylorthosilicate (TEOS) and ethanol were mixed and stirred vigorously for 10 min at room temperature. Th en 0.1 M HCl was gradually added to the solutions, until a water to TEOS molar ratio of R = 2 was attained. Additional deionised water was added to give solutions with R = 3, 4 and 5, so that for all solutions the molecular ration TEOS:HCl was maintained, as summarised in Table 1. Th e solutions were placed in the refluxing bath immediately aft er mixing, and the temperature of the bath was increased to 70°C in 15 min, while stirring, and kept there for 2 h. Th e solutions were then aged for 24 h at room temperature, before being diluted with an equal volume of EtOH and stirred for 10 min, to give the solution used for spin coating. All the chemicals were obtained from Aldrich Chemicals Ltd.<br><code>将等体积的原硅酸四乙酯（TEOS）和乙醇混合并在室温下剧烈搅拌10分钟。然后将0.1M HCl逐渐加入到溶液中，直到水与TEOS的摩尔比达到R = 2。将另外的去离子水加入到给予溶液中，得到R = 3、4和5的溶液，使得对于所有溶液，保持TEOS：HCl的分子比，如表1中总结的。在混合后立即将溶液置于水浴中，在搅拌的同时在15分钟内将浴的温度升高至70°C，并在此保持2小时。然后将溶液在室温下老化24小时，然后用等体积的EtOH稀释并搅拌10分钟，得到给予用于旋涂的溶液。所有化学品均购自Aldrich Chemicals Ltd.。</code><br>The sols were dispensed on p-type, 75 mm diameter silicon wafers, through a 0.1 µm filter (PTFE Whatman, obtained from BDH Merk Ltd), and thereafter the substrate was spun at 2000 rpm for 15 s. Th e coated substrate was baked at 100°C for 5 min, and then cleaved into 10 pieces. Each piece was baked in air at a different temperature, in the range from 100 to 1000°C, for 30 min. Th e samples were kept in covered petri dishes for a few days in room conditions before the experiments were continued; this allows the completion of surface hydroxylation, and gave reproducible ellipsometer results when water is used as an adsorbate.<br><code>通过0.1 μm过滤器（PTFE Whatman，得自BDH Merk Ltd）将溶胶分配在p型、75 mm直径的硅晶片上，然后将基底以2000 rpm旋转15 s。将涂覆的基底在100°C下烘烤5分钟，然后切成10片。将每片样品在空气中在不同温度（100至1000°C）下烘烤30 min。在继续实验之前，将样品在室内条件下在带盖培养皿中保存数天;这允许完成表面羟基化，并在水用作吸附质时给出可重现的椭圆偏振仪结果。</code><br>Th e thickness and refractive index of the samples were measured using a Rudolph AutoEl III ellipsometer, with an operating wavelength of 633 nm, and precisions of about ±0.002 and ±3 Å in index and thickness, respectively. For microporous films, the measured index is strongly dependent on relative humidity, because of condensation of water in the pores. By measuring the dependence of index on humidity, information about porosity can be obtained. We have extended this technique to the use of diff erent adsorbate species, in order to probe pore sizes [3]; this, for the sake of brevity, we call molecular probe ellipsometry. In this technique, the film is placed in a sealed chamber on the sample stage of the ellipsometer; first dry N2 gas is passed through the chamber to empty the pores of any condensed adsorbate, and then N2 having been bubbled through the liquid adsorbate is passed over the sample to fill the pores; in each case the refractive index is measured. By assuming that all the accessible pores in dry and saturated atmospheres are completely empty or filled with adsorbate, respectively, the pore volume and index of the solid skeleton can be determined by an extension of the Lorentz-Lorenz relation [8] where nf, ns and np are the refractive indices of the film, solid skeleton and pores, respectively, and vp is the volume fraction porosity. Measurement of nf for both the dry and saturated films allows both vp and ns to be determined with the assumption that np has the same value as that of the bulk adsorbate in the saturated case, and of air (np = 1) in the dry case.<br><code>样品的厚度和折射率使用Rudolph AutoEl III椭偏仪测量，工作波长为633 nm，折射率和厚度的精度分别为约±0.002和±3 μ m。对于微孔膜，测量的折射率强烈依赖于相对湿度，因为水在孔中冷凝。通过测量折射率对湿度的依赖性，可以获得关于孔隙度的信息。我们已经将这种技术扩展到使用不同的吸附物种类，以探测孔径[3];为了简洁起见，我们称之为分子探针椭圆偏振法。在该技术中，将膜放置在椭圆偏振仪的样品台上的密封室中;首先使干燥的N2气体通过室以清空任何冷凝的吸附物的孔，然后使已经鼓泡通过液体吸附物的N2通过样品以填充孔;在每种情况下测量折射率。通过假设干燥和饱和气氛中的所有可接近孔分别完全空或充满吸附物，固体骨架的孔体积和指数可以通过洛伦兹-洛伦兹关系式[8]的扩展来确定，其中nf，ns和np分别是膜，固体骨架和孔的折射率，vp是体积分数孔隙率。干燥和饱和膜的nf测量允许vp和ns两者在假设np具有与饱和情况下的本体吸附物和干燥情况下的空气（np = 1）的值相同的值的情况下被确定。</code><br>In order to empty the pores, an initial high fl ow rate of N2 was used for a few minutes and the rate was then reduced to 1000 sccm (standard c.c per minute) for 15 min. the fl ow rate was kept at 100 sccm for 15 min to fill the pores. The low flow rate in this case reduces the likelihood of cooling of the sample surface, which could cause condensation on the external film surface. Comparison of the measured film thickness for wet and dry atmospheres indicated that this did not occur. The temperature inside the chamber was monitored by a thermocouple to ensure that there was no drift or alteration due to gas fl ow. In each case, the measurement was recorded once repeatable readings were obtained. Th e adsorbates used are listed in Table 2. Their average diameters were estimated using a combination of bond length data [9] and Van der Waals atomic radii [10]. All were obtained from Aldrich Chemical Ltd, except C24H44O8 obtained from Fluka Chemie AG.<br><code>为了清空孔，使用初始高流速的N2持续几分钟，然后将流速降低至1000 sccm（标准c.c./分钟）持续15分钟。将流速保持在100 sccm持续15分钟以填充孔。在这种情况下，低流速降低了样品表面冷却的可能性，这可能导致外膜表面上的冷凝。比较潮湿和干燥气氛下测得的膜厚度表明，这并没有发生。通过热电偶监测腔室内的温度，以确保不存在由于气体流动引起的漂移或改变。在每种情况下，一旦获得可重复读数，就记录测量值。所用吸附物列于表2中。使用键长数据[9]和货车范德华原子半径[10]的组合来估计它们的平均直径。除了C24 H44 O 8购自Fluka Chemie AG外，所有均购自Aldrich Chemical Ltd。</code><br>The optical quality of the films was first studied qualitatively by visual examination, and by optical microscopy. The homogeneity of the films was then investigated quantitatively by measuring the intensity of scattered light resulting from oblique reflection of a laser beam from the film-coated silicon substrate. A helium-neon laser beam, having a wavelength of 633 nm, was directed onto the sample, through a chopping wheel, at an angle 59° from the normal. Th e specularly reflected beam was absorbed onto a black card, and the scattered light was collected at normal incidence to the sample using a ×10 microscope objective, and measured using a silicon photodiode and a lock-in amplifier. Th e position of lens and angle of incidence were fixed during measurements.<br><code>首先通过目视检查和光学显微镜定性研究膜的光学质量。然后，通过测量激光束从膜涂覆的硅衬底的斜反射产生的散射光的强度，定量地研究膜的均匀性。将波长为633 nm的氦氖激光束通过斩波轮以与法线成59°的角度引导到样品上。镜面反射的光束被吸收到一张黑卡上，散射光以垂直入射到样品的方式使用×10显微镜物镜收集，并使用硅光电二极管和锁定放大器进行测量。在测量过程中固定透镜的位置和入射角。</code><br>The film stress, σf, can be determined by measuring the resulting substrate curvature [11], according to Stoney’s formula:<br><code>根据Stoney公式，可以通过测量所得衬底曲率[11]来确定薄膜应力σf：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-23.png"><br>where rs and rf are the radii of curvature of the bare substrate and substrate with film, respectively; Es, ts and vs are the Young’s modulus, thickness and Poisson’s ratio of the silicon substrate, respectively, and tf is the thickness of the film. Tensile stresses are positive and compressive stresses negative; thus, a positive radius of curvature denotes a convex film surface. Entire 75 mm diameter wafers were used, and curvature was measured from plots of surface profile along 30 mm lines over the central part of the film surface using a Dektak IIA auto-levelling profilometer. To reduce inaccuracy caused by lack of axial symmetry in the wafer curvature, two scans were made, in orthogonal directions, for each measurement, and the inverse radii thus obtained were averaged. Care was taken not to use wafers which had a substantially asymmetric curvature before deposition. Wafer thicknesses, measured with a micrometer, were 390 ± 3 µm. Final film thicknesses were measured by ellipsometry and checked by patterned etching and profilometry, and interim thicknesses were estimated by interpolation. Equivalent single-layer thickness measurements indicate that the assumption that final thickness is proportional to number of layers is sufficiently accurate. For Es/ (1 − vs), the value 180 GPa was used [11].<br><code>其中rs和rf分别是裸衬底和具有膜的衬底的曲率半径; Es、ts和vs分别是硅衬底的杨氏模量、厚度和泊松比，并且tf是膜的厚度。拉伸应力为正，压缩应力为负;因此，正的曲率半径表示凸的膜表面。使用整个75 mm直径的晶片，并使用Dektak IIA自动调平轮廓仪从膜表面中心部分上沿沿着30 mm线的表面轮廓图测量曲率。为了减少由于晶片曲率缺乏轴对称性而引起的不准确性，对于每次测量，在正交方向上进行两次扫描，并且将由此获得的倒数半径平均。注意不要使用在沉积之前具有基本上不对称曲率的晶片。用千分尺测量的晶片厚度为390 ± 3 µm。最终的膜厚度测量椭圆偏振法和检查图案化蚀刻和轮廓，和临时厚度估计插值。等效单层厚度测量表明，最终厚度与层数成比例的假设是足够准确的。对于Es/（1-vs），使用了180 GPa的值[11]。</code><br>In order to give an indication of the effect of water content on stress, 10 layers were deposited for each R value, using 10 s rapid thermal annealing at 1000°C in all cases.<br><code>为了给予水含量对应力的影响的指示，对于每个R值沉积10层，在所有情况下在1000 ℃下使用10 s快速热退火。</code></p></blockquote><blockquote><p>Infrared imaging of defects heated by a sonic pulse<br><code>声脉冲加热下缺陷的红外成像</code><br>ii) Experiment<br><code>ii）实验</code><br>Our experimental setup is shown in Fig. 1. The source of the sonic excitation is a Branson, Model 900 MA 20 kHz ultrasonic welding generator, with a Model GK-5 hand-held gun. Th e source has a maximum power of 1 kW, and is triggered to provide a short (typically 50–200 ms duration) output pulse to the gun. The gun contains a piezoelectric transducer that couples to the specimen through the 1.3-cm-diam tip of a steel horn. In the laboratory setup, as can be seen in Fig. 1, we use a mechanical fixture to hold the sonic horn firmly against the sample surface. Th is setup uses a machine slide to provide reproducible alignment of the horn. Typically, a piece of soft Cu sheet is placed between the tip of the horn and the specimen to provide good sound transmission. Th e location of the source on the sample is chosen primarily for convenience of geometrical alignment, and since it has minimal effect on the resulting sonic IR images, typically is not changed during the course of the inspection. Sound waves at frequencies of 20 kHz in metals such as aluminium or steel have wavelengths on the order of tens of centimetres, and propagate with appreciable amplitude over distances much longer than a wavelength. For typical complex-shaped industrial parts (see, for example, the aluminium automotive part shown in Fig. 1), reflections from various boundaries of the specimen introduce countless conversions among the vibrational modes, leading to a very complicated pattern of sound within the specimen during the time that the pulse is applied. Since the speed of sound in solids is typically on the order of a few km/s, this sound field completely insonifies the regions under inspection during the time that the excitation pulse is applied. If a subsurface interface is present, say a fatigue crack in a metal, or a delamination in a composite structure, the opposing surfaces at the interface will be caused to move by the various sound modes present there. Th e complexity of the sound is such that relative motion of these surfaces will ordinarily have components both in the plane of the crack and normal to it. Thus, the surfaces will ‘rub’ and ‘slap’ against one another, with a concomitant local dissipation of mechanical energy. Th is energy dissipation causes a temperature rise, which propagates in the material through thermal diffusion. We monitor this dissipation through its effect on the surface temperature distribution. Th e resolution of the resulting images depends on the depth of the dissipative source as well as on the time at which the imaging is carried out.<br><code>我们的实验装置如图1所示。声激励源是Branson 900 MA型20 kHz超声波焊接发生器，带有GK-5型手持焊枪。该源的最大功率为1 kW，并且被触发以向枪提供短（通常为50-200 ms持续时间）输出脉冲。该枪包含一个压电传感器，通过1.3厘米直径的钢喇叭尖端耦合到试样。在实验室设置中，如图1所示，我们使用机械夹具将声波喇叭牢固地固定在样品表面。此设置使用机器滑块来提供喇叭的可再现对准。通常情况下，一块软铜片放置在喇叭的尖端和样品之间，以提供良好的声音传输。源在样品上的位置主要是为了便于几何对准而选择的，并且由于其对所得到的声波IR图像具有最小的影响，因此通常在检查过程中不改变。在诸如铝或钢的金属中，频率为20 kHz的声波具有几十厘米量级的波长，并且以明显的振幅在比波长长得多的距离上传播。对于典型的形状复杂的工业部件（例如，见图1所示的铝汽车部件），来自试样的各种边界的反射在振动模式之间引入了无数的转换，导致在施加脉冲的时间内试样内的声音非常复杂的模式。由于固体中的声速通常在几km/s的量级上，因此在施加激励脉冲的时间期间，该声场完全声穿透被检查的区域。如果存在表面下界面，比如金属中的疲劳裂纹，或者复合结构中的分层，则界面处的相对表面将被存在于那里的各种声音模式引起移动。声音的复杂性使得这些表面的相对运动通常在裂纹平面内和垂直于裂纹平面的方向上都有分量。因此，表面将相互“摩擦”和“拍打”，伴随着机械能的局部耗散。这种能量耗散导致温度上升，其通过热扩散在材料中传播。我们通过其对表面温度分布的影响来监测这种耗散。所得到的图像的分辨率取决于耗散源的深度以及进行成像的时间。</code><br>The IR camera that we used in the setup that is shown in Fig. 1 is a Raytheon Radiance HS that contains a 256×256 InSb focal plane array, and operates in the 3–5 µm spectral region. It is sensitive (with a 1 ms integration time) to surface temperature changes of ~0.03°C, and can be operated at full frame rates up to 140 Hz with that sensitivity. We have also observed the effects reported here with a considerably less expensive, uncooled, microbolometer focal plane array camera, operating in the long wavelength (7–10 µm) of the IR.<br><code>我们在图1所示的设置中使用的红外相机是Raytheon Radiance HS，它包含256×256 InSb焦平面阵列，工作在3-5 µm光谱区域。它对约0.03°C的表面温度变化敏感（积分时间为1 ms），并且可以在高达140 Hz的全帧速率下工作。我们还观察到了这里报告的效果，使用了一种相当便宜的非制冷微测辐射热计焦平面阵列相机，在IR的长波长（7-10 µm）下工作。</code></p></blockquote><blockquote><p>The height of biomolecules measured with the atomic force microscope depends on electrostatic interactions<br><code>用原子力显微镜测量的生物分子的高度取决于静电相互作用</code><br><em><strong>MATERIALS AND METHODS</strong></em><br><code>材料和方法</code><br><em><strong>Biological samples</strong></em><br><code>生物样品</code><br>Aquaporin-1 (AQP1) from human erythrocyte solubilized in octyl-f3-glucopyranoside was reconstituted in the presence of Escherichia coli phospholipids to form two-dimensional (2D) crystalline sheets (Walz et al., 1994). Th e 2D crystals were prepared at a concentration of -0.5 mg protein/ml and 0.25 mg/ml lipid in 0.25 M NaCl, 20 mM MgCl2, 20 mM 2-(N-morpholino) ethanesulfonic acid (MES) (pH 6).<br><code>在大肠杆菌磷脂的存在下重构溶解在辛基-β-吡喃葡萄糖苷中的来自人红细胞的水通道蛋白-1（AQP 1）以形成二维（2D）结晶片（Walz等人，1994年）。在0.25 M NaCl、20 mM MgCl 2、20 mM 2-（N-吗啉代）乙磺酸（MES）（pH 6）中以约0.5 mg蛋白质/ml和0.25 mg/ml脂质的浓度制备2D晶体。</code><br>Hexagonally packed intermediate (HPI) layer from Deinococcus radiodurans, a kind gift of Dr. W. Baumeister, was extracted from whole cells (strain SARK) with lithium dodecyl sulfate, and purified on a Percoll density gradient (Baumeister et al., 1982). A stock solution (1 mg/ml protein) was stored in distilled water at 4°C.<br><code>耐辐射奇球菌的受阻包装中间层（HPI），是W。Baumeister，用十二烷基硫酸锂从全细胞（菌株SARK）中提取，并在Percoll密度梯度上纯化（Baumeister等人，1982年）。将储备溶液（lmg/ml蛋白质）在4°C下储存在蒸馏水中。</code><br>Purple membranes of Halobacterium salinarium strain ET1001 were isolated as described by Oesterhelt and Stoeckenius (1974). Th e membranes were frozen and stored at −70°C. Aft er thawing, stock solutions (10 mg protein/ml) were kept in distilled water at 4°C.<br><code>如Oesterhelt和Stoeckenius（1974）所述分离盐生盐杆菌菌株ET 1001的紫色膜。将膜冷冻并储存在−70°C下。解冻后，将储备溶液（10 mg蛋白质/ml）保持在4°C的蒸馏水中。</code><br>Porin OmpF trimers from E. coli strain BZ 1 10/PMY222 (Hoenger et al., 1993) solubilized in octyl-polyoxyethylene were mixed with solubilised dimyristoyl phosphatidylcholine (99% purity; Sigma Chemical Co., St. Louis, MO) at a lipid-to-protein ratio (w/w) of 0.2 and a protein concentration of 1 mg/ml. Th e mixture was reconstituted as previously described (Hoenger et al., 1993) in a temperature-controlled dialysis device (Jap et al., 1992). Th e dialysis buff er was 20 mM HEPES, pH 7.4, 100 mM NaCl, 20 mM MgCl2, 0.2 mM dithiothreitol, 3 mM azide.<br><code>孔蛋白OmpF三聚体来自E.大肠杆菌菌株BZ 110/PMY 222（Hoenger等人，1993）与溶解的二肉豆蔻酰磷脂酰胆碱（99%纯度; Sigma Chemical Co.，St. Louis，MO），脂质与蛋白质之比（w/w）为0.2，蛋白质浓度为lmg/ml。如前所述重构混合物（Hoenger等人，1993）在温控透析装置中（Jap等，1992年）。透析缓冲液为20 mM HEPES，pH 7.4，100 mM NaCl，20 mM MgCl 2，0.2 mM二硫苏糖醇，3 mM叠氮化物。</code><br>1,2-Dipalmitoyl-phosphatidylethanolamine (DPPE) from Sigma was solubilized in chloroform:hexane (1:1) to a concentration of 1 mg/ml. Th e resulting solution was diluted in buff er solution (150 mM KCl, 10 mM Tris, pH 8.4) to a concentration of 100 µg/ml.<br><code>将来自Sigma的1，2-二棕榈酰-磷脂酰乙醇胺（DPPE）溶解在氯仿：己烷（1：1）中至浓度为1 mg/ml。将所得溶液在缓冲液（150 mM KCl，10 mM Tris，pH 8.4）中稀释至100 µg/ml的浓度。</code><br><em><strong>Layered crystals</strong></em><br><code>层状晶体</code><br>MoTe2, a layered crystal of the family of transition metal dichalcogenides (Wilson and Yoff e, 1969), was employed to calibrate the piezo scanner of the AFM. It was prepared by chemical vapor transport (CVT), with chlorine or bromine as carrier gases in a temperature gradient of 100°C across the quartz ampule (Jungblut et al., 1992), and was a kind gift of Y. Tomm.<br><code>MoTe 2是过渡金属二硫属化物族的层状晶体（Wilson和Yoff e，1969），用于校准AFM的压电扫描器。其通过化学气相传输（CVT）制备，其中氯或溴作为载气，在100°C的温度梯度下穿过石英安瓿（Jungblut等人，1992），是Y.汤姆</code><br>Muscovite mica (Mica New York Corp., New York) was used as the solid support for all samples. Mica minerals are characterized by their layered crystal structure, and show a perfect basal cleavage that provides atomically fl at surfaces over several hundreds of square microns. Their hydrophilicity and relative chemical inertness (Bailey, 1984) make them suitable for the adsorption of biological macromolecules.<br><code>白云母（云母纽约公司，纽约）用作所有样品的固体载体。云母矿物的特征在于它们的层状晶体结构，并且显示出完美的基底解理，其提供了超过数百平方微米的原子级平坦表面。它们的亲水性和相对化学惰性（Bailey，1984）使它们适合于生物大分子的吸附。</code><br><em><strong>Atomic force microscopy</strong></em><br><code>原子力显微镜</code><br>A commercial AFM (Nanoscope III; Digital Instruments, Santa Barbara, CA), equipped with a 120-µm scanner (j-scanner) and a liquid cell, was used. Before use, the liquid cell was cleaned with normal dish cleaner, gently rinsed with ultrapure water, sonicated in ethanol (50 kHz), and sonicated in ultrapure water (50 kHz). Mica was punched to a diameter of −5 mm and glued with waterinsoluble epoxy glue (Araldit; Ciba Geigy AG, Basel, Switzerland) onto a Teflon disc. Its diameter of 25 mm was slightly larger than the diameter of the supporting steel disc. Th e steel disc was required to magnetically mount the sample on to the piezoelectric scanner. Imaging was performed in the error signal mode, acquiring the deflection and height signal simultaneously. The deflection signal was minimized by optimizing gains and scan speed. Th e height images presented were recorded in the contact mode. Th e scan speed was roughly linear to the scan size, at 4–8 lines/s for lower magnifications (frame size 1–25 µm). Th e applied force was corrected manually to compensate for thermal drift . To achieve reproducible forces, cantilevers were selected from a restricted area of one wafer. Th e dimensions of one tip were measured in a scanning electron microscope to calculate the mechanical properties of the cantilever (Butt et al., 1993). Th e 120-µm-long cantilevers purchased from Olympus Ltd. (Tokyo, Japan) had a force constant of k = 0.1 N/m, and the 200-µm-long cantilevers purchased from Digital Instruments had a force constant of 0.15 N/m. All cantilevers used had oxide-sharpened Si3N4 tips.<br><code>使用配备有120 μm扫描仪（j-扫描仪）和液体池的商业AFM（Nanoscope III; Digital Instruments，圣巴巴拉，CA）。使用前，用普通洗碟剂清洁液体池，用超纯水轻轻冲洗，在乙醇（50 kHz）中超声处理，并在超纯水（50 kHz）中超声处理。将云母冲压成-5 mm的直径，并用水不溶性环氧胶（Araldit; Ciba Geigy AG，巴塞尔，瑞士）胶合到特氟隆盘上。其直径为25 mm，略大于支撑钢盘的直径。需要钢盘将样品磁性安装到压电扫描仪上。在误差信号模式下进行成像，同时采集偏转和高度信号。通过优化增益和扫描速度使偏转信号最小化。在接触模式下记录所呈现的高度图像。扫描速度与扫描尺寸大致呈线性关系，对于较低放大倍数（帧尺寸1-25 µm），扫描速度为4-8线/秒。手动校正施加的力以补偿热漂移。为了实现可再现的力，从一个晶片的受限区域选择杠杆。在扫描电子显微镜中测量一个尖端的尺寸以计算悬臂的机械性能（Butt等人，1993年）。购自Olympus Ltd.（Tokyo，Japan）的120 μ m长的杠杆具有k = 0.1 N/m的力常数，购自Digital Instruments的200 μ m长的杠杆具有0.15 N/m的力常数。所有使用的杠杆都有氧化物锐化的Si 3 N4尖端。</code><br><em><strong>Sample preparation</strong></em><br><code>样品制备</code><br>To minimize contamination of surfaces during exposure to ambient air, sample supports were prepared immediately before use. All buffers were made with ultrapure water (−18 MDcm−1; Branstead, Boston, MA). Th is water contains fewer hydrocarbons than conventional bidistilled water and fewer macroscopic contaminants, both of which can influence the imaging process. Chemicals were grade p.a. and purchased from Sigma Chemie AG (Buchs, Switzerland). Th e buffers used were Tris-(hydroxymethyl)-aminomethane (from pH 10.2 to pH 7.2), MES (from pH 6.5 to pH 5.5), and citric acid (from pH 5.4 to pH 3.0). Macromolecular samples were checked before use by conventional negative stain electron microscopy (Bremer et al., 1992) and/or by sodium dodecyl sulfate-gel electrophoresis.<br><code>为了最大限度地减少暴露于环境空气期间的表面污染，在使用前立即制备样品载体。所有缓冲液均用超纯水（−18 MDcm−1; Branstead，Boston，MA）制备。这种水比传统的重蒸馏水含有更少的碳氢化合物和更少的宏观污染物，这两者都可以影响成像过程。化学品为p.a.级。购自Sigma Chemie AG（布克斯，瑞士）。使用的缓冲液为三-（羟甲基）-氨基甲烷（pH 10.2至pH 7.2）、MES（pH 6.5至pH 5.5）和柠檬酸（pH 5.4至pH 3.0）。在使用前通过常规负染色电子显微镜检查大分子样品（Bremer等人，1992）和/或通过十二烷基硫酸钠凝胶电泳。</code><br>The samples were diluted to a concentration of 5–10 µg/ml in buff er solution (pH 8.2, 20 mM Tris-HCl, 2100 mM; monovalent electrolyte; except for DPPE, which was not further diluted) before adsorption to freshly cleaved mica. Aft er an adsorption time of 10–60 min, the samples were gently washed with the measuring buff er to remove weakly attached membranes. Th is allowed height measurements at low electrolyte concentrations, at which samples adsorb sparsely to mica (Muller et al., 1997a and 1997b). Experiments requiring constant pH were performed at pH 8.2. Th e isoelectric points of bacteriorhodopsin, AQP1, DPPE, and OmpF are 5.2 (Ross et al., 1989), 6.95 (calculated), −10 (Tatulian, 1993), and 4.64 (calculated), respectively. Th us, at this pH, all samples had a net negative charge, except for DPPE, which had a net positive charge.<br><code>将样品在缓冲溶液（pH 8.2，20 mM Tris-HCl，2100 mM;单价电解质;除DPPE外，其未进一步稀释）中稀释至5-10 μg/ml的浓度，然后吸附至新鲜切割的云母。在10-60分钟的吸附时间之后，用测量缓冲液轻轻洗涤样品以除去弱附着的膜。这是允许在低电解质浓度下进行高度测量的，在该浓度下，样品稀疏地吸附到云母上（Muller等人，1997年a和1997年b）。需要恒定pH的实验在pH 8.2下进行。细菌视紫红质、AQP 1、DPPE和OmpF的等电点为5.2（Ross等人，1989）、6.95（计算）、-10（Tatulian，1993）和4.64（计算）。因此，在该pH下，所有样品都具有净负电荷，除了DPPE，其具有净正电荷。</code></p></blockquote><p>Now do the same in your target articles. We hope you obtain good confirmation of the model and can now answer the questions in Section 2.1:<br><code>现在在你的目标文章中做同样的事情。我们希望您能够对模型进行充分的确认，并能够回答第2.1节中的问题：</code></p><ul><li>How do I start this section? What type of sentence should I begin with?<br><code>如何开始这一部分？我开始应该用什么类型的句子？</code></li><li>What type of information should be in this section, and in what order?<br><code>本节应包含哪些类型的信息，顺序如何？</code></li><li>How do I end this section?<br><code>我如何结束这一节？</code></li></ul><h3 id="2-4-Vocabulary"><a href="#2-4-Vocabulary" class="headerlink" title="2.4 Vocabulary"></a>2.4 Vocabulary</h3><p><code>2.4词汇</code><br>In order to complete the information you need to write this section of your paper you now need to find appropriate vocabulary for each part of the model. Th e vocabulary in this section is taken from over 600 research articles in different fields, all of which were written by native speakers and published in science journals. Only words/phrases which appear frequently have been included; this means that the vocabulary lists contain words and phrases which are considered normal and acceptable by both writers and editors.<br><code>为了完成你需要写这部分论文的信息，你现在需要为模型的每个部分找到合适的词汇。本节中的词汇来自不同领域的600多篇研究文章，所有这些文章都是由母语人士撰写并发表在科学期刊上的。只有经常出现的单词/短语被包括在内;这意味着词汇表包含作者和编辑认为正常和可接受的单词和短语。</code><br>In the next section we will look at vocabulary for the following seven areas of the model:<br><code>在下一节中，我们将研究模型的以下七个领域的词汇表：</code><br><em><strong>1. PROVIDE A GENERAL INTRODUCTION AND OVERVIEW OF THE MATERIALS/METHODS and GIVE THE SOURCE OF MATERIALS/ EQUIPMENT USED</strong></em><br><code>1.提供材料/方法的一般介绍和概述，并给予所用材料/设备的来源</code><br>This includes phrases such as In this study, most of the samples were tested using a… as well as verbs such as were supplied by. A good list of commonly-used words and expressions will encourage you to include this in your first sentences.<br><code>这包括短语，如在这项研究中，大多数样本使用...以及动词，如由提供。一个好的常用词和表达的列表会鼓励你在你的第一句话中包括这一点。</code><br><em><strong>2. SUPPLY ESSENTIAL BACKGROUND INFORMATION</strong></em><br><code>2.基本背景资料</code><br>This list provides words and phrases used to describe instruments, equipment or locations, and includes items such as parallel to and equidistant. They are essential because the reader needs them in order to visualise or recreate your work.<br><code>该列表提供了用于描述仪器、设备或位置的单词和短语，并包括平行和等距等项目。它们是必不可少的，因为读者需要它们来想象或重新创造你的作品。</code><br><em><strong>3. PROVIDE SPECIFIC AND PRECISE DETAILS ABOUT MATERIALS AND METHODS (i.e. quantities, temperatures, duration, sequence, conditions, locations, sizes)</strong></em><br><code>3.提供有关材料和方法的具体和精确的说明（即数量、温度、持续时间、顺序、条件、位置、尺寸）</code><br>This includes verbs which specifically describe what you did/used. Instead of writing only was done or was used, a more specific verb such as optimise or extract can save you time by explaining exactly what was ‘done’.<br><code>这包括具体描述你所做/所用的动词。而不是只写被完成或被使用，一个更具体的动词，如优化或提取可以通过准确地解释什么是“做”来保存你的时间。</code><br>4.JUSTIFY CHOICES MADE<br><code>4.做出的合理选择</code><br>This includes phrases that introduce the reasons for the choices you made, such as in order to. It also includes a list of verbs that specify the advantages of the choices you made, like enable and facilitate.<br><code>这包括介绍你所做选择的原因的短语，例如为了。它还包括一个动词列表，指定您所做选择的优点，如启用和促进。</code><br>5.INDICATE THAT APPROPRIATE CARE WAS TAKEN<br><code>5.表明已采取适当护理</code><br>This includes adjectives (careful) as well as adverbs (carefully), so as to give you maximum flexibility when you are constructing sentences.<br><code>这包括形容词（小心）和副词（小心），以便在你造句时给予最大的灵活性。</code><br>6.RELATE MATERIALS/METHODS TO OTHER STUDIES<br><code>6.将材料/方法用于其他研究</code><br>This provides you with ways to distinguish between procedures/materials/ tests which were exactly the same as those used by other researchers, procedures/materials/tests which were similar to those used by other researchers and procedures/materials/tests which were significantly different.<br><code>这为您提供了区分与其他研究人员使用的程序/材料/测试完全相同的程序/材料/测试、与其他研究人员使用的程序/材料/测试相似的程序/材料/测试以及显著不同的程序/材料/测试的方法。</code><br>7.INDICATE WHERE PROBLEMS OCCURRED<br><code>7.指出问题发生的位置</code><br>This list includes ways of minimising the problem, minimising your responsibility, maximising the good aspects and suggesting a solution to the problem.<br><code>这个列表包括最小化问题的方法，最小化你的责任，最大化好的方面和建议解决问题的方法。</code></p><h5 id="2-4-1-Vocabulary-task"><a href="#2-4-1-Vocabulary-task" class="headerlink" title="2.4.1 Vocabulary task"></a>2.4.1 Vocabulary task</h5><p><code>2.4.1词汇任务</code><br>Look through the Methodology sections in this unit and the Methodology or Experimental sections in your target articles. Underline or highlight all the words and phrases that you think could be used in the seven areas above.<br><code>浏览本单元的方法学部分和你的目标文章的方法学或实验部分。划出或突出所有你认为可以用在以上七个方面的单词和短语。</code><br>A full list of useful language can be found on the next pages. This includes all the appropriate words and phrases you highlighted along with some other common ones. Read through them and check the meaning of any you don’t know in the dictionary. This list will be useful for many years.<br><code>有用的语言的完整列表可以在接下来的页面中找到。这包括所有适当的单词和短语，你突出显示沿着与其他一些常见的。通读一遍，并在字典中查一下你不知道的意思。这份清单将在许多年内有用。</code></p><h5 id="2-4-2-Vocabulary-for-the-Methodology-section"><a href="#2-4-2-Vocabulary-for-the-Methodology-section" class="headerlink" title="2.4.2 Vocabulary for the Methodology section"></a>2.4.2 Vocabulary for the Methodology section</h5><p><code>2.4.2方法学部分的词汇</code><br><em><strong>1. PROVIDE A GENERAL INTRODUCTION AND OVERVIEW OF THE MATERIALS/METHODS and GIVE THE SOURCE OF MATERIALS/ EQUIPMENT USED</strong></em><br><code>1.提供材料/方法的一般介绍和概述，并给予所用材料/设备的来源</code><br>Some of the vocabulary you need for this is in the Introduction vocabulary list; for example, many of the verbs that describe what you did/used can be found there.<br><code>你需要的一些词汇在介绍词汇表中;例如，许多描述你做了什么/使用了什么的动词都可以在那里找到。</code><br>These verbs fall into three categories: the first includes general verbs related to academic research, such as attempt, consider, conduct, determine, investigate, report, suggest, verify, and most of these can be found in the Introduction vocabulary list. Th e second category contains verbs that specify what you did, such as calculate, extract, isolate, formulate, incorporate, modify, plot, simulate, and these can be found in the vocabulary list below. Th e third category includes verbs which are specific to your field and your research, but which are not useful in other fields, for example clone, dissect, isotype, infuse. Also try:<br><code>这些动词分为三类：第一类包括与学术研究有关的一般动词，如尝试，考虑，进行，确定，调查，报告，建议，验证，其中大部分可以在介绍词汇表中找到。第二类包含的动词指定你做了什么，如计算，提取，隔离，公式化，合并，修改，绘图，模拟，这些都可以在下面的词汇表中找到。第三类动词是你的领域和研究所特有的，但在其他领域没有用，例如克隆、解剖、同种型、注入。还可以尝试：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-24.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-25.png"></p><ul><li>Here are some examples of how these are used:<br>The impact tests used in this work were a modified version of…<br><code>在这项工作中使用的冲击测试是一个修改版本的...</code></li><li>All reactions were performed in a 27 ml glass reactor…<br><code>所有反应均在27 ml玻璃反应器中进行。</code></li><li>All cell lines were generated as previously described in…<br><code>所有细胞系均如先前在...</code></li><li>In the majority of the tests, buffers with a pH of 8 were used in order to…<br><code>在大多数测试中，使用pH为8的缓冲液，以...</code></li><li>Both experiments were performed in a greenhouse so that…<br><code>这两个实验都是在温室里进行的，所以...</code></li><li>The substrate was obtained from the Mushroom Research Centre…<br><code>该基质是从蘑菇研究中心获得的。</code></li><li>SSCE glass structures were used in this study to perform…<br><code>SSCE玻璃结构在这项研究中被用来执行…</code></li><li>The cylindrical lens was obtained from Newport USA and is shown in Fig. 3.<br><code>圆柱形透镜购自美国纽波特，如图3所示。</code></li><li>The material investigated was a standard aluminium alloy; all melts were modified with sodium.<br><code>所研究的材料是标准铝合金;所有熔体都用钠改性。</code></li><li>Topographical examination was carried out using a 3-D stylus instrument.<br><code>使用3-D触针仪器进行地形检查。</code></li><li>The experiments were conducted at a temperature of 0.5ºC.<br><code>实验在0.5 ℃的温度下进行。</code></li></ul><p><em><strong>2. SUPPLY ESSENTIAL BACKGROUND INFORMATION</strong></em><br><code>2.基本背景资料</code><br>As well as describing standard procedures and techniques you may need to describe the equipment/apparatus or instrument you used or constructed. In order to do this accurately you need good control over the language of spatial location. Make sure you know how to use the words/phrases below. If you are not sure, write down the dictionary definition and use a concordance sampler (which you can find on the Internet) to see how they are used. was/were performed (by/in) was/were provided (by) was/were purchased (from) was/were supplied (by) was/were used as supplied was/were investigated<br><code>除了描述标准程序和技术外，您可能还需要描述您使用或建造的设备/仪器或仪器。为了准确地做到这一点，你需要很好地控制空间位置的语言。确保你知道如何使用下面的单词/短语。如果你不确定，写下字典的定义，并使用一个索引采样器（你可以在互联网上找到），看看他们是如何使用的。被调查的人被调查的人</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-26.png"><br>Here are some examples of how these are used:<br><code>下面是一些如何使用这些的例子：</code></p><ul><li>Porosity was measured at the near end and at the far end of the polished surface.<br><code>在抛光表面的近端和远端测量孔隙率。</code></li><li>The compression axis is aligned with the rolling direction…<br><code>压缩轴与轧制方向一致...</code></li><li>The source light was polarised horizontally and the sample beam can be scanned laterally.<br><code>源光水平偏振，样品光束可以横向扫描。</code></li><li>The mirrors are positioned near the focal plane.<br><code>反射镜位于焦平面附近。</code></li><li>Electrodes comprised a 4 mm diam disk of substrate material embedded in a Teflon disk of 15 mm diam.<br><code>电极包括嵌入15 mm直径的Teflon盘中的4 mm直径的基底材料盘。</code></li><li>The intercooler was mounted on top of the engine…<br><code>中间冷却器安装在发动机顶部...</code></li><li>The concentration of barium decreases towards the edge…<br><code>钡的浓度越靠近边缘越低...</code></li><li>Similar loads were applied to the front and side of the box…<br><code>箱子的正面和侧面都有类似的负荷...</code></li><li>A laminar flow element was located downstream of the test section of the wind tunnel…<br><code>层流元件位于风洞试验段的下游。</code></li></ul><p>In which sentence(s) below was the table closest to the wall?<br><code>在下面的哪个句子中，桌子最靠近墙？</code><br>The table was placed against the wall.<br>The table was placed next to the wall.<br>The table was placed flush with the wall.<br>The table was placed in contact with the wall.<br>The table was placed right against the wall.<br>The table was placed alongside the wall.<br><code>桌子靠墙放着。桌子靠墙放着。桌子与墙平齐.这张桌子被放在紧靠着墙的地方。桌子紧靠着墙放着。桌子靠墙放着。</code><br>In which sentence(s) below was the clock closest to the door?<br><code>在下面哪句话中，时钟离门最近？</code><br>The clock was located just above the door.<br>The clock was located slightly above the door.<br>The clock was located immediately above the door.<br>The clock was located directly above the door.<br>The clock was located right above the door.<br><code>钟就在门的上方。时钟位于门的上方。钟就在门的正上方。钟就在门的正上方。钟就在门的正上方。</code><br>Note that half as wide (as) = half the width (of); half as heavy (as) = half the weight (of); twice as long (as) = twice the length (of) and twice as strong (as) = twice the strength (of). Also note that with/having a weight of 20 kg = weighing 20 kg and with/having a width/length of 20 cm = 20 cm wide/long.<br><code>请注意，一半的宽度（as）=一半的宽度（of）;一半的重量（as）=一半的重量（of）;两倍的长度（as）=两倍的长度（of）和两倍的强度（as）=两倍的强度（of）。还请注意，具有/具有20 kg的重量=重20 kg，并且具有/具有20 cm的宽度/长度= 20 cm宽/长。</code><br><em><strong>3. PROVIDE SPECIFIC AND PRECISE DETAILS ABOUT MATERIALS AND METHODS</strong></em><br><code>3.提供有关材料和方法的具体和精确说明</code><br>These verbs fall into three categories: the first includes general verbs used in academic research, such as attempt, consider, conduct, determine, investigate, report, suggest, verify, and these can be found in the Introduction vocabulary list (Section 1.4). Th e second category contains technical verbs which are specific to your field and your research, but which are not useful in other fields, for example anneal, calibrate, centrifuge, dissect, fertilise, ionise, infuse. These will not be given here because they are not generally useful. The third category is a set of less technical verbs that specify what was done or used, such as calculate, extract, isolate, formulate, incorporate, modify, plot, simulate. These usually occur in the passive (was/were isolated) and can be found in the vocabulary list below.<br><code>这些动词分为三类：第一类包括学术研究中使用的一般动词，如attempt、consider、conduct、determine、investigate、report、suggest、verify，这些动词可以在引言词汇表（1.4节）中找到。第二类是专门针对你的领域和研究的技术动词，但在其他领域没有用，例如退火、校准、离心分离、解剖、施肥、电离、注入。这里不会给出这些，因为它们通常没有用处。第三类是一组技术性较低的动词，用于指定所做或使用的内容，如计算、提取、分离、公式化、合并、修改、绘图、模拟。这些通常出现在被动语态中（was/were isolated），可以在下面的词汇表中找到。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-27.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-28.png"><br>**See Section 1.2.2 for other examples of signalling language **<br><strong>See box below for infinitives, -ing forms and noun forms of useful verbs. ∅ indicates that a noun form is not available or is not common in this type of structure</strong><br><code>* 见第1.2.2节的其他信号语言的例子 ** 见下面的方框的不定式，-ing形式和名词形式的有用的动词。∅表示名词形式在这类结构中不可用或不常见</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-29.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-30.png"><br>Here are some examples of how these are used:<br><code>下面是一些如何使用这些的例子：</code></p><ul><li>To validate the results from the metroscale model, samples were collected from all groups.<br><code>为了验证大城市模型的结果，从所有组中收集了样本。</code></li><li>The method of false nearest neighbours was selected in order to determine the embedding dimension.<br><code>为了确定嵌入维数，选择了伪最近邻方法。</code></li><li>For the sake of simplicity, only a single value was analysed.<br><code>为简单起见，只分析了一个数值。</code></li><li>By partitioning the array, all the multipaths could be identified.<br><code>通过对阵列进行分区，可以识别所有的多路径。</code></li><li>Zinc oxide was drawn into the laminate with the intention of enhancing delaminations and cracks.<br><code>氧化锌被拉入层压板中，目的是增强分层和裂纹。</code></li><li>The advantage of using three-dimensional analysis was that the out-of-plane stress field could be obtained.<br><code>三维分析的优点是可以得到平面外的应力场。</code></li><li>Because FITC was used for both probes, enumeration was carried out using two different slides.<br><code>由于两种探针均使用FITC，因此使用两种不同的载玻片进行计数。</code></li><li>The LVDTs were unrestrained, so allowing the sample to move freely.<br><code>LVDT不受限制，因此允许样品自由移动。</code></li><li>The cylinder was constructed from steel, which avoided problems of water absorption.<br><code>圆筒由钢制成，避免了吸水问题。</code></li></ul><p><em><strong>5. INDICATE THAT APPROPRIATE CARE WAS TAKEN</strong></em><br>Most of the items in the box below are in adverb form, but they also occur in adjective form (e.g. accurate).<br><code>下面方框中的大多数项目是副词形式，但它们也出现在形容词形式中（例如准确）。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-31.png"><br>Here are some examples of how these are used:<br><code>下面是一些如何使用这些的例子：</code></p><ul><li>A mechanical fixture was employed to hold the sonic horn firmly in place.<br><code>一个机械固定装置被用来将声波喇叭牢固地固定在适当的位置。</code></li><li>After being removed, the mouse lungs were frozen and thawed at least three times.<br><code>取出后，将小鼠肺冷冻和解冻至少三次。</code></li><li>The specimen was monitored constantly for a period af 24 hours.<br><code>持续监测样本24小时。</code></li><li>They were then placed on ice for immediate FACS analysis.<br><code>然后将其置于冰上进行即时FACS分析。</code></li><li>Frequent transducer readings were taken to update the stress conditions smoothly.<br><code>频繁读取传感器读数，以顺利更新应力条件。</code></li><li>The samples were slowly and carefully sheared to failure.<br><code>将样品缓慢且小心地剪切至失效。</code></li></ul><p><em><strong>6.RELATE MATERIALS/METHODS TO OTHER STUDIES</strong></em><br><code>6.将材料/方法用于其他研究</code><br>There are three ways in which you might want to relate your materials/ methods to those used in other studies.<br><code>有三种方法可以将您的材料/方法与其他研究中使用的材料/方法联系起来。</code><br><em><strong>Option 1: The procedure/material you used is exactly the same as the one you cite.</strong></em><br><code>选项1：您使用的程序/材料与您引用的程序/材料完全相同。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-32.png"><br><em>by and of are usually followed by the name of the researcher or research team (by Ross or using the method of Ross et al.) and in is usually followed by the work (in Ross et al. (2003)). Another option is simply to give the research reference at the appropriate place in the sentence, either in brackets or using a superscript number.</em><br><code>*by和of通常后跟研究人员或研究团队的名称（由Ross或使用Ross等人的方法）。而in通常跟在work之后（in Ross et al.（2003））。另一种选择是简单地在句子的适当位置给予研究参考，或者在括号中，或者使用上标数字。</code><br><em><strong>Option 2: The procedure/material you used is similar to the one you cite.</strong></em><br><code>选项2：您使用的程序/材料与您引用的程序/材料相似。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-33.png"><br><em><strong>Option 3: The procedure/material you used is significantly different from the one you cite.</strong></em><br><code>选项3：您使用的程序/材料与您引用的程序/材料明显不同。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-34.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-35.png"><br><em><strong>as you can see, these can be used in Option 2 as well as Option 3. When you use them in Option 2 you may not need to state the differences between the procedure/material you used and the one you cite if they are not significant. In Option 3 those differences or modifications are significant and you should say what they were, especially if they were modifications which improved the procedure/material.</strong></em><br><code>* 正如您所看到的，这些可以在选项2和选项3中使用。当你在选项2中使用它们时，如果它们不重要，你可能不需要说明你使用的程序/材料与你引用的程序/材料之间的差异。在选项3中，这些差异或修改是重要的，您应该说明它们是什么，特别是如果它们是改进程序/材料的修改。</code><br>Here are some examples of how these are used:<br><code>下面是一些如何使用这些的例子：</code></p><ul><li>Developmental evaluation was carried out using the Bayley Scales of Infant Development (Bayley, 1969).<br><code>使用Bayley婴儿发育量表（Bayley，1969）进行发育评价。</code></li><li>The size of the Gaussians was adjusted as in (Krissian et al., 2000).<br><code>如在（Krissian et al.，2000年）。</code></li><li>The centrifuge is a slightly modified commercially available model, the Beckman J6-HC.<br><code>离心机是稍微改进的市售型号Beckman J6-HC。</code></li><li>The protein was overexpressed and purified as reported previously.<br><code>如先前报道的，过表达和纯化蛋白质。</code></li><li>A revised version of the Structured Clinical Interview (4th edition) was used.<br><code>使用了结构化临床访谈（第4版）的修订版。</code></li><li>We modified the Du and Parker filter to address these shortcomings and we refer to this modified filter as the MaxCurve filter.<br><code>我们修改了Du和帕克滤波器以解决这些缺点，我们将此修改后的滤波器称为MaxCurve滤波器。</code></li><li>In our implementation we followed Sato et al. (1998) by using a discrete kernel size.<br><code>在我们的实现中，我们遵循Sato et al.（1998）使用离散的内核大小。</code></li></ul><p>7.INDICATE WHERE PROBLEMS OCCURRED<br><code>7.指出问题发生的位置</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-36.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/2-37.png"><br><em><strong>There is an interesting difference between the phrase future work should and the phrase future work will. When you write future work should you are suggesting a direction for future work and inviting the research community in your field to take up the challenge and produce the research. When you write future work will you are communicating your own plans and intentions to the research community and it should be understood that these plans and intentions belong to you — you’re saying ‘hands off !’ to the rest of the research community and describing a research plan of your own</strong></em><br><code>*在未来的工作应该和未来的工作将之间有一个有趣的区别。当你写未来的工作时，你应该为未来的工作提出一个方向，并邀请你所在领域的研究团体接受挑战并进行研究。当你写未来的工作时，你是在向研究界传达你自己的计划和意图，应该理解这些计划和意图属于你-你在说“放手！”向其他研究团体介绍你自己的研究计划</code><br>Here are some examples of how these are used:<br><code>下面是一些如何使用这些的例子：</code><br>Inevitably, considerable computation was involved.<br><code>不可避免地，涉及到大量的计算。</code><br>Only a brief observation was feasible, however, given the number in the sample.<br><code>然而，鉴于样本中的数量，只能进行简短的观察。</code><br>Although centrifugation could not remove all the excess solid drug, the amount remaining was negligible.<br><code>虽然离心不能除去所有多余的固体药物，但剩余的量可以忽略不计。</code><br>Solutions using (q = 1) differed slightly from the analytical solutions.<br><code>使用（q = 1）的解决方案与分析解决方案略有不同。</code><br>Continuing research will examine a string of dc-dc converters to determine if the predicted efficiencies can be achieved in practice.<br><code>继续的研究将检查一串直流-直流转换器，以确定预测的效率是否可以在实践中实现。</code><br>While the anode layer was slightly thicker than 13 μm, this was a minor deficit.<br><code>虽然阳极层略厚于13 μm，但这是一个微小的缺陷。</code></p><h3 id="2-5-Writing-a-Methodology-Section"><a href="#2-5-Writing-a-Methodology-Section" class="headerlink" title="2.5 Writing a Methodology Section"></a>2.5 Writing a Methodology Section</h3><p><code>2.5撰写方法论章节</code><br>In the next task, you will bring together and use all the information in this unit. You will write a Methodology section according to the model, using the grammar and vocabulary you have learned, so make sure that you have both the model (Section 2.3.3) and the vocabulary (Section 2.4) in front of you.<br><code>在下一个任务中，你将把本单元中的所有信息结合起来使用。您将根据模型编写方法论部分，使用您所学的语法和词汇，因此请确保您面前有模型（第2.3.3节）和词汇（第2.4节）。</code><br>In this unit you have seen the conventional model of the Methodology and the vocabulary conventionally used has been collected. Remember that when you write, your sentence patterns should also be conventional, so use the sentence patterns you have seen in the Methodology samples in this unit and in your target articles as models for your writing.<br><code>在本单元中，你已经看到了方法论的传统模型，并且收集了传统使用的词汇。请记住，当你写作时，你的句型也应该是常规的，所以使用你在本单元的方法论样本和你的目标文章中看到的句型作为你写作的模式。</code><br>Follow the model exactly this time, and in future, use it to check the Methodology of your work so that you can be sure that the information is in an appropriate order and that you have done what your readers expect you to do in this section.<br><code>这一次完全按照这个模型来做，将来，用它来检查你的工作方法，这样你就可以确保信息的顺序是正确的，你已经做了读者希望你在这一节做的事情。</code><br>Although a model answer is provided in the Key, you should try to have your own answer checked by a native speaker of English if possible, to make sure that you are using the vocabulary correctly.<br><code>虽然答案中提供了一个标准答案，但如果可能的话，您应该尝试让母语为英语的人检查您自己的答案，以确保您正确使用词汇。</code></p><h5 id="2-5-1-Write-a-Methodology-section"><a href="#2-5-1-Write-a-Methodology-section" class="headerlink" title="2.5.1 Write a Methodology section"></a>2.5.1 Write a Methodology section</h5><p><code>2.5.1编写方法部分</code><br>The aim of this task is for you to learn how to describe what you did and used so that any reader can repeat exactly what you did and obtain exactly the same result as you obtained. Remember that you are expected to show that you carried out your work with due care and that you had good reasons for doing what you did. The message is: This is exactly what I did, I did it carefully and I had good reasons for doing it in this way.<br><code>这个任务的目的是让你学习如何描述你做了什么和使用了什么，这样任何读者都可以准确地重复你做了什么，并获得与你获得的完全相同的结果。记住，你应该表现出你以应有的谨慎来完成你的工作，并且你有很好的理由去做你所做的事情。我所传达的信息是：这正是我所做的，我做得很仔细，我有很好的理由这样做。</code><br>To complete the task, imagine that you are writing up a research project which has carried out the first-ever attempt to cook chicken. Imagine that until now, everyone ate it raw. The task is to write a recipe for cooking chicken as if it were the Materials/Methods section of a research paper.<br><code>为了完成任务，想象你正在写一个研究项目，该项目首次尝试烹饪鸡肉。想象一下，直到现在，每个人都吃它生。任务是写一份烹饪鸡肉的食谱，就好像它是一篇研究论文的材料/方法部分。</code><br>As an example, instead of starting by writing something like Cut the chicken into four pieces, you could perhaps start with an overview of the entire procedure, or by giving the source of your chicken. Did you obtain it from a supermarket? Was it supplied by a laboratory facility? You will need to say what you used to cut the chicken up; using an axe gives a very different result from using a 4 cm Sabatier steel knife! Instead of writing Now put the chicken in a hot oven for about an hour and a half, you should write something like: Th e sample was then placed on a 300 × 600 mm stainless steel sheet and heated in a Panasonic E458 × 500 w standard fan-assisted oven for 90 minutes at 350°C.<br><code>举个例子，你可以从整个过程的概述开始，或者从鸡肉的来源开始，而不是从写“把鸡肉切成四块”开始。你是从超市买的吗？是否由实验室提供？你需要说你用什么来切鸡肉;使用斧头和使用4厘米的萨巴蒂尔钢刀会产生非常不同的结果！不要写Now put the chicken in a hot oven for about a half hour，你应该写这样的话：然后将样品放在300 × 600 mm的不锈钢板上，并在松下E458 × 500 w标准风扇辅助烤箱中在350°C下加热90分钟。</code><br>Don’t worry if you don’t know how to cook chicken — it doesn’t matter if you report that you cooked it by boiling it in vodka, but you must give the exact quantity and the brand name of the vodka you used, so that your method and results can be replicated by someone else. Remember to use the passive voice and the appropriate tense.<br><code>如果你不知道如何烹饪鸡肉，不要担心--如果你报告说你是用伏特加煮鸡肉的，那也没关系，但你必须给予准确的数量和你使用的伏特加的品牌名称，这样你的方法和结果就可以被其他人复制。记住使用被动语态和适当的时态。</code><br>The title of the research paper in which you report the new process is: AN APPROACH TO THE PREPARATION OF CHICKEN. Th e Introduction to your paper looks like this:<br><code>您报告新工艺的研究论文的标题是：鸡的制备方法。你的论文的引言是这样的：</code></p><blockquote><p>Introduction<br>Chicken preparation techniques are used in a range of applications both in homes and in restaurants. Chicken is easily available and can be locally produced in most areas; in addition it is easily digested and low in calories.<br><code>鸡肉制备技术用于家庭和餐馆的一系列应用中。鸡肉很容易获得，在大多数地区都可以在当地生产;此外，它很容易消化，热量低。</code><br>鸡肉制备技术用于家庭和餐馆的一系列应用中。鸡肉很容易获得，在大多数地区都可以在当地生产;此外，它很容易消化，热量低。<br><code>自从邓迪的开创性工作报告了鸡肉制备的“自然”方法（邓迪等人，1990年），鸡被杀死，然后用盐生吃，有重大的创新。在法国，人们已经在改进鸡的屠宰方法方面做了大量的工作，而在美国，研究人员则专注于改善鸡的大小。“自然”方法被广泛使用，因为该过程所需的时间非常短;然而，一些问题仍未解决。使用邓迪方法制备的鸡肉的味道通常被认为是令人不快的，并且有充分的证据表明食用生肉会导致细菌感染的风险。</code><br>The aim of this study was to develop a preparation method that would address these two problems. In this report, we describe the new method, which uses seasoning to improve the flavour while heating the chicken in order to kill bacteria prior to eating.<br><code>本研究的目的是开发一种制备方法，将解决这两个问题。在这份报告中，我们描述了一种新的方法，它使用调味料来改善风味，同时加热鸡肉，以便在食用前杀死细菌。</code></p></blockquote><p>Now write the Methodology section of this paper. You should write approximately 250–400 words. If you get stuck and don’t know what to write next, use the model and the vocabulary to help you move forward. Don’t look at the Key until you have finished writing.<br><code>现在写这篇论文的方法论部分。你应该写大约250-400字。如果你陷入了困境，不知道下一步该写什么，使用模型和词汇表来帮助你前进。写完后再看钥匙。</code></p><h5 id="2-5-2-Key"><a href="#2-5-2-Key" class="headerlink" title="2.5.2 Key"></a>2.5.2 Key</h5><p><code>2.5.2密钥</code><br>Here is a sample answer. When you read it, think about which part of the model is represented in each sentence.<br><code>下面是一个示例答案。当你读它的时候，想想模型的哪一部分在每个句子中被代表。</code></p><blockquote><p>Two experiments were carried out using diff erent combinations of seasoning and varying cooking temperatures. A 4.5 kg frozen organic chicken was purchased from Buyrite Supermarket. Buyrite only sell grade ‘A’ chickens approved by the Organic Farmers Association, thus ensuring both the homogeneity of the sample and the quality of the product. Seasonings were obtained from SeasonInc UK and were used as supplied.<br><code>使用不同的调味料组合和不同的烹饪温度进行了两个实验。4.5kg冷冻有机鸡购自Buyrite超市。Buyrite只销售有机农民协会批准的A级鸡，从而确保样品的同质性和产品的质量。调味料从SeasonInc UK获得，并按供应使用。</code><br>According to the method described by Hanks et al. (1998), the chicken was first immersed in freshly boiled water cooled to a temperature of 20°C and was subsequently rinsed thoroughly in a salt solution so as to reduce the level of bacteria on the surface of the chicken. In order to obtain two samples of equal size and weight for testing, the chicken was first skinned using a standard BS1709 Skin-o-matic; the flesh was then removed from the bone with a 4 cm steel Sabatier knife, after which it was cut into 3 cm-cubes, each weighing 100 g.<br><code>根据Hanks等人（1998年）描述的方法，首先将鸡浸入冷却至20°C的新鲜沸水中，随后在盐溶液中彻底冲洗，以减少鸡表面的细菌水平。为了获得相同尺寸和重量的两个样品用于测试，首先使用标准BS 1709 Skin-o-matic将鸡剥皮;然后用4cm钢Sabatier刀从骨头上去除肉，之后将其切成3cm立方体，每个重100 g。</code><br>Two of the cubes thus obtained were randomly selected for testing. Th e cubes were dried individually in a Phillips R2D2 Dehydrator for 10 minutes. Immediately aft er removing each cube from the dehydrator it was coated with the selected seasoning mixture and left to stand on a glass plate for 30 minutes at room temperature (16°C) in order to enhance absorption of the seasoning prior to heating. Seasoning quantities were measured used standard domestic kitchen scales and were therefore only approximate.<br><code>随机选择由此获得的两个立方体进行测试。将立方体在菲利普斯R2D2脱水器中单独干燥10分钟。在从蒸煮器中取出每个立方体之后，立即将其用所选择的调味品混合物涂覆，并在室温（16°C）下在玻璃板上静置30分钟，以在加热之前增强调味品的吸收。使用标准家用厨房秤测量调味料量，因此仅为近似值。</code><br>Each cube was then placed on an ovenproof dish and transferred to a pre-heated Panasonic Model 33KY standard electric fan-assisted oven at 150°C for 10 minutes. Th e product was removed from the oven and allowed to come to equilibrium, aft er which the cubes were assessed according to the TTS test developed by Dundee (Dundee, 1997).<br><code>然后将每个立方体放置在耐热皿上，并转移到预热的Panasonic Model 33KY标准电风扇辅助烘箱中，在150°C下持续10分钟。从烘箱中取出产品并使其达到平衡，之后根据邓迪开发的TTS测试（邓迪，1997）评估立方体。</code></p></blockquote></body></html>]]></content>
      
      
      <categories>
          
          <category> sciencewriting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【目标检测】Object Detection in 20 Years A Survey</title>
      <link href="/2024/02/25/mu-biao-jian-ce-object-detection-in-20-years-a-survey/"/>
      <url>/2024/02/25/mu-biao-jian-ce-object-detection-in-20-years-a-survey/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><em><strong>Object Detection in 20 Years A Survey</strong></em><br><em><strong>目标检测20年综述</strong></em></p><h1 id="GPT解读"><a href="#GPT解读" class="headerlink" title="GPT解读"></a>GPT解读</h1><p> 这篇论文《Object Detection in 20 Years: A Survey》全面回顾了过去二十年对象检测技术的发展历程，强调了其在计算机视觉领域的重要性和挑战性。以下是论文的核心内容概述：</p><ol><li><p><strong>对象检测的重要性</strong>：对象检测是计算机视觉中的基础任务，涉及在数字图像中检测特定类别的视觉对象实例。它为计算机视觉应用提供了基本的知识：对象在哪里。</p></li><li><p><strong>技术演进</strong>：论文详细回顾了从1990年代到2022年的技术发展，包括里程碑式的检测器、数据集、评估指标、检测系统的基本构建块、加速技术以及最新的检测方法。</p></li><li><p><strong>里程碑检测器</strong>：介绍了历史上的重要检测器，如Viola-Jones检测器、HOG检测器、DPM、R-CNN系列、YOLO系列、SSD、RetinaNet、CornerNet和DETR等。</p></li><li><p><strong>数据集与评估指标</strong>：讨论了多个著名的对象检测数据集，如PASCAL VOC、ILSVRC、MS-COCO等，以及评估对象检测性能的指标，如平均精度（AP）和交并比（IoU）。</p></li><li><p><strong>技术发展</strong>：分析了多尺度检测、上下文引导、硬负样本挖掘、损失函数和非极大值抑制（NMS）等关键技术的演进。</p></li><li><p><strong>加速技术</strong>：探讨了提高检测器速度的方法，包括特征图共享计算、级联检测、网络剪枝和量化、轻量级网络设计以及数值加速。</p></li><li><p><strong>最新进展</strong>：回顾了近年来的一些最新技术，如超越滑动窗口检测、旋转和尺度变化的鲁棒检测、更好的背景网络、改进的定位方法、与分割损失的学习、对抗性训练、弱监督对象检测和领域适应。</p></li><li><p><strong>未来方向</strong>：提出了未来研究的可能方向，包括轻量级对象检测、端到端对象检测、小对象检测、3D对象检测、视频中的检测、跨模态检测以及开放世界检测。</p></li><li><p><strong>作者信息</strong>：论文由Zhengxia Zou、Keyan Chen、Zhenwei Shi、Yuhong Guo和Jieping Ye共同撰写，他们在计算机视觉、模式识别和机器学习领域有着丰富的研究和实践经验。</p></li></ol><p>这篇论文为读者提供了对象检测技术的全面视角，强调了其在计算机视觉领域的演变，并对未来的研究方向提供了深入的分析。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【目标检测】Underwater target detection based on improved YOLOv7</title>
      <link href="/2024/02/25/mu-biao-jian-ce-underwater-target-detection-based-on-improved-yolov7/"/>
      <url>/2024/02/25/mu-biao-jian-ce-underwater-target-detection-based-on-improved-yolov7/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><em><strong>Underwater target detection based on improved YOLOv7</strong></em><br><em><strong>基于改进YOLOv7的水下目标检测</strong></em></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Underwater target detection is a crucial aspect of ocean exploration. However, conventional underwater target detection methods face several challenges such as inaccurate feature extraction, slow detection speed and lack of robustness in complex underwater environments. To address these limitations, this study proposes an improved YOLOv7 network (YOLOv7-AC) for underwater target detection. The proposed network utilizes an ACmixBlock module to replace the 3x3 convolution block in the E-ELAN structure, and incorporates jump connections and 1x1 convolution architecture between ACmixBlock modules to improve feature extraction and network reasoning speed. Additionally, a ResNet-ACmix module is designed to avoid feature information loss and reduce computation, while a Global Attention Mechanism (GAM) is inserted in the backbone and head parts of the model to improve feature extraction. Furthermore, the K-means++ algorithm is used instead of K-means to obtain anchor boxes and enhance model accuracy. Experimental results show that the improved YOLOv7 network outperforms the original YOLOv7 model and other popular underwater target detection methods. The proposed network achieved a mean average precision (mAP) value of 89.6% and 97.4% on the URPC dataset and Brackish dataset, respectively, and demonstrated a higher frame per second (FPS) compared to the original YOLOv7 model.<br><code>水下目标检测是海洋探测的一个重要方面。然而，在复杂的水下环境下，传统的水下目标检测方法面临特征提取不准确、检测速度慢、鲁棒性不足等挑战。针对这些局限性，提出了一种改进的YOLOv7网络(YOLOv7- ac)用于水下目标检测。该网络利用ACmixBlock模块替代E-ELAN结构中的3x3卷积块，并在ACmixBlock模块之间融入跳跃连接和1x1卷积架构，以提高特征提取和网络推理速度。此外，设计了ResNet-ACmix模块以避免特征信息损失并减少计算量，同时在模型的主干和头部部分插入了全局注意力机制(GAM)以改进特征提取。使用k -means++算法代替K-means算法获取锚框，提高模型精度。实验结果表明，改进的YOLOv7网络优于原始的YOLOv7模型和其他流行的水下目标检测方法。所提网络在URPC数据集和Brackish数据集上分别取得了89.6%和97.4%的平均精度均值(mAP)值，并且相比原始YOLOv7模型表现出更高的每秒帧数(FPS)。</code><br>The source code for this study is publicly available at <a href="https://github.com/NZWANG/YOLOV7-AC">https://github.com/NZWANG/YOLOV7-AC</a>.<br>In conclusion, the improved YOLOv7 network proposed in this study represents a promising solution for underwater target detection and holds great potential for practical applications in various underwater tasks.<br><code>综上所述，本文提出的改进YOLOv7网络为水下目标检测提供了一种很好的解决方案，在水下各种任务的实际应用中具有很大的潜力。</code><br><em><strong>Keywords</strong></em><br>Underwater target detection; Marine resources; Computer vision; Image analysis; YOLOv7-AC; GAM; K-means++<br><code>水下目标检测;海洋资源;计算机视觉;图像分析;YOLOv7-AC;GAM;k - means + +</code></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The oceans occupy a significant portion of the Earth’s surface and are a valuable source of oil, gas, minerals, chemicals, and other aquatic resources, attracting the attention of professionals, adventurers, and researchers, leading to an increase in marine exploration activities [1]. To support these exploration efforts, various underwater tasks such as target location, biometric identification, archaeology, object search, environmental monitoring, and equipment maintenance must be performed [2]. In this context, underwater target detection technology plays a crucial role. Underwater target detection can be categorized into acoustic system detection and optical system detection [3], and image analysis, including classification, identification, and detection, can be performed based on the obtained image information. Optical images, compared to acoustic images, offer higher resolution and a greater volume of information and are more cost-effective in terms of acquisition methods [4, 5]. As a result, underwater target detection based on optical systems is receiving increased attention.Target detection, being as a core branch of computer vision, encompasses fundamental tasks such as target classification and localization. The existing approaches to target detection can be broadly classified into two categories: traditional target detection methods and deep learning-based target detection methods [6].<br><code>海洋占据了地球表面的重要部分，是石油、天然气、矿物、化学品和其他水生资源的宝贵来源，吸引了专业人士、探险家和研究人员的注意，导致海洋勘探活动的增加。为了支持这些探索工作，必须执行各种水下任务，如目标定位、生物特征识别、考古、物体搜索、环境监测和设备维护。在此背景下，水下目标检测技术起着至关重要的作用。水下目标探测分为声学系统探测和光学系统探测[3]，可以根据获得的图像信息进行图像分析，包括分类、识别和探测。与声学图像相比，光学图像具有更高的分辨率和更大的信息量，在获取方法方面更具成本效益[4,5]。因此，基于光学系统的水下目标检测受到越来越多的关注。目标检测作为计算机视觉的核心分支，包含目标分类、定位等基本任务。现有的目标检测方法大致可以分为两类:传统的目标检测方法和基于深度学习的目标检测方法[6]。</code><br>Traditional algorithms for target detection are typically structured into three phases: region selection, feature extraction, and feature classification [7]. The goal of region selection is to localize the target, as the position and aspect ratio of the target may vary in the image. This phase is typically performed by traversing the entire image using a sliding window strategy [8], wherein different scales and aspect ratios are considered. Subsequently, feature extraction algorithms such as Histogram of Oriented Gradients (HOG) [9] and Scale Invariant Feature Transform (SIFT) [10] are employed to extract relevant features. Finally, the extracted features are classified using classifiers such as Support Vector Machines (SVM) [11] and Adaptive Boosting (Ada-Boost) [12]. However, the traditional target detection method has two major limitations: (1) the region selection using sliding windows lacks specificity and leads to high time complexity and redundant windows, and (2) the hand-designed features are not robust to variations in pose.<br><code>传统的目标检测算法通常分为3个阶段:区域选择、特征提取和特征分类。由于目标在图像中的位置和长宽比可能会发生变化，因此区域选择的目的是定位目标。这一阶段通常通过使用滑动窗口策略[8]遍历整个图像来执行，其中考虑了不同的尺度和长宽比。然后，采用方向梯度直方图(HOG)[9]和尺度不变特征变换(SIFT)[10]等特征提取算法提取相关特征;最后，利用支持向量机(SVM)[11]和自适应增强(Ada-Boost)[12]等分类器对提取的特征进行分类。然而，传统的目标检测方法存在两个主要的局限性:(1)使用滑动窗口的区域选择缺乏特异性，导致时间复杂度高和窗口冗余;(2)手工设计的特征对姿态变化的鲁棒性不强。</code><br>The advent of deep learning has revolutionized the field of target detection and has been extensively applied in computer vision. Convolutional neural networks (CNNs) have demonstrated their superior ability to extract and model features for target detection tasks, and numerous studies have demonstrated that deep learning-based methods outperform traditional methods relying on hand-designed features [13]. Currently, there are two main categories of deep learning-based target detection algorithms: region proposal-based algorithms and regression-based algorithms. The former category, also referred to as Two-Stage target detection methods, are based on the principle of coarse localization and fine classification, where candidate regions containing targets are first identified and then classified. Representative algorithms in this category include R-FCN (Region-based Fully Convolutional Networks) [15] and the R-CNN (Region-CNN) family of algorithms (R-CNN [16], Fast-RCNN [17], Faster-RCNN [18], Mask-RCNN [19], Cascade-RCNN [20], etc.). Although region-based algorithms have high accuracy, they tend to be slower and may not be suitable for real-time applications. In contrast, regression-based target detection algorithms, also known as One-Stage target detection algorithms, directly extract features through CNNs for the prediction of target classification and localization. Representative algorithms in this category include the SSD (Single Shot MultiBox Detector) [21] and the YOLO (You Only Look Once) family of algorithms (YOLO [23], YOLO9000 [24], YOLOv3 [25], YOLOv4 [26], YOLOv5 [27], YOLOv6 [28], YOLOv7 [29]). Due to the direct prediction of classification and localization, these algorithms offer a faster detection speed, making them a popular research area in the field of target detection, with ongoing efforts aimed at improving their accuracy and performance.<br><code>深度学习的出现彻底改变了目标检测领域，并被广泛应用于计算机视觉。卷积神经网络(cnn)已经证明了其在目标检测任务中提取和建模特征的卓越能力，大量研究表明，基于深度学习的方法优于依赖手工设计特征[13]的传统方法。目前，基于深度学习的目标检测算法主要有两大类:基于区域提议的算法和基于回归的算法。前一类也称为两阶段目标检测方法，基于粗定位和细分类的原则，首先识别出包含目标的候选区域，然后再进行分类。该类别中的代表性算法包括R-FCN(基于区域的全卷积网络)[15]和R-CNN(区域- cnn)系列算法(R-CNN[16]、Fast-RCNN[17]、Faster-RCNN[18]、Mask-RCNN[19]、Cascade-RCNN[20]等)。基于区域的算法虽然精度高，但速度较慢，不适合实时应用。相比之下，基于回归的目标检测算法，也称为单阶段目标检测算法，直接通过cnn提取特征用于目标分类和定位的预测。这一类中的代表性算法包括SSD (Single Shot MultiBox Detector)[21]和YOLO (You Only Look Once)系列算法(YOLO[23]、YOLO9000[24]、YOLOv3[25]、YOLOv4[26]、YOLOv5[27]、YOLOv6[28]、YOLOv7[29])。由于直接预测分类和定位，这些算法具有更快的检测速度，成为目标检测领域的研究热点，人们一直在努力提高它们的精度和性能。</code><br>The commercial viability of underwater robots equipped with highly efficient and accurate target detection algorithms is being actively pursued in the field of underwater environments [30]. In this regard, researchers have made significant contributions to the development of target detection algorithms. For instance, in 2017, Zhou et al. [31] integrated image enhancement techniques into an expanded VGG16 feature extraction network and employed a Faster R-CNN network with feature mapping for the detection and identification of underwater biological targets using the URPC dataset. In 2020, Chen et al. [32] introduced a new sample distribution-based weighted loss function called IMA (Invert Multi-Class AdaBoost) to mitigate the adverse effect of noise on detection performance. In 2021, Qiao et al. [33] proposed a real-time and accurate underwater target classifier, leveraging the combination of LWAP (Local Wavelet Acoustic Pattern) and MLP (Multilayer Perceptron) neural networks, to tackle the challenging problem of underwater target classification. Nevertheless, the joint requirement of localization and classification, in addition to classification, makes the target detection task especially challenging in underwater environments where images are often plagued by severe color distortion and low visibility caused by mobile acquisition.With the aim of enhancing the accuracy, achieving real-time performance, and promoting the portability of the underwater target detection capability, the most advanced YOLOv7 model of the YOLO series has been selected for improvement, resulting in the proposed YOLOv7-AC model, designed to address the difficulties encountered in this field. The effectiveness of the proposed model has been demonstrated through experiments conducted on underwater images.The innovations of this paper are as follows:<br><code>在水下环境[30]领域，配备高效、准确目标检测算法的水下机器人的商业可行性正在积极追求。为此，研究人员为目标检测算法的发展做出了重大贡献。例如，2017年，Zhou等[31]将图像增强技术集成到扩展的VGG16特征提取网络中，并利用具有特征映射的Faster R-CNN网络对URPC数据集进行水下生物目标的检测和识别。2020年，Chen等人在[32]中引入了一种新的基于样本分布的加权损失函数IMA (inverse Multi-Class AdaBoost)来缓解噪声对检测性能的不利影响。2021年，Qiao et al.[33]提出了一种实时准确的水下目标分类器，利用LWAP (Local Wavelet Acoustic Pattern)和MLP (Multilayer Perceptron)神经网络的结合来解决水下目标分类的挑战性问题。然而，除了分类之外，还需要定位和分类，这使得水下环境下的目标检测任务尤其具有挑战性。在水下环境中，由于移动采集，图像往往存在严重的颜色失真和低能见度。为了提高水下目标检测的准确性、实时性和便携性，选取YOLO系列中最先进的YOLOv7模型进行改进，提出了YOLOv7- ac模型，以解决该领域遇到的困难。通过在水下图像上的实验验证了所提模型的有效性。本文的创新点如下:</code><br>   (1) In order to extract more informative features, the integration of the Global Attention Mechanism (GAM) [39] is proposed. This mechanism effectively captures both the channel and spatial aspects of the features and increases the significance of cross-dimensional interactions.<br>   <code>(1)为了提取信息量更大的特征，提出了融合全局注意力机制(GAM)的[39]。这种机制有效地捕获了特征的通道和空间方面，并增加了跨维度交互的重要性。</code><br>(2) To further enhance the performance of the network, the ACmix (A mixed model incorporating the benefits of self-Attention and Convolution) [40] is introduced.<br>   <code>(2)为了进一步提升网络的性能，引入了ACmix (A mixed model combination of self-Attention and Convolution)[40]模型。</code><br>   (3) The design of the ResNet-ACmix module in YOLOv7-AC aims to enhance the feature extraction capability of the backbone network and to accelerate the convergence of the network by capturing more informative features.<br>   <code>(3) YOLOv7-AC中ResNet-ACmix模块的设计旨在增强骨干网的特征提取能力，并通过捕获更有信息量的特征来加速网络的收敛。</code><br>   (4) The E-ELAN module in the YOLOv7 network is optimized by incorporating Skip Connections and a 1x1 convolutional structure between modules and replacing the 3x3 Convolutional layer with the ACmixBlock. This results in an enhanced feature extraction ability and improved speed during inference.<br>   <code>(4)对YOLOv7网络中的E-ELAN模块进行优化，在模块之间加入跳跃连接和1x1卷积结构，并将3x3卷积层替换为ACmixBlock。这增强了特征提取能力，提高了推理速度。</code><br>The rest of this paper is organized as follows. Section 2 describes the architecture of YOLOv7 model and related methods. Section 3 presents the proposed YOLOv7-AC model and its theoretical foundations. The performance of the YOLOv7-AC model is evaluated and analyzed through experiments conducted on underwater image datasets in Section 4. The limitations and drawbacks of the proposed method are discussed in Section 5. Finally, we provide a conclusion of this work in Section 6.<br><code>本文的其余部分组织如下。第2节介绍YOLOv7模型的体系结构和相关方法。第3节介绍了YOLOv7-AC模型及其理论基础。第四节通过在水下图像数据集上进行实验，评估和分析了YOLOv7-AC模型的性能。第5节讨论了所提出方法的局限性和缺点。最后，在第6节对本文工作进行了总结。</code></p><h1 id="2-Related-Works"><a href="#2-Related-Works" class="headerlink" title="2. Related Works"></a>2. Related Works</h1><h2 id="2-1-YOLOv7"><a href="#2-1-YOLOv7" class="headerlink" title="2.1. YOLOv7"></a>2.1. YOLOv7</h2><p>The YOLOv7 model, developed by Chien-Yao Wang and Alexey Bochkovskiy et al. in 2022, integrates strategies such as E-ELAN (Extended efficient layer aggregation networks) [34], model scaling for concatenation-based models [35], and model re-parameterization [36] to achieve a favorable balance between detection efficiency and precision. As shown in Figure 1, the YOLOv7 network consists of distinct four modules: the Input module, the Backbone network, the Head network and the Prediction network.<br><code>由Chien-Yao Wang和Alexey Bochkovskiy等人在2022年开发的YOLOv7模型集成了E-ELAN (Extended efficient layer aggregation networks)[34]、基于级联的模型扩展[35]和模型重新参数化[36]等策略，以实现检测效率和精度之间的良好平衡。如图1所示，YOLOv7网络由四个不同的模块组成:输入模块、主干网络、头部网络和预测网络。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/6-1.png"></p><blockquote><p>Figure 1. The network structure of YOLOv7 [29].<br><code>图1.YOLOv7的网络结构[29]。</code></p></blockquote><p>Input module: The pre-processing stage of the YOLOv7 model employs both mosaic and hybrid data enhancement techniques and leverages the adaptive anchor frame calculation method established by YOLOv5 to ensure that the input color images are uniformly scaled to a 640x640 size, thereby meeting the requirements for the input size of the backbone network.<br><code>输入模块：YOLOv7模型的预处理阶段采用了马赛克和混合数据增强技术，并利用YOLOv5建立的自适应锚帧计算方法，以确保输入的彩色图像均匀缩放到640 x640大小，从而满足骨干网络对输入大小的要求。</code><br>Backbone network: The YOLOv7 network comprises three main components: CBS, E-ELAN, and MP1. The CBS module is composed of convolution, batch normalization, and SiLU activation functions. The E-ELAN module maintains the original ELAN design architecture and enhances the network’s learning ability by guiding different feature group computational blocks to learn more diverse features, preserving the original gradient path. MP1 is composed of CBS and MaxPool and is divided into upper and lower branches. The upper branch uses MaxPool to halve the image’s length and width and CBS with 128 output channels to halve the image channels. The lower branch halves the image channels through a CBS with a 1x1 kernel and stride, halves the image length and width with a CBS of 3x3 kernel and 2x2 stride, and finally fuses the features extracted from both branches through the concatenation (Cat) operation. MaxPool extracts the maximum value information of small local areas while CBS extracts all value information of small local areas, thereby improving the network’s feature extraction ability.<br><code>骨干网络:YOLOv7网络由三个主要组件组成:CBS、E-ELAN和MP1。CBS模块由convolution、batch normalization和SiLU激活函数组成。E-ELAN模块保持了原有ELAN设计架构，通过引导不同特征组计算块学习更多样化的特征，增强了网络的学习能力，保留了原有的梯度路径。MP1由CBS和MaxPool组成，分为上分支和下分支。上面的分支使用MaxPool将图像的长度和宽度减半，使用具有128个输出通道的CBS将图像通道减半。下分支通过具有1x1核和步幅的CBS将图像通道减半，使用具有3x3核和2x2步幅的CBS将图像长宽减半，最后通过concatenation (Cat)操作将从两个分支提取的特征进行融合。MaxPool提取局部小区域的最大值信息，CBS提取局部小区域的所有值信息，从而提高网络的特征提取能力。</code><br>Head network: The Head network of YOLOv7 is structured using the Feature Pyramid Network (FPN) architecture, which employs the PANet design. This network comprises several Convolutional, Batch Normalization and SiLU activation (CBS) blocks, along with the introduction of a Spatial Pyramid Pooling and Convolutional Spatial Pyramid Pooling (Sppcspc) structure, the extended efficient layer aggregation network (E-ELAN), and MaxPool-2 (MP2). The Sppcspc structure improves the network’s perceptual field through the incorporation of a Convolutional Spatial Pyramid (CSP) structure within the Spatial Pyramid Pooling (SPP) structure, along with a large residual edge to aid optimization and feature extraction. The ELAN-H layer, which is a fusion of several feature layers based on E-ELAN, further enhances feature extraction. The MP2 block has a similar structure to the MP1 block, with a slight modification to the number of output channels.<br><code>头网络:YOLOv7的头网络采用特征金字塔网络(Feature Pyramid network, FPN)架构构建，采用PANet设计。该网络由多个卷积、批量归一化和SiLU激活(CBS)块组成，同时引入了空间金字塔池化和卷积空间金字塔池化(Sppcspc)结构、扩展的高效层聚合网络(E-ELAN)和MaxPool-2 (MP2)。Sppcspc结构通过在空间金字塔池化(SPP)结构中融入卷积空间金字塔(CSP)结构来改善网络的感知场，并使用较大的残差边缘来辅助优化和特征提取。在E-ELAN的基础上融合多个特征层，进一步增强特征提取能力。MP2块与MP1块具有相似的结构，对输出通道的数量略有修改。</code></p><h1 id="GPT解读"><a href="#GPT解读" class="headerlink" title="GPT解读"></a>GPT解读</h1><p>这篇文章的核心内容是关于一种改进的YOLOv7网络（YOLOv7-AC）用于水下目标检测的研究。以下是文章的主要点：</p><ol><li><strong>研究背景</strong>：水下目标检测对于海洋探索至关重要，但传统的水下目标检测方法面临着特征提取不准确、检测速度慢和在复杂水下环境中缺乏鲁棒性等挑战。</li><li><strong>改进的YOLOv7网络（YOLOv7-AC）</strong>：为了解决这些限制，研究提出了一种改进的YOLOv7网络。这个网络通过以下方式进行优化：<ul><li>使用ACmixBlock模块替换E-ELAN结构中的3x3卷积块。</li><li>在ACmixBlock模块之间加入跳跃连接和1x1卷积架构以提高特征提取和网络推理速度。</li><li>设计ResNet-ACmix模块以避免特征信息丢失并减少计算量。</li><li>在模型的骨干和头部部分插入全局注意力机制（GAM）以提高特征提取。</li><li>使用K-means++算法代替K-means来获取锚框，以提高模型精度。</li></ul></li><li><strong>实验结果</strong>：改进的YOLOv7网络在URPC数据集和Brackish数据集上分别实现了89.6%和97.4%的平均精度（mAP），并且与原始YOLOv7模型相比，帧率（FPS）更高。</li><li><strong>研究贡献</strong>：文章提出了以下创新点：<ul><li>引入全局注意力机制（GAM）以更有效地捕获特征。</li><li>引入ACmix（结合自注意力和卷积的优点）。</li><li>设计ResNet-ACmix模块以增强骨干网络的特征提取能力。</li><li>优化YOLOv7网络的E-ELAN模块，通过引入跳跃连接和1x1卷积结构。</li></ul></li><li><strong>实验设置</strong>：详细描述了实验环境、超参数设置、测试数据集以及锚框的优化方法。</li><li><strong>性能比较</strong>：与YOLOv7、YOLOv6、YOLOv5s、SSD等流行的目标检测模型进行了性能比较，证明了YOLOv7-AC模型在水下目标检测中的优越性。</li><li><strong>讨论与结论</strong>：尽管YOLOv7-AC模型在水下目标检测方面表现出色，但在高度复杂的水下环境中仍存在误检和漏检的情况。未来的研究将致力于收集更多样化的水下数据集，并采用图像增强技术来提高水下图像质量，这对于水下目标检测至关重要。</li></ol><p>文章还提供了源代码的公开链接：<a href="https://github.com/NZWANG/YOLOV7-AC%EF%BC%8C%E4%BB%A5%E4%BE%BF%E5%85%B6%E4%BB%96%E7%A0%94%E7%A9%B6%E8%80%85%E5%8F%AF%E4%BB%A5%E8%AE%BF%E9%97%AE%E5%92%8C%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/NZWANG/YOLOV7-AC，以便其他研究者可以访问和使用。</a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> YOLO </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【目标检测】Underwater target detection based on Faster R-CNN and adversarial occlusion network</title>
      <link href="/2024/02/06/mu-biao-jian-ce-underwater-target-detection-based-on-faster-r-cnn-and-adversarial-occlusion-network-1/"/>
      <url>/2024/02/06/mu-biao-jian-ce-underwater-target-detection-based-on-faster-r-cnn-and-adversarial-occlusion-network-1/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><em><strong>Underwater target detection based on Faster R-CNN and adversarial occlusion network</strong></em><br><em><strong>基于Faster R-CNN和对抗性遮挡网络的水下目标检测</strong></em></p><h1 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h1><p>Underwater target detection is an important part of ocean exploration, which has important applications in military and civil fields. Since the underwater environment is complex and changeable and the sample images that can be obtained are limited, this paper proposes a method to add the adversarial occlusion network (AON) to the standard Faster R-CNN detection algorithm which called Faster R-CNN-AON network. The AON network has a competitive relationship with the Faster R-CNN detection network, which learns how to block a given target and make it difficult for the detecting network to classify the blocked target correctly. Faster R-CNN detection network and the AON network compete and learn together, and ultimately enable the detection network to obtain better robustness for underwater seafood. The joint training of Faster R-CNN and the adversarial network can effectively prevent the detection network from overfitting the generated fixed features. The experimental results in this paper show that compared with the standard Faster R-CNN network, the increase of mAP on VOC07 data set is 2.6%, and the increase of mAP on the underwater data set is 4.2%.<br><code>水下目标检测是海洋探测的重要组成部分，在军事和民用领域都有重要的应用。针对水下环境复杂多变，可获取的样本图像有限的特点，提出在标准Faster R-CNN检测算法中加入对抗性遮挡网络（AON）的Faster R-CNN-AON网络。AON网络与Faster R-CNN检测网络存在竞争关系，Faster R-CNN检测网络学习如何阻止给定目标，使检测网络难以正确分类被阻止的目标。更快的R-CNN检测网络和AON网络一起竞争和学习，最终使检测网络对水下海鲜获得更好的鲁棒性。Faster R-CNN和对抗网络的联合训练可以有效防止检测网络过度拟合生成的固定特征。本文的实验结果表明，与标准Faster R-CNN网络相比，VOC 07数据集上的mAP提高了2.6%，水下数据集上的mAP提高了4.2%。</code></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1><p>Oceans make up most of the earth’s total area, and about 71% of the earth’s surface is covered by sea water. In recent years, with the development of science and technology and the continuous increase of human survival needs, the resources on land is overexploited. Many developed countries in the world have turned their attention to marine resources. Nowadays, countries are increasingly competing for maritime territory, which makes the exploration, development and utilization of the ocean more and more important. At present, there are two main ways to obtain underwater information: underwater sonar technology and underwater optical imaging technology. Compared with underwater sonar technology, underwater optical imaging technology has the advantages of intuitive target detection, high imaging resolution and high information content, which is more suitable for target detection in a short distance. In recent years, most of the underwater detection depends on the divers. The long-term diving operation and the complex underwater environment bring a great burden to their health. Therefore, the research on underwater object detection is particularly important.<br><code>海洋占地球总面积的大部分，大约71%的地球表面被海水覆盖。近年来，随着科学技术的发展和人类生存需求的不断增加，陆地上的资源被过度开发。世界上许多发达国家都把目光投向了海洋资源。如今，各国对海洋领土的争夺日益激烈，这使得对海洋的勘探、开发和利用变得越来越重要。目前，水下信息的获取主要有两种方式：水下声纳技术和水下光学成像技术。与水下声纳技术相比，水下光学成像技术具有目标探测直观、成像分辨率高、信息量大等优点，更适合于近距离目标探测。近年来，水下探测大多依靠潜水员进行。长期的潜水作业和复杂的水下环境给他们的健康带来了很大的负担。因此，对水下目标检测的研究就显得尤为重要。</code><br>At present, there have been some different classifications for underwater target detection. In the paper, in order to simplify the analysis, the current underwater target detection methods are classified into two major categories. One is based on traditional target detection method, and the other is based on deep learning target detection method.<br><code>目前，水下目标检测技术已经有了不同的分类。为了简化分析，本文将现有的水下目标检测方法分为两大类。一种是基于传统的目标检测方法，另一种是基于深度学习的目标检测方法。</code><br>For optical image, the traditional method is based on pixel and feature. For sonar image, echo based method is a proper way to deal with target detection problem. The principle of pixel-based detection method is to detect the target by comparing whether the gray level of each pixel exceeds a certain threshold value. It includes two aspects. One is constant false alarm rate (CFAR) algorithm (Kalyan and Balasuriya, 2004) which includes cell averaging-constant false alarm rate (CA-CFAR) algorithm (Aalo et al., 2015), order statisticsconstant false alarm rate (OS-CFAR) algorithm (Villar et al., 2015), accumulated cell averaging-constant false alarm rate (ACA-CFAR) algorithm (Tanuja, 2016) and accumulated cell averaging-constant false alarm rate 2-D(ACA-CFAR 2-D) algorithm (Acosta and Villar, 2015). Another is change detection algorithm which includes image difference algorithm (Radke et al., 2005) and bitemporal change detection algorithm (Wei and Leung, 2012). The limitation of pixel-based detection method is that it needs a priori hypothesis because of its slow operation speed, and the detection ability declines when there is multi-target interference. The principle of feature-based detection method is to determine the region of interest by detecting an obvious feature of underwater target. It includes two aspects: one is symbol analysis algorithm, another is change detection algorithm. The limitation of feature-based detection method lies in the large amount of calculation, the difficulty of geometric modeling, and the need to match different features one by one through the model. The principle of echo-based detection method is that the echo signal is different for the material, shape, size, and the like of the underwater target, and a useful echo signal is extracted from the interference signal for detection. Beijbom et al. (2012) proposed a coral coverage method through a number of different scales by color descriptors and texture features. Chuang et al. (2016) estimated the significance of extracted underwater fish texture feature images through phase Fourier transform (Chuang et al., 2016). Zhu et al. (2016) proposed an underwater detection method for feature area fusion. The limitation of the echo detection method is that there is no accurate physical model, the false alarm rate is high and the threshold selection is difficult. And for the severe quality degradation due to light absorption and scattering in water, image enhancement and image restoration is also a useful tool to improve the underwater target detection, and a system review about image enhancement and image restoration is presented by Wang et al. (2019).<br><code>对于光学图像，传统的方法是基于像素和特征的。对于声纳图像，基于回波的目标检测方法是一种较好的目标检测方法。基于像素的检测方法的原理是通过比较每个像素的灰度值是否超过一定的阈值来检测目标。它包括两个方面。一种是恒定虚警率（CFAR）算法（Kalyan和Balasuriya，2004），其包括小区平均恒定虚警率（CA-CFAR）算法（Aalo等人，2015）、阶矩恒虚警率（OS-CFAR）算法（Villar等人，2015）、累积单元平均恒定虚警率（ACA-CFAR）算法（Tanuja，2016）和累积单元平均恒定虚警率2-D（ACA-CFAR 2-D）算法（Acosta和Villar，2015）。另一种是变化检测算法，其包括图像差分算法（Radke等人，2005）和双时变化检测算法（Wei和Leung，2012）。基于像素的检测方法的局限性在于运算速度慢，需要先验假设，且存在多目标干扰时检测能力下降。基于特征的检测方法的原理是通过检测水下目标的一个明显特征来确定感兴趣区域。它包括两个方面：一是符号分析算法，二是变化检测算法。基于特征的检测方法的局限性在于计算量大，几何建模困难，需要通过模型逐个匹配不同的特征。基于回波的探测方法的原理是，对于水下目标的材质、形状、大小等，回波信号是不同的，从干扰信号中提取有用的回波信号进行探测。Beijbom等人（2012年）提出了一种珊瑚覆盖方法，通过颜色描述符和纹理特征通过许多不同的尺度。Chuang等人（2016）通过相位傅里叶变换估计了提取的水下鱼类纹理特征图像的重要性（Chuang等人，2016年）。Zhu et al.（2016）提出了一种特征区域融合的水下检测方法。回波检测方法的局限性在于没有精确的物理模型，虚警率高，阈值选取困难。由于水中光的吸收和散射会导致图像质量严重下降，图像增强和图像恢复也是改善水下目标检测的有用工具，Wang等人（2019）对图像增强和图像恢复进行了系统综述。</code><br>Compared with traditional target detection methods, the target detection method based on deep learning significantly improves the accuracy of target detection. For sonar data set with small valid sample and low Signal-to-Noise Ratios (SNR), Kong et al. (2020) proposed an improved YOLOv3 algorithm for real-time detection called as YOLOv3DPFIN to accomplish the accurate detection of noise-intensive multicategory sonar targets with minimum time consumption. Moniruzzaman et al. (2017) presented a survey for underwater marine object detection based on deep learning while the analysis approaches are categorized according to the object of detection. Lee et al. (2019) reported the empirical evaluation using deep learning based object detection method via style-transferred underwater sonar images. Sung et al. (2019) proposed a detection and removal of crosstalk noise method using a convolutional neural network in the images of forward scan sonar. According to whether candidate regions need to extracts, target detection algorithms based on convolution neural network can divide into two categories: target detection algorithm based on candidate regions and target detection algorithm based on regression (Shen et al., 2017).<br><code>与传统的目标检测方法相比，基于深度学习的目标检测方法显著提高了目标检测的准确性。针对小有效样本和低信噪比的声纳数据集，Kong等（2020）提出了一种改进的YOLOv3实时检测算法YOLOv3DPFIN，以最小的时间消耗实现对噪声密集型多类别声纳目标的准确检测。Moniruzzaman等人（2017）提出了一项基于深度学习的水下海洋物体检测调查，而分析方法根据检测对象进行分类。Lee et al.（2019）报告了使用基于深度学习的对象检测方法通过风格转换的水下声纳图像进行的经验评估。Sung等人（2019）提出了一种在前向扫描声纳图像中使用卷积神经网络检测和去除串扰噪声的方法。根据是否需要提取候选区域，基于卷积神经网络的目标检测算法可以分为两类：基于候选区域的目标检测算法和基于回归的目标检测算法（Shen et al.，2017年）。</code><br>The target detection algorithm based on candidate regions first extracts candidate regions from the whole image, then classifies and regresses the candidate regions to obtain the detection results. Girshick et al. (2014) proposes a method combining candidate regions with convolution neural network: R-CNN. R-CNN has obvious improvement in detection effect, but there is a large number of repeated calculations. Therefore, Girshick (2015) improve a more efficient deep convolutional network: Fast R-CNN, which improves the efficiency of detection. Ren et al. (2017) further proposes Region Proposal Network(RPN), which can realize the sharing of convolution features and reduce the resource consumption of Network training. On the basis of the above algorithm, the improved network, such as Mask R-CNN (He et al., 2017), R-FCN (Dai et al., 2016), has also achieve good detection results.<br><code>基于候选区域的目标检测算法首先从整幅图像中提取候选区域，然后对候选区域进行分类和回归，得到检测结果。Girshick等人（2014）提出了一种将候选区域与卷积神经网络相结合的方法：R-CNN。R-CNN在检测效果上有明显的提升，但存在大量的重复计算。因此，Girshick（2015）改进了一种更有效的深度卷积网络：Fast R-CNN，提高了检测效率。Ren等人（2017）进一步提出了区域建议网络（RPN），可以实现卷积特征的共享，减少网络训练的资源消耗。在上述算法的基础上，改进的网络，如Mask R-CNN（He et al.，2017）、R-FCN（Dai等人，2016年），也取得了良好的检测效果。</code><br>The target detection method based on regression eliminates the operation of candidate region extraction and is an end-to-end target detection algorithm, such as YOLO (Redmon et al., 2016), SSD (Liu et al., 2016), DSSD (Fu et al., 2017) etc. These methods treat target detection as a regression problem and directly use neural network to detect and locate targets from the whole image. This kind of detection method is faster, but the detection effect is slightly inferior.<br><code>基于回归的目标检测方法省去了候选区域提取的操作，是一种端到端的目标检测算法，如YOLO（雷德蒙et al.，2016）、SSD（Liu等人，2016）、DSSD（Fu等人，这些方法将目标检测视为回归问题，直接使用神经网络从整个图像中检测和定位目标。这种检测方法速度较快，但检测效果稍差。</code><br>The current mainstream deep learning target detection algorithm mainly focuses on solving the problem of land scene (Sun et al., 2018). In this paper, the target detection algorithm based on convolutional neural network is introduced into the underwater scene for underwater target detection. Considering the detection speed and accuracy of the algorithm, this paper chooses Faster R-CNN as the basic detection network.<br><code>目前主流的深度学习目标检测算法主要集中在解决陆地场景的问题（Sun et al.2018年）。本文将基于卷积神经网络的目标检测算法引入到水下场景中进行水下目标检测。考虑到算法的检测速度和准确性，本文选择Faster R-CNN作为基本检测网络。</code><br>However, the underwater environment is complex and changeable, the number of underwater target images can be obtained is limited and cannot cover all scenes. In order to solve this problem, this paper proposes to add the adversarial network to the Faster R-CNN detector for training. The goal of the adversarial network is to generate samples that make it difficult for the target detector to classify. On the one hand, the generated samples contain more scenes and increase training samples. On the other hand, they compete with the detection network to learn from each other to improve the detection ability of the detection network.<br><code>然而，水下环境复杂多变，所能获得的水下目标图像数量有限，无法覆盖所有场景。为了解决这个问题，本文提出将对抗网络添加到Faster R-CNN检测器中进行训练。对抗网络的目标是生成样本，使目标检测器难以进行分类。一方面，生成的样本包含更多的场景，增加了训练样本。另一方面，它们与检测网络竞争，相互学习，以提高检测网络的检测能力。</code></p><h1 id="2-The-proposed-Faster-RCNN-AON-network"><a href="#2-The-proposed-Faster-RCNN-AON-network" class="headerlink" title="2.The proposed Faster-RCNN-AON network"></a>2.The proposed Faster-RCNN-AON network</h1><p>This paper considers a feature called occlusion generated by adversarial networks competing with Faster R-CNN detector. This paper proposes an Adversarial Occlusion Network (AON), which learns how to occlude a given target, making it difficult for Faster R-CNN detectors to classify it correctly. Faster R-CNN can acquire better robustness of target detection by competing and learning together with adversarial networks. This paper combine the Faster R-CNN and adversarial network for training, which can effectively prevent the detector from over-fitting the fixed generated features.<br><code>本文考虑了一种称为遮挡的特征，它是由对抗网络与Faster R-CNN检测器竞争生成的。本文提出了一种对抗性遮挡网络（AON），它学习如何遮挡给定的目标，使得更快的R-CNN检测器很难正确地对其进行分类。更快的R-CNN可以通过与对抗网络的竞争和学习来获得更好的目标检测鲁棒性。本文将Faster R-CNN和对抗网络相结合进行训练，可以有效地防止检测器过度拟合固定生成的特征。</code></p><h2 id="2-1-Overview-of-Faster-R-CNN"><a href="#2-1-Overview-of-Faster-R-CNN" class="headerlink" title="2.1. Overview of Faster R-CNN"></a>2.1. Overview of Faster R-CNN</h2><p><code>2.1.更快的R-CNN概述</code><br>In this section, the basic idea of Faster R-CNN is introduced since it is the basis of our proposed new algorithm for underwater target detection.<br><code>在本节中，介绍了Faster R-CNN的基本思想，因为它是我们提出的水下目标检测新算法的基础。</code><br>Faster R-CNN algorithm is a classic work in the field of target detection. Compared with another target detection algorithm Fast RCNN , Faster R-CNN has slightly improved the accuracy of PASCA VOC 2007 (Everingham et al., 2010) data set and greatly improved the speed of image processing. In the test, Faster R-CNN is 10 times faster than Fast R-CNN, and the image processing speed can basically reach 17 FPS (17 frames per second), which can achieve the capability of quasi-real-time processing.<br><code>更快的R-CNN算法是目标检测领域的经典工作。与另一种目标检测算法Fast RCNN相比，Faster R-CNN略微提高了PASCA VOC 2007的准确性（Everingham等人，2010）数据集，大大提高了图像处理的速度。在测试中，Faster R-CNN比Fast R-CNN快10倍，图像处理速度基本可以达到17 FPS（每秒17帧），可以达到准实时处理的能力。</code><br>The overall structure of Faster R-CNN algorithm is shown in Fig. 1, which can be divided into four parts: VOC data set, feature extractor, RPN (Region Proposal Network) and Fast R-CNN network.<br><code>Faster R-CNN算法的整体结构如图1所示，可以分为四个部分：VOC数据集、特征提取器、RPN（区域建议网络）和Fast R-CNN网络。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-1.png"></p><blockquote><p>Fig. 1. Structure of Faster R-CNN. The feature selector used in this paper is a convolution neural network VGG16 used for large-scale image recognition.<br><code>图1 更快的R-CNN结构。本文采用的特征选择器是一个用于大规模图像识别的卷积神经网络VGG 16。</code></p></blockquote><p><em><strong>PASCAL VOC data set.</strong></em><br><code>PASCAL VOC数据集。</code><br>Data set refers to the image data used to train and test the model, which is processed into the required data format. Currently, the data set commonly used by the Faster R-CNN algorithm is VOC, and store the collected image data in the format of the PASCAL VOC 2007 data set.<br><code>数据集是指用于训练和测试模型的图像数据，这些数据被处理成所需的数据格式。目前，Faster R-CNN算法常用的数据集是VOC，并以PASCAL VOC 2007数据集的格式存储采集的图像数据。</code><br>The VOC data set is used as the input data of Faster R-CNN algorithm, and three values are return as the input of model training after the corresponding input layer processing. These three values are img, bbox and label.<br><code>VOC数据集作为Faster R-CNN算法的输入数据，经过相应的输入层处理后返回三个值作为模型训练的输入。这三个值分别是img、bbox和label。</code><br>Feature Proposal Network. The feature extractor used in this paper is a convolutional neural network VGG16 (Simonyan and Zisserman, 2014) for large-scale image recognition. The whole structure consists of 13 convolutional layers, which are divided into 5 segments according to the pattern of ‘‘2+2+3+3+3”. Each convolutional layer is followed by a maximum pooling layer. In practical application, the more convolutional layers used, the more image features extract, and the better the recognition effect of the network on unknown images.<br><code>特色提案网络。本文中使用的特征提取器是用于大规模图像识别的卷积神经网络VGG16（Simonyan和Zisserman，2014）。整个结构由13个卷积层组成，按照“2 +2+3+3+3”的模式分为5段。每个卷积层后面是最大池化层。在实际应用中，使用的卷积层越多，提取的图像特征就越多，网络对未知图像的识别效果就越好。</code><br>The 13 convolutional layers of VGG16 network can well extract the useful features of the target in the image and obtain the corresponding feature graph, which can be used for the subsequent model training. Here, the maximum pooling layer mainly controls the dimension of feature graph extracted by the convolution layer on the basis of maintaining most features, making the network insensitive to the change of the input image, so as to maintain the scale invariance of the image to the greatest extent.<br><code>VGG16网络的13个卷积层可以很好地提取图像中目标的有用特征，得到相应的特征图，可以用于后续的模型训练。这里，最大池化层主要是在保持大部分特征的基础上，控制卷积层提取的特征图的维数，使网络对输入图像的变化不敏感，从而最大程度地保持图像的尺度不变性。</code></p><p><em><strong>Region Proposal Networks.</strong></em><br><code>区域提案网络。</code><br>RPN is the convolution of network structure, and it can also predict the input images produced by the location of the target candidate box and the probability of the target as part of the real goal. At the same time, in the process of training the network, through the RPN network and Fast R-CNN interval training, network can share the characteristics of convolution, which greatly reduces the amount of parameters need in training and improves the training efficiency. The core contribution of Faster R-CNN is to propose an RPN network instead of the traditional Selective Search method, which reduces the time overhead of candidate region extraction to almost zero (from 2 s to 0.01 s).<br><code>RPN是网络结构的卷积，它还可以预测输入图像产生的目标候选框的位置和目标作为真实的目标的概率部分。同时，在训练网络的过程中，通过RPN网络和Fast R-CNN区间训练，网络可以共享卷积的特性，大大减少了训练时需要的参数量，提高了训练效率。Faster R-CNN的核心贡献是提出了一种RPN网络，而不是传统的选择性搜索方法，将候选区域提取的时间开销降低到几乎为零（从2 s到0.01 s）。</code></p><p><em><strong>Fast R-CNN.</strong></em><br>Fast R-CNN is responsible for classifying the region of interest and fine-tuning the location border, judging whether the region of interest identified by RPN contains the target and the category of the target, and modifying the location coordinates of the border. RPN only gives 2000 candidate boxes. The Fast R-CNN network needs to continue the classification and position parameter regression on the given 2000 candidate boxes.<br><code>Fast R-CNN负责对感兴趣区域进行分类并微调位置边界，判断RPN识别的感兴趣区域是否包含目标以及目标的类别，并修改边界的位置坐标。RPN仅提供2000个候选框。Fast R-CNN网络需要继续对给定的2000个候选框进行分类和位置参数回归。</code></p><p><em><strong>2.2. Adversarial occlusion network</strong></em><br><code>2.2.对抗性遮挡网络</code><br>Compared with the direct occlusion on the input image to generate the adversarial sample images (or pixel), this paper finds that the operation on the feature map is more efficient. Because the direct generation of counter sample images (or pixels) not only generates a large amount of additional data storage, but also increases the time of network training and reduces efficiency. Therefore, this paper designs an adversarial network to modify features, making the target more difficult to detect.<br><code>与直接在输入图像上进行遮挡以生成对抗样本图像（或像素）相比，本文发现在特征图上的操作效率更高。因为直接生成反样本图像（或像素）不仅会产生大量的额外数据存储，而且会增加网络训练的时间，降低效率。因此，本文设计了一个对抗网络来修改特征，使得目标更难被检测。</code><br>This paper proposes an adversarial occlusion network(AON) to generate occlusion based on the foreground target feature map. In the standard Faster R-CNN network, this paper takes the convolution features of each target candidate region from the output of ROI-pooling layer as the input of the adversarial network. For each feature of the target, AON can attempt to generate an occlusion mask indicating which parts of the feature map is obscured (that is the feature can clear to zero) so that the detector cannot correctly identify the target.<br><code>本文提出了一种对抗性遮挡网络（AON）的基础上产生的前景目标特征图的遮挡。在标准Faster R-CNN网络中，本文将ROI池层输出的每个目标候选区域的卷积特征作为对抗网络的输入。对于目标的每个特征，AON可以尝试生成遮挡掩模，该遮挡掩模指示特征图的哪些部分被遮挡（即特征可以被清除到零），使得检测器不能正确地识别目标。</code><br>This paper use the standard Faster R-CNN as the backbone network structure. This paper also use the ImageNet (Deng et al., 2009) pretrained model to initialize our network. Adversarial network and Faster R-CNN detector share convolution layer and ROI-pooling layer, and then use their own independent full connection layer. Moreover, this paper does not share the parameters of Faster R-CNN in adversarial networks.<br><code>本文采用标准的Faster R-CNN作为骨干网络结构。本文还使用了ImageNet（Deng et al.，2009）预训练模型来初始化我们的网络。对抗网络和Faster R-CNN检测器共享卷积层和ROI池化层，然后使用自己独立的全连接层。此外，本文没有在对抗网络中共享Faster R-CNN的参数。</code><br>In the experiment, this paper first trained the Faster R-CNN detector alone and iterated it for 20k times without joining the adversarial network. On the premise of the model which can identify the target preliminarily, all the parameters of fixed layers are used to train the adversarial network model.<br><code>在实验中，本文首先单独训练了Faster R-CNN检测器，并在不加入对抗网络的情况下迭代了20 k次。在模型能够初步识别目标的前提下，利用固定层的所有参数训练对抗网络模型。</code><br>In Fig. 2, the dotted line part is the adversarial occlusion network proposed in this paper. The overall network framework is combination of the standard Faster R-CNN network and the adversarial occlusion network, which eventually forms the Faster-RCNN-AON network. The training steps of the algorithm in this paper mainly consist of three stages. The first stage is to train the Faster R-CNN network separately for some iterations (10K for example) to obtain a model that can preliminarily identify seafood. In the second stage, the model obtained in the first stage is used to train the adversarial occlusion network and obtain the pre-training model of the anti-occlusion network. The third stage is to make a parameter copy of the model obtained in the first two stages to initialize and train the joint training model. Fig. 3 is the training flow chart of the algorithm in this paper.<br><code>图2中虚线部分是本文提出的对抗性遮挡网络。整个网络框架是标准Faster R-CNN网络和对抗性遮挡网络的组合，最终形成Faster-RCNN-AON网络。本文算法的训练步骤主要包括三个阶段。第一阶段是单独训练Faster R-CNN网络进行一些迭代（例如10 K），以获得可以初步识别海鲜的模型。在第二阶段中，使用第一阶段中获得的模型来训练对抗性遮挡网络，并获得抗遮挡网络的预训练模型。第三阶段是对前两个阶段得到的模型进行参数复制，以初始化和训练联合训练模型。图3是本文算法的训练流程图。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-2.png"></p><blockquote><p>Fig. 2. The structure of Faster-RCNN-AON network. The Adversarial Occlusion Network (AON) uses the output of the ROI pooling layer as its input. AON network predicts the occlusion mask, and then uses it to mask the eigenvalue of the corresponding part on the feature map and passes it to Faster R-CNN for classification.<br><code>图二 Faster-RCNN-AON网络的结构。对抗性遮挡网络（AON）使用ROI池化层的输出作为其输入。AON网络预测遮挡掩模，然后用它来掩盖特征图上对应部分的特征值，并将其传递给Faster R-CNN进行分类。</code></p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-3.png"></p><blockquote><p>Fig. 3. Training flow chart of the proposed algorithm.<br><code>图三.给出了该算法的训练流程图。</code></p></blockquote><h2 id="2-3-Loss-function"><a href="#2-3-Loss-function" class="headerlink" title="2.3. Loss function"></a>2.3. Loss function</h2><p><code>2.3.损失函数</code><br>The loss function of the standard Faster R-CNN network can be expressed as:<br><code>标准Faster R-CNN网络的损失函数可以表示为：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-4.png"><br>Here, 𝑖 is the index of an anchor in a mini-batch $𝑃<em>𝑖$ is the prediction probability of anchor 𝑖 as the target<br><code>这里，是小批量中的锚的索引，是作为目标的锚的预测概率</code>$𝑃_𝑖$<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-5.png"><br>The classification loss function is expressed as<br><code>分类损失函数表示为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-6.png"><br>$𝑡_𝑖$ = {$𝑡_𝑥$, $𝑡_𝑦$, $𝑡_𝑤$, $𝑡_ℎ$} is a vector, represents the offset predicted by this anchor. 𝑡∗ 𝑖 is a vector with the same dimension as 𝑡𝑖, represents the actual offset of anchor relative to the ground-truth box.<br>$𝑡_𝑖$ = {$𝑡_𝑥$, $𝑡_𝑦$, $𝑡_𝑤$, $𝑡_ℎ$} <code>是一个向量，表示该锚预测的偏移量。𝑡𝑖 是一个维度与 𝑡𝑖 相同的向量，表示锚点相对于地面实况箱的实际偏移量。</code><br>The bbox regression loss function can be expressed as<br><code>bbox回归损失函数可以表示为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-7.png"><br>where 𝑅 is the $𝑠𝑚𝑜𝑜𝑡ℎ</em>{𝐿𝑖}$ function<br><code>其中，R是函数</code>$𝑠𝑚𝑜𝑜𝑡ℎ_{𝐿𝑖}$<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-8.png"><br>This paper assumes that the original target detector network is represented as 𝑌(𝑋) and the adversarial network is identified as 𝐴(𝑋), where 𝑋 represents 𝐴 candidate region. Assume 𝐶 is the true category of 𝑋. The loss function of the adversarial network can be expressed as:<br><code>本文假设原始目标检测器网络表示为𝑌(𝑋)，对抗网络表示为𝐴(𝑋)，其中X表示A候选区域。假设C是的真范畴X。对抗网络的损失函数可以表示为：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-9.png"><br>The loss function of the target detector remains unchanged. Now the mini-batch contains not only fewer original samples but also some generated adversarial samples. On the one hand, if the adversarial samples generated by the adversarial network can be easily identified and classified by the target detector, the adversarial network can get a higher loss. On the other hand, if the adversarial samples generated by the adversarial network are difficult to identify and classify for the target detector, the adversarial network can get a lower loss, while the target detector can get a higher loss.<br><code>目标检测器的损失函数保持不变。现在，小批量不仅包含更少的原始样本，还包含一些生成的对抗样本。一方面，如果由对抗网络生成的对抗样本能够被目标检测器容易地识别和分类，则对抗网络可以获得更高的损失。另一方面，如果对抗网络生成的对抗样本对于目标检测器来说是难以识别和分类的，那么对抗网络可以得到较低的损失，而目标检测器可以得到较高的损失。</code></p><h2 id="2-4-Joint-training"><a href="#2-4-Joint-training" class="headerlink" title="2.4. Joint training"></a>2.4. Joint training</h2><p><code>2.4.联合训练</code><br>In this paper, we train the adversarial occlusion network and the standard Faster R-CNN detector jointly, and optimize the parameters of the two networks in each training iteration. During the period of forward propagation, for training the Faster R-CNN detector, the adversarial network is first used to generate the blocking mask on the feature map obtained after ROI pooling layer. This paper can obtain binary masks through sampling and use them to delete the values in the corresponding positions on the feature graph after the ROI-pooling layer. Then the modified features are trained forward and calculate the corresponding loss value, and finally conduct end-to-end training on the detector.<br><code>在本文中，我们联合训练对抗性遮挡网络和标准Faster R-CNN检测器，并在每次训练迭代中优化两个网络的参数。在前向传播期间，为了训练Faster R-CNN检测器，对抗网络首先用于在ROI池化层之后获得的特征图上生成块掩码。本文通过采样得到二值掩码，利用二值掩码删除ROI池层后特征图上相应位置的值。然后对修改后的特征进行前向训练并计算相应的损失值，最后对检测器进行端到端训练。</code></p><h1 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h1><p><code>3.实验</code><br>The purpose of this experiment is to verify the effectiveness and superiority of the algorithm based on the combination of Faster R-CNN and adversarial network in underwater target detection. By applying this algorithm to underwater target detection, the work burden is reduced and the efficiency of underwater target detection is improved.<br><code>实验旨在验证Faster R-CNN与对抗性网络相结合的算法在水下目标检测中的有效性和优越性。将该算法应用于水下目标检测中，减少了工作负担，提高了水下目标检测的效率。</code></p><h2 id="3-1-Data-set"><a href="#3-1-Data-set" class="headerlink" title="3.1. Data set"></a>3.1. Data set</h2><p><code>3.1.数据集</code><br>This experiment tests the superiority of our algorithm through two data sets. One is the commonly used open source data set PASAL VOC2007, and the other is the underwater image data set collected by our lab. We collected nearly 8,000 underwater target images from the deep-sea fishing ground using the underwater robot in the laboratory, and the ratio of training set to testing set was 3:2. Targets in each image of the whole data set are labeled manually as shown in Fig. 4, where (a) refers to the tagging of sea cucumber targets, (b) refers to the tagging of sea urchin targets, (c) refers to the tagging of starfish targets, and (d) refers to the tagging of scallop targets. All sample images are processed and stored in the format of PASAL VOC2007 sample set, and then the training set is randomly divided into training set and verification set by using random function (the number ratio is 1:1). Our data set includes four kind seafood: echinus, holothurian, starfish and scallops.<br><code>实验通过两组数据验证了该算法的优越性。一个是常用的开源数据集PASAL VOC 2007，另一个是我们实验室收集的水下图像数据集。我们在实验室使用水下机器人从深海渔场采集了近8,000幅水下目标图像，训练集与测试集的比例为3：2。如图4所示，对整个数据集的每个图像中的目标进行手动标记，其中（a）是指海参目标的标记，（b）是指海胆目标的标记，（c）是指海星目标的标记，以及（d）是指扇贝目标的标记。所有样本图像均以PASAL VOC 2007样本集的格式进行处理和存储，然后利用随机函数将训练集随机分为训练集和验证集（数量比为1：1）。我们的数据集包括四种海产品：海胆，海参，海星和扇贝。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-10.png"></p><blockquote><p>Fig. 4. Manual calibration of seafood.<br><code>见图4。手动校准海鲜。</code></p></blockquote><h2 id="3-2-Evaluation-index"><a href="#3-2-Evaluation-index" class="headerlink" title="3.2. Evaluation index"></a>3.2. Evaluation index</h2><p><code>3.2.评价指标</code><br>In target detection, the mAP (mean Average Precision) is used to measure the detection accuracy, which refers to the average accuracy of multiple categories. AP refers to the average accuracy of a single category, which measures the recognition effect of the model on that category, while mAP measures the recognition effect of the model on all categories. In fact, mAP is the average value of all AP. The value of mAP is between 0–1, and the larger the value indicates the better the recognition accuracy of the model. In this experiment, mAP is used to judge the effect of model recognition.<br><code>在目标检测中，mAP（mean Average Precision）用来衡量检测精度，它是指多个类别的平均精度。AP是指单个类别的平均准确率，衡量模型对该类别的识别效果，而mAP则衡量模型对所有类别的识别效果。mAP是所有AP的平均值。mAP的值在0-1之间，值越大表示模型的识别精度越好。在本实验中，使用mAP来判断模型识别的效果。</code></p><h2 id="3-3-Experimental-result"><a href="#3-3-Experimental-result" class="headerlink" title="3.3. Experimental result"></a>3.3. Experimental result</h2><p><code>3.3.实验结果</code><br>The server operating system of this test is Ubuntu18.04, CUDA 10.0, CUDNN 7.0, OpenCV 2.4.0, while the hardware includes Intel Core I78700 CPU (3.2 GHz), the graphics card Nvidia GeForce RTX 2080 (8 GB memory), 32GB RAM. The deep learning framework Caffe (Jia et al., 2014) is built on the server, and the running environment of Faster R-CNN is configured. Then the calibrated sample data set is trained by Faster R-CNN and the adversarial network. The basic feature extraction network is based on VGG16 (Simonyan and Zisserman, 2014) network, and the model is trained by joint training. In the first 60k iterations, the learning rate was set to 0.001, in the 60k to 80k iterations, the learning rate was reduced to 0.0001, and stopped training after 80k iterations.<br><code>本次测试的服务器操作系统为Ubuntu 18.04，CUDA 10.0，CUDNN 7.0，OpenCV 2.4.0，硬件包括Intel Core I78700 CPU（3.2 GHz），显卡Nvidia GeForce RTX 2080（8 GB内存），32 GB RAM。深度学习框架Caffe（Jia et al.，2014）构建在服务器上，配置Faster R-CNN的运行环境。然后用Faster R-CNN和对抗网络训练校准后的样本数据集。基本特征提取网络基于VGG 16（Simonyan和Zisserman，2014）网络，模型通过联合训练进行训练。在前60 k次迭代中，学习率设置为0.001，在60 k到80 k次迭代中，学习率降低到0.0001，并在80 k次迭代后停止训练。</code><br>The test results on the open source data set PASCAL VOC2007 and on underwater data set are shown in Tables 1 and 2. In order to show the efficiency of the detection result via the proposed method, two benchmark method is added to conduct contrast experiment. FRCN(ASDN) refers to A-Fast-RCNN (Wang et al., 2017) with our training environment, and Faster R-CNN refers the work (Ren et al., 2017).<br><code>在开放源数据集PASCAL VOC 2007和水下数据集上的测试结果如表1和表2所示。为了验证该方法检测结果的有效性，增加了两种基准方法进行对比实验。FRCN（ASDN）是指A-Fast-RCNN（Wang等人，2017）与我们的训练环境，更快的R-CNN指的是工作（任等人，2017年）。</code><br>It can be seen from Table 1 that in the detection results of PASCAL VOC2007, the method proposed in this paper (72.5% mAP) is 2.6% higher than Faster R-CNN (69.9% mAP) and 1.7% higher than FRCN (ASDN) (70.8% mAP), indicating that the method proposed in this paper has a certain improvement in the detection effect. Table 2 shows that in the detection results of underwater data set, the method proposed in this paper (72.1% mAP) is 4.2% higher than Faster R-CNN (67.9% mAP) and 6.4% higher than FRCN (ASDN) (65.7% mAP). It can be seen from Table 2 that the mAP of the three methods has decreased in the detection results of the underwater data set. The reason is that the underwater light is uneven and environment is complex and diverse, which leads to the imaging effect not as good as that on land, and the number of images obtained under the water is limited, which affects the detection effect. It can be seen that image quality has the greatest impact on the detection of FRCN (ASDN). However, the influence on this method in this paper is not obvious, which fully reflects the superiority of the method in this paper.<br><code>从表1可以看出，在PASCAL VOC 2007的检测结果中，本文提出的方法（72.5%mAP）比Faster R-CNN（69.9%mAP）高2.6%，比FRCN（ASDN）（70.8%mAP）高1.7%，说明本文提出的方法在检测效果上有一定的提升。表2显示，在水下数据集的检测结果中，本文提出的方法（72.1%mAP）比Faster R-CNN（67.9%mAP）高4.2%，比FRCN（ASDN）（65.7%mAP）高6.4%。从表2中可以看出，三种方法的mAP在水下数据集的检测结果中均有所下降。原因是水下光线不均匀，环境复杂多样，导致成像效果不如陆地，水下获取的图像数量有限，影响了探测效果。可以看出，图像质量对FRCN（ASDN）的检测影响最大。但对本文方法的影响并不明显，充分体现了本文方法的优越性。</code><br>As can be seen from Figs. 5 and 6, the method proposed in this paper performs very well in the detection of multi-category objects and singlecategory objects in the four scenarios of near distance and clear, near distance but blur, a distance and clear and a distance but blur. In all the above results, the detection threshold set in this paper is 0.8. Only when the target detection score exceeds or equals to 0.8, the detection accuracy and target box of the target can be displayed correctly in the detection result. However, if the detection score of the target is lower than 0.8, the category of the target and the detection box cannot display in the detection results. This is often referred to as missed detection. In our detection results, all objects and targets in the image are detected accurately and correctly, and there are few missed objects.<br><code>从图5和6中可以看出，本文提出的方法在近距离清晰、近距离模糊、远距离清晰和远距离模糊四种情况下对多类目标和多类目标的检测都有很好的效果。在上述所有结果中，本文设定的检测阈值为0.8。只有当目标检测得分大于等于0.8时，检测结果中才能正确显示目标的检测精度和目标框。但是，如果目标的检测分数低于0.8，则目标的类别和检测框不能显示在检测结果中。这通常被称为错过检测。在我们的检测结果中，图像中的所有物体和目标都被准确正确地检测出来，几乎没有遗漏的物体。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-11.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-12.png"></p><blockquote><p>Fig. 5. The detection results of underwater multi class object. (a), (b) represents the situation of multiple targets at close range and clear. (c), (d) represents the situation of multiple targets at close but fuzzy. (e), (f) represents the situation of multiple targets at a distance and clear. (g) and (h) represents the situation of multiple targets at a distance but fuzzy.<br><code>图五 水下多类目标的检测结果。(a)、（B）表示近距离和清晰的多个目标的情况。(c)（d）表示多个目标接近但模糊的情况。(e)，（f）表示多个目标在一定距离和清晰的情况。(g)以及（h）表示在一定距离处但模糊的多个目标的情况。</code></p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-13.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-14.png"></p><blockquote><p>Fig. 6. The detection results of underwater single class object. (a), (b) represents the situation of single class target at close range and clear. (c), (d) represents the situation of single class target at close but fuzzy. (e), (f) represents the situation of single class target at a distance and clear. (g) and (h) represents the situation of single class target at a distance but fuzzy.<br><code>见图6 水下单类目标的检测结果。(a)、（B）表示近距离、清晰的单类目标情况。(c)（d）表示近距离但模糊的单类目标的情况。(e)、（f）表示单类目标在远距离且清晰的情况。(g)（h）表示单类目标在一定距离上的模糊情况。</code></p></blockquote><p>In order to give an intuitive understanding of the detection result, two comparison study under different scenarios is conducted, as shown in Figs. 7 and 8. Fig. 7 is the detection result of single class target in a blurred scene. Fig. 8 is the detection result of multiple class in a blurred scene. (a) is the original picture, (b) is the detection result of FRCN network training for 40,000 times. (c) is the detection result of the model after 40,000 times of training using the Faster R-CNN network. (d) is the detection result of the model via the proposed Faster R-CNNAON network training for 40,000 times. From the simulation result, it is quite clear that our proposed method has a higher detection result in most of the scenarios. Some seafood cannot be detected with FRCN method while Faster R-CNN has a close performance but still worse than our proposed method.<br><code>为了直观地了解检测结果，进行了两种不同场景下的对比研究，如图7和图8所示。图7为模糊场景下的单类目标检测结果。图8为模糊场景下的多类检测结果。(a)为原始图片，(b)为FRCN网络训练40000次后的检测结果。(c)为使用Faster R-CNN网络训练40000次后的模型检测结果。(d)是通过提出的Faster R-CNNAON网络训练40000次后的模型检测结果。从仿真结果可以看出，我们的方法在大多数场景下都有较高的检测结果。FRCN方法不能检测某些海鲜，而Faster R-CNN方法与FRCN方法有相近的性能，但仍低于所提出的方法。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-15.png"></p><blockquote><p>Fig. 7. Detection result of single class target in a blurred scene.<br><code>图7 模糊场景中单类目标的检测结果。</code></p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/4-16.png"></p><blockquote><p>Fig. 8. Detection result of multiple class target in a blurred scene.<br><code>图8 模糊场景中多类目标检测结果。</code></p></blockquote><p>From the above simulation experiment, it can be concluded that the detection result with our proposed Faster-RCNN-AON method is superior to the compared two benchmark methods. It is more obvious for the underwater target detection since the underwater image is greatly influenced by light and turbidity, etc. Due to the character of AON network, it can generate images that is difficult for the network to detect which is quite applicable for the underwater case. Therefore, the detection rate with our method is still quite higher than other two methods without significant decrease compared with VOC 2007 set. Generally speaking, the method in this paper has a good detection effect in most of the underwater case.<br><code>从上述仿真实验可以得出结论，我们提出的Faster-RCNN-AON方法的检测结果优于所比较的两种基准方法上级。由于水下图像受光照、浊度等因素的影响较大，这一点在水下目标检测中表现得更为明显。由于AON网络的特性，它可以生成网络难以检测的图像，非常适用于水下情况。因此，与VOC 2007集相比，我们的方法的检测率仍然比其他两种方法高得多，而没有显着下降。总的来说，本文的方法在大多数水下情况下都有较好的检测效果。</code></p><h1 id="4-Conclusion"><a href="#4-Conclusion" class="headerlink" title="4. Conclusion"></a>4. Conclusion</h1><p>For underwater target detection, aiming at the problem of insufficient underwater image data, this paper presents an underwater target detection algorithm based on Faster R-CNN and adversarial network. The key idea is to learn an adversary in conjunction with original object detector. This adversary creates different examples of occlusion in training, such that these occlusions make it difficult for original object detector to classify correctly. Instead of generating samples in pixel space, our adversarial network modifies the features to simulate occlusion. Experiments show that the method is effective and superior in underwater target detection. Compared with some up-to-date well-known methods, this method has a significant improvement in underwater target detection. By joining the adversarial network on the standard Faster R-CNN detection network for joint training, we not only increases the number of training samples but also improves the detection capability of the detection network.<br><code>在水下目标检测方面，针对水下图像数据不足的问题，提出了一种基于Faster R-CNN和对抗网络的水下目标检测算法。其关键思想是结合原始对象检测器来学习对手。这个对手在训练中创建了不同的遮挡示例，使得这些遮挡使得原始对象检测器难以正确分类。我们的对抗网络不是在像素空间中生成样本，而是修改特征来模拟遮挡。实验表明，该方法在水下目标检测中是有效的，具有上级的优越性。与目前一些著名的方法相比，该方法在水下目标检测方面有明显的改善。通过在标准Faster R-CNN检测网络上加入对抗网络进行联合训练，我们不仅增加了训练样本的数量，还提高了检测网络的检测能力。</code></p><h1 id="GPT解读"><a href="#GPT解读" class="headerlink" title="GPT解读"></a>GPT解读</h1><p> 这篇文章的核心内容是关于一种基于Faster R-CNN（区域卷积神经网络）和对抗性遮挡网络（Adversarial Occlusion Network, AON）的新型水下目标检测方法。文章的主要贡献和结构如下：</p><ol><li><p><strong>研究背景</strong>：</p><ul><li>水下目标检测对于海洋探索具有重要意义，尤其在军事和民用领域。</li><li>水下环境复杂多变，可用样本图像有限，这给水下目标检测带来了挑战。</li></ul></li><li><p><strong>研究目的</strong>：</p><ul><li>提出一种结合Faster R-CNN和AON的新型网络结构，称为Faster R-CNN-AON网络，以提高水下目标检测的鲁棒性。</li></ul></li><li><p><strong>方法介绍</strong>：</p><ul><li><strong>Faster R-CNN</strong>：作为基础检测网络，使用VGG16作为特征提取器，通过Region Proposal Network (RPN) 提高检测速度。</li><li><strong>AON</strong>：对抗性遮挡网络，与Faster R-CNN检测网络竞争，学习如何遮挡给定目标，使其难以被正确分类。AON通过生成遮挡来增加训练样本的多样性，同时与检测网络共同学习，提高检测网络的鲁棒性。</li></ul></li><li><p><strong>实验结果</strong>：</p><ul><li>在PASCAL VOC2007数据集上，与标准Faster R-CNN相比，mAP（平均精度均值）提高了2.6%。</li><li>在水下数据集上，mAP提高了4.2%。</li></ul></li><li><p><strong>实验方法</strong>：</p><ul><li>使用两个数据集进行测试：PASCAL VOC2007和实验室收集的水下图像数据集。</li><li>使用mAP作为评估指标，衡量模型的检测准确性。</li></ul></li><li><p><strong>结论</strong>：</p><ul><li>提出的Faster R-CNN-AON方法在水下目标检测中表现出色，尤其是在水下图像受光照和浑浊度影响较大的情况下。</li><li>通过联合训练对抗性网络，不仅增加了训练样本数量，还提高了检测网络的检测能力。</li></ul></li></ol><p>文章还详细介绍了Faster R-CNN的工作原理、AON的设计、损失函数的定义以及联合训练的步骤。此外，文章还讨论了实验设置、硬件环境、训练过程和结果分析。</p><h2 id="1-前人的研究方法"><a href="#1-前人的研究方法" class="headerlink" title="1.前人的研究方法"></a>1.前人的研究方法</h2><p>在这篇文章中，提到了前人研究的方法包括基于传统方法和基于深度学习方法的水下目标检测算法。</p><ol><li>基于传统方法的水下目标检测算法：传统方法主要依赖于手工设计的特征提取器和机器学习算法。这些方法通常使用针对水下图像特点设计的特征，如颜色、纹理、形状等，通过与事先定义的模型进行比较，识别和检测水下目标。常见的传统方法包括基于颜色统计的方法、基于纹理分析的方法、基于轮廓提取的方法等。然而，传统方法在处理水下图像的颜色失真、光照变化和水下散射等问题时效果不佳。</li><li>基于深度学习方法的水下目标检测算法：深度学习方法已经在很多计算机视觉任务中取得了显著的成果。在水下目标检测任务中，基于深度学习的方法也被广泛应用。这些方法主要基于卷积神经网络（CNN）和目标检测算法进行水下目标的检测。常见的基于深度学习的水下目标检测算法包括基于Faster R-CNN、YOLO（You Only Look Once）和SSD（Single Shot MultiBox Detector）等。这些方法通过在数据集上进行端到端的训练，可以自动学习图像中的特征和目标的位置信息，从而实现准确的水下目标检测。</li></ol><p>在这篇文章中，作者采用了基于深度学习的水下目标检测方法，具体是基于Faster R-CNN算法。Faster R-CNN是一种常用的目标检测算法，它通过联合训练区域建议网络（RPN）和Fast R-CNN网络实现目标检测。RPN网络负责生成候选框的位置和得分，而Fast R-CNN网络负责对这些候选框进行分类和位置回归。为了增加训练样本的多样性并提高检测鲁棒性，作者还引入了对抗性遮挡网络（AON），用于生成遮挡蒙版，使得检测网络难以正确分类被遮挡的目标。通过联合训练Faster R-CNN和AON网络，最终实现了对水下目标的准确检测。</p><p>总而言之，前人研究的水下目标检测方法主要包括基于传统方法和基于深度学习方法。传统方法依赖于手工设计的特征和机器学习算法，而基于深度学习的方法则通过训练深度神经网络来实现自动学习特征和目标位置信息。在这篇文章中，作者提出了一种基于Faster R-CNN和AON网络的水下目标检测方法，通过引入对抗性网络来增加训练样本的多样性和提高检测的鲁棒性。</p><h2 id="2-作者要解决的问题"><a href="#2-作者要解决的问题" class="headerlink" title="2.作者要解决的问题"></a>2.作者要解决的问题</h2><p>本文旨在解决水下目标检测的问题。水下目标检测是海洋探索中的重要问题，对于军事和民用领域都具有重要的应用价值。然而，由于水下环境复杂多变，难以获取大量的样本图像，因此传统的水下目标检测方法存在一定的局限性。因此，本文提出了一种改进的目标检测算法，将对抗性网络引入标准的Faster R-CNN算法中，提高水下目标检测的准确性和鲁棒性。</p><p>具体而言，本文提出了一种名为Faster R-CNN-AON（Faster R-CNN和对抗性遮挡网络）的目标检测算法。该算法中的对抗性遮挡网络学习如何生成遮挡图像，使得检测器难以正确识别目标。Faster R-CNN检测器和对抗性遮挡网络通过联合训练，相互竞争和学习，从而提高检测网络在水下环境中的鲁棒性和检测能力。</p><p>通过联合训练Faster R-CNN和对抗性遮挡网络，可以有效防止检测网络对生成的固定特征过拟合。实验结果表明，在PASCAL VOC2007数据集和水下数据集上，相比于标准的Faster R-CNN网络，本文提出的方法在检测性能上有明显的提升。在VOC07数据集上，mAP提高了2.6%，在水下数据集上，mAP提高了4.2%。这表明本文所提出的Faster R-CNN-AON算法能够在水下目标检测中取得更好的效果，提高检测准确性，并具备对抗水下环境变化的能力。</p><p>因此，本文旨在解决水下目标检测中的挑战，提出了一种利用对抗性遮挡网络改进Faster R-CNN的方法，通过联合训练两个网络，提高水下目标检测的性能和鲁棒性。这对于海洋探索和应用领域具有重要的实际意义。</p><h2 id="3-作者通过什么样的方法来解决所提出的研究问题"><a href="#3-作者通过什么样的方法来解决所提出的研究问题" class="headerlink" title="3.作者通过什么样的方法来解决所提出的研究问题"></a>3.作者通过什么样的方法来解决所提出的研究问题</h2><p>根据这篇文章，作者提出了一种基于Faster R-CNN和对抗性遮挡网络（AON）相结合的方法来解决水下目标检测的问题。</p><p>首先，作者使用Faster R-CNN作为基础目标检测算法。Faster R-CNN是一种常用的目标检测算法，它通过联合训练区域建议网络（RPN）和Fast R-CNN网络来实现目标检测。RPN网络用于生成候选框的位置和得分，而Fast R-CNN网络负责对这些候选框进行分类和位置回归。</p><p>为了增加训练样本的多样性，并提高检测鲁棒性，作者引入了对抗性遮挡网络（AON）。AON网络与Faster R-CNN检测网络存在竞争关系，它学习如何生成遮挡蒙版，即哪些特征图区域会被遮挡（即特征值被清零），从而使得检测网络难以正确分类被遮挡的目标。AON网络和Faster R-CNN网络进行联合训练，并共享卷积层和ROI池化层，同时使用各自独立的全连接层。</p><p>通过联合训练Faster R-CNN和AON网络，可以有效地防止检测网络过拟合生成的固定特征。实验结果表明，相较于标准的Faster R-CNN网络，在VOC07数据集上的mAP提高了2.6%，在水下数据集上的mAP提高了4.2%。这表明该方法能够提高水下目标检测网络的检测性能和鲁棒性。</p><p>综上所述，作者通过将对抗性遮挡网络引入Faster R-CNN目标检测算法中，实现了水下目标检测的改进。这一方法能够增加训练样本的多样性，并提高检测网络的鲁棒性，从而提升水下目标检测的准确性和性能。</p><h2 id="4-作者通过哪些实验得出来了这些结果"><a href="#4-作者通过哪些实验得出来了这些结果" class="headerlink" title="4.作者通过哪些实验得出来了这些结果"></a>4.作者通过哪些实验得出来了这些结果</h2><p>根据这篇文章，作者通过一系列实验来验证他们所提出的方法在水下目标检测任务中的有效性。下面是他们进行的实验和得出的结果：</p><ol><li>数据集和实验设置：作者使用了PASCAL VOC2007数据集和水下数据集进行实验。他们在Ubuntu 18.04操作系统、CUDA 10.0、CUDNN 7.0和OpenCV 2.4.0的环境下搭建了基于Caffe深度学习框架的运行环境。实验所使用的硬件配置包括Intel Core I7 8700 CPU、Nvidia GeForce RTX 2080显卡和32GB内存。</li><li>模型训练和评估：作者使用提出的Faster R-CNN-AON网络对校准样本数据集进行训练。他们将VGG16网络作为特征提取器，并通过联合训练Faster R-CNN和AON网络来提高检测效果。训练过程中，学习率在不同迭代阶段有不同的设定。训练完成后，作者使用训练好的模型在测试集上进行评估，并计算mAP（mean Average Precision）来衡量检测结果的准确性。</li><li>与基准方法的对比实验：为了证明所提出方法的优越性，作者进行了与两种基准方法的对比实验。一种基准方法是标准的Faster R-CNN网络，另一种是FRCN (ASDN)方法。通过与这两种方法的对比，作者证明了所提出方法在PASCAL VOC2007和水下数据集上的mAP值都有显著的提升。</li><li>检测结果分析：通过实验结果的图示，作者展示了所提出方法在不同情景下对多类别和单一类别对象的检测效果。同时，作者还对不同距离和清晰度的情况下的检测结果进行了分析。他们设置了一个检测阈值来控制检测结果的显示，当目标的检测分数大于等于阈值时，才显示目标的检测精度和边界框。</li></ol><p>通过这些实验，作者验证了所提出的Faster R-CNN-AON方法在水下目标检测任务中的有效性，并与其他基准方法进行了对比分析，从而得出了实验结果。这些结果表明，所提出的方法在PASCAL VOC2007和水下数据集上的检测效果明显优于传统的Faster R-CNN方法和FRCN (ASDN)方法。</p><h2 id="5-实验部分的不足"><a href="#5-实验部分的不足" class="headerlink" title="5.实验部分的不足"></a>5.实验部分的不足</h2><p>根据这篇文章，实验部分存在以下一些不足之处：</p><ol><li>数据集选择较为有限：作者在实验中仅使用了PASCAL VOC2007数据集和水下数据集进行验证。这两个数据集可能无法完全覆盖水下目标检测任务中的各种场景和情况，因此可能存在一定的偏差。为了更全面地评估所提出的方法的有效性，可以考虑使用更多样化、更具代表性的水下数据集进行实验。</li><li>实验环境和硬件配置：虽然作者提到了实验所使用的操作系统、深度学习框架和硬件配置等信息，但并未对其选择进行详细的解释和说明。例如，为何选择了特定的硬件配置和软件环境，是否有其他选择可行等。更详细的实验环境说明可以提供更多的信息，以便他人能够重现实验并对结果进行进一步验证。</li><li>与更多基准方法的对比：作者在实验中仅与标准的Faster R-CNN网络和FRCN (ASDN)方法进行了对比。虽然这两种方法是相关的和常用的基准方法，但与更多的相关工作进行对比可能更有说服力。对比更多的基准方法可以更全面地评估所提出方法的优劣，并更好地揭示其相对优势。</li></ol><p>综合来看，虽然实验部分展示了所提出方法在水下目标检测任务中的有效性，但仍有一些不足之处需要改进和完善。解决这些不足之处可以进一步增强实验的可靠性和说服力。</p><h2 id="6-作者使用该方法的优点"><a href="#6-作者使用该方法的优点" class="headerlink" title="6.作者使用该方法的优点"></a>6.作者使用该方法的优点</h2><p>根据这篇文章，使用作者提出的方法在水下目标检测任务中具有以下优点：</p><ol><li>提高目标检测鲁棒性：通过引入对抗性遮挡网络（AON），该方法可以学习生成遮挡蒙版来遮挡目标的部分特征，从而使得检测网络对被遮挡的目标难以正确分类。这样的对抗学习机制能够提高目标检测网络对复杂和变化的水下环境的适应性，提高了检测的鲁棒性。</li><li>扩展训练样本：AON网络可以生成具有不同遮挡程度的样本，从而增加了训练样本的多样性。通过与Faster R-CNN联合训练，使得检测网络能够学习到更多不同情况下的目标特征，提高了模型的泛化能力。</li><li>防止过拟合：联合训练Faster R-CNN和AON网络可以有效地防止检测网络过拟合生成的固定特征。AON网络与Faster R-CNN网络在训练过程中相互竞争学习，通过动态生成遮挡蒙版，减少了模型对固定特征的依赖，提高了模型的泛化能力。</li><li>提高检测性能：实验结果显示，与标准的Faster R-CNN网络相比，该方法在PASCAL VOC2007数据集上的mAP提高了2.6%，在水下数据集上的mAP提高了4.2%。这表明该方法能够显著提高水下目标检测的准确性和性能。</li></ol><p>综上所述，该方法通过引入对抗性遮挡网络，扩展了训练样本并提高了模型的鲁棒性和泛化能力，从而在水下目标检测任务中表现出优秀的性能。</p><h2 id="7-作者使用该方法的缺点"><a href="#7-作者使用该方法的缺点" class="headerlink" title="7.作者使用该方法的缺点"></a>7.作者使用该方法的缺点</h2><p>根据这篇文章，该方法存在以下一些缺点：</p><ol><li>目标遮挡生成的挑战：尽管作者提出的对抗性遮挡网络（AON）能够学习生成遮挡的蒙版，但是在实际情况下，目标的遮挡可能由于光照变化、水下散射等因素而变得复杂和不确定。因此，生成的遮挡蒙版可能存在不准确或者不理想的情况，这会对目标的检测结果产生一定的影响。</li><li>训练复杂度增加：引入对抗性网络 (AON) 后，模型的训练复杂度会增加。联合训练 Faster R-CNN 和 AON 网络要求增加额外的计算资源和时间，这对于一些资源有限的环境可能带来一定的挑战。</li><li>数据集限制：作者在实验中使用了有限的数据集进行验证，如 PASCAL VOC2007 数据集和水下数据集。这些数据集可能无法完全覆盖所有的水下场景和目标类型，因此对于该方法的鲁棒性和泛化性能在更广泛的数据集上的表现还需要进一步研究。</li><li>参数调优的困难：在训练过程中，联合训练 Faster R-CNN 和 AON 网络需要调优的参数较多，调整好各个参数的取值是一项具有挑战性的任务。可能需要更多的实验和调优才能获得最佳的模型性能。</li></ol><p>综上所述，尽管该方法在水下目标检测中具有一定的优点，但也面临着一些挑战和限制，需要进一步的研究和改进。</p><h2 id="8-论文的创新点主要体现在以下几个方面"><a href="#8-论文的创新点主要体现在以下几个方面" class="headerlink" title="8.论文的创新点主要体现在以下几个方面"></a>8.论文的创新点主要体现在以下几个方面</h2><p>根据这篇文章，论文的创新点主要体现在以下几个方面：</p><ol><li>提出了将对抗性遮挡网络（AON）引入Faster R-CNN目标检测算法的方法。在传统的目标检测算法中，往往只通过增加训练样本数量来提高模型的鲁棒性，而AON网络通过与Faster R-CNN网络的竞争学习，可以生成遮挡蒙版，增加样本的多样性，并提高模型对遮挡目标的检测能力。</li><li>通过联合训练Faster R-CNN和AON网络，提高了模型的泛化能力。传统的目标检测算法往往在具体场景下表现较好，但在其他场景下泛化能力较弱。而通过联合训练，模型可以学习到更多不同情况下的目标特征，提高了模型的泛化能力和鲁棒性。</li><li>实验证明了所提方法在水下目标检测任务中的有效性。通过与基准方法的对比实验证明了所提出的方法相对于传统方法和单独使用Faster R-CNN而言，在水下目标检测任务中具有更好的性能和鲁棒性，取得了较好的检测结果。</li></ol><p>综上所述，该论文的创新点主要在于引入对抗性遮挡网络，并通过联合训练提高模型的泛化能力和鲁棒性，以及在水下目标检测任务中取得了较好的实验结果。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【目标检测】Underwater object detection algorithm based on feature enhancement and progressive dynamic aggregation strategy</title>
      <link href="/2024/02/03/mu-biao-jian-ce-underwater-object-detection-algorithm-based-on-feature-enhancement-and-progressive-dynamic-aggregation-strategy/"/>
      <url>/2024/02/03/mu-biao-jian-ce-underwater-object-detection-algorithm-based-on-feature-enhancement-and-progressive-dynamic-aggregation-strategy/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><em><strong>Underwater object detection algorithm based on feature enhancement and progressive dynamic aggregation strategy</strong></em><br><em><strong>基于特征增强和渐进动态聚合策略的水下目标检测算法</strong></em></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>To solve the problems that the conventional object detector is hard to extract features and miss detection of small objects when detecting underwater objects due to the noise of underwater environment and the scale change of objects, this paper designs a novel feature enhancement &amp; progressive dynamic aggregation strategy, and proposes a new underwater object detector based on YOLOv5s. Firstly, a feature enhancement gating module is designed to selectively suppress or enhance multi-level features and reduce the interference of underwater complex environment noise on feature fusion. Then, the adjacent feature fusion mechanism and dynamic fusion module are designed to dynamically learn fusion weights and perform multi-level feature fusion progressively, so as to suppress the conflict information in multiscale feature fusion and prevent small objects from being submerged by the conflict information. At last, a spatial pyramid pool structure (FMSPP) based on the same size quickly mixed pool layer is proposed, which can make the network obtain stronger description ability of texture and contour features, reduce the parameters, and further improve the generalization ability and classification accuracy. The ablation experiments and multi-method comparison experiments on URPC and DUT-USEG data sets prove the effectiveness of the proposed strategy. Compared with the current mainstream detectors, our detector achieves obvious advantages in detection performance and efficiency.<br><code>针对传统目标检测器在检测水下目标时，由于水下环境噪声和目标尺度变化等原因，存在特征提取困难和小目标漏检的问题，设计了一种新的特征增强渐进动态聚合策略，提出了一种基于YOLOv 5s的水下目标检测器。首先设计了特征增强选通模块，有选择地抑制或增强多层次特征，降低水下复杂环境噪声对特征融合的干扰。然后，设计了相邻特征融合机制和动态融合模块，动态学习融合权值，逐步进行多尺度特征融合，以抑制多尺度特征融合中的冲突信息，防止小目标被冲突信息淹没。最后，提出了一种基于相同尺寸快速混合池层的空间金字塔池结构（FMSPP），使网络获得更强的纹理和轮廓特征描述能力，减少参数，进一步提高泛化能力和分类精度。在URPC和DUT-USEG数据集上的烧蚀实验和多方法对比实验证明了该策略的有效性。与目前主流的检测器相比，该检测器在检测性能和效率上都有明显的优势。</code></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>In recent years, marine information processing technology has developed vigorously, and the application of underwater object detection technology has become more and more extensive, involving naval coastal defense, fishery and aquaculture, salvage of sunken ships on the seabed, research of marine ecosystem and other fields. Underwater optical image has high resolution and abundant information, which is very suitable for close range underwater object detection. The special underwater imaging environment leads to the problems of color distortion, too much interference noise, blurred texture features and low contrast in underwater images [1] . So, how to accurately, quickly and stably detect objects with poor image visibility is a huge challenge [2] .<br><code>近年来，海洋信息处理技术蓬勃发展，水下目标探测技术的应用越来越广泛，涉及到海军海防、渔业养殖、海底沉船打捞、海洋生态系统研究等多个领域。水下光学图像分辨率高、信息量丰富，非常适合近距离水下目标探测。特殊的水下成像环境导致水下图像存在颜色失真、干扰噪声多、纹理特征模糊、对比度低等问题[1]。因此，如何准确、快速、稳定地检测图像可见性差的物体是一个巨大的挑战[2]。</code><br>With the continuous iteration of convolutional neural network, researchers have proposed many underwater object detection methods based on deep learning, which are fast, high-precision and have good generalization performance. Xu et al. [3] proposed an improved ocean object detector based on attention-based spatial pyramid pool network and bidirectional feature fusion strategy to alleviate the problem of feature weakening. Lin et al [4] proposed an underwater image enhancement method to improve the detection accuracy of occluded and blurred objects. Liu et al. [5] constructed a universal underwater object detector using many underwater data sets, which has good environmental adaptability. Zhou et al. [6] proposed a new algorithm by combining important environmental features, and the average recognition rate of this model is 80.07%. Zhangyan et al. [7] proposed an algorithm based on channel attention and feature fusion, which effectively improved the detection accuracy. Li et al. [8] proposed an algorithm by combining channel attention, which improved the robustness of the detector. Wang et al [9] proposed an underwater object edge detection method based on ant colony optimization and reinforcement learning.<br><code>随着卷积神经网络的不断迭代，研究人员提出了许多基于深度学习的水下目标检测方法，这些方法速度快，精度高，泛化性能好。Xu等人[3]提出了一种基于注意力的空间金字塔池网络和双向特征融合策略的改进海洋目标检测器，以缓解特征弱化问题。Lin等人[4]提出了一种水下图像增强方法，以提高遮挡和模糊目标的检测精度。Liu等人[5]利用多个水下数据集构建了一个通用的水下目标探测器，具有良好的环境适应性。Zhou等人[6]提出了一种结合重要环境特征的新算法，该模型的平均识别率为80.07%。Zhangyan等人[7]提出了一种基于通道注意和特征融合的算法，有效提高了检测精度。Li等人[8]提出了一种结合信道注意力的算法，提高了检测器的鲁棒性。Wang等[9]提出了一种基于蚁群优化和强化学习的水下目标边缘检测方法。</code><br>Now, the underwater image object detector still faces two challenges: 1) small objects are fuzzy and tiny 2) The distinction between object and background is low. The underwater object scale is quite different, and there are many small objects in the underwater scene. Due to the existence of suspended matter and uneven illumination in the underwater environment, it is more difficult to detect small objects, as (a) and (b) in Fig. 1 . At present, in the mainstream public data set, the number of small objects is far less than that of large objects, so that the small object has less influence on the loss function, and the direction of network con vergence is constantly tilting towards the larger objects. There are still prominent problems of missed detection and false detection of small objects. The underwater environment is complex, and due to image degradation and natural camouflage, the appearance and physical structure of some underwater objects in images are highly similar to the surrounding environment, as (c) and (d) in Fig. 1 , which is an important challenge for underwater image object detection.<br><code>目前，水下图像目标检测还面临两个挑战：1）小目标的模糊性和微小性; 2）目标与背景的区分度低。水下物体尺度差异较大，水下场景中有许多小物体。由于水下环境中悬浮物的存在和光照不均匀，检测小物体更加困难，如图1中的（a）和（b）。目前，在主流的公共数据集中，小对象的数量远远少于大对象，从而小对象对损失函数的影响较小，网络汇聚的方向不断向大对象倾斜。小目标的漏检、误检问题仍然比较突出。水下环境复杂，由于图像退化和自然伪装，图像中某些水下目标的外观和物理结构与周围环境高度相似，如图1中的（c）和（d），这是水下图像目标检测的重要挑战。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-1.png"></p><blockquote><p>Fig. 1. Two Challenges of Underwater Image Object Detection.<br><code>图1 水下图像目标检测的两个挑战。</code></p></blockquote><p>In summary, this paper proposes a novel underwater object detection method, which specifically improves the detection accuracy by designing the following strategies:<br><code>综上所述，本文提出了一种新颖的水下目标检测方法，具体通过设计以下策略来提高检测精度：</code><br>1)Firstly, a new feature enhancement gating module is designed, which is used to capture the important information of channel, space and global dimensions of features. And, it expands the receptive field of features, selectively enhances or suppresses multi-scale features extracted by feature pyramid network (FPN), reduces the interference of background information on subsequent fusion results, and improves the ability to express features.<br><code>1)首先，设计了一种新的</code><em><strong>特征增强门控模块</strong></em><code>，用于捕获特征的通道、空间和全局维度的重要信息;扩大特征感受野，选择性地增强或抑制特征金字塔网络(feature pyramid network, FPN)提取的多尺度特征，减少背景信息对后续融合结果的干扰，提高特征表达能力。</code><br>2)Then, a feature dynamic fusion module is designed to aggregate the features of adjacent layers to obtain semantic context information, which can establish the relationship between the object scale of the input image and feature fusion, and dynamically learn the fusion weight according to the object scale. Moreover, a simple and effective progressive aggregation strategy is proposed to avoid the loss of small object features caused by hopping feature fusion and realize the progressive aggregation of multi-level features.<br><code>2)然后，设计了</code><em><strong>特征动态融合模块</strong></em><code>，对相邻层的特征进行聚合，获得语义上下文信息，可以建立输入图像的对象尺度与特征融合之间的关系，并根据对象尺度动态学习融合权值。此外，提出了一种简单有效的渐进式聚合策略，避免了跳变特征融合造成的小目标特征丢失，实现了多层次特征的渐进式聚合。</code><br>3)Finally, aiming at the problem that the texture of underwater objects is not clear and the discrimination between underwater objects and the surrounding environment is low, this paper proposes a fast spatial pyramid mixed pooling module (FMSPP) composed of mixed pooling layers of the same size to replace the spatial pyramid pooling (SPP) module composed of maximum pooling layers of different sizes in the original yolov5s model, so that the network can obtain stronger texture and contour feature description ability.<br><code>3)最后，针对水下目标纹理不清晰、水下目标与周围环境区分度低的问题，本文提出了一种由大小相同的混合池化层组成的</code><em><strong>快速空间金字塔混合池化模块</strong></em><code>（FMSPP），以取代原yolov5s模型中由大小不同的最大池化层组成的空间金字塔池化模块（SPP），从而使网络获得更强的纹理和轮廓特征描述能力。</code></p><h1 id="2-Related-works"><a href="#2-Related-works" class="headerlink" title="2. Related works"></a>2. Related works</h1><p><em><strong>Underwater object detection technology .</strong></em><br><code>水下目标探测技术。</code><br>Object detection algorithms based on deep learning are mainly divided into two cate gories: One-stage detection algorithm based on regression thought; Two-stage detection algorithm based on generating candidate regions, first extracts candidate regions from the input image, and then realize object classification and position correction by these candidate regions. Li et al. [10] applied high-precision Fast-R-CNN to detect fish in complex underwater environment. One-stage detection algorithm directly uses convolutional neural network to classify and correct the position of the object from the input image. Compared with double-step detection algorithm, one-stage detection algorithm greatly reduces the calculation cost of the model because it doesn’t need to generate candidate areas before classifying and correcting the position of the object, which meet the needs of rapid underwater object detection. Therefore, many researchers study underwater object detection algorithm based on one-stage detection. Sung et al. [11] proposed a model based on YOLO, the classification accuracy rate reached 93%. Liu Ping et al. [12] based on YOLOv3 algorithm and using GAN (Generative Adversarial Networks) model realized the identification of marine organisms. Chen et al. [2] proposed a method of underwater biological objects in low light based on improved YOLOV5s to solve the problem of low biometric identification accuracy caused by serious light attenuation in underwater, complex image environment and moving shooting equipment.<br><code>基于深度学习的目标检测算法主要分为两类：基于回归思想的一级检测算法;基于候选区域生成的两级检测算法，首先从输入图像中提取候选区域，然后利用这些候选区域实现目标分类和位置校正。Li等人[10]应用高精度Fast-R-CNN在复杂的水下环境中检测鱼类。一级检测算法直接使用卷积神经网络从输入图像中分类并校正目标的位置。与双步检测算法相比，一步检测算法在对目标进行分类和位置校正之前不需要生成候选区域，大大降低了模型的计算开销，满足了快速水下目标检测的需要。因此，许多研究人员研究了基于单阶段检测的水下目标检测算法。Sung等人[11]提出了一种基于YOLO的模型，分类准确率达到93%。刘平等[12]基于YOLOv 3算法，利用GAN（Generative Adversarial Networks）模型实现了海洋生物的识别。Chen等[2]提出了一种基于改进YOLOV 5s的弱光下水下生物目标识别方法，以解决水下光照衰减严重、图像环境复杂、拍摄设备移动等导致的生物特征识别精度低的问题。</code></p><p><em><strong>Multi-scale feature fusion technology .</strong></em><br><code>多尺度特征融合技术。</code><br>Generating discriminative multi-scale features is an effective method to solve the problem of large scale differences and high similarity between classes of objects, which can greatly improve the performance of object detection. However, at present, most fusion methods fuse features of different scales with fixed weights. Feature pyramid (FPN) [13] enhances the semantic information of top-level features through lateral connection and top-down paths. Then, many feature fusion networks are derived from FPN. Liu et al. [14] proposed PANet, which adds bottom-up paths to feature pyramid FPN to fuse features of adjacent scales. Vishnu Chalavadi et al. [15] proposed a novel network for multi-scale object detection in aerial images using hierarchical dilated convolutions, called as mSODANet. Zhao et al. [16] designed a pyramid network of multi-level and multiscale feature fusion, which cascaded and fused the multi-scale features at different stages of the backbone network. It greatly enriches the semantic information of features. Ghiasi et al. [17] used the neural architecture search method to search the optimal feature pyramid structure, which greatly improved the discrimina tion of features, but the neural architecture search required huge GPU resources, which restricted the universality of pyramid network. Xie et al. [18] proposed a dynamic feature fusion network for remote sensing object detection. This network consists of feature gating module and dynamic fusion module, which can realize dynamic fusion of multi-scale features.<br><code>产生具有区分性的多尺度特征是解决目标类间尺度差异大、相似度高的问题的有效方法，可以大大提高目标检测的性能。然而，目前，大多数融合方法融合不同尺度的特征与固定的权重。特征金字塔（FPN）[13]通过横向连接和自顶向下路径增强顶层特征的语义信息。然后，从FPN中衍生出多种特征融合网络。Liu等人。[14]提出了PANet，它将自下而上的路径添加到特征金字塔FPN中，以融合相邻尺度的特征。Vishnu Chalavadi等人[15]提出了一种新的网络，用于使用分层扩张卷积进行航空图像中的多尺度对象检测，称为mSODANet。Zhao等人[16]设计了一种多级多尺度特征融合的金字塔网络，将骨干网络不同阶段的多尺度特征进行级联融合。它极大地丰富了特征的语义信息。Ghiasi等人[17]使用神经架构搜索方法搜索最优特征金字塔结构，大大提高了特征的区分度，但神经架构搜索需要巨大的GPU资源，限制了金字塔网络的通用性。Xie等人[18]提出了一种用于遥感目标检测的动态特征融合网络。该网络由特征选通模块和动态融合模块组成，可实现多尺度特征的动态融合。</code></p><p><em><strong>Data enhancement technology .</strong></em><br><code>数据增强技术。</code><br>Deep learning is a data-based method, so the preprocessing of training data is very important. Common data preprocessing methods include rotation, distortion, random erasure, random occlusion and illumination distortion. Xiao et al. [19] put forward a copy-reduce-paste small object enhancement method to improve the contribution of small objects to the loss function and make the training more balanced.<br><code>深度学习是一种基于数据的方法，因此训练数据的预处理非常重要。常用的数据预处理方法有旋转、畸变、随机擦除、随机遮挡和光照畸变等。Xiao et al. [19]提出了复制-减少-粘贴小对象增强方法，以提高小对象对损失函数的贡献，使训练更加均衡。</code><br>Deep learning is a data-based method, so the preprocessing of training data is very important. Common data preprocessing methods include rotation, distortion, random erasure, random occlusion and illumination distortion. Xiao et al. [19] put forward a copy-reduce-paste small object enhancement method to improve the contribution of small objects to the loss function and make the training more balanced.<br><code>深度学习是一种基于数据的方法，因此训练数据的预处理非常重要。常用的数据预处理方法有旋转、畸变、随机擦除、随机遮挡和光照畸变等。Xiao et al. [19]提出了复制-减少-粘贴小对象增强方法，以提高小对象对损失函数的贡献，使训练更加均衡。</code></p><p><em><strong>YOLOv5s model .</strong></em><br><code>YOLOv5s模型。</code><br>YOLO series algorithms are based on PyTorch framework, which is easy to extend to mobile devices and belongs to lightweight network. YOLOv5 is a model with strong performance and versatility in YOLO series at present, and YOLOv5s is the first choice for lightweight networks, which is easy to deploy to embedded devices [20] . YOLOv5s network adopts OneStage structure, which consists of four parts: Input terminal, Backbone network, Neck network layer and Head output terminal. Input has Mosaic data enhancement, adaptive anchor box computing and adaptive picture scaling functions. Backbone network includes Focus structure, CSP structure and SPP structure of spatial pyramid pool, and features of different levels in the image are extracted by deep convolution operation. Neck network layer consists of FPN and path aggregation network structure (PAN). As the final detection of head, different size objects are predicted on different size feature maps [2] .<br><code>YOLO系列算法基于PyTorch框架，易于扩展到移动的设备，属于轻量级网络。YOLOv5是目前YOLO系列中性能和通用性较强的机型，YOLOv5s是轻量级网络的首选，易于部署到嵌入式设备[20]。YOLOv5s网络采用OneStage结构，由四部分组成：输入端、骨干网、Neck网络层和Head输出端。输入具有马赛克数据增强、自适应锚盒计算和自适应图片缩放功能。骨干网络包括空间金字塔池的Focus结构、CSP结构和SPP结构，通过深度卷积运算提取图像中不同层次的特征。颈部网络层由FPN和路径汇聚网络结构（PAN）组成。作为头部的最终检测，在不同大小的特征图上预测不同大小的对象[2]。</code></p><h1 id="3-Proposed-method"><a href="#3-Proposed-method" class="headerlink" title="3. Proposed method"></a>3. Proposed method</h1><h2 id="3-1-Overall-framework"><a href="#3-1-Overall-framework" class="headerlink" title="3.1. Overall framework"></a>3.1. Overall framework</h2><p><code>3.1.总体框架</code><br>As shown in Fig. 2 , the proposed model takes the lightweight object detection model YOLOv5s as the basic model, and consists of six parts: Input terminal, Backbone network, Neck network layer, feature gating module, feature dynamic fusion and purification module and Prediction output terminal.<br><code>如图2所示，该模型以轻量级目标检测模型YOLOv5s为基础模型，由输入端、骨干网络、颈网络层、特征门控模块、特征动态融合与净化模块、预测输出端六部分组成。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-2.png"></p><blockquote><p>Fig. 2. The framework of our model.<br><code>图二 我们的模型框架。</code></p></blockquote><p>Firstly, the underwater image is corrected by using the image adaptive enhancement module [28] to suppress the environmental noise. Backbone network includes Focus structure, CSP structure and FMSPP structure, and features of different levels in the image are extracted by deep convolution operation. Neck network layer is composed of FPN and path aggregation network (PAN), which extracts multi-scale features from images. The feature enhancement gating module is used to adjust the size of multi-scale features, and then selectively enhance or suppress the adjusted features with the same size. The dynamic feature fusion module establishes the relationship between the input object scale and feature fusion, learns the fusion weight according to the input object scale, and assigns different fusion weights to multi-scale features to realize the dynamic fusion of multi-scale features, and suppress the conflict information after multi-scale feature fusion and prevent small objects from drowning in the conflict information. Prediction, as the final detection output.<br><code>首先，利用图像自适应增强模块[28]对水下图像进行校正，抑制环境噪声。骨干网络包括Focus结构、CSP结构和FMSPP结构，通过深度卷积运算提取图像中不同层次的特征。颈部网络层由FPN和路径聚合网络（PAN）组成，从图像中提取多尺度特征。特征增强选通模块用于调整多尺度特征的大小，然后选择性地增强或抑制具有相同大小的调整后的特征。动态特征融合模块建立输入对象尺度与特征融合之间的关系，根据输入对象尺度学习融合权值，为多尺度特征分配不同的融合权值，实现多尺度特征的动态融合，并抑制多尺度特征融合后的冲突信息，防止小对象淹没在冲突信息中。预测，作为最终检测输出。</code></p><h2 id="3-2-Fast-spatial-mixed-pooling-pyramid"><a href="#3-2-Fast-spatial-mixed-pooling-pyramid" class="headerlink" title="3.2. Fast spatial mixed pooling pyramid"></a>3.2. Fast spatial mixed pooling pyramid</h2><p><code>3.2.快速空间混合池金字塔</code><br>Due to the complicated underwater environment and low visibility, the image texture features are lost, which makes the distinction between underwater objects and the surrounding environment low and the texture unclear. So, as shown in Fig. 3 , this paper proposes a fast spatial mixed pooling pyramid (FMSSP) based on the same size fast mixed pooling layer to replace the spatial pyramid pooling (SPP) module in the original YOLOv5s model, which consists of different sizes of maximum pooling layers.<br><code>由于水下环境复杂，能见度低，图像纹理特征丢失，使得水下目标与周围环境的区分度低，纹理不清晰。因此，如图3所示，本文提出了一种基于相同大小的快速混合池化层的快速空间混合池化金字塔（FMSSP），以取代原始YOLOv5s模型中由不同大小的最大池化层组成的空间金字塔池化（SPP）模块。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-3.png"></p><blockquote><p>Fig. 3. The schematic diagram of Copy-reduce-rotate-paste data enhancement strategy.<br><code>图三 Copy-reduce-rotate-paste数据增强策略示意图。</code></p></blockquote><p>The FMSSP module performs mixed pooling with the scale of 5 × 5 [32] on the input feature map as shown in formula (1).<br><code>FMSSP模块在输入特征图上执行比例为5 × 5 [32]的混合池化，如公式（1）所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-4.png"><br>where λ is a random value of 0 or 1, which means that the max pooling or average pooling is selected. The mixed pooling changes the rules of pool adjustment in a random way, which will solve the problems encountered in maximum pooling and average pooling to some extent [25] .<br><code>其中λ是0或1的随机值，这意味着选择最大池化或平均池化。混合池以随机的方式改变了池的调整规则，这将在一定程度上解决最大池和平均池所遇到的问题[25]。</code><br>Then, channel connection is made to the output of the pool layer in each stage, and the process is shown in formula (2) , the catch used in this paper is the concatenate operation.<br><code>然后，在每一级中对池层的输出进行通道连接，过程如公式（2）所示，本文中使用的catch是concatenate操作。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-5.png"><br>While avoiding the loss of local features, FMSPP can reduce redundant information and retain prominent texture features, so that the network can obtain stronger description ability of texture and contour features, thus reducing the influence caused by the inconspicuous contour and texture features of underwater images. Compared with SPP module, FMSPP module has a faster forward propagation speed, thus alleviating the detection speed loss caused by incorporating feature enhancement gating module [2] . Furthermore, FMSPP can solve the over-fitting problem and improve the classification accuracy through mixed pooling without setting any hyper-parameters for adjustment.<br><code>FMSPP在避免局部特征丢失的同时，减少冗余信息，保留突出的纹理特征，使网络获得更强的纹理和轮廓特征描述能力，从而减少水下图像不明显的轮廓和纹理特征带来的影响。与SPP模块相比，FMSPP模块具有更快的前向传播速度，从而减轻了由于结合特征增强门控模块而导致的检测速度损失[2]。此外，FMSPP可以解决过拟合问题，并通过混合池提高分类精度，而无需设置任何超参数进行调整。</code></p><h2 id="3-3-Feature-enhancement-gating-module"><a href="#3-3-Feature-enhancement-gating-module" class="headerlink" title="3.3. Feature enhancement gating module"></a>3.3. Feature enhancement gating module</h2><p><code>3.3.特征增强选通模块</code><br>Due to the complex background of underwater images, direct fusion of multi-scale features will introduce irrelevant background features, reduce the discrimination between foreground and background, and affect the accuracy of detection results. So it is essential to selectively enhance or suppress multi-scale features before fusion. In this paper, a feature gating module [21] is designed to adjust the size of multi-scale features, and then selectively enhance or suppress the adjusted features of the same size. The size adjustment is mainly realized by interpolation and convolution, and the selective enhancement or suppression of features is realized by gating unit.<br><code>由于水下图像背景复杂，直接融合多尺度特征会引入无关的背景特征，降低前景与背景的区分度，影响检测结果的准确性。因此在融合前有选择地增强或抑制多尺度特征是非常必要的。在本文中，设计了一个特征门控模块[21]来调整多尺度特征的大小，然后选择性地增强或抑制调整后的相同大小的特征。尺寸调整主要通过插值和卷积实现，特征的选择性增强或抑制通过选通单元实现。</code><br>The feature enhancement gating module is shown in Fig. 4 , which combines channel attention, spatial attention, global attention, residual connection and receptive field (RF) operation. Channel attention is to establish the correlation between channel features without introducing learning parameters, and adaptively enhance or suppress channel features according to the correlation. Spatial attention pays more attention to the position information, focusing on the area with more effective features in the feature map, which is a supplement to channel attention. Global attention is to learn a global attention coefficient by full connection layer to achieve the overall enhancement or suppression of input features [22] . RF operation integrates more discriminative feature representation by expanding the receptive field range to achieve effective enhancement of features. The gating unit designed in this paper does not need to set any threshold. It is an input feature driven gating mechanism.<br><code>特征增强选通模块如图4所示，其组合了通道注意力、空间注意力、全局注意力、残余连接和感受野（RF）操作。信道关注是在不引入学习参数的情况下建立信道特征之间的相关性，并根据相关性自适应地增强或抑制信道特征。空间注意力更关注位置信息，关注特征图中有效特征更多的区域，是对通道注意力的补充。全局注意力是通过全连接层学习一个全局注意力系数，实现对输入特征的整体增强或抑制[22]。RF操作通过扩大感受野范围来集成更多的鉴别特征表示，以实现特征的有效增强。本文设计的门控单元不需要设置任何阈值。它是一种输入特征驱动的选通机制。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-6.png"></p><blockquote><p>Fig. 4. The feature enhancement gating module.<br><code>图4 特征增强门控模块。</code></p></blockquote><p>For the input feature P ∈ C×H×W , the feature a ∈ C×HW is obtained by dimensional transformation (R). Feature b ∈ HW ×C is obtained by dimensional transformation plus transposition (RT). The relationship matrix c ∈ C×C between the feature channels is obtained by matrix multiplication of the features a and b, and the normalized relationship matrix d ∈ C×C is obtained by softmax operation on the relationship matrix c. Softmax operation is shown in formula (3) .<br><code>对于输入特征P ∈ C×H×W，通过维数变换（R）得到特征a ∈ C×HW。特征B ∈ HW ×C通过维数变换加转置（RT）得到。通过特征a和b的矩阵相乘得到特征通道之间的关系矩阵c ∈ C×C，通过对关系矩阵c进行softmax运算得到归一化关系矩阵d ∈ C×C。Softmax运算如公式（3）所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-7.png"><br>where $d_{i j}$ and $c_{i j}$ represent the values in row i and column j of the relationship matrices d and c, respectively. Then the relationship matrix d is multiplied with P to obtain feature e ∈ C×H×W . The feature e is input to the spatial attention module (SAM), and the feature map e is compressed in the channel dimension by using average pooling and maximum pooling to obtain two two-dimensional feature maps, then concat 2 feature maps based on the channel to get a feature map with 2 channel. To ensure that the final features are consistent with the input e in spatial dimension, a hidden layer with a single convolution kernel is used to convolve the spliced feature maps, and finally a spatial attention weight f ∈ C×H is generated by the sigmoid operation, as shown in formula (4) .<br><code>其中</code>$d_{i j}$<code>和</code>$c_{i j}$<code>分别表示关系矩阵d和c的行i和列j中的值。然后将关系矩阵d与P相乘，得到特征e ∈ C×H×W。将特征e输入到空间注意力模块（SAM）中，利用平均池化和最大池化对特征图e进行通道维压缩，得到两个二维特征图，然后基于通道对两个特征图进行拼接，得到一个双通道特征图。为了保证最终特征在空间维度上与输入e一致，使用具有单个卷积核的隐藏层对拼接的特征图进行卷积，最后通过sigmoid运算生成空间注意力权重f ∈ C×H，如公式（4)所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-8.png"><br>In addition, to adaptively suppress or enhance the input features, we use a global attention mechanism. The global attention mechanism is implemented by learning the global feature conditioning coefficients with the fully connected layer. The first step is to perform global average pooling (GAP) on features P to get feature vector u ∈ C . The GAP is implemented as shown in formula (5) .<br><code>此外，为了自适应地抑制或增强输入特征，我们使用了全局注意力机制。全局注意机制通过学习全连接层的全局特征条件系数来实现。第一步是对特征P执行全局平均池化（GAP）以得到特征向量u ∈ C。差距的实现如公式（5）所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-9.png"><br>where $(P)<em>l$ represents the l ∈ [1 , C] th channel of P,and u( l ) represents the value of the l th channel of P after global averaging pooling . The second step takes u as the input and uses the fully connected layers $f</em>{c/ 16}$ and $f_{16 / 1}$ to learn the global feature conditioning factor i , as shown in formula (6) .<br><code>其中</code>$(P)<em>l$<code>表示P的第l ∈ [1，C]个通道，并且u（l）表示全局平均池化之后P的第l个通道的值。第二步以u为输入，使用全连接层</code>$f</em>{c/ 16}$<code>和</code>$f_{16 / 1}$<code>学习全局特征条件因子i，如公式（6)所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-10.png"><br>Where $f_{c/ 16}$ represents the first fully connected layer (the input dimension is C and the output dimension is 16).$ f_{16 / 1}$ represents the second fully connected layer (the input dimension is 16 and the output dimension is 1). Finally, the global feature adjustment coefficient i is applied to the feature e to output the feature g ∈ C×H×W , and the residual connection is made with the output feature h ∈ C×H×W through SAM.<br><code>其中</code>$f_{c/ 16}$<code>表示第一个全连通层（输入维度为C，输出维度为16）。</code>$ f_{16 / 1}$ <code>表示第二个全连接层（输入维度为16，输出维度为1)。最后，将全局特征调整系数i应用于特征e以输出特征g ∈ C×H×W，并通过SAM与输出特征h ∈ C×H×W进行残差连接。</code><br>In the human visual system, a set of differently sized population receptive fields (PRFs) help to highlight regions close to the fovea, which are sensitive to small spatial displacements. This inspires us to use the RF module [30] to expand the feature receptive field and obtain global context information to further enhance the expression of features and obtain the output feature P . The RF module can be expressed in detail as Formula (7) and Formula (8) .<br><code>在人类视觉系统中，一组不同大小的群体感受野（PRFs）有助于突出靠近中央凹的区域，这些区域对小的空间位移敏感。这启发我们使用RF模块[30]来扩展特征感受野并获得全局上下文信息，以进一步增强特征的表达并获得输出特征P。RF模块可以详细地表示为公式（7）和公式（8）。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-11.png"><br>Where $b_n$ in the formula (7) represents the characteristic diagram of the branch n of the RF module; $\mathrm{BC}<em>{3,3}^{2 n-1}$ represents the sequential operation of 3 × 3 convolution operation with expansion rate 2 n − 1 and batch normalization; $\mathrm{Cat}</em>{k =1}^{4}$ means splicing the first four branches to integrate the acquired context information; γ represents the ReLU (Rectified Linear Unit) activation function. In reference [23] . The output dimension of all convolution operations in RF module is set to 32 in order to maintain the balance between computation and performance.<br><code>其中，式（7）中的</code>$b_n$<code>表示RF模块的分支n的特征图; </code>$\mathrm{BC}<em>{3,3}^{2 n-1}$<code>表示扩展率为2n-1的3 × 3卷积运算和批量归一化的顺序运算;</code> $\mathrm{Cat}</em>{k =1}^{4}$<code>表示将前四个分支拼接，整合获取的上下文信息; γ表示ReLU（Rectified Linear Unit)激活函数。在参考文献[23]中。RF模块中所有卷积运算的输出维数都设置为32，以保持计算和性能之间的平衡。</code></p><h2 id="3-4-Progressive-feature-dynamic-aggregation-strategy"><a href="#3-4-Progressive-feature-dynamic-aggregation-strategy" class="headerlink" title="3.4. Progressive feature dynamic aggregation strategy"></a>3.4. Progressive feature dynamic aggregation strategy</h2><p><code>3.4.渐进式特征动态聚合策略</code><br>In order to fully and effectively aggregate the multi-level feature information, this paper adopts a progressive dynamic aggregation strategy [24] to achieve the aggregation of multi-level features. This strategy dynamically aggregates adjacent features step by step to avoid the noise problem caused by cross-level fusion, and effectively im proves the adaptability of feature fusion. Progressive aggregation strategy has a lateral contraction structure, in which adjacent features are contracted by adjacent aggregation modules, and the contracted new features are used as the input of the next adjacent aggregation module. In the process of aggregation, in order to effectively integrate the information of adjacent features, this paper designs a Dynamic Aggregation Module (DAM) to keep the important information of adjacent features and suppress noise. As shown in Fig. 5 , the Adjacency Dynamic Aggregation Network (ADAN) adopts three dynamic aggregation modules to progressively aggregate pairs of adjacent features, so as to realize the accurate detection of underwater difficult-to-detect objects.<br><code>为了充分有效地聚合多层次特征信息，本文采用渐进式动态聚合策略[24]来实现多层次特征的聚合。该策略通过逐步动态聚合相邻特征，避免了跨层融合带来的噪声问题，有效地提高了特征融合的适应性。渐进式聚合策略具有横向收缩结构，相邻聚合模块对相邻特征进行收缩，收缩后的新特征作为下一个相邻聚合模块的输入。在聚合过程中，为了有效地融合相邻特征的信息，本文设计了动态聚合模块（DAM）来保持相邻特征的重要信息并抑制噪声。如图5所示，邻接动态聚合网络（ADAN）采用三个动态聚合模块，对相邻特征对进行渐进聚合，实现水下难探测目标的精确检测。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-12.png"></p><blockquote><p>Fig. 5. The schematic diagram of progressive feature dynamic aggregation strategy.<br><code>图五 渐进式特征动态聚合策略示意图。</code></p></blockquote><p>In object detection, fusing multi-scale features with fixed weights will make all the input images share the fusion mode, ignoring the influence of the object size on feature fusion, which restricts the detection performance. Therefore, it is necessary to dynamically adjust the fusion weight according to the input object scale to improve the adaptability of feature fusion [18] . Based on the above ideas, we designed a dynamic fusion module. This module establishes the relationship between the input object scale and feature fusion, learns the fusion weights according to the input object scale, and assigns different fusion weights to multi-scale features. The dynamic fusion module is mainly realized by the fusion weight learner, and the learner is shown in Fig. 6 . It takes feature with the same size as input, and outputs a set of fusion weights, and the learning of weights is realized through full connection layer.<br><code>在目标检测中，固定权值的多尺度特征融合会使所有输入图像共享融合模式，忽略了目标大小对特征融合的影响，从而限制了检测性能。因此，有必要根据输入对象尺度动态调整融合权重，以提高特征融合的适应性[18]。基于上述思想，设计了动态融合模块。该模块建立了输入目标尺度与特征融合之间的关系，根据输入目标尺度学习融合权值，为多尺度特征分配不同的融合权值。动态融合模块主要由融合权重学习器实现，学习器如图6所示。该方法以相同大小的特征作为输入，输出一组融合权值，权值的学习通过全连接层实现。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-13.png"></p><blockquote><p>Fig. 6. The schematic diagram of dynamic fusion weight learner.<br>图6 动态融合权值学习器原理图。</p></blockquote><p>Firstly, the feature { P 1 , P 2 } with the same size is cascaded to get the feature V ∈ 2 C×H×W , and then the feature V is globally averaged and pooled to get the feature vector S ∈ 2 C . To reduce the operation cost, we use two fully connected layers $f_{2 C / ( C / 2 )}$ and $f_{2 C / ( C / 2 )}$ to learn the feature fusion weight Z ∈ 2 [21] . To improve the stability of the training of the fusion weight learner, we perform softmax operation on Z to obtain the normalized fusion weight { w 1 , w 2 } . The learned fusion weight here represents the importance of multiscale features in fusion. Finally, according to the learned fusion weight, the input feature { P 1 , P 2 } is fused linearly to obtain the feature F ∈ C×H×W . The linear fusion is calculated as shown in formula (9) .<br><code>首先将相同大小的特征{P1，P2}级联得到特征V ∈ 2 C×H×W，然后对特征V进行全局平均和池化得到特征向量S ∈ 2 C。为了降低操作成本，我们使用两个全连接层</code>$f_{2 C / ( C / 2 )}$<code>和</code>$f_{2 C / ( C / 2 )}$<code>来学习特征融合权重Z ∈ 2 [21]。为了提高融合权值学习器训练的稳定性，对Z进行softmax运算，得到归一化的融合权值{w1，w2}。这里学习的融合权重表示融合中多尺度特征的重要性。最后，根据学习得到的融合权值，对输入特征{P1，P2}进行线性融合，得到特征F ∈ C×H×W。如公式（9）所示计算线性融合。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-14.png"></p><h2 id="3-5-Data-enhancement-strategy"><a href="#3-5-Data-enhancement-strategy" class="headerlink" title="3.5. Data enhancement strategy"></a>3.5. Data enhancement strategy</h2><p><code>3.5.数据增强策略</code><br>In the current mainstream public data sets, the number of small objects or the number of pictures containing small objects is far less than that of large objects. At the same time, the number of positive samples generated by small objects is far less than that of large objects, resulting in the contribution rate of small objects to the loss function being far less than that of large objects, which makes the direction of network convergence continue to tilt toward large objects. Inspired by literature [19] , this paper uses a new Copy-reduce-rotate-paste data enhancement strategy in the training process to increase the number of positive samples generated by small objects and the contribution value to the loss function, so that the training is more balanced. As shown in Fig. 6 , the solid line frame in Fig. 7 is the original object, and the dotted line frame is the pasted object. First, copy the large object image block, then the image block is reduced and randomly rotated, and finally pasted to different positions of the original image.<br><code>在目前主流的公开数据集中，小物体的数量或包含小物体的图片数量远远少于大物体。同时，小对象产生的正样本数量远少于大对象，导致小对象对损失函数的贡献率远小于大对象，使得网络收敛的方向不断向大对象倾斜。受文献[19]的启发，本文在训练过程中采用了一种新的Copy-reduce-rotate-paste数据增强策略，增加了小对象产生的正样本数和对损失函数的贡献值，使训练更加均衡。如图6所示，图7中的实线框为原始对象，虚线框为粘贴对象。首先复制大对象图像块，然后对图像块进行缩小和随机旋转，最后粘贴到原始图像的不同位置。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-15.png"></p><blockquote><p>Fig. 7. The schematic diagram of Copy-reduce-rotate-paste data enhancement strategy.<br><code>图7 Copy-reduce-rotate-paste数据增强策略示意图。</code></p></blockquote><h2 id="3-6-Loss-function-of-our-model"><a href="#3-6-Loss-function-of-our-model" class="headerlink" title="3.6. Loss function of our model"></a>3.6. Loss function of our model</h2><p><code>3.6.模型的损失函数</code><br>The loss function of YOLOv5s includes objective loss function, classification loss function and bounding box regression loss function. YOLOv5s uses BCE With Logits as the object loss function and classification loss function L BCE , as shown in formula (10) . P l represents the probability of output through Sigmoid activation function, and y is the real sample label, with a value of 0 or 1.<br><code>YOLOv5s的损失函数包括目标损失函数、分类损失函数和边界框回归损失函数。YOLOv5s使用BCE With Logits作为对象损失函数和分类损失函数L BCE，如公式（10）所示。P l表示通过Sigmoid激活函数输出的概率，y是真实的样本标签，值为0或1。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-16.png"><br>The part of the image containing the object is a positive sample, and the rest is a negative sample. For positive samples, the greater the output probability, the smaller the loss function; for negative samples, the smaller the output probability, the smaller the loss function. In underwater images, the proportion of background is obviously larger than that of object. For the One-Stage object detection algorithm, the unbalance between positive and negative samples is more prominent. Most of the loss values obtained by the loss function are negative sample background losses, and most of the negative sample backgrounds are simple and easily separable, which has little effect on the model’s convergence. So we introduces the focus loss function L Focal [26] to balance the influence of positive and negative samples on the loss function, and divides the samples into difficult samples and easy samples to reduce the weight of easy samples on the total loss function, as shown in formula (11) .<br><code>图像中包含对象的部分是正样本，其余部分是负样本。对于正样本，输出概率越大，损失函数越小;对于负样本，输出概率越小，损失函数越小。在水下图像中，背景所占的比例明显大于目标。对于单阶段目标检测算法，正样本和负样本之间的不平衡更加突出。由损失函数得到的损失值大多为负样本背景损失，且大多数负样本背景简单易分离，对模型的收敛性影响不大。所以我们引入焦点损失函数L Focal [26]来平衡正负样本对损失函数的影响，并将样本分为困难样本和容易样本，以减少容易样本对总损失函数的权重，如公式（11）所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-17.png"><br>Among them, the balance factor $a_1$ is to control the contribution of positive and negative samples to the loss, but it can’t affect the loss of easy-to-distinguish and difficult-to-distinguish samples. Therefore, ω ∈ [ 0 , 5 ] is used to control the size of modulation factor, and the weights of difficult-to-distinguish samples and easy-to-distinguish samples are controlled by modulation factors $\left(1-p_l\right)^\omega$ and $\left(p_l\right)^\omega$. To sum up, the total loss function L total is in formula (12) .<br><code>其中，平衡因子</code>$a_1$<code>是为了控制正负样本对损失的贡献，但不能影响易区分样本和难区分样本的损失。因此，使用ω ∈ [0，5]来控制调制因子的大小，并且通过调制因子</code>$\left(1-p_l\right)^\omega$<code>和</code>$\left(p_l\right)^\omega$<code>来控制难区分样本和易区分样本的权重。综上所述，总损失函数L total在公式（12)中。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-18.png"></p><h1 id="4-Results"><a href="#4-Results" class="headerlink" title="4. Results"></a>4. Results</h1><h2 id="4-1-Datasets-and-evaluation-metrics"><a href="#4-1-Datasets-and-evaluation-metrics" class="headerlink" title="4.1. Datasets and evaluation metrics"></a>4.1. Datasets and evaluation metrics</h2><p><code>4.1.数据集和评价指标</code><br>In this paper, underwater optical image data set URPC2021 [27] and underwater semantic segmentation data set of real scene (DUT-USEG) [28] are used as experimental data sets. URPC2021 data set provides a total of 8,200 training images, and there is no inter-frame continuity between these images. Including underwater images of holothurian, echinus, starfish and scallops, and the annotation of the corresponding images. DUT-USEG data set includes 6,617 underwater images, including four categories: holothurian, echinus, scallop and starfish, of which 1,487 images have manually added semantic segmentation labels and instance segmentation labels, and the remaining 5,130 images have object detection box labels. To test the optimization effect of our strategy on underwater small object detection, 200 images containing low-resolution small objects (pixel size less than 10 × 10) were selected from URPC and DUT-USEG, and the underwater lowresolution small object test set USO was constructed.<br><code>本文使用水下光学图像数据集URPC 2021 [27]和真实的场景水下语义分割数据集（DUT-USEG）[28]作为实验数据集。URPC 2021数据集提供了总共8,200张训练图像，这些图像之间没有帧间连续性。包括海参、海胆、海星和扇贝的水下图像，以及相应图像的注释。DUT-USEG数据集包含6,617张水下图像，包括海参、海胆、扇贝和海星四个类别，其中1,487张图像手动添加了语义分割标签和实例分割标签，其余5,130张图像有对象检测框标签。为了验证该策略在水下小目标检测中的优化效果，从URPC和DUT-USEG中选取了200幅包含低分辨率小目标（像素尺寸小于10 × 10）的图像，构建了水下低分辨率小目标测试集USO。</code><br>In this paper, Precision P, Recall R, Average Precision AP, mean Average Precision mAP and single image reasoning time t are used to evaluate the effectiveness and real-time performance of the detector. The calculation methods of all evaluation metrics are adopt PASCAL VOC2007 standard, which can represent the performance of the detector [2] .<br><code>本文采用查准率P、查全率R、平均查准率AP、平均平均查准率mAP和单幅图像推理时间t等指标来评价检测器的有效性和实时性。所有评价指标的计算方法均采用PASCAL VOC 2007标准，能够代表探测器的性能[2]。</code></p><h2 id="4-2-Experimental-environment-and-application-details"><a href="#4-2-Experimental-environment-and-application-details" class="headerlink" title="4.2. Experimental environment and application details"></a>4.2. Experimental environment and application details</h2><p><code>4.2.实验环境和应用细节</code><br>The hardware platform used in model training is Intel Xeon e52680 V4 CPU, 32 GB memory and NVIDIA geforce RTX 3060 Ti GPU. The software platform is Windows operating system and the deep learning framework is pytoch. During model training, the object categories is 4, the image size is 640 × 640, the optimizer is SGD [29] , the batch size is 8, the learning rate is 0.0 0 01, and the decay rate of the weight is 5 × 10 −4 , the momentum is set to 0.9, and the learning rate adjustment strategy is fixed step attenuation.<br><code>模型训练中使用的硬件平台是Intel Xeon e52680 V4 CPU，32 GB内存和NVIDIA geforce RTX 3060 Ti GPU。软件平台是Windows操作系统，深度学习框架是pytoch。在模型训练期间，对象类别为4，图像大小为640 × 640，优化器为SGD [29]，批量大小为8，学习率为0.0 0 01，权重的衰减率为5 × 10 −4，动量设置为0.9，学习率调整策略为固定步长衰减。</code><br>In this paper, Focal loss mainly depends on two hyperparameters $a_1$ and ω. in order to better integrate focal loss function and yolov5s to meet the needs of underwater object detection tasks, determining a set of optimal values of a$a_1$ and ω becomes the content of further research. According to the experimental results of reference [29] , $a_1$ takes 0.5 or 0.75, and ω takes an integer in the range of [ 1 , 5 ]. Following the principle of the control variable method, only a 1 and ω have different values in each reference group. The test results on URPC and DUT-USEG data sets show that when $a_1$ = 0.5and ω= 3, the network achieves the best detection results, and the three evaluation indexes are higher than other combinations.<br><code>在本文中，焦点损失主要取决于两个超参数</code>ω 1<code>和ω。为了更好地将焦损函数和yolov 5s结合起来，以满足水下目标探测任务的需要，确定一组</code>a$a_1$<code>和ω的最优值成为进一步研究的内容。根据文献[29]的实验结果，</code>$a_1$<code>取0.5或0.75，ω取[ 1，5 ]范围内的整数。遵循控制变量法的原理，在每个参考组中只有</code>$a_1$<code>和ω具有不同的值。在URPC和DUT-USEG数据集上的测试结果表明，当</code>$a_1$ <code>= 0.5，ω= 3时，网络达到了最佳的检测效果，且3个评价指标均高于其他组合。</code></p><h2 id="4-3-Ablation-experiments"><a href="#4-3-Ablation-experiments" class="headerlink" title="4.3. Ablation experiments"></a>4.3. Ablation experiments</h2><p><code>4.3.消融实验</code></p><h3 id="4-3-1-Gating-unit-ablation-experiment"><a href="#4-3-1-Gating-unit-ablation-experiment" class="headerlink" title="4.3.1. Gating unit ablation experiment"></a>4.3.1. Gating unit ablation experiment</h3><p><code>4.3.1.选通单元烧蚀实验</code><br>In this paper, based on YOLOv5s, a gating unit module is added after the Neck network layer, and many ablation experiments are carried out on URPC and DUT-USEG data sets. The effects of channel attention (CA), spatial attention (SA), global attention (GA), RF module and residual connection (RC) in the gating unit are verified respectively, the results are shown in Table 1 .<br><code>本文在YOLOv 5s的基础上，在Neck网络层后增加了选通单元模块，并在URPC和DUT-USEG数据集上进行了多次烧蚀实验。分别验证了门控单元中通道注意力（CA）、空间注意力（SA）、全局注意力（GA）、RF模块和剩余连接（RC）的效果，结果如表1所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-19.png"></p><blockquote><p>Table 1 The ablation experiment results of gate control unit on URPC and DUT-USEG data sets.<br><code>表1 门控单元在URPC和DUT-USEG数据集上的烧蚀实验结果。</code></p></blockquote><p>The mAp of the baseline method on URPC and DUT-USEG datasets was 76.70% and 70.36% respectively, which increased by 1.51% and 1.37% respectively after adding the designed gating unit, and the average processing time of an image increased by 12ms. On the URPC dataset, when the gating unit only fuses the channel attention features, the mAp increases by 0.41% compared with the baseline model; when the gating unit only fuses the spatial attention features, the mAp increases by 0.22% compared with the baseline; when the gating unit only fuses the global attention features, the mAp increases by 0.38% compared with the baseline. At the same time, three attention features were introduced, which improved the mAp by 0.59% compared with the baseline model. On this basis, the RF module was introduced, which improved the mAp by 0.53%. Continue to add residual connections, increasing by 0.39%. On the DUT-USEG dataset, when the gating unit only fuses channel attention features, the mAp increases by 0.45% compared with the baseline model; when the gating unit only fuses spatial attention features, the mAp increases by 0.18% compared with the baseline model; when the gating unit only fuses global attention features, the mAp increases by 0.46% compared with the baseline model. Three attention features were introduced at the same time, and compared with the baseline model, the mAp increased by 0.57%. On this basis, the RF module was introduced, and the mAp increased by 0.49%. Continue to add residual connections, increasing by 0.31%.<br><code>基线方法在URPC和DUT-USEG数据集上的mAp分别为76.70%和70.36%，加入设计的门控单元后分别提高了1.51%和1.37%，图像的平均处理时间增加了12 ms。在URPC数据集上，当门控单元仅融合通道注意力特征时，mAp较基线模型增加0.41%;当门控单元仅融合空间注意力特征时，mAp较基线增加0.22%;当门控单元仅融合全局注意力特征时，mAp较基线增加0.38%。同时，引入了三个注意力特征，与基线模型相比，mAp提高了0.59%。在此基础上，引入了射频模块，使mAp提高了0.53%。继续增加剩余连接，增加0.39%。在DUT-USEG数据集上，当门控单元仅融合通道注意特征时，mAp较基线模型增加0.45%;当门控单元仅融合空间注意特征时，mAp较基线模型增加0.18%;当门控单元仅融合全局注意特征时，mAp较基线模型增加0.46%。同时引入三个注意特征，与基线模型相比，mAp提高了0.57%。在此基础上，引入了射频模块，mAp提高了0.49%。继续增加剩余连接，增加0.31%。</code><br>In Table 1 , the accuracy of the baseline on the URPC and DUTUSEG is 76.31% and 73.54%, respectively. After the introduction of the gating unit, it has increased by 5.02% and 2.84%, respectively. The accuracy has been greatly improved, but the recall rate has indeed decreased by 0.8% and 1.1% respectively. Due to the addition of multiple attention mechanisms and the increase of the number of network layers, when similar objects are learned to the bottom of the neural network, the feature differentiation is not big, resulting in the object can be detected, but the category has been detected incorrectly, so the recall rate is reduced. But the introduction of RF module and residual connection effectively im proves the value of R .<br><code>在表1中，URPC和DUTUSEG的基线准确度分别为76.31%和73.54%。引入门控单元后，分别提高了5.02%和2.84%。准确率有了很大的提高，但召回率确实分别下降了0.8%和1.1%。由于加入了多重注意机制和网络层数的增加，当相似的对象学习到神经网络底层时，特征区分度不大，导致对象可以检测到，但类别已经被错误检测到，因此召回率降低。但射频模块和剩余连接的引入有效地提高了R值。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-19.png"></p><blockquote><p>Table 1 The ablation experiment results of gate control unit on URPC and DUT-USEG data sets.<br><code>表1 门控单元在URPC和DUT-USEG数据集上的烧蚀实验结果。</code></p></blockquote><p>In summary, in the gating unit, the gain brought by RF module, channel attention and global attention is the most obvious, and the gain of residual connection is higher than that of spatial attention. Moreover, the feature enhanced gating module can effectively improve the accuracy, but will not reduce the operation efficiency too much.<br><code>综上所述，在门控单元中，射频模块、通道注意力和全局注意力带来的增益最为明显，剩余连接的增益高于空间注意力。此外，特征增强选通模块可以有效地提高准确性，但不会降低操作效率太多。</code></p><h3 id="4-3-2-Feature-enhancement-gating-Progressive-feature-dynamic-aggregation-strategy"><a href="#4-3-2-Feature-enhancement-gating-Progressive-feature-dynamic-aggregation-strategy" class="headerlink" title="4.3.2. Feature enhancement gating &amp; Progressive feature dynamic aggregation strategy"></a>4.3.2. Feature enhancement gating &amp; Progressive feature dynamic aggregation strategy</h3><p><code>4.3.2.特征增强门控&amp;渐进式特征动态聚集策略</code><br>To verify the effectiveness of the Feature enhancement gating &amp; Progressive feature dynamic aggregation strategy, ablation experiments were conducted on URPC, and the baseline mode is YOLOv5s, the results are shown in Table 2 . The mAP, P and R of the baseline model on URPC data set are 76.7%, 76.31% and 83.22%, respectively. After adding feature gating unit after different scale features in Neck network layer, the mAP of 78.02% increased by 1.32%, the P of 78.65% increased by 2.34%, and the R of 84.65% increased by 1.43%. In order to verify the gain brought by dynamic fusion, without feature gating, adding dynamic fusion to the baseline model increases the mAP by 1.82%, P by 3.55% and R by 3.1%. There is no feature gating here, which mainly unifies the three different scale features output by Neck network layer, then dynamically fuses them to obtain features, and finally obtains multi-scale features by resampling. To sum up, the gain of dynamic feature fusion is greater than that of feature gating. By adding feature gating and dynamic feature fusion to the benchmark method, the mAP is increased by 3.17%, P by 5.32% and R by 4.71%.<br><code>为了验证特征增强门控&amp;渐进式特征动态聚合策略的有效性，在URPC上进行了消融实验，基线模式为YOLOv5s，结果如表2所示。基线模型在URPC数据集上的mAP、P和R分别为76.7%、76.31%和83.22%。在Neck网络层的不同尺度特征后添加特征选通单元后，78.02%的mAP增加了1.32%，78.65%的P增加了2.34%，84.65%的R增加了1.43%。为了验证动态融合带来的增益，在没有特征门控的情况下，将动态融合添加到基线模型中，mAP增加了1.82%，P增加了3.55%，R增加了3.1%。这里没有特征选通，主要是将Neck网络层输出的三个不同尺度的特征统一起来，然后动态融合得到特征，最后通过重排序得到多尺度特征。综上所述，动态特征融合的增益大于特征选通。通过在基准方法的基础上增加特征门和动态特征融合，mAP提高了3.17%，P提高了5.32%，R提高了4.71%。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-20.png"></p><blockquote><p>Table 2 The ablation experiment results on URPC data set.<br><code>表2 URPC数据集上的烧蚀实验结果。</code></p></blockquote><p>The results on USO are shown in Table 3 . Both dynamic fusion and feature enhancement strategies can effectively improve the detection accuracy of small objects, among which dynamic fusion can improve the detection performance more obviously, increasing mAP by 6.17%, P by 6.15% and R by 5.89%.<br><code>USO的结果见表3。动态融合和特征增强策略都能有效提高小目标的检测精度，其中动态融合对检测性能的提升更为明显，mAP提高了6.17%，P提高了6.15%，R提高了5.89%。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-21.png"></p><blockquote><p>Table 3 The ablation experiment results on USO data set.<br><code>表3 USO数据集上的烧蚀实验结果。</code></p></blockquote><p>Fig. 8 shows the feature fusion weights learned by dynamic feature fusion network with different scale objects as input, where red represents the maximum fusion weight and blue represents the minimum fusion weight. When the object scale of the input image is small, the bottom features $P_1$ and $P_2$ contain the detailed information of the object, which is beneficial to the detection of small objects. In feature fusion, the bottom features P 1 and $P_2$ will adopt larger weights, as shown in the first row of Fig. 8 . When the object scale of the input image is large, the semantic information extracted by the top feature $P_3$ is beneficial to the detection of large objects, and the top features will adopt larger weights, as shown in the second row of Fig. 8 .<br><code>图8示出了以不同尺度对象作为输入的动态特征融合网络学习的特征融合权重，其中红色表示最大融合权重，蓝色表示最小融合权重。当输入图像的目标尺度较小时，底部特征</code>$P_1$<code>和</code>$P_2$<code>包含了目标的详细信息，有利于小目标的检测。在特征融合中，底部特征</code>$P_1$<code>和</code>$P_2$<code>将采用较大的权重，如图8的第一行所示。当输入图像的对象尺度较大时，顶部特征</code>$P_3$<code>提取的语义信息有利于大对象的检测，顶部特征将采用较大的权重，如图8的第二行所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-22.png"></p><blockquote><p>Table 8 Experimental results of performance comparison of various detectors on USO data set.<br><code>表8 各种探测器在USO数据集上的性能比较实验结果。</code></p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-23.png"></p><blockquote><p>Fig. 8. The schematic diagram of Copy-reduce-rotate-paste data enhancement strategy.<br><code>图8 Copy-reduce-rotate-paste数据增强策略示意图。</code></p></blockquote><p>To verify the effectiveness of the adjacent aggregation structure, three groups of ablation experiments are designed on URPC, and the results are shown in Table 4 . Fig. 9 shows the specific structure of each experimental scheme, in which (a) is a 3-2-adjacent structure, (b) is a 2-3-adjacent structure, and (c) is a 2-adjacent structure.<br><code>为了验证相邻聚集结构的有效性，在URPC上设计了三组烧蚀实验，结果如表4所示。图9示出了每个实验方案的具体结构，其中（a）是3-2-相邻结构，（B）是2-3-相邻结构，以及（c）是2-相邻结构。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-24.png"></p><blockquote><p>Table 4 The ablation experiment results on USO data set.<br><code>表4 USO数据集上的烧蚀实验结果。</code></p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-25.png"></p><blockquote><p>Fig. 9. Three different Combination structure.<br><code>图9 三种不同的组合结构。</code></p></blockquote><p>In Table 4 , compared with 3-2-adjacent structure and 2-3-adjacent structure, the 2-adjacent structure achieves the best detection performance and detection efficiency, which verifies the superiority of two-proximity aggregation.<br><code>在表4中，与3-2-邻接结构和2-3-邻接结构相比，2-邻接结构实现了最佳的检测性能和检测效率，验证了两邻近聚合的优越性。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-24.png"></p><blockquote><p>Table 4 The ablation experiment results on USO data set.<br><code>表4 USO数据集上的烧蚀实验结果。</code></p></blockquote><p>The activation heat map of the intermediate feature is obtained as shown in Fig. 10 , the lower-level features contain more texture details, which is beneficial to the detection of small objects, while the higher-level features contain obvious position information. Each progressive operation fuses two adjacent features with small differences. In Fig. 10 , the irrelevant noise is suppressed in the process of feature aggregation, and the features of objects are enhanced, achieving the desired effect.<br><code>中间特征的激活热图如图10所示，低层特征包含更多的纹理细节，有利于小物体的检测，而高层特征包含明显的位置信息。每个渐进操作融合两个具有微小差异的相邻特征。在图10中，在特征聚合的过程中抑制了无关噪声，增强了对象的特征，达到了预期的效果。</code><br>The activation heat map of the intermediate feature is obtained as shown in Fig. 10 , the lower-level features contain more texture details, which is beneficial to the detection of small objects, while the higher-level features contain obvious position information. Each progressive operation fuses two adjacent features with small differences. In Fig. 10 , the irrelevant noise is suppressed in the process of feature aggregation, and the features of objects are enhanced, achieving the desired effect.<br><code>中间特征的激活热图如图10所示，低层特征包含更多的纹理细节，有利于小物体的检测，而高层特征包含明显的位置信息。每个渐进操作融合两个具有微小差异的相邻特征。在图10中，在特征聚合的过程中抑制了无关噪声，增强了对象的特征，达到了预期的效果。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-26.png"></p><blockquote><p>Fig. 10. The activation heat map of intermediate features.<br><code>图10 中间特征的激活热图。</code></p></blockquote><h3 id="4-3-3-FMSPP-data-enhancement-strategy"><a href="#4-3-3-FMSPP-data-enhancement-strategy" class="headerlink" title="4.3.3. FMSPP &amp; data enhancement strategy"></a>4.3.3. FMSPP &amp; data enhancement strategy</h3><p><code>4.3.3.FMSPP和数据增强策略</code><br>To investigate the impact of FMSPP module and data enhancement strategy on the overall performance of the network, ablation experiments were conducted on the URPC, the results are shown in Table 5 . The object detection algorithm of YOLOv5s is used as the baseline method, and then FMSPP and data enhancement strategies are added, which are noted as baseline + FMSPP and baseline + Crrp, respectively.The addition of data enhancement strategy improves the algorithm’s P, R and mAP by 0.9%, 1.2% and 0.5%, respectively. the replacement of FMSPP improves the algorithm’s P and mAP improved by 2.2% and 0.6%, respectively, and the singleframe image processing time was reduced by 4 ms and R decreased by 2.1%. The introduction of both FMSPP and data enhancement strategies in the prediction network improves P, R and mAP by 3.1%, 1.9% and 1.3%, respectively, demonstrating that the two strategies can effectively enhance the algorithm’s understanding of the object and realize the improvement of detection accuracy.<br><code>为了研究FMSPP模块和数据增强策略对网络整体性能的影响，在URPC上进行了消融实验，结果如表5所示。以YOLOv 5s的目标检测算法为基线方法，加入FMSPP和数据增强策略，分别记为baseline + FMSPP和baseline + Crrp，数据增强策略的加入使算法的P、R和mAP分别提高了0.9%、1.2%和0.5%。替换FMSPP后，算法的P和mAP分别提高了2.2%和0.6%，单帧图像处理时间减少了4 ms，R降低了2.1%。在预测网络中同时引入FMSPP和数据增强策略，使P、R和mAP分别提高了3.1%、1.9%和1.3%，证明两种策略能够有效增强算法对对象的理解，实现检测精度的提升。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-27.png"></p><blockquote><p>Table 5 Results of FMSPP&amp; data enhancement strategy ablation on URPC data set.<br><code>表5 FMSPP和数据增强策略在URPC数据集上消融的结果。</code></p></blockquote><p>To verify the effectiveness of small object data enhancement strategy and the influence of pasting times on the performance of small object detection, an ablation experiment was conducted on USO data set, the results are shown in Table 6 . The results show that pasting one object is the best setting. With the increase of pasting times, the detection rate of small objects gradually decreases, even lower than the baseline model. This may be due to the gradual destruction of the distribution of original data with the increase of pasting times, resulting in poor performance in the test set.<br><code>为了验证小目标数据增强策略的有效性以及粘贴次数对小目标检测性能的影响，在USO数据集上进行了消融实验，结果如表6所示。结果表明，粘贴一个对象是最佳设置。随着粘贴次数的增加，小物体的检测率逐渐降低，甚至低于基线模型。这可能是由于原始数据的分布随着粘贴次数的增加而逐渐被破坏，导致在测试集中表现不佳。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-28.png"></p><blockquote><p>Table 6 Results of small object data enhancement strategy on USO.<br><code>表6 USO小目标数据增强策略的结果。</code></p></blockquote><h2 id="4-4-Qualitative-analysis"><a href="#4-4-Qualitative-analysis" class="headerlink" title="4.4. Qualitative analysis"></a>4.4. Qualitative analysis</h2><p><code>4.4.定性分析</code><br>The partial image test results of our model on the URPC and DUT-USEG datasets are shown in Fig. 11 . In Fig. 11 , in A) there are easily leaking objects that are highly similar to the background, B) and D) have problems such as occlusion, blur, color shift and uneven light, and C) has many objects and a complex environment background. The results show that our model achieves better detection results, good environmental adaptability and stable performance. Blue bounding box represents starfish, red bounding box represents echinus, purple bounding box represents holothurian, and yellow bounding box represents scallop.<br><code>我们的模型在URPC和DUT-USEG数据集上的部分图像测试结果见图11。在图11中，在A）中，存在与背景高度相似的容易泄漏的对象，B）和D）具有诸如遮挡、模糊、颜色偏移和不均匀光的问题，以及C）具有许多对象和复杂的环境背景。实验结果表明，该模型具有较好的检测效果、良好的环境适应性和稳定的性能。蓝色边框代表海星，红色边框代表海胆，紫色边框代表海参，黄色边框代表扇贝。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-29.png"></p><blockquote><p>Fig. 11. The results of our model on the URPC and DUT-USEG datasets<br><code>图11 我们的模型在URPC和DUT-USEG数据集上的结果</code></p></blockquote><p>For the detection effect of small and fuzzy objects, the comparison between the detection results of our model and YOLOv5s on USO data set is shown in Fig. 12 , (a) is the result of YOLOv5s, and (b) is the result of our model.<br><code>对于小目标和模糊目标的检测效果，我们的模型和YOLOv5s在USO数据集上的检测结果对比如图12所示，（a）是YOLOv5s的结果，（b）是我们的模型的结果。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-30.png"></p><blockquote><p>Fig. 12. The comparison between the detection results of our model and the YOLOv5s on USO data set.<br><code>图12 我们的模型和YOLOv5s在USO数据集上的检测结果的比较。</code></p></blockquote><p>Both scene images contain a large number of small objects and fuzzy objects. By comparing 1) and 2), it can be seen that 2) the sea cucumber in the purple box that 1) failed to detect and some fuzzy tiny sea urchin objects are detected. It can be seen from the comparison between 3) and 4) that in 4) the starfish in the blue box and the faint tiny sea urchin object in the distance that 3) failed to detect are detected. The above results prove that our optimization strategy can effectively im prove the detection performance for underwater fuzzy objects and small objects.<br><code>这两种场景图像都包含大量的小对象和模糊对象。通过比较1）和2），可以看出2）1）未能检测到的紫色盒子中的海参和一些模糊的微小海胆物体被检测到。从3）和4）的比较可以看出，在4）中检测到了蓝色盒子中的海星和3）未能检测到的远处微弱的微小海胆物体。实验结果表明，该优化策略能有效地提高水下模糊目标和小目标的检测性能。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-30.png"></p><blockquote><p>Fig. 12. The comparison between the detection results of our model and the YOLOv5s on USO data set.<br><code>图12 我们的模型和YOLOv5s在USO数据集上的检测结果的比较。</code></p></blockquote><p>Another major challenge for underwater object detection is the interference caused by complex background. Fig. 13 shows the detection results of our model on URPC and DUT-USEG data sets for objects which are easy to mix and leak in complex background. In a) and c), the background is very complex, the object sea urchin is small, and the object starfish features are weak; B) and d) are highly similar in the background characteristics of the object sea, and the characteristics of the object starfish are weak, which makes it difficult to detect. Our model has achieved good detection results in the above scenarios, which verifies the validity of our strategy in detecting small objects and confusing weak objects.<br><code>水下目标检测的另一个主要挑战是复杂背景造成的干扰。图13显示了我们的模型在URPC和DUT-USEG数据集上对复杂背景中容易混合和泄漏的对象的检测结果。在a）和c）中，背景非常复杂，目标海胆较小，目标海星特征较弱; B）和d）中目标海的背景特征高度相似，目标海星特征较弱，难以检测。该模型在上述场景下均取得了较好的检测效果，验证了该策略在检测小目标和混淆弱目标方面的有效性。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-31.png"></p><blockquote><p>Fig. 13. The detection results of our model on URPC and DUT-USEG.<br><code>图十三 我们的模型在URPC和DUT-USEG上的检测结果。</code></p></blockquote><h2 id="4-5-Quantitative-analysis"><a href="#4-5-Quantitative-analysis" class="headerlink" title="4.5. Quantitative analysis"></a>4.5. Quantitative analysis</h2><p><code>4.5.定量分析</code><br>To further analyze the accuracy of object detection and positioning of our model, it is suggested to classify each extracted object in the positive training images into one of the following five situations: correct positioning (overlap ≥ 50%), recommendation (completely) inside the ground truth, recommendation (completely) inside the ground truth, and none of the above, but nonzero overlap, low overlap and no overlap. The detection results on URPC and DUT-USEG datasets are shown in (a) and (b) of FIG. 14 , respectively, showing the frequency of these five cases for each object category and the average error of all categories of our model. In Fig. 14 , the average positioning accuracy of our model can reach more than 70% for a variety of different types of objects on the two data sets, which basically avoids the error problem of no overlap.<br><code>为了进一步分析我们的模型的对象检测和定位的准确性，建议将正训练图像中的每个提取的对象分类为以下五种情况之一：正确定位（重叠≥ 50%），推荐（完全）在地面真理内，推荐（完全）在地面真理内，以及以上都没有，但非零重叠，低重叠和无重叠。图14的（a）和（B）分别示出了URPC和DUT-USEG数据集上的检测结果，示出了针对每个对象类别的这五种情况的频率和我们的模型的所有类别的平均误差。在图14中，我们的模型对于两个数据集上的各种不同类型的物体，平均定位精度可以达到70%以上，基本上避免了没有重叠的误差问题。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-32.png"></p><blockquote><p>Fig. 14. The positioning results of our model on URPC and DUT-USEG.<br><code>图14 模型在URPC和DUT-USEG上的定位结果。</code></p></blockquote><p>To verify the advancement and superiority of our model, several current most advanced models are used to conduct comparative experiments on URPC dataset with the detector proposed in this paper. YOLOv5s, HCE [30] , UARO [6] , APAN [31] and FAYOLOV5 [32] are single-stage detectors, and YOLOv5s is lightweight detectors. ThunderNet3 [33] , SA-FPN [34] , and LUDet [35] are lightweight two-stage detectors. To make a fair comparison, this paper directly uses the test results provided by the literatures or the open source code to evaluate the models, and all models use the same image enhancement method.<br><code>为了验证该模型的先进性和优越性，我们使用了几种当前最先进的模型，并在URPC数据集上与本文提出的检测器进行了对比实验。YOLOv 5s、HCE [30]、UARO [6]、APAN [31]和FAYOLOV 5 [32]是单级探测器，YOLOv 5s是轻量级探测器。ThunderNet 3 [33]，SA-FPN [34]和LUDet [35]是轻量级的两级检测器。为了进行公平的比较，本文直接使用文献提供的测试结果或开源代码对模型进行评价，所有模型都使用相同的图像增强方法。</code><br>The comparative results are shown in Fig. 15 and Table 7 , our model achieves the optimal detection effect of 88.56% and 85.16% in the P and mAp respectively, and the R ranks the second with 86.82%. The detection efficiency can meet the requirement of realtime, and the inference time of a single frame is 63ms.<br><code>对比结果如图15和表7所示，我们的模型在P和mAp中分别达到了88.56%和85.16%的最佳检测效果，R以86.82%排名第二。检测效率满足实时性要求，单帧推理时间为63 ms。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-33.png"></p><blockquote><p>Fig. 15. Results of performance comparison of various detectors on URPC data set.<br><code>图15 各种检测器在URPC数据集上的性能比较结果。</code></p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-34.png"></p><blockquote><p>Table 7 Results of performance comparison of various detectors on URPC data set.<br><code>表7 各种检测器在URPC数据集上的性能比较结果。</code></p></blockquote><p>We further compare our model with several other advanced one-stage detectors FA- YOLOv5, APAN and UARO on URPC and DUT-USEG data sets, and the corresponding precision-regression curves are shown in Fig. 16 . The accuracy and regression rate of our model are obviously better than those of other one-stage detectors.<br><code>我们进一步在URPC和DUT-USEG数据集上将我们的模型与其他几个先进的一级检测器FA-YOLOv 5、APAN和UARO进行比较，相应的精度回归曲线如图16所示。该模型的准确率和回归速度明显优于其他一级检测器。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-35.png"></p><blockquote><p>Fig. 16. The P-R curve of multi-class models on URPC and DUT-USEG.<br><code>图16 URPC和DUT-USEG上多类模型的P-R曲线。</code></p></blockquote><p>We further compare our model with several other advanced detectors FA- YOLOv5, APAN, SA-FPN on USO, the results are shown in Table 8 . The performance of our model on underwater fuzzy small object is obviously better than other models, with P increasing by 9% ∼10%, R increasing by 7% ∼8% and mAP increasing by about 7%.<br><code>我们进一步将我们的模型与USO上的其他几种先进探测器FA-YOLOv 5、APAN、SA-FPN进行了比较，结果如表8所示。该模型对水下模糊小目标的识别性能明显优于其他模型，P提高了9%~ 10%，R提高了7%~ 8%，mAP提高了约7%。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-36.png"></p><blockquote><p>Table 8 Experimental results of performance comparison of various detectors on USO data set.<br><code>表8各种探测器在USO数据集上的性能比较实验结果。</code></p></blockquote><p>Table 9 compares the complexity of our model with that of FA- YOLOv5, YOLOV5, SA-FPN on UPRC data set, and usually describes the complexity of the model by the amount of computation (FLOPs) and the number of Parameters [44]. Our network size is only 5.86M, which is 65.3% of SA-FPN model parameters, and FLOPs drops by about 60%. Our model parameters are similar to YOLOv5s, and FLOPs only increases about 22.5% compared with YOLOv5s.<br><code>表9在UPRC数据集上比较了我们的模型与FA-YOLOV 5、YOLOV 5、SA-FPN的复杂度，通常通过计算量（FLOP）和参数数量来描述模型的复杂度[44]。我们的网络规模仅为5.86M，是SA-FPN模型参数的65.3%，FLOPs下降了约60%。我们的模型参数与YOLOv 5s相似，FLOP与YOLOv 5s相比仅增加约22.5%。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/3-37.png"></p><blockquote><p>Table 9 Complexity comparison results of multiple models on UPRC data set.<br><code>表9 UPRC数据集上多个模型的复杂性比较结果。</code></p></blockquote><h1 id="5-Conclusions"><a href="#5-Conclusions" class="headerlink" title="5. Conclusions"></a>5. Conclusions</h1><p>This paper designs a novel feature enhancement and progressive dynamic aggregation strategy, and proposes a new underwater object detection model based on YOLOv5s model to solve two challenges the Underwater object detection faces: 1) small objects are fuzzy and tiny 2) The distinction between object and background is low. The adjacent aggregation mechanism and a dynamic aggregation module can suppress conflicting information in multi-scale feature fusion and prevent small objects from being submerged. The Copy-reduce-rotate-paste data augmentation strategy can improve the contribution of the small object to the loss function, making the training more balanced. The feature enhancement gating module and FMSPP module is proposed to obtain stronger texture and contour feature description capabilities and reduce the interference of underwater complex environmental noise. Our model performs well on the URPC and DUT-USEG data sets, and has significant advantages in detection accuracy and efficiency compared with the current mainstream object detectors. However, the complexity and computational efficiency of our model still have much room for improvement. In the next step, we will continue to study how to further improve the detection efficiency on the premise of ensuring the detection accuracy, and make lightweight improvements to the model to apply to the autonomous object detection of underwater robots.<br><code>针对水下目标检测面临的两大挑战：1）小目标模糊、微小; 2）目标与背景的区分度低。设计了一种新的特征增强和渐进式动态聚合策略，提出了一种基于YOLOv 5s模型的水下目标检测模型。相邻聚合机制和动态聚合模块可以抑制多尺度特征融合中的冲突信息，防止小目标被淹没。复制-减少-旋转-粘贴数据增强策略可以提高小对象对损失函数的贡献，使训练更加平衡。为了获得更强的纹理和轮廓特征描述能力，降低水下复杂环境噪声的干扰，提出了特征增强选通模块和FMSPP模块。我们的模型在URPC和DUT-USEG数据集上表现良好，与当前主流的目标检测器相比，在检测精度和效率上具有显著优势。然而，我们的模型的复杂性和计算效率仍然有很大的改进空间。下一步，我们将继续研究如何在保证检测精度的前提下进一步提高检测效率，并对模型进行轻量化改进，以应用于水下机器人的自主目标检测。</code></p><h1 id="GPT解读"><a href="#GPT解读" class="headerlink" title="GPT解读"></a>GPT解读</h1><p> 这篇文章提出了一种基于YOLOv5s的新型水下目标检测算法，旨在解决水下环境中小目标检测的挑战，包括小目标的模糊性和目标与背景区分度低的问题。文章的核心内容和结构如下：</p><ol><li><p><strong>研究背景</strong>：</p><ul><li>水下目标检测技术在海军沿海防御、渔业养殖、沉船打捞、海洋生态系统研究等领域有广泛应用。</li><li>水下成像环境的特殊性导致颜色失真、干扰噪声多、纹理特征模糊和对比度低，给目标检测带来挑战。</li></ul></li><li><p><strong>研究目的</strong>：</p><ul><li>设计一种新的特征增强和渐进式动态聚合策略，提高水下小目标的检测准确性和效率。</li></ul></li><li><p><strong>方法介绍</strong>：</p><ul><li><strong>特征增强门控模块</strong>：用于选择性地增强或抑制多级特征，减少水下复杂环境噪声对特征融合的干扰。</li><li><strong>特征动态融合模块</strong>：通过动态学习融合权重，实现多级特征的渐进式聚合，避免小目标被冲突信息淹没。</li><li><strong>快速空间金字塔混合池化（FMSPP）</strong>：基于相同大小的快速混合池化层，增强网络对纹理和轮廓特征的描述能力，减少参数数量，提高泛化能力和分类准确性。</li></ul></li><li><p><strong>实验结果</strong>：</p><ul><li>在URPC和DUT-USEG数据集上的消融实验和多方法比较实验证明了所提策略的有效性。</li><li>与当前主流检测器相比，所提出的检测器在检测性能和效率上有明显优势。</li></ul></li><li><p><strong>实验方法</strong>：</p><ul><li>使用URPC2021水下光学图像数据集和DUT-USEG水下语义分割数据集进行测试。</li><li>使用精确度（P）、召回率（R）、平均精度（AP）、平均平均精度（mAP）和单张图像推理时间（t）作为评估指标。</li></ul></li><li><p><strong>结论</strong>：</p><ul><li>提出的模型在URPC和DUT-USEG数据集上表现良好，与当前主流目标检测器相比，在检测准确性和效率上有显著优势。</li><li>未来的工作将集中在进一步提高检测效率和模型轻量化，以适应水下机器人的自主目标检测需求。</li></ul></li></ol><p>文章还详细介绍了相关研究工作、提出的模型的整体框架、特征增强门控模块、渐进式特征动态聚合策略、数据增强策略、损失函数以及实验环境和应用细节。</p><h2 id="1-前人研究的方法"><a href="#1-前人研究的方法" class="headerlink" title="1.前人研究的方法"></a>1.前人研究的方法</h2><p>这篇文章中提到了前人在水下目标检测方面的研究方法，并对其进行了讨论。以下是前人研究方法的详细说明：</p><ol><li><p>传统的水下目标检测方法：传统的水下目标检测方法主要是基于图像处理和计算机视觉技术。这些方法通常依赖于手工设计的特征提取器和分类器，如滤波、边缘检测和模板匹配等。然而，由于水下环境的复杂性和噪声干扰等因素，这些方法的性能往往不够稳定和准确。</p></li><li><p>基于机器学习的水下目标检测方法：近年来，随着深度学习的兴起，基于机器学习的水下目标检测方法取得了显著进展。这些方法使用卷积神经网络（CNN）来学习特征表示，并通过后续的分类器来进行目标检测。其中，基于Faster R-CNN的方法被广泛应用于水下目标检测中。这些方法通过引入区域建议网络（RPN）来生成候选目标框，然后在这些框上进行目标分类和位置回归。</p></li><li><p>基于深度学习的单阶段检测器：除了基于两阶段检测器的方法，单阶段检测器也被应用于水下目标检测中。这些方法通常具有简单和高效的优势，如YOLO（You Only Look Once）和SSD（Single Shot MultiBox Detector）。YOLO将目标检测任务视为回归问题，直接通过网络输出目标的边界框和类别概率。这种方法在水下目标检测中也取得了一定的效果。</p></li><li><p>数据增强方法：为了增强模型的泛化能力和对水下环境的适应性，一些研究者提出了各种数据增强技术。例如，通过改变图像的颜色、亮度、对比度、模糊度和形态学变换等方式，来模拟水下环境中的不同影响因素。此外，还有一些特定的增强方法，如复制-减少-旋转-粘贴（Copy-reduce-rotate-paste）策略，用于增加小目标样本的数量和贡献。</p></li></ol><p>总之，前人研究水下目标检测方法主要包括传统方法和基于深度学习的方法。深度学习方法主要包括两阶段检测器和单阶段检测器。此外，数据增强方法也被广泛应用于提高水下目标检测模型的性能。然而，传统方法在复杂水下环境中往往表现不佳，深度学习方法虽然取得了一定的进展，但仍然存在一些挑战和问题。以上述方法为基础，本文提出了一种基于特征增强和动态融合策略的水下目标检测算法，并通过实验证明了其有效性。</p><h2 id="2-作者要解决的问题"><a href="#2-作者要解决的问题" class="headerlink" title="2.作者要解决的问题"></a>2.作者要解决的问题</h2><p>作者的研究旨在解决水下目标检测领域面临的几个问题。首先，由于水下环境的噪声和目标尺度的变化，传统的目标检测方法很难从水下图像中提取特征，并且容易错过检测小尺寸目标。其次，水下环境中存在色彩失真、干扰噪声、纹理特征模糊和低对比度等问题，进一步增加了水下目标检测的挑战。</p><p>为了解决这些问题，作者设计了一种新颖的特征增强和渐进动态聚合策略，并提出了一种基于YOLOv5s的水下目标检测算法。首先，他们设计了一个特征增强门控模块，通过插值和卷积调整特征的尺寸，并利用门控单元有选择地增强或抑制特征，以减少水下复杂环境噪声对特征融合的干扰。然后，他们设计了相邻特征融合机制和动态融合模块，动态学习融合权重并逐步进行多级特征融合，以抑制多尺度特征融合中的冲突信息，防止小目标被冲突信息淹没。最后，他们提出了一种基于快速混合池化的空间金字塔池结构（FMSPP），该结构可以使网络获得更强的纹理和轮廓特征描述能力，减少参数，并进一步提高泛化能力和分类精度。</p><p>通过在URPC和DUT-USEG数据集上进行剖析实验证明了所提策略的有效性。与当前主流的检测器相比，他们的检测器在检测性能和效率方面都取得了明显优势。</p><p>总的来说，作者的研究旨在解决水下目标检测领域中特征提取困难和小目标漏检等问题。通过特征增强和动态聚合策略，他们提出了一种新的水下目标检测算法，有效地处理了水下环境的挑战，并取得了显著的检测性能提升。</p><h2 id="3-作者通过什么样的方法来解决所提出的研究问题"><a href="#3-作者通过什么样的方法来解决所提出的研究问题" class="headerlink" title="3.作者通过什么样的方法来解决所提出的研究问题"></a>3.作者通过什么样的方法来解决所提出的研究问题</h2><p>根据这篇文章，作者提出了一种新颖的特征增强和动态融合策略，以解决传统目标检测方法在水下环境中提取特征困难、小目标漏检等问题。下面是详细的方法说明：</p><ol><li><p>特征增强门控模块：设计了一个特征增强门控模块，利用插值和卷积操作来调整特征的大小，并通过门控单元来有选择地增强或抑制特征。该模块采用通道注意力、空间注意力、全局注意力、残差连接和RF操作等技术，以增强特征描述能力和提高检测精度。</p></li><li><p>动态特征融合模块：设计了相邻特征融合机制和动态融合模块，以动态地学习融合权重并逐步地进行多层次特征融合。这样可以抑制多尺度特征融合中的冲突信息，防止小目标被冲突信息淹没。</p></li><li><p>快速混合池塘的FMSPP结构：提出了一种基于相同尺寸的快速混合池塘结构（FMSPP），它能够使网络获得更强的纹理和轮廓特征描述能力，减少参数，进一步提高泛化能力和分类精度。</p></li></ol><p>此外，使用了Copy-reduce-rotate-paste的数据增强策略，通过增加小目标样本的数量和对损失函数的贡献，解决了训练数据中小目标和大目标之间的不平衡问题。</p><p>通过实验验证了所提出方法的有效性，对URPC和DUT-USEG数据集进行了割除实验和多种方法的比较实验，结果表明，与当前主流的目标检测器相比，该方法在检测性能和效率上具有明显的优势。</p><p>总之，作者通过设计特征增强和动态融合策略以及快速混合池塘结构，结合数据增强策略，解决了水下目标检测中特征提取和小目标漏检等问题。通过实验证明了该方法的有效性和性能优势</p><h2 id="4-作者通过哪些实验得出来了这些结果"><a href="#4-作者通过哪些实验得出来了这些结果" class="headerlink" title="4.作者通过哪些实验得出来了这些结果"></a>4.作者通过哪些实验得出来了这些结果</h2><p>在这篇文章中，作者通过多个实验和试验分析来验证和评估提出的水下目标检测算法的性能。</p><p>首先，作者进行了一系列的消融实验来研究动态融合和特征增强策略对检测准确度的影响。实验结果显示，动态融合对检测性能的改进更为显著，相较于特征增强，提升了mAP、P和R等评价指标。此外，作者还验证了相邻特征融合结构的有效性，其中2个相邻结构实现了最佳的检测性能和效率。</p><p>其次，对模型在不同数据集上的检测结果进行了质量分析，展示了其在准确检测小而模糊的目标、适应复杂背景和处理具有挑战性的场景方面的能力。在定量分析方面，对目标检测和定位的准确性进行了评估，结果显示模型在各种类型目标上达到了超过70%的平均定位精度。</p><p>此外，作者还将模型的性能与其他先进检测器进行了比较，包括单阶段和两阶段的模型。结果显示，提出的模型在精确度、mAP和R等方面优于其他模型，尤其在检测水下模糊小目标方面表现出更高的优势。</p><p>总的来说，通过在多个实验中验证和分析，本文得出了提出的水下目标检测算法的有效性，并强调了其在水下环境中检测小、模糊和具有挑战性目标方面的改进性能。</p><h2 id="5-实验部分的不足"><a href="#5-实验部分的不足" class="headerlink" title="5.实验部分的不足"></a>5.实验部分的不足</h2><p>根据文章中的描述，实验部分存在以下一些不足之处：</p><ol><li><p>实验设计方面：文章没有明确提到采用的随机分割训练集和测试集的方法，也未详细说明采用的交叉验证策略。这些细节对于确保实验的可靠性和结果的泛化能力非常重要。</p></li><li><p>数据集选择：文章提到使用了URPC和DUT-USEG数据集进行实验评估，但在论文中没有提供这些数据集的详细信息，包括数据集的规模、数据集的特点以及用于评估的标准。这使得读者难以评估实验结果的全面性和可靠性。</p></li><li><p>对比实验设计不足：文章提到了与其他先进检测器的比较，但没有提供详细的对比实验设置和结果分析。比较的范围和依据也未明确说明，这些信息对于读者更好地理解该算法在检测性能方面的优势和特点很重要。</p></li><li><p>参数选择和敏感性分析：文章没有提及对于算法中的关键参数进行敏感性分析，即改变参数后检测结果的变化情况。这样的分析可以帮助读者更好地理解参数设置的影响，并确定算法的鲁棒性和稳定性。</p></li></ol><p>综上所述，虽然文章在实验部分通过一系列的实验和分析来验证算法的性能，但是仍然存在一些不足之处，包括实验设计的缺失、缺乏对数据集的详细说明、对比实验设计的不足以及缺乏参数敏感性分析。完善这些方面将有助于提高实验的可信度和结果的解释性。</p><h2 id="6-作者使用该方法的优点"><a href="#6-作者使用该方法的优点" class="headerlink" title="6.作者使用该方法的优点"></a>6.作者使用该方法的优点</h2><p>根据这篇文章，作者使用的水下目标检测方法具有以下优点：</p><ol><li><p>特征增强策略：作者设计了特征增强门控模块，用于有选择地增强或抑制多尺度特征，并降低水下复杂环境噪声对特征融合的干扰。这样可以提高前景和背景之间的区分度，从而改善水下目标检测的准确性。</p></li><li><p>动态融合策略：作者提出的动态融合模块可以根据目标的尺度动态调整特征融合的权重。这种策略可以逐步地聚合相邻层的特征信息，避免并解决了多尺度特征融合中的冲突问题。通过动态融合，可以更好地保留重要信息并抑制噪声干扰。</p></li><li><p>快速空间金字塔混合池化（FMSPP）：文章提出了一种基于相同尺寸的快速空间金字塔混合池化结构，它能够提供更强的纹理和轮廓特征描述能力，同时减少参数数量，进一步提高模型的泛化能力和分类准确性。</p></li><li><p>数据增强策略：作者采用了Copy-reduce-rotate-paste的数据增强策略，增加了小目标生成的正样本数量，并通过增加它们在损失函数中的贡献来平衡训练过程。这有效地提高了模型对小目标的检测能力。</p></li><li><p>检测性能优势：通过与其他先进的水下目标检测器进行比较实验，作者的方法在检测性能和效率方面取得了明显的优势。相较于当前主流的目标检测器，本方法在精确度、mAP、召回率等评估指标上表现出明显的优势。</p></li></ol><h2 id="7-作者使用该方法的缺点"><a href="#7-作者使用该方法的缺点" class="headerlink" title="7.作者使用该方法的缺点"></a>7.作者使用该方法的缺点</h2><p>根据这篇文章，作者使用的水下目标检测方法在实验部分存在一些缺点：</p><ol><li><p>实验设计方面：文章没有明确说明随机分割训练集和测试集的方法，也未详细说明采用的交叉验证策略，这些细节对实验的可靠性和结果的泛化能力非常重要。</p></li><li><p>数据集选择：文章提到使用了URPC和DUT-USEG数据集进行实验评估，但未提供这些数据集的详细信息，如数据集规模、数据集特点和评估标准。这使得读者难以评估实验结果的全面性和可靠性。</p></li><li><p>对比实验设计不足：文章提到与其他先进检测器的比较，但未提供详细的对比实验设置和结果分析。比较的范围和依据也未明确说明，这些信息有助于读者更好地理解该算法在检测性能方面的优势。</p></li><li><p>缺乏详细的计算和内存消耗分析：文章未提供关于该方法的计算和内存消耗的详细分析，这对于评估算法的实用性和可部署性非常重要。</p></li></ol><p>综上所述，这些缺点可能限制了读者对于该水下目标检测方法在不同场景和数据集上推广应用的能力。</p><h2 id="8-论文的创新点主要体现在以下几个方面"><a href="#8-论文的创新点主要体现在以下几个方面" class="headerlink" title="8.论文的创新点主要体现在以下几个方面"></a>8.论文的创新点主要体现在以下几个方面</h2><p>根据这篇文章，论文的创新点主要体现在以下几个方面：</p><ol><li><p>特征增强和动态融合策略：作者设计了特征增强门控模块，能够有选择地增强或抑制多尺度特征，以减少水下复杂环境噪音对特征融合的干扰。动态融合模块可以根据目标尺度动态调整融合权重，逐步聚合相邻特征，并有效地抑制冲突信息。这两个策略的结合，提升了水下目标检测的准确性和鲁棒性。</p></li><li><p>快速空间金字塔混合池化（FMSPP）：该方法引入了FMSPP模块，通过随机选择最大池化和平均池化来解决两种不同池化方法的问题，并利用通道连接将池化层的输出进行连接。这种池化方法综合了两种方法的优点，有效提取特征，并减少了特征维度。</p></li><li><p>邻近特征动态聚合结构：论文提出了一种邻近特征动态聚合结构，通过动态聚合不同尺度的邻近特征，有效地抑制冲突信息，提高目标检测的准确性和鲁棒性。该结构逐步聚合相邻特征，避免了噪声问题，并通过动态聚合模块保留重要信息。</p></li><li><p>实验评估和性能分析：作者通过实验对提出的水下目标检测算法进行了全面的评估和性能分析。实验结果表明，该方法在水下目标检测任务中表现出较高的准确性和鲁棒性，并超越了其他先进的检测器。这些实验结果对于验证算法的效果和可行性非常重要，可以为进一步改进和应用提供参考。</p></li></ol></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【水声信道】Channel State Information Prediction for Adaptive Underwater Acoustic Downlink OFDMA System: Deep Neural Networks Based Approach</title>
      <link href="/2024/01/29/shui-sheng-xin-dao-channel-state-information-prediction-for-adaptive-underwater-acoustic-downlink-ofdma-system-deep-neural-networks-based-approach/"/>
      <url>/2024/01/29/shui-sheng-xin-dao-channel-state-information-prediction-for-adaptive-underwater-acoustic-downlink-ofdma-system-deep-neural-networks-based-approach/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><em><strong>Channel State Information Prediction for Adaptive Underwater Acoustic Downlink OFDMA System: Deep Neural Networks Based Approach<br>基于深度神经网络的自适应水声下行OFDMA系统信道状态信息预测</strong></em></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>In underwater acoustic (UWA) adaptive communication system, due to time-varying channel, the transmitter often has outdated channel state information (CSI), which results in low efficiency. UWA channels are much more difficult to estimate and predict than terrestrial wireless channels, given the more severe multipath environments with varying propagation speeds in different locations, non-linear propagation paths, several-order higher propagation latency, mobile transceiver and obstacles in the sea, etc. To handle the complexity, this paper proposes an efficient online CSI prediction model for UWA CSI prediction considering the complicated correlationship of UWA channels in both the time and frequency domains. This paper designs a learning model called CsiPreNet, which is an integration of a one-dimensional convolutional neural network (CNN) and a long short term memory (LSTM) network. The performance is compared with the widely used recursive least squares (RLS) predictor, the approximate linear dependency recursive kernel least-squares (ALD-KRLS), and two common conventional deep neural networks (DNN) predictors, i.e., back propagation neural network (BPNN) and LSTM network using the measured data recorded in the South China Sea. To validate the efficacy of prediction, we investigate the prediction of CSI in simulated downlink UWA orthogonal frequency division multiple access (OFDMA)systems. Specifically, themeasuredUWA channel is used in the OFDMA system. A joint subcarrier-bitpower adaptive allocation scheme is used for resource allocation. To further improve the performance, we develop an offline-online prediction scheme, enabling theprediction results tobemore stable. Simulation and experimental results show that the CsiPreNet has superior performance than the existing solutions, thanks to its capability in capturing both the temporal and frequency correlation of the UWA CSIs.<br><code>在水声（UWA）自适应通信系统中，由于信道的时变特性，发射端往往具有过时的信道状态信息（CSI），导致系统效率低下。考虑到在不同位置具有不同传播速度的更严重的多径环境、非线性传播路径、几阶更高的传播延迟、移动的收发器和海中的障碍物等，UWA信道比陆地无线信道更难以估计和预测。摘要针对水声信道在时域和频域上的复杂相关性，提出了</code><em><strong>一种有效的水声信道状态信息在线预测模型</strong></em><code>。本文设计了一种称为</code><em><strong>CsiPreNet的学习模型</strong></em><code>，它是一维卷积神经网络（CNN）和长短期记忆（LSTM）网络的集成。性能与广泛使用的递归最小二乘（RLS）预测器，近似线性依赖递归核最小二乘（ALD-KRLS）和两种常见的常规深度神经网络（DNN）预测器进行了比较，即，利用南海实测数据，对BP神经网络（BPNN）和LSTM网络进行了仿真研究。为了验证预测的有效性，我们研究了信道状态信息的预测在下行链路的模拟水声正交频分多址（OFDMA）系统。具体地，在OFDMA系统中使用所测量的UWA信道。一个联合的子载波比特功率自适应分配方案用于资源分配。为了进一步提高预测性能，我们提出了离线-在线预测方案，使预测结果更加稳定.仿真和实验结果表明，CsiPreNet具有比现有方法更优越的上级性能，这要归功于它能够同时捕获水声CSI的时间和频率相关性.</code><br>Index Terms—Underwater acoustic (UWA) channel, adaptive communication system, channel state information (CSI)prediction, deep neural network (DNN), orthogonal frequency division multiple access (OFDMA).<br><code>索引词——水下声学（UWA）信道，自适应通信系统，信道状态信息（CSI）预测，深度神经网络（DNN），正交频分多址（OFDMA）。</code></p><h1 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h1><p>Due to the low speed of sound propagation underwater (about 1500 m/s), limited bandwidth, serious multipath and Doppler effects, etc., underwater acoustic (UWA) channels are one of the most challenging communication media [1]. Specifically, UWA channels exhibit larger time dispersions (on the order of hundreds of milliseconds), several-order higher than those with terrestrial radio channels. As such, UWA channels often experience severe inter symbol interference, which requires sophisticated and computationally expensive equalization techniques [2], [3]. Moreover, small-scale phenomena, e.g., scattering caused by surface waves, also contribute to the fast temporal variability of UWA channels [4]. Time-varying multipath propagation and limited bandwidth severely affect the performance of the UWA communication system. To improve spectrum utilization, adaptive communication technology is applied to UWA communication system [5]–[10].<br><code>由于声在水下传播速度较低（约1500 m/s）、带宽有限、多径和多普勒效应严重等原因，水声信道是最具挑战性的通信介质之一。具体而言，UWA信道表现出更大的时间分散（数百毫秒的数量级），几阶高于地面无线电信道。因此，UWA信道经常经历严重的符号间干扰，这需要复杂且计算昂贵的均衡技术[2]、[3]。此外，小规模的现象，例如，由表面波引起的散射也有助于UWA信道的快速时间变化[4]。时变多径传播和有限带宽严重影响了水声通信系统的性能。为了提高频谱利用率，自适应通信技术被应用于水声通信系统[5]-[10]。</code><br>The performance of the adaptive communication systems depend on the knowledge of channel state information (CSI) provided by the feedback of the receiver. Specifically, CSI determines the physical-layer parameters and setting of adaptive UWA communications. For example, the transmitter needs to implement low-order modulation schemes at the physical layer in the case of poor channel status, and vice versa. Obviously, inaccurate CSI can lead to improper modulation schemes which in turn leads to lowcommunication efficiency. Furthermore, CSI also has a significant impact on resource allocation [11]–[13] in adaptive UWA orthogonal frequency division multiple access (OFDMA) systems. Therefore, obtaining accurate CSI is important for improving the performance of the adaptive UWA communication systems.<br><code>自适应通信系统的性能依赖于接收机反馈的信道状态信息（CSI）。具体地，CSI确定自适应UWA通信的物理层参数和设置。例如，在信道状态差的情况下，发射机需要在物理层实现低阶调制方案，反之亦然。显然，不准确的CSI会导致不适当的调制方案，这反过来又会导致通信效率低下。此外，CSI还对自适应UWA正交频分多址（OFDMA）系统中的资源分配具有显著影响[11]-[13]。因此，获得准确的CSI对于提高自适应水声通信系统的性能具有重要意义。</code><br>However, because of the large propagation delay and rapid changing of UWA channel, CSI received by the transmitter is usually outdated. Especially, in the UWA communication networks, the central node needs to take more time to obtain the CSIs from all users. This makes the problem of outdated CSI even more severe. In recent years, many researches have studied the performance of adaptive UWA communication systems using outdated CSI. In [14], an adaptive UWA communication scheme for multiantenna transmissions based on the partial CSI was proposed. In [12], long-term statistics of CSI were considered for channel feedback to mitigate the impact of outdated CSI. In [13], the CSI used for feedback was selected between instantaneous CSI and average CSI for system performance improvement. In addition to the extensive research on how to deal with the negative impact of the outdated CSIs, it is found that channel prediction is a desirable method to fully address the problem. Since the speed of sound is very low, the ability to predict the CSI at least one transmission round ahead highly affects the performance of the adaptive communication systems. It is indeed challenging forUWAcommunications in the range of several kilometers, which significant limits the use of feedback.<br><code>然而，由于水声信道的传播时延大、变化快，发射端接收到的信道状态信息往往是过时的。特别是在水声通信网络中，中心节点需要花费更多的时间来获得来自所有用户的CSI。这使得过时的CSI问题更加严重。近年来，许多研究都在研究使用过时CSI的自适应水声通信系统的性能。在[14]中，提出了一种基于部分CSI的用于多天线传输的自适应UWA通信方案。在[12]中，信道反馈考虑了CSI的长期统计，以减轻过时CSI的影响。在[13]中，在瞬时CSI和平均CSI之间选择用于反馈的CSI，以改善系统性能。除了对如何处理过时CSI的负面影响进行了广泛的研究之外，还发现信道预测是完全解决该问题的理想方法。由于声音的速度非常低，因此在至少一个传输回合之前预测CSI的能力高度影响自适应通信系统的性能。在几公里范围内的UWA通信确实具有挑战性，这大大限制了反馈的使用。</code><br>With a reasonable tracking ability and simple design, a linear predictor applying the recursive least-squares (RLS) algorithm has been widely used for UWA channel impulse responses (CIRs) prediction [8], [15]–[18]. For seasonal UWA time-varying channels, autoregressive (AR) processs [20] and Holt-Winters [21], [24] were introduced to model the seasonal correlation. An efficient adaptive predictor operating in the delay-Doppler domain for UWA time-varying channels was proposed in [25], which does not require any prior knowledge of channel dynamic model and noise statistics.<br><code>采用递归最小二乘（RLS）算法的线性预测器具有合理的跟踪能力和简单的设计，已被广泛用于UWA信道脉冲响应（CIR）预测[8]，[15]-[18]。对于季节性UWA时变信道，引入自回归（AR）过程[20]和Holt-Winters [21]，[24]来模拟季节相关性。在[25]中提出了一种用于UWA时变信道的在延迟-多普勒域中操作的有效自适应预测器，其不需要信道动态模型和噪声统计的任何先验知识。</code><br>For adaptive UWA OFDM and OFDMA systems, existing channel predictors were often realized at several significant channel taps in the time domain [8], [17]–[19]. Few researchers applied the predictor to each OFDM subcarrier in the frequency domain. This is because the variation at each subcarrier is a combined variation of multiple taps, which makes it difficult to predict using a linear predictor. Moreover, prediction on a large number of OFDM subcarriers leads to high complexity. The traditional linear predictor can not handle frequency domain prediction well. Kernel adaptive filtering (KAF) methods are rapidly gaining popularity to solve a wide variety of prediction, identification and regression problems, since they provide stateof-the-art performance in many real-world applications [22]. In particular, the kernel RLS (KRLS) algorithm based on the RLS algorithm is one ofthe most popular algorithms inKAF[23], and shows good performance in channel prediction [22]. The rapid development of deep neural networks (DNNs) [26], [27], [31], [32] also makes it possible to perform channel prediction in the frequency domain. As long as there are sufficient data to train the neural network, DNNs can capture the complex correlation between different data in both the time and frequency domains. In the past thirty years, DNNs have been widely applied in wireless communications [28]. Reasonable application of DNNs can effectively optimize wireless communication and network system [29]. DNNs will be an indispensable tool for the design and operation of future wireless communication networks [30]. It convinces us that DNNs will also be promising for future research in UWA communications and networks. In wireless communications, many researches focused on channel prediction based on DNNs [33]–[37]. Although some studies have applied DNNs to UWA communications [38], to the best of our knowledge, no research ever used DNNs for channel prediction in UWA communication systems.<br><code>对于自适应UWA OFDM和OFDMA系统，现有的信道预测器通常在时域中的几个重要信道抽头处实现[8]，[17]-[19]。很少有研究者在频域中将预测器应用于每个OFDM子载波。这是因为每个子载波处的变化是多个抽头的组合变化，这使得难以使用线性预测器进行预测。此外，对大量OFDM子载波的预测导致高复杂度。传统的线性预测器不能很好地处理频域预测。核自适应滤波（KAF）方法正在迅速普及，以解决各种预测，识别和回归问题，因为它们在许多现实世界的应用中提供了最先进的性能[22]。特别是，基于RLS算法的核RLS（KRLS）算法是KAF中最流行的算法之一[23]，并且在信道预测中显示出良好的性能[22]。深度神经网络（DNN）的快速发展[26]，[27]，[31]，[32]也使得在频域中进行信道预测成为可能。只要有足够的数据来训练神经网络，DNN就可以在时域和频域中捕获不同数据之间的复杂相关性。在过去的三十年里，DNN已经广泛应用于无线通信[28]。DNN的合理应用可以有效地优化无线通信和网络系统[29]。DNN将成为未来无线通信网络设计和运营不可或缺的工具[30]。它使我们相信，DNN也将在未来的研究在西澳大学的通信和网络。在无线通信中，许多研究集中在基于DNN的信道预测[33]-[37]。虽然一些研究已经将DNN应用于UWA通信[38]，但据我们所知，没有研究将DNN用于UWA通信系统中的信道预测。</code><br>UWA channels are much more difficult to predict than terrestrial wireless channels, given the more severe multipath environmentswith varying propagation speeds in different locations, non-linear propagation paths, several-order higher propagation latency, mobile transceiver and obstacles in the sea, etc. The existing CSI prediction approaches, e.g., the RLS/KRLS-based prediction methods, only consider the amplitude of the channel tap, and do not consider the variety of the phase. However, when predicting UWA channel, the variety of channel delay cannot be ignored. In addition, due to the requirement of low power consumption of UWA communication hardware design, its computing power is far less than that of wireless communication system, which makes the traditional CSI prediction approaches not fully applicable for UWA channel prediction. In UWAnetworks, such as the Internet ofunderwater things (IoUT) with extremely large-scale deployment of sensor nodes [39], the above difficulties are more serious. To handle the complexity, in this paper, we propose an DNN-based efficient online CSI prediction model for CSI prediction and study the performance according to measured UWA channels in South China Sea. To validate the efficacy of prediction, we investigate the prediction of the measured channels in the simulated UWA OFDMA system. Furthermore, an offline-online prediction scheme is developed to improve system performance. The main contributions of this paper are three-fold.<br><code>考虑到在不同位置具有变化的传播速度、非线性传播路径、几阶更高的传播延迟、移动的收发器和海中的障碍物等的更严重的多径环境，UWA信道比陆地无线信道更难以预测。基于RLS/KRLS的预测方法，只考虑了信道抽头的幅度，没有考虑相位的变化。然而，在预测水声信道时，不能忽略信道时延的变化。另外，由于水声通信硬件设计对低功耗的要求，其计算能力远低于无线通信系统，使得传统的CSI预测方法不能完全适用于水声信道预测。在UWA网络中，例如具有超大规模传感器节点部署的水下物联网（IOUT）[39]，上述困难更为严重。为了处理复杂性，本文提出了一种</code><em><strong>基于DNN的有效的在线CSI预测模型</strong></em><code>，并根据在南中国海测量的水声信道进行性能研究。为了验证预测的有效性，我们研究了预测的测量信道在模拟的UWA OFDMA系统。此外，离线-在线预测方案的开发，以提高系统的性能。本文的主要贡献有三个方面。</code></p><ul><li>We design a learning model called CsiPreNet for UWA CSI prediction, that is a combination of a one-dimensional convolutional neural network (CNN) and a long short term memory (LSTM) network. The proposed CsiPreNet can exploit the complicated frequency-temporal correlationship of UWA channels to conduct the CSI prediction effectively. The prediction performance is evaluated with measured channel data recorded in South China Sea. The proposed CsiPreNet has excellent performance on UWA CSI prediction, based on its ability to capture the information in both the frequency and time domains.<br><code>我们为UWA CSI预测设计了一个名为CsiPreNet的学习模型，它是一维卷积神经网络（CNN）和长短期记忆（LSTM）网络的组合。CsiPreNet可以利用水声信道复杂的频时相关性进行有效的CSI预测。用南海实测航道资料对预报性能进行了评价。建议的CsiPreNet具有优异的性能UWA CSI预测的基础上，它的能力，以捕捉在频域和时域的信息。</code></li><li>We design a simulated downlink UWA OFDMA system, and investigate the impact of CSI prediction. Specifically, the measured UWA channels are used in the simulated OFDMA system as ground truth. A joint subcarrier-bit-power adaptive allocation scheme is used for resource allocation by using the predicted CSI for feedback.<br><code>本文设计了一个仿真的下行UWA OFDMA系统，并研究了CSI预测对下行UWA OFDMA系统性能的影响。具体地，所测量的UWA信道在仿真的OFDMA系统中被用作地面实况。联合子载波比特功率自适应分配方案用于通过使用用于反馈的预测CSI进行资源分配。</code></li><li>We design an offline-online prediction scheme to improve the stability of the developed learning models when applying it to the simulated OFDMA system. A large amount of historical CSI is input into the offline part to train the learning models, and the trained model is applied to the online part for CSI prediction. The predicted CSI is used as feedback for resource allocation. Furthermore, we update the training data of the offline part after several transmission rounds to ensure the stability of the learning models. Simulation results demonstrate the superior performance of the proposed offline-online prediction based on CsiPreNet model.<br><code>我们设计了一个离线-在线预测方案，以提高稳定性的开发的学习模型时，将其应用到模拟的OFDMA系统。离线部分输入大量的历史CSI数据对学习模型进行训练，训练后的模型应用于在线部分进行CSI预测。预测的CSI被用作资源分配的反馈。此外，我们在几轮传输后更新离线部分的训练数据，以确保学习模型的稳定性。仿真结果表明，基于CsiPreNet模型的离线-在线预测具有良好的上级性能。</code></li></ul><p>The rest of this paper is organized as follows. Section II introduces the related work. In Section III, we describe the downlink UWA OFDMA system, the UWA channel model, and the measured data recorded in sea test. In Section IV, we propose the BPNN, LSTM, and CsiPreNet predictionmodels. In Section V, we present the joint subcarrier-bit-power allocation scheme, the limited feedback for UWA OFDMA system, and the proposed offline-online prediction scheme. In SectionVI, we analyze the prediction performance of the three DNN prediction models using measured CSI, and apply these three models to the simulated downlink UWAOFDMA system for performance evaluation, and then analyze the computational complexity and the running time of all models, followed by the concluding remarks and further research issues in Section VII.<br><code>本文的其余部分组织如下。第二节介绍了相关工作。第三部分介绍了水声OFDMA系统的下行链路、水声信道模型以及海上试验的实测数据。在第四节中，我们提出了BPNN，LSTM和CsiPreNet预测模型。在第五节中，我们提出了联合子载波比特功率分配方案，有限反馈的水声OFDMA系统，并提出了离线在线预测方案。在第六节中，我们使用测量的CSI分析了三种DNN预测模型的预测性能，并将这三种模型应用于仿真的下行UWAOFDMA系统中进行性能评估，然后分析了所有模型的计算复杂度和运行时间，然后在第七节中进行了总结和进一步的研究问题。</code></p><h1 id="II-RELATED-WORK"><a href="#II-RELATED-WORK" class="headerlink" title="II. RELATED WORK"></a>II. RELATED WORK</h1><p>Due to the fast changing nature and the extremely limited bandwidth of the UWA channels, an outdated CSI can lead to severe performance degradation, so accurate channel prediction is critical. Although many scholars have studied channel prediction in terrestrial wireless communications [33]–[37], [40], [41], [43], the prediction of UWA channels has received little attention until recent years [8], [15]–[21], [24], [25].<br><code>由于水声信道的快速变化和带宽的限制，过时的CSI会导致性能严重下降，因此准确的信道预测至关重要。虽然许多学者研究了地面无线通信中的信道预测[33]-[37]，[40]，[41]，[43]，但直到最近几年[8]，[15]-[21]，[24]，[25]，UWA信道的预测才得到很少的关注。</code><br>The prediction methods are generally divided into two categories, i.e., model-based and non-model-based. The modelbased methods assume that certain knowledge and variation about the channel are known, then the correctly defined model can improve the prediction accuracy. For example, the channel variations and parameters were based on an AR model and tracked by an extended Kalman filter in [40], and it assumed that the channel tap coefficients are uncorrelated. By contrast, in [41], it used the RLS algorithm with a postfilter for channel tracking, and considered the multipath correlation. If the real channel does not match the assumed model, the performance of the above-mentioned methods can not be satisfactory. On the contrary, the non-model-based adaptive prediction methods do not rely on the prior knowledge of the channel. Thus it is more suitable for the real communication systems [8], [15]–[21], [24], [25]. Generally speaking, the adaptive predictors can adaptively track channel variations and adjust itself. Least Mean Square (LMS) andRLS are two widely used adaptive algorithms forCSI prediction [8], [15]–[19], [43]. Compared with LMS algorithm, RLS algorithm has the better tracking ability at the cost of a higher computational complexity [42]. The common method using RLS predictor for time domain prediction is shown in Fig. 1 [18], [43]. However, these studies only considered the variety of channel taps and assumed that the delay of channel taps is stable for a certain period of time.<br><code>预测方法一般分为两类，即，基于模型和非基于模型。基于模型的方法假设信道的某些知识和变化是已知的，那么正确定义的模型可以提高预测精度。例如，信道变化和参数基于AR模型，并由[40]中的扩展卡尔曼滤波器跟踪，并且假设信道抽头系数不相关。相比之下，在[41]中，它使用RLS算法和后置滤波器进行信道跟踪，并考虑了多径相关性。如果真实的信道与假设的模型不匹配，上述方法的性能不能令人满意。相反，非基于模型的自适应预测方法不依赖于信道的先验知识。因此它更适合于真实的通信系统[8]，[15]-[21]，[24]，[25]。一般来说，自适应预测器可以自适应地跟踪信道变化并调整自身。最小均方（LMS）和RLS是用于CSI预测的两种广泛使用的自适应算法[8]，[15]-[19]，[43]。与LMS算法相比，RLS算法具有更好的跟踪能力，但计算复杂度较高[42]。使用RLS预测器进行时域预测的常用方法如图1 [18]、[43]所示。然而，这些研究仅考虑了信道抽头的变化，并假设信道抽头的延迟在一定时间内是稳定的。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-1.png"></p><blockquote><p>Fig. 1. Time domain predictor for sparse channel based on RLS algorithm.<br><code>图1 基于RLS算法的稀疏信道时域预测器。</code></p></blockquote><p>On the other hand, the rapid development of DNNs makes it possible to capture complex channel variations accurately. In wireless communications, DNNs has been used to predict channel [33]–[37]. It inspires us to apply DNNs to channel prediction inUWA communication systems, which have drastically different channel characteristics.<br><code>另一方面，DNN的快速发展使得准确捕获复杂的信道变化成为可能。在无线通信中，DNN已被用于预测信道[33]-[37]。它启发我们将DNN应用于具有截然不同信道特性的UWA通信系统中的信道预测。</code><br>Obviously, combining the channel prediction with the adaptive UWA communication system can significantly improve the system performance. In [8], RLS predictor was embedded in an adaptive UWA OFDM system. In [19], a precoding based channel prediction scheme was designed for UWA OFDM system. In [25], the adaptive delay-Doppler spreading function (DDSF) prediction scheme was proposed, and used for UWA communication system under several simulated UWA channels. There is currently no deep learning based prediction model for adaptive UWA communication systems. This work is motivated to fill the gap.<br><code>显然，将信道预测与自适应水声通信系统相结合，可以显著提高系统性能。在文献[8]中，RLS预测器被嵌入到自适应UWA OFDM系统中。在[19]中，为UWA OFDM系统设计了一种基于预编码的信道预测方案。在文献[25]中，提出了自适应延迟-多普勒扩展函数（DDSF）预测方案，并将其用于几种模拟水声信道下的水声通信系统。目前还没有用于自适应UWA通信系统的基于深度学习的预测模型。这项工作的动机是填补差距。</code></p><h1 id="III-SYSTEM-MODEL-AND-DATASET-DESCRIPTION"><a href="#III-SYSTEM-MODEL-AND-DATASET-DESCRIPTION" class="headerlink" title="III. SYSTEM MODEL AND DATASET DESCRIPTION"></a>III. SYSTEM MODEL AND DATASET DESCRIPTION</h1><p><code>三.系统模型和数据集描述</code><br>The UWA channel is time-spatial correlated, but in a very complicated way. Due to the characteristics of the long propagation delay (in seconds), curved propagation ray due to different acoustic velocity in different height, relative motions and changing propagation multi-paths, lack of synchronization in UWA channels [1], the parameter design and experiment setup for UWA communication is important.<br><code>水声信道具有复杂的时空相关性。由于水声信道具有传播延迟长（秒）、不同高度声速导致的传播射线弯曲、相对运动和传播多径变化、信道不同步等特点，水声通信的参数设计和实验装置非常重要。</code></p><p><em><strong>A. Downlink UWA OFDMA System</strong></em><br><code>A.下行UWA OFDMA系统</code><br>As shown in Fig. 2(a), we consider a simulated downlink UWA OFDMA system with U spatially separated users and a central node.<br><code>如图2（a）所示，我们考虑具有U个空间上分离的用户和中心节点的模拟下行链路UWA OFDMA系统。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-2.png"></p><blockquote><p>Fig. 2. System model: (a) Downlink UWA OFDMA system, (b) OFDMA handshaking process.<br><code>图二 系统模型：（a）下行链路UWA OFDMA系统，（B）OFDMA握手过程。</code></p></blockquote><p>Consider an OFDMA setup, where a total number of K subcarriers are allocated to all users. $K_u$ non-overlapping subcarriers are allocated to user u, where $\sum_{u=1}^U K_u=K$.The OFDM symbol period is T, the cyclic prefix (CP) length is $T_{c p}$, and the subcarrier interval is 1/T. The center frequency is $f_c$, and then the subcarrier frequency $f_k$ = $f_c$ + k/T, k = −K/2,…,K/2 − 1. Define d[k] as the coded information on the k-th subcarrier, and then the transmitted signal is<br><code>考虑OFDMA设置，其中总共K个子载波被分配给所有用户。</code>$K_u$ <code>个不重叠的子载波被分配给用户u，其中，</code>$\sum_{u=1}^U K_u=K$<code>。OFDM符号周期是T，循环前缀（CP）长度是</code>$T_{c p}$<code>，并且子载波间隔是1/T。中心频率是</code>$f_c$<code>，然后子载波频率</code>$f_k$ = $f_c$ + k/T，k =-K/2，.，K/2 − 1<code>。将d[k]定义为第k个子载波上的编码信息，然后发送的信号为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-3.png"><br>where S is the subcarrier index set, including the data subcarrier index set $S_D$ and the combpilot index set $S_P$, S = $S_D$ ∪ $S_P$. All users use the same pilot symbols for channel estimation, and the k-th subcarrier can only be allocated to one user, where k ∈ $S_D$. The candidate modulation schemes of d[k] are binary phase shift keying (BPSK), quadrature phase-shift keying (QPSK), 8-quadrature amplitude modulation (8QAM),and 16-quadrature amplitude modulation (16QAM) with 2-D Gray mapping. In other words, for the k-th subcarrier, where k ∈ $S_D$, the modulation level $M_K$ ∈{2, 4, 8, 16}, and if no data are transmitted, $M_K$ = 1. We assumed that the pilot symbols (k ∈ $S_P$) use the modulation scheme of QPSK.<br><code>其中S为子载波索引集，包括数据子载波索引集</code>SD<code>和梳状导频索引集</code>$S_P$，S = $S_D$ ∪ $S_P$<code>，所有用户使用相同的导频符号进行信道估计，第k个子载波只能分配给一个用户，其中</code>k ∈ $S_D$<code>。d[k]的候选调制方案是具有2-D格雷映射的二进制相移键控（BPSK）、正交相移键控（QPSK）、8正交幅度调制（8 QAM）和16正交幅度调制（16 QAM）。换句话说，对于第k个子载波，其中</code>k ∈ $S_D$<code>，调制级别</code>$M_K$ ∈{2，4，8，16} <code>，并且如果没有数据被发送，则</code>$M_K$ = 1<code>。我们假设导频符号</code>（k ∈ $S_P$)<code>使用QPSK的调制方案。</code><br>Assuming that the receiver (user u) has correctly compensated for the Doppler frequency offset caused by the relative motion, the time-varying UWA multipath channel model transmitted from the central node to user u in a CP-OFDM block can be expressed as<br><code>假设接收器（用户u）已经正确地补偿了由相对运动引起的多普勒频率偏移，则在CP-OFDM块中从中心节点发送到用户u的时变UWA多径信道模型可以表示为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-4.png"><br>where $N_{p, u}$ is the number of channel paths from the central node to user u; $A_{p, u}(t)$ is the attenuation coefficient of the p-th path in a CP-OFDM block; $T_{p, u}(t)$ is the time delay corresponding to the p-th path. Assuming that the CP length $T_{c p}$ is longer than the maximum multipath delay, the signal received by user u is<br><code>其中</code>$N_{p, u}$<code>为中心节点到用户u的信道路径数; </code>$A_{p, u}(t)$``为CP-OFDM块中第p条路径的衰减系数;<code> $T_{p, u}(t)$</code>为第p条路径对应的时延，假设CP长度<code>$T_{c p}$</code>大于最大多径时延，则用户u接收到的信号为<code>![](https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-5.png) where w(t) is the additive noise. Substituting (1) into (3), after removing CP and undergoing discrete fourier transform (DFT) transformation, the frequency domain baseband reception vector of user u on the k-th subcarrier can be obtained by</code>其中w(t)是加性噪声。将（1）代入（3），在去除CP并进行离散傅立叶变换（DFT）之后，可以通过下式获得用户u在第k个子载波上的频域基带接收向量：<code>![](https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-6.png) where $Z_u[k]$ and $d_u[k]$ represent the received data and the transmitted data on the k-th subcarrier allocated to user u, respectively. $H _u[k]$ represents the channel gain of user u on the k-th subcarrier. $V_u[k]$ is the frequency domain additive noise in the frequency domain. CSI can be measured in the form of Signal-to-noise ratio (SNR), which can be expressed as</code>其中<code>$Z_u[k]$</code>和<code>$d_u[k]$</code>分别表示分配给用户u的第k个子载波上的接收数据和发送数据。<code>$H _u[k]$ </code>表示用户u在第k个子载波上的信道增益。<code>$V_u[k]$</code>是频域中的频域加性噪声。CSI可以以信噪比（SNR）的形式测量，其可以表示为<code>![](https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-7.png) where $\sigma_s^2$  is the symbol energy, and $\sigma_u^2$ is the noise variance of user u.</code>其中<code>$\sigma_s^2$</code>是符号能量，<code>$\sigma_u^2$</code>是用户u的噪声方差。<code>Therefore, based on the knowledge of the transmitted and received signals, i.e., d[k] and z[k], the CSI can be estimated. So far, scholars have developed several CSI estimation methods, e.g., maximum likelihood (ML), minimum mean square error (MMSE), least squares (LS), and compressed sensing (CS) methods. In this paper, we use the orthogonal matching pursuit (OMP) algorithm [44] for CSI estimation, which is one of the CS methods.</code>因此，基于对所发送和接收的信号的了解，即，d[k]和z[k]，可以估计CSI。到目前为止，学者们已经开发了几种CSI估计方法，最大似然（ML）、最小均方误差（MMSE）、最小二乘（LS）和压缩传感（CS）方法。在本文中，我们使用正交匹配追踪（OMP)算法[44]进行CSI估计，这是CS方法之一。<code>Furthermore, the handshaking process of this adaptive downlink UWA OFDMA system is shown in Fig. 2(b). The CSI of each user is estimated by the request-to-send (RTS) packet, and then embedded into the clear-to-send (CTS) packet. The central node collects all users’ CSIs, and performs the adaptive resource allocation scheme based on them. Furthermore, the allocation table is embedded in the announcement (ANC) packet, and broadcasted with the DATA packet to all users. Upon reception, each user decodes its own data based on the allocation table. Therefore, the CSI embedded in a CTS packet is the CSI based on the RTS packet in slot T1, which outdates while using it for resource allocation in slot T3, under time-varyingUWAchannel.</code>此外，图2（B）示出了该自适应下行链路UWA OFDMA系统的握手过程。每个用户的CSI通过请求发送（RTS）分组估计，然后嵌入到清除发送（CTS）分组中。中心节点收集所有用户的CSI，并根据它们执行自适应资源分配方案。此外，分配表被嵌入在通知（ANC）分组中，并且与DATA分组一起被广播给所有用户。在接收时，每个用户基于分配表解码其自己的数据。因此，在时变UWA信道下，嵌入在CTS分组中的CSI是基于时隙T1中的RTS分组的CSI，其在用于时隙T3中的资源分配时过时。`<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-2.png"></p><blockquote><p>Fig. 2. System model: (a) Downlink UWA OFDMA system, (b) OFDMA handshaking process.<br><code>图二 系统模型：（a）下行链路UWA OFDMA系统，（B）OFDMA握手过程。</code></p></blockquote><p><em><strong>B. Experimental Dataset Description</strong></em><br><code>B.实验数据集描述</code><br>We carried out the sea trial in Lingshui Bay, South China Sea on May 8, 2014. In the experiment, two ships were used for communication. The sea depth was 70 meters. One ship was the transmitter with a transducer deployed 27 meters depth. Another ship was the receiver with a four-element line array deployed at the depth of 30 meters, and array element spacing was 0.6 meters. Transmission distance ranged from 1 km to 5 km. The geometry ofthe experiment and the setup ofthe system are given in Fig. 3(a).<br><code>我们于2014年5月8日在南海陵水湾进行了海试。在实验中，两艘船被用于通信。海深70米。一艘船是发射器，其换能器部署在27米深的地方。另一艘是接收船，四元线阵，部署在30米深，阵元间距0.6米。传输距离从1公里到5公里不等。实验的几何形状和系统的设置如图3（a）所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-8.png"></p><blockquote><p>Fig. 3. Experimental setup: (a) The geometry and the setup of the communication system. (b) Structure of one frame OFDM signal. (c) Organized CSI image.<br><code>图三 实验装置：（a）通信系统的几何形状和装置。(b)一帧OFDM信号的结构。(c)有条理的犯罪现场调查。</code></p></blockquote><p>As shown in Fig. 3(b), the transmitter sends frames of CPOFDM blocks. Each frame contains 8 OFDM blocks. One OFDM block has 681 subcarriers, which contains 595 data subcarriers and 86 pilot subcarriers, and the pilot interval is 8. CSI is estimated from the pilot subcarriers. CP length, block length and frame length areTcp=25 milliseconds (ms),Tblock=171 ms, and Tframe=1.568 seconds (s), respectively. The transmission interval between two adjacent frames is 4 s. The bandwidth is 4 kHz (kHz), and the central frequency is 8 kHz. LFM A signal is used for signal synchronization, whileLFMB signal andLFM C signal are the same signal that are used for Doppler estimation.<br><code>如图3（B）所示，发射机发送CPOFDM块的帧。每个帧包含8个OFDM块。一个OFDM块具有681个子载波，其包含595个数据子载波和86个导频子载波，并且导频间隔为8。从导频子载波估计CSI。CP长度、块长度和帧长度分别为Tcp =25毫秒（ms）、Tblock=171 ms和Tframe=1.568秒（s）。两个相邻帧之间的传输间隔为4s。带宽为4 kHz（kHz），中心频率为8 kHz。LFM A信号用于信号同步，而LFMB信号和LFM C信号是用于多普勒估计的相同信号。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-8.png"></p><blockquote><p>Fig. 3. Experimental setup: (a) The geometry and the setup of the communication system. (b) Structure of one frame OFDM signal. (c) Organized CSI image.<br><code>图三 实验装置：（a）通信系统的几何形状和装置。(b)一帧OFDM信号的结构。(c)有条理的犯罪现场调查。</code></p></blockquote><p>The channel of each block can be estimated by the pilot signal in the block. The time interval between two channels of the same block in adjacent frames is 4 s. We record a large amount of measured UWA CSI. We organize the CSI into a two-dimensional image as shown in Fig. 3(c). One dimension is the frequency dimension, representing the k-th subcarrier, and another dimension is the time dimension, representing the time t.<br><code>每个块的信道可以通过该块中的导频信号来估计。相邻帧中同一块的两个通道之间的时间间隔为4s。我们记录了大量测量的UWA CSI。我们将CSI组织成如图3（c）所示的二维图像。一个维度是频率维度，表示第k个子载波，另一个维度是时间维度，表示时间t。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-8.png"></p><blockquote><p>Fig. 3. Experimental setup: (a) The geometry and the setup of the communication system. (b) Structure of one frame OFDM signal. (c) Organized CSI image.<br><code>图三 实验装置：（a）通信系统的几何形状和装置。(b)一帧OFDM信号的结构。(c)有条理的犯罪现场调查。</code></p></blockquote><p>We can also express the CSI image as a matrix<br><code>我们还可以将CSI图像表示为矩阵</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-9.png"><br>where $C_{t, k}$ represents the CSI of the k-th subcarrier at the time t.<br><code>其中</code>$C_{t, k}$<code>表示在时间t的第k个子载波的CSI。</code><br>This paper selects two different measured channels with a transmission distance of 3 km and 5 km for analysis. In this case, the two ships anchored at a distance of 3 km and 5 km, respectively. But the ships still drift randomly in a small range with the water surface waves, resulting in time-varying of the channel. The analytical results will be presented in Section VI.<br><code>本文选取了传输距离为3 km和5 km的两个不同的实测信道进行分析。在这种情况下，两艘船分别在3公里和5公里的距离抛锚。但船舶仍随水面波在小范围内随机漂移，导致航道时变性。分析结果见第VI节。</code></p><h1 id="IV-PREDICTION-MODEL-BASED-ON-DNN"><a href="#IV-PREDICTION-MODEL-BASED-ON-DNN" class="headerlink" title="IV. PREDICTION MODEL BASED ON DNN"></a>IV. PREDICTION MODEL BASED ON DNN</h1><p><code>四.基于DNN的预测模型</code><br>The long propagation delay makes it critical to conduct accurate channel prediction, while the complicated mobile UWA environment makes the traditional solutions less effective. Therefore, we investigate the deep-learning approach for UWA channel prediction.<br><code>长传播时延使得进行准确的信道预测变得至关重要，而复杂的移动的水声环境使得传统的解决方案效果不佳。因此，我们研究了用于UWA信道预测的深度学习方法。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-10.png"><br>In this section, we will introduce three DNN models, i.e., BPNN, LSTM, and the proposed CsiPreNet. We use these three models for CSI prediction. Fig. 4 shows the training process of a typic DNN. Generally speaking, a DNN prediction model consists of two parts, the training part and the prediction part. Before the prediction part works, the network must be trained by the training symbols in the training part. First, the training samples are input into the neurons in the hidden layers and then are propagated to the output layer. If the learning errors between the DNN output results and the desired outputs are not reached the training threshold, the learning errors will be back-propagated from the output layer to the hidden layers. Meanwhile, the weights and biases in the neurons will be updated according to the learning errors. The training part will be repeated until the training threshold are reached, such as a given number of training rounds have been completed, or a certain learning error has been met. Finally, the well-trained model is used for CSI prediction.<br><code>在本节中，我们将介绍三种DNN模型，即，BPNN、LSTM和CsiPreNet。我们使用这三个模型进行CSI预测。图4示出了典型DNN的训练过程。一般来说，DNN预测模型由两部分组成，训练部分和预测部分。在预测部分工作之前，网络必须由训练部分中的训练符号进行训练。首先，将训练样本输入到隐藏层的神经元中，然后将其传播到输出层。如果DNN输出结果和期望输出之间的学习误差没有达到训练阈值，则学习误差将从输出层反向传播到隐藏层。同时，神经元中的权值和偏置将根据学习误差进行更新。训练部分将重复进行，直到达到训练阈值，例如已经完成给定数量的训练轮，或者已经满足特定的学习错误。最后，训练好的模型用于CSI预测。</code></p><p><em><strong>A. Back Propagation Neural Network (BPNN)</strong></em><br><code>A.反向传播神经网络</code><br>BPNN can be described as a multilayer perceptron (MLP) neural network with multiple hidden layers trained by the back propagation algorithm [45], [46]. Fig. 5 illustrates a typical BPNN, which includes an input layer, multiple hidden layers, and an output layer.<br><code>BPNN可以描述为多层感知器（MLP）神经网络，具有通过反向传播算法训练的多个隐藏层[45]，[46]。图5示出了典型的BPNN，其包括输入层、多个隐藏层和输出层。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-11.png"></p><blockquote><p>Fig. 5. Typical architecture of BPNN model.<br><code>图五 BPNN模型的典型结构。</code></p></blockquote><p>Then the back propagation algorithm [45], [46] is used to update the weights U and biases v in the neurons according to the learning errors. As shown in Fig. 5, the training part of the BPNN can be summarized as the forward propagation of training symbols and the back propagation of updating errors.<br><code>然后使用反向传播算法[45]，[46]根据学习误差更新神经元中的权重U和偏置v。如图5所示，BPNN的训练部分可以概括为训练符号的前向传播和更新误差的反向传播。</code><br>The update of BPNN can be briefly summarized as follows<br><code>BPNN的更新可以简单地总结如下</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-13.png"><br>where BPNN(·) presents all propagation process of BPNN, and P represents all the parameters in the BPNN.<br><code>其中BPNN(·)表示BPNN的所有传播过程，P表示BPNN中的所有参数。</code><br>For the CSI prediction, we replace x and y with $C_t$,$C_{t−1}$,…,$C_{t−n}$ and $C_{t+1}$ defined in (7), respectively. The prediction expression can be expressed as<br><code>对于CSI预测，我们用</code>$C_t$,$C_{t−1}$,…,$C_{t−n}$ <code>和</code> $C_{t+1}$<code>分别定义在（7）中。预测表达式可以表示为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-14.png"></p><p><em><strong>B. Long Short Term Memory (LSTM)</strong></em><br><code>B.长短期记忆（LSTM）</code><br>The LSTM network is explicitly designed to handle the long-term dependency problem, which is suitable for time series prediction [48]. LSTM network uses the approximate gradient calculation algorithm, which updates the weight matrices after each time step. However, the back-propagation algorithm can be used to calculate the entire gradient.<br><code>LSTM网络被明确设计用于处理长期依赖性问题，这适用于时间序列预测[48]。LSTM网络使用近似梯度计算算法，该算法在每个时间步后更新权重矩阵。然而，反向传播算法可用于计算整个梯度。</code><br>In a typical LSTM network, each cell has four gates, interacting in a special way. Fig. 6 illustrates a single LSTM memory cell.<br><code>在典型的LSTM网络中，每个单元有四个门，以特殊的方式相互作用。图6示出了单个LSTM存储器单元。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/2-12.png"></p><blockquote><p>Fig. 6. Typical architecture of LSTM cell.<br><code>见图6 LSTM单元的典型架构。</code></p></blockquote><h1 id="GPT解读"><a href="#GPT解读" class="headerlink" title="GPT解读"></a>GPT解读</h1><h2 id="水声通信具有许多独特的特点，主要有以下几个方面："><a href="#水声通信具有许多独特的特点，主要有以下几个方面：" class="headerlink" title="水声通信具有许多独特的特点，主要有以下几个方面："></a>水声通信具有许多独特的特点，主要有以下几个方面：</h2><ol><li>声传播速度低：在水中，声波的传播速度较低，通常约为1500米/秒，相对于空气中的声速约为4倍。这使得水声通信系统中的信号传输延迟较高。</li><li>信道环境复杂：水中存在多种复杂的信道环境，包括多径传播、声波反射、折射和散射等。这些环境特性导致信号在水中传播时会发生衰减、时延扩展和多径干扰等问题。</li><li>多径干扰：由于水中存在不同的障碍物和反射面，导致声波在传播过程中经历多条不同路径。这将导致接收到的信号是多径组合的结果，增加了信道估计和信号解调的复杂性。</li><li>环境噪声：水中存在各种噪声源，如自然水流、动物、船只等。这些噪声源会干扰通信信号，降低通信系统的性能。</li><li>频率损耗：在水中，高频信号的传播损耗比低频信号更大。这意味着在水声通信系统中，需要有针对性地选择合适的频率范围来保证信号的传输可靠性。</li></ol><p>总体而言，水声通信由于特殊的传播介质和环境条件，与传统的无线通信系统有很大的差异。在设计水声通信系统时，需要充分考虑水声信道的特点，并采取相应的信号处理和调制技术来提高系统性能和可靠性。</p><h2 id="前人研究的方法"><a href="#前人研究的方法" class="headerlink" title="前人研究的方法"></a>前人研究的方法</h2><ol><li>递归最小二乘（RLS）预测器：这种方法使用递归最小二乘算法来估计和预测水声信道的状态信息（CSI）。它根据历史观测值和预测误差来更新预测器的权重系数。RLS预测器能够提供高准确性的CSI预测，但在计算复杂度和存储需求方面较高。</li><li>基于预编码的方法：这种方法将预编码应用于水声通信中，通过对发送信号进行调制和编码来抵消信号在复杂水声信道中的衰落和干扰。预编码方法可以提供更好的抗干扰能力和传输可靠性，但需要较高的计算资源和功耗。</li><li>基于深度学习的方法：前人研究也探索了使用深度学习技术来解决水声信道预测问题。深度学习模型如卷积神经网络（CNN）和长短期记忆（LSTM）神经网络可用于建模和预测复杂的水声信道状态。这些方法可以通过学习频率和时间域上信道的相关性来提高预测性能。</li></ol><h2 id="作者要解决的问题"><a href="#作者要解决的问题" class="headerlink" title="作者要解决的问题"></a>作者要解决的问题</h2><p>根据这篇文章，作者主要要解决的问题是在水声自适应通信系统中，由于水声信道的时变性，发送方常常面临过时的信道状态信息（CSI），从而降低了系统的效率。由于水声信道相对于陆地无线信道更加复杂，很难准确估计和预测信道状况。因此，作者旨在提出一个有效的在线CSI预测模型，以考虑水声信道在时间和频域中的复杂相关性，从而改善自适应水声通信系统的性能。具体而言，论文的创新点包括设计了一个名为CsiPreNet的学习模型，它是一维卷积神经网络（CNN）和长短期记忆（LSTM）网络的结合。通过结合这两种网络结构，CsiPreNet模型可以捕捉到水声信道在频率和时间域上的复杂关联性，从而实现对CSI的准确预测。此外，论文还提出了离线-在线预测方案，用于改善系统的稳定性，并探讨了不同预测模型的性能比较，以及预测结果对于水声OFDMA系统性能的影响。通过解决这些问题，作者旨在提高水声自适应通信系统的性能，并推动水声通信领域的发展。</p><h2 id="作者通过什么样的方法来解决所提出的研究问题"><a href="#作者通过什么样的方法来解决所提出的研究问题" class="headerlink" title="作者通过什么样的方法来解决所提出的研究问题"></a>作者通过什么样的方法来解决所提出的研究问题</h2><p>作者通过采用深度神经网络（DNN）的方法来解决所提出的研究问题。具体而言，作者设计了一个名为CsiPreNet的学习模型，它是一维卷积神经网络（CNN）和长短期记忆（LSTM）网络的结合。这种组合的结构能够充分捕捉水声信道在时间和频率域上的复杂相关性。</p><p>首先，通过一维CNN模型，CsiPreNet模型可以有效提取相邻子载波之间的频率相关性。通过应用多个滤波器对信道状态信息（CSI）图像进行处理，CNN模型能够计算卷积结果并通过最大池化提取出显著特征。最后，将所有滤波器的输出结果进行组合并展开为一个向量。</p><p>然后，通过LSTM模型，CsiPreNet模型能够捕捉到CSI在时间上的相关性。使用这个模型，可以对当前向量进行预测。最后，使用完全连接的softmax层生成预测结果的概率分布。</p><p>为了训练CsiPreNet模型，作者使用了在南海收集的实测CSI数据，并对模型进行了参数设置和超参数优化。通过在实验中比较CsiPreNet模型与基于RLS和ALD-KRLS的预测器以及其他DNN模型的性能，作者证明了CsiPreNet模型在准确性方面的优势，并对其性能进行了评估和分析。</p><p>综上所述，作者通过设计组合了CNN和LSTM网络的CsiPreNet模型来解决水声通信系统中的CSI预测问题，从而提高了水声自适应通信系统的性能。</p><h2 id="作者通过哪些实验得出来了这些结果"><a href="#作者通过哪些实验得出来了这些结果" class="headerlink" title="作者通过哪些实验得出来了这些结果"></a>作者通过哪些实验得出来了这些结果</h2><p>作者通过一系列实验来验证他们提出的方法和研究结果。首先，他们使用在南海采集到的水声信道状态信息（CSI）数据集进行了训练和测试。这个数据集包含了在水下环境中记录的真实信道数据。</p><p>在实验中，作者比较了不同深度神经网络模型（包括BPNN，LSTM和CsiPreNet）以及基准预测器（RLS和ALD-KRLS）的性能。他们通过测量均方根误差（RMSE）和平均绝对百分比误差（MAPE）来评估预测误差。</p><p>此外，作者还通过绘制累积分布函数（CDF）来分析不同模型的预测误差分布情况。他们还考虑了不同参数设置和训练轮数对于模型性能的影响。</p><p>为了验证他们的方法对自适应水声通信系统的实际影响，作者还在一个模拟的OFDMA系统中进行了性能评估。他们比较了使用预测CSI和实际CSI进行资源分配的系统效果，以评估预测的准确性和对系统性能的影响。</p><p>通过这些实验，作者得出了他们提出的CsiPreNet模型在水声通信系统中预测信道状态信息方面的优越性能，并验证了他们的方法对自适应通信系统的效果。</p><h2 id="实验部分的不足"><a href="#实验部分的不足" class="headerlink" title="实验部分的不足"></a>实验部分的不足</h2><ol><li>缺乏更大规模的实际水声信道数据集：文章中作者所使用的实验数据集是在南海采集的真实信道数据，但规模相对较小。更多和更广泛的实际数据集可能有助于更全面地评估所提出方法的性能。</li><li>缺乏与其他算法的直接比较：文章中对所提出方法的性能进行了比较，但并未直接与其他现有的水声通信信道预测算法进行详细对比。直接与其他方法进行比较可以更好地了解所提出方法的优势和局限性。</li><li>缺乏对不同实际环境条件的探索：文章中所选择的实验环境是南海，在其他不同的水下环境条件下，信道特性可能会有所不同。更多针对不同实际环境的实验将有助于验证所提出方法的鲁棒性和适应性。</li><li>缺乏性能受到参数选择的检验：在实验中，所提出方法的性能高度依赖于网络结构和参数选择。然而，在文章中并没有进行详细的参数选择实验，这可能导致一些参数的选择不够优化。</li></ol><p>综上所述，将来的研究可以通过增加规模更大的实际数据集，与其他现有算法进行直接比较，并在更多的实际环境条件下进行实验来进一步探索所提出方法的优势和不足之处。此外，对网络结构和参数选择进行更多的实验和分析也是一个重要的方向</p><h2 id="作者使用该方法的优点"><a href="#作者使用该方法的优点" class="headerlink" title="作者使用该方法的优点"></a>作者使用该方法的优点</h2><p>作者使用所提出的方法（CsiPreNet模型）具有以下优点：</p><ol><li>高准确性：CsiPreNet模型通过结合一维卷积神经网络（CNN）和长短期记忆（LSTM）网络，能够捕捉水声信道在时间和频域上的复杂相关性。与传统的方法相比，CsiPreNet在水声信道状态信息（CSI）的预测方面表现更为准确。</li><li>考虑时变性：水声信道具有时变性，因此需要实时进行CSI的预测。CsiPreNet模型通过在线学习和预测的方式来处理时变信道，使得系统能够及时适应信道变化。</li><li>适用性广泛：CsiPreNet模型在实验中展示了在不同信道环境和不同信噪比条件下的高性能。它可以应用于各种水声通信场景，提供更稳定和高效的通信性能。</li><li>低计算复杂度：与传统的递归最小二乘（RLS）方法相比，CsiPreNet模型具有较低的计算复杂度。这使得它在实时水声通信系统中具有可行性。</li></ol><p>综上所述，作者使用CsiPreNet模型具有高准确性、时变性适应性、广泛适用性和较低计算复杂度等优点，有望提高水声自适应通信系统的性能。</p><h2 id="作者使用该方法的缺点"><a href="#作者使用该方法的缺点" class="headerlink" title="作者使用该方法的缺点"></a>作者使用该方法的缺点</h2><p>根据文章内容，以下是作者使用该方法（CsiPreNet模型）的一些可能的缺点：</p><ol><li>训练复杂度高：文章中提到，训练DNN模型（包括CsiPreNet模型）的过程具有较高的计算复杂度。这可能需要大量的训练时间和计算资源。</li><li>参数调节：CsiPreNet模型的性能可能受到参数调节的影响。找到最佳的模型参数和超参数（如网络深度、神经元数量等）需要一定的经验和实验调节。</li><li>数据需求量大：DNN模型通常需要较大规模的数据集进行训练，以获得良好的性能。在实际应用中，获取足够数量和质量的水声信道数据可能具有挑战性。</li></ol><p>请注意，以上缺点是根据对文章内容的理解进行的推测，具体的缺点可能需要进一步的研究和验证。</p><h2 id="论文的创新点主要体现在以下几个方面："><a href="#论文的创新点主要体现在以下几个方面：" class="headerlink" title="论文的创新点主要体现在以下几个方面："></a>论文的创新点主要体现在以下几个方面：</h2><ol><li>水声信道状态信息（CSI）的预测模型：论文提出了一种新颖的水声信道状态信息（CSI）预测模型，称为CsiPreNet。该模型将一维卷积神经网络（CNN）和长短期记忆（LSTM）网络相结合，以捕捉水声信道在时域和频域的复杂关联性。相比于传统的预测模型，CsiPreNet能够更准确地预测CSI，从而提高自适应水声通信系统的性能。</li><li>在线CSI预测模型：论文提出了一种在线CSI预测模型，能够实时地对水声信道的状态进行估计和预测。通过将历史CSI数据与训练的CsiPreNet模型相结合，能够在实时通信过程中快速地预测当前的CSI信息，从而改善传输效率和系统性能。</li><li>基于联合子载波-比特-功率分配的资源分配方案：论文还提出了一种基于联合子载波-比特-功率分配的资源分配方案，用于自适应水声OFDMA系统。该方案考虑了计算复杂性、功耗和系统性能等因素，可以根据通信环境和需求，灵活地分配子载波、分配比特和调整功率，以优化系统性能。</li><li>离线-在线预测方案：为了应对水声信道的时变特性，论文提出了一种离线-在线预测方案。离线部分主要包括对历史CSI数据的训练和更新，而在线部分则通过训练得到的模型进行实时的CSI估计和预测。该方案能够有效地应对水声信道的动态变化，提高通信系统的稳定性和性能。</li></ol></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 水声信道 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【信道估计】Sparse Channel Estimation for OFDM-Based Underwater Acoustic Systems in Rician Fading With a New OMP-MAP Algorithm</title>
      <link href="/2024/01/28/xin-dao-gu-ji-sparse-channel-estimation-for-ofdm-based-underwater-acoustic-systems-in-rician-fading-with-a-new-omp-map-algorithm/"/>
      <url>/2024/01/28/xin-dao-gu-ji-sparse-channel-estimation-for-ofdm-based-underwater-acoustic-systems-in-rician-fading-with-a-new-omp-map-algorithm/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><em><strong>基于OMP-MAP算法的莱斯衰落ofdm水声系统稀疏信道估计</strong></em></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>In this paper, a new channel estimation algorithm is proposed that exploits channel sparsity in the time domain for an orthogonal frequency division multiplexing (OFDM)-based underwater acoustical communications systems in the presence of Rician fading. A path-based channel model is used, in which the channel is described by a limited number of paths, each characterized by a delay, Doppler scale, and attenuation factor. The resulting algorithm initially estimates the overall sparse channel tap delays and Doppler shifts using a compressed sensing approach, in the form of the orthogonal matching pursuit (OMP) algorithm. Then a computationally efficient and novel channel estimation algorithm is developed by combining the OMP and maximum a posteriori probability (MAP) techniques for estimating the sparse complex channel path gains whose prior densities have complex Gaussian distributions with unknown mean and variance vectors, where a computationally efficient maximum likelihood (ML) algorithm is proposed for their estimation. Monte Carlo simulation results show that the mean square error (MSE) and symbol error rate (SER) performances of the OMP-MAP algorithm uniformly outperforms the conventional OMP-based channel estimation algorithm, in case of uncoded OFDM-based underwater acoustic (UWA) communications systems.<br><code>在本文中，提出了一种新的信道估计算法，利用信道的稀疏性在时域中的正交频分复用（OFDM）为基础的水声通信系统中存在的莱斯衰落。使用基于路径的信道模型，其中信道由有限数量的路径描述，每个路径的特征在于延迟、多普勒尺度和衰减因子。所得到的算法最初估计的整体稀疏信道抽头延迟和多普勒频移使用压缩感知方法，在正交匹配追踪（OMP）算法的形式。然后，一个计算有效的和新的信道估计算法相结合的OMP和最大后验概率（MAP）技术估计稀疏的复杂的信道路径增益的先验密度具有复杂的高斯分布与未知的均值和方差向量，其中一个计算有效的最大似然（ML）算法提出了他们的估计。Monte Carlo仿真结果表明，在未编码的OFDM水声通信系统中，OMP-MAP算法的均方误差（MSE）和误符号率（SER）性能均优于传统的基于OMP的信道估计算法。</code><br>Index Terms—Underwater Acoustic Channel Estimation, Equalization, OFDM, Orthogonal Matching Pursuit, MAP estimation.<br><code>索引词——水声信道估计，均衡，正交频分复用，正交匹配追踪，MAP估计。</code></p><h1 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h1><p>Underwater wireless communication has received increased attention over the past decade. In particular, there is growing interest in providing high-speed wireless links with high link reliability in various underwater applications such as offshore oil field exploration/monitoring, oceanographic data collection, maritime archaeology, seismic observations, environmental monitoring, port and border security among many others. While these tasks can be achieved through radio, optical, or sound (acoustic) waves, underwater acoustic (UWA) transmission is the most practical and commonly employed method due to the favorable propagation characteristics of sound waves in the underwater environment, and research efforts method due to the favorable propagation characteristics of sound waves in the underwater environment, and research efforts therefore have focused on this area. However, the UWA channel presents many challenges, such as long propagation delays, multipath and fading, limited bandwidth, and potentially high spatial and temporal variability. In addition, there is no typical underwater acoustic channel; different bodies of water exhibit quantifiably different properties. Consequently, current modems, implemented in hardware with a fixed, conservative set of transmission parameters, can face severe performance degradation in such varied UWA channels. The four distinguishing characteristics of UWA channels are frequency-dependent propagation loss, severe multipath with much longer delay spreads [1], Doppler spread, and low speed of sound propagation. None of these characteristics are nearly as pronounced in land-based radio channels, which makes underwater wireless communication comparatively quite difficult, and necessitates a dedicated system design.<br><code>水下无线通信在过去的十年中受到越来越多的关注。特别是，在各种水下应用中，例如近海油田勘探/监测、海洋学数据收集、海洋考古学、地震观测、环境监测、港口和边界安全等，对提供具有高链路可靠性的高速无线链路的兴趣日益增加。虽然这些任务可以通过无线电、光或声（声）波来实现，但由于声波在水下环境中的有利传播特性，水下声学（UWA）传输是最实用和最常用的方法，并且由于声波在水下环境中的有利传播特性，研究工作集中在该领域。然而，水下航行器信道提出了许多挑战，如长的传播延迟，多径和衰落，有限的带宽，以及潜在的高空间和时间的变化。此外，没有典型的水声信道;不同的水体表现出量化的不同属性。因此，当前的调制解调器，在硬件中实现一个固定的，保守的一组传输参数，可能会面临严重的性能下降，在这种变化的UWA信道。UWA信道的四个显著特征是频率相关的传播损耗、具有更长延迟扩展的严重多径[1]、多普勒扩展和声音传播的低速。这些特性在陆基无线电信道中几乎都不明显，这使得水下无线通信相对相当困难，并且需要专用的系统设计。</code><br>Space-time coding and multi-input multi-output (MIMO) configurations as well as orthogonal frequency division multiplexing (OFDM)-based communication systems, which were originally introduced in the context of terrestrial radiofrequency (RF) wireless communication, have been successfully applied to underwater communications in studies [2], [3], [4]. These techniques seem to be primary candidates for next generation UWA systems, due to their high information capacity and robustness to large multipath spreads [5] [6] and bring significant improvements in both throughput rate and error rate performance through channel estimation and equalization processes [7]. On the other hand, when the deployment of multi transmit/receive elements is not possible due to space or power limitations and path loss becomes a performance limiting factor, relay-assisted (cooperative) communication has also been applied to UWA systems to take advantage of diversity benefits. These works have been mostly focused on capacity and power allocation [8] for UWA relay channels with intersymbol interference (ISI), distributed channel coding and space time cooperative schemes for UWA channels [9] [10], adaptive relay-aided OFDM UWA communications using the amplify-and- forward (AF) protocol [11]. On the other hand, channel estimation and equalization for amplify-and-forward cooperative relay based OFDM systems in UWA channels was investigated in [12]. Notably, in [12], assuming Rayleigh fading channels between source, relay and destination, an efficient algorithm is developed based on the space-alternating generalized expectation-maximization (SAGE) technique. The fundamental performance bounds of such system generalized expectation-maximization (SAGE) technique.<br><code>空时编码和多输入多输出（MIMO）配置以及基于正交频分复用（OFDM）的通信系统，最初是在陆地射频（RF）无线通信的背景下引入的，已在研究[2]，[3]，[4]中成功应用于水下通信。这些技术似乎是下一代UWA系统的主要候选者，因为它们的高信息容量和对大的多径扩展的鲁棒性[5] [6]，并且通过信道估计和均衡过程[7]在吞吐率和错误率性能方面都带来了显着的改进。另一方面，当由于空间或功率限制而不可能部署多个发射/接收元件并且路径损耗成为性能限制因素时，中继辅助（协作）通信也已经应用于UWA系统以利用分集益处。这些工作主要集中在具有符号间干扰（ISI）的UWA中继信道的容量和功率分配[8]，UWA信道的分布式信道编码和空时协作方案[9] [10]，使用放大转发（AF）协议的自适应中继辅助OFDM UWA通信[11]。另一方面，在[12]中研究了UWA信道中基于放大转发协作中继的OFDM系统的信道估计和均衡。值得注意的是，在[12]中，假设源、中继和目的地之间存在瑞利衰落信道，基于空间交替广义期望最大化（SAGE）技术开发了一种有效的算法。这种系统的基本性能界广义期望最大化（SAGE）技术。</code></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 水声信道 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unit 1 How to Write an Introduction</title>
      <link href="/2024/01/26/unit-1-how-to-write-an-introduction/"/>
      <url>/2024/01/26/unit-1-how-to-write-an-introduction/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><code>第一单元 如何写介绍</code></p><h1 id="1-1-Structure"><a href="#1-1-Structure" class="headerlink" title="1.1 Structure"></a>1.1 Structure</h1><p><code>1.1结构</code><br>Until now, much of your science writing has focused on writing reports in which you simply described what you did and what you found. Although this will help you write the central ‘report’ sections (Methodology and Results) of a research paper or thesis, it doesn’t prepare you for writing an Introduction to a full-length research article; this is a new task that faces you once you move on to research writing.<br><code>到目前为止，你的大部分科学写作都集中在写报告上，在报告中你简单地描述了你做了什么和你发现了什么。虽然这会帮助你撰写研究论文或论文的中心“报告”部分（方法论和结果），但它并不能让你为撰写一篇完整的研究文章做好准备;这是一项新的任务，一旦你转向研究写作。</code><br>In practice, you will find that you need to be certain about what you have done and what you have found in order to write the Introduction, and so the best time to write it will be after you have written, or at least draft ed, the report sections. However, in this book, the structure of a research article is presented in the order in which it appears in a paper/thesis so that you can trace the connections between each part and see the sequence in which information is presented to the reader.<br><code>在实践中，你会发现，为了写引言，你需要确定你做了什么和你发现了什么，所以写引言的最佳时间是在你写好报告部分之后，或者至少是起草好报告部分之后。然而，在这本书中，研究文章的结构是按照它在论文/论文中出现的顺序呈现的，这样你就可以追踪每个部分之间的联系，并看到信息呈现给读者的顺序。</code></p><p>You may want to start your Introduction by describing the problem you are trying to solve, or the aim of your work, but as you will see when you examine published work, this is not how most research papers begin — and therefore it is not the best way for you to begin. In order to help you write the Introduction to your own research, the model you build must answer the following three questions:</p><p><code>你可能想通过描述你试图解决的问题或你工作的目标来开始你的介绍，但是当你检查已发表的作品时，你会发现这不是大多数研究论文的开始方式——因此这不是你开始的最佳方式。为了帮助您撰写自己研究的介绍，您构建的模型必须回答以下三个问题：</code></p><ul><li>How do writers normally start the Introduction?<br><code>作者通常如何开始引言？</code></li><li>What type of information should be in my Introduction, and in what order?<br><code>我的简介中应该包含哪些类型的信息，以及顺序如何？</code></li><li>How do writers normally end the Introduction?<br><code>作者通常如何结束引言？</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-1.png" alt="Fig. 1"><br>The first thing you may notice about Fig. 1 is that it is symmetrical. This is because many of the things you need to do in the Introduction are done — in reverse order — in the Discussion/Conclusion. For example, you need to write an opening sentence which enables you and your reader to ‘get in’ or start your paper/thesis and you also need to ‘get out’ at the end of the Discussion/Conclusion by finding an acceptable way to end the paper/thesis. In addition, you must look for a way to interface with the central report section at the end of the Introduction, and again — in reverse — when you move out of the central section to start the Discussion/ Conclusion.<br><code>你可能注意到的第一件事就是图1是对称的。这是因为你在引言中需要做的许多事情都在讨论/结论中完成了--以相反的顺序。例如，你需要写一个开场白，让你和你的读者“进入”或开始你的论文/论文，你还需要在讨论/结论的结尾找到一个可以接受的方式来结束论文/论文。此外，您必须寻找一种方法来与引言末尾的中心报告部分进行接口，并在您移出中心部分以开始讨论/结论时再次进行接口。</code><br>Something else you should notice about the shape of the diagram is that it narrows towards the central report section, and widens after it. This represents the way information is ordered in the Introduction and the Discussion/Conclusion: in the Introduction you start out by being fairly general and gradually narrow your focus, whereas the opposite is true in the Discussion/Conclusion.<br><code>关于图的形状，您应该注意的另一点是，它向中心报告部分变窄，然后变宽。这代表了引言和讨论/结论中信息的排序方式：在引言中，你开始时相当笼统，然后逐渐缩小你的重点，而在讨论/结论中则相反。</code><br>Read the Introduction below. Don’t worry if the subject matter is not familiar or if you have difficulty understanding individual words, especially technical terms like polylactide. Just try to get a general understanding at this stage and familiarise yourself with the type of language used.<br><code>阅读下面的介绍。不要担心，如果主题不熟悉，或者如果你有困难理解个别单词，特别是技术术语，如聚乳酸。试着在这个阶段获得一个大致的理解，并熟悉所使用的语言类型。</code></li></ul><blockquote><p>The synthesis of flexible polymer blends from polylactide and rubber<br><code>聚乳酸/橡胶柔性共混物的合成</code><br>Introduction</p><ol><li>Polylactide (PLA) has received much attention in recent years due to its biodegradable properties, which offer important economic benefits.<br><code>聚乳酸（PLA）由于其可生物降解的特性，近年来受到了广泛的关注，这提供了重要的经济效益。</code></li><li>PLA is a polymer obtained from corn and is produced by the polymerisation of lactide.<br><code>PLA是从玉米获得的聚合物，并且通过丙交酯的聚合产生。</code></li><li>It has many possible uses in the biomedical field and has also been investigated as a potential engineering material.<br><code>它在生物医学领域有许多可能的用途，也被研究作为一种潜在的工程材料。</code></li><li>However, it has been found to be too weak under impact to be used commercially.<br><code>然而，已经发现其在冲击下太弱而不能商业使用。</code></li><li>One way to toughen polymers is to incorporate a layer of rubber particles and there has been extensive research regarding the rubber modification of PLA.<br><code>增韧聚合物的一种方法是掺入一层橡胶颗粒，并且已经对PLA的橡胶改性进行了广泛的研究。</code></li><li>For example, Penney et al. showed that PLA composites could be prepared using blending techniques and more recently, Hillier established the toughness of such composites.<br><code>例如，Penney et al.表明PLA复合材料可以使用共混技术制备，并且最近，Hillier建立了这种复合材料的韧性。</code></li><li>However, although the effect of the rubber particles on the mechanical properties of copolymer systems was demonstrated over two years ago, little attention has been paid to the selection of an appropriate rubber component.<br><code>然而，尽管橡胶颗粒对共聚物体系的机械性能的影响在两年前就已得到证实，但很少注意选择合适的橡胶组分。</code></li><li>The present paper presents a set of criteria for selecting such a component.<br><code>本文提出了一套标准，选择这样的组件。</code></li><li>On the basis of these criteria it then describes the preparation of a set of polymer blends using PLA and a hydrocarbon rubber (PI).<br><code>在这些标准的基础上，然后描述了使用PLA和烃橡胶（PI）制备一组聚合物共混物。</code></li><li>This combination of two mechanistically distinct polymerisations formed a novel copolymer in which the incorporation of PI significantly increased flexibility.<br><code>两种机械上不同的聚合的这种组合形成了一种新型的共聚物，其中PI的掺入显著增加了柔性。</code></li></ol></blockquote><h1 id="1-2-Grammar-and-Writing-Skills"><a href="#1-2-Grammar-and-Writing-Skills" class="headerlink" title="1.2 Grammar and Writing Skills"></a>1.2 Grammar and Writing Skills</h1><p><code>1.2语法和写作技巧</code><br>This section deals with four language areas which are important in the Introduction:<br><code>本节涉及引言中重要的四个语言领域：</code><br>TENSE PAIRS<br><code>时态对</code><br>SIGNALLING LANGUAGE<br><code>信令语言</code><br>PASSIVE/ACTIVE USE<br><code>被动/主动使用</code><br>PARAGRAPHING<br><code>分段</code></p><h2 id="1-2-1-Tense-pairs"><a href="#1-2-1-Tense-pairs" class="headerlink" title="1.2.1 Tense pairs"></a>1.2.1 Tense pairs</h2><p><code>1.2.1时态对</code><br><em><strong>Present Simple/Present Continuous</strong></em><br><code>现在简单/现在连续</code><br>In order to use tenses correctly in the Introduction, you fi rst need to look at the diff erence between the way the Present Simple tense and the Present Continuous tense are used.<br><code>为了在引言中正确地使用时态，你首先需要看看现在进行时和现在进行时之间的不同之处。</code><br>Look at these two sentences:<br><code>看看这两句话：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-2.png"><br>(a) describes a permanent situation and (b) describes a temporary situation. Because of this, the Present Simple tense is used in science writing to state accepted facts and truths — but what qualifi es as an accepted fact or truth is oft en, surprisingly, your decision. Sometimes the writer considers that research fi ndings have the status of a fact; in that case, s/he can decide to state them in the Present Simple, usually followed by the appropriate research reference. Here is an example from the Introduction in Section 1.1:<br><code>(a)描述了永久性的情况，而（b）描述了临时性的情况。正因为如此，一般现在时态在科学写作中被用来陈述公认的事实和真理--但令人惊讶的是，什么是公认的事实或真理，往往是由你决定的。有时认为研究成果具有事实地位;在这种情况下，她/他可以决定在现在的简单陈述中陈述它们，通常后面是适当的研究参考。下面是1.1节引言中的一个例子：</code></p><blockquote><p>5 One way to toughen polymers is to incorporate a layer of rubber particles and there has been extensive research regarding the rubber modification of PLA.<br><code>5增韧聚合物的一种方法是加入一层橡胶颗粒，并且已经对PLA的橡胶改性进行了广泛的研究。</code></p></blockquote><p>Later on, in the Results section, you can even decide to state your own findings this way. Look at these two sentences which describe results: (a) We found that the pressure increased as the temperature rose, which indicated that temperature played a significant role in the process. (b) We found that the pressure increases as the temperature rises, which indicates that temperature plays a significant role in the process.<br><code>稍后，在结果部分，您甚至可以决定以这种方式陈述自己的发现。看看这两个描述结果的句子：（a）我们发现压力随温度的升高而增大，说明温度在此过程中起着重要作用。(b)我们发现压力随着温度的升高而增加，这表明温度在这个过程中起着重要的作用。</code><br>Which sentence is ‘stronger’? In (a), using the Past Simple tense means that your findings are linked only to your own research, and you do not claim your deductions should be considered as accepted or established facts, or even that another researcher will necessarily get the same results. In (b), using the Present Simple tense means that you believe your findings and deductions are strong enough to be considered as facts or truths. The Present Simple communicates this reliability and your readers will respond to your work accordingly. There will be more about this later, in the unit on Results.<br><code>哪个句子更“强”？在（a）中，使用一般过去时意味着你的发现只与你自己的研究有关，你不主张你的推论应该被认为是公认的或既定的事实，甚至不主张另一个研究者必然会得到同样的结果。在（B）中，使用一般现在时意味着你相信你的发现和推论足够有力，可以被认为是事实或真理。现在的简单传达这种可靠性和你的读者会回应你的工作相应的。这将是更多关于这一点后，在单位的结果.</code><br><em><strong>Past Simple/Present Perfect</strong></em><br><code>过去简单/现在完成</code><br>Another tense pair you need in the Introduction is the Past Simple tense and the Present Perfect tense. You will need both, and you need to know when and why to switch from one to the other. Look at these sentences:<br><code>你在引言中需要的另一对时态是一般过去时和现在完成时。您将需要两者，并且您需要知道何时以及为什么从一个切换到另一个。看看这些句子：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-3.png"><br>You probably learned the difference between (a) and (b) years ago: that one of the differences between Past Simple and Present Perfect is the ‘time’ of the verb, i.e. when it happened. The difference between (c) and (d) is harder to understand and more important for you as a writer of science research.<br><code>你可能在几年前就知道了（a）和（b）的区别：一般过去时和现在完成时的区别之一是动词的“时间”，即我就知道了（c）和（d）之间的区别很难理解，但对你作为一个科学研究的作者来说更重要。</code><br>In (c) and (d), ‘time’, i.e. when the verb happened, isn’t really what separates the two sentences; it’s possible that both (c) and (d) happened last month, this morning, or one nanosecond ago. What is important is that the event in (d) is considered more relevant to the situation now than the event in (c), which is why it is given in the Present Perfect. Why is this idea of relevance useful when you write an Introduction? Look at these sentences from the Introduction in Section 1.1:</p><blockquote><p><code>在（c）和（d）中，“时间”，即当动词发生时，并不是分隔两个句子的地方有可能（c）和（d）都发生在上个月，今天早上，或者一纳秒之前。重要的是，（d）中的事件被认为比（c）中的事件与现在的情况更相关，这就是为什么它在现在完成时给出。为什么在你写引言时，这种相关性的想法是有用的？看看1.1节引言中的这些句子：</code></p></blockquote><p>**For example, Penney et al. showed that PLA composites could be prepared using blending techniques and more recently, Hillier established the toughness of such composites. However, although the effect of the rubber particles on the mechanical properties of copolymer systems was demonstrated over two years ago, little attention has been paid to the selection of an appropriate rubber component. **</p><p><code>例如，Penney et al.研究表明，PLA复合材料可以使用共混技术制备，最近，Hillier建立了这种复合材料的韧性。然而，尽管橡胶颗粒对共聚物体系的机械性能的影响在两年多前被证明，很少注意选择合适的橡胶组分。</code></p><ul><li>Note: a little means ‘a small amount’, but little means ‘virtually none’.<br><code>* 注：a little的意思是“少量”，但little的意思是“几乎没有”。</code></li></ul><p>Where does the tense change? Why do you think the writer changes from the Past Simple to the Present Perfect? Could it be because this research article is NOW paying attention to the selection of an appropriate rubber component?</p><p><code>时态在哪里变化？你认为作者为什么要从过去的简单变成现在完成时？这是否是因为这篇研究文章现在正在关注选择合适的橡胶成分？</code></p><p>Now look at what happens if the writer forgets to change tense and continues in the Past Simple:</p><p><code>现在看看如果作者忘记改变时态而继续使用一般过去时会发生什么：</code></p><p><em><strong>However, although the effect of the rubber particles on the mechanical properties of copolymer systems was demonstrated over two years ago, little attention was paid to the selection of an appropriate rubber component.</strong></em></p><p><code>然而，尽管橡胶颗粒对共聚物体系的机械性能的影响在两年前就已得到证实，但很少注意选择合适的橡胶组分。</code><br>Suddenly, the sentence means that little attention was paid THEN, i.e. two years ago. Perhaps attention has been paid to this problem since then; perhaps the problem has even been solved! Tense changes are always meaningful, and they always signal a change in the function of the information — so don’t change tense randomly and make sure you remember to change tense when you should.<br><code>突然，这个句子意味着当时，也就是两年前，很少有人注意到。也许从那时起，这个问题就得到了关注;也许这个问题已经得到了解决！时态的变化总是有意义的，它们总是标志着信息功能的变化-所以不要随意改变时态，并确保你记得在应该改变时态的时候改变时态。</code><br>Now check what you have learned about tenses by looking carefully at the way the Past Simple and Present Perfect are used in the Introductions of your target articles. Look in particular at the way the Past Simple tense and the Present Perfect tense are used to refer to previous research.<br><code>现在，通过仔细观察你的目标文章的引言中一般过去时和现在完成时的使用方式，检查你所学到的关于时态的知识。特别注意过去一般时和现在完成时是如何被用来参考以前的研究的。</code></p><h2 id="1-2-2-Signalling-language"><a href="#1-2-2-Signalling-language" class="headerlink" title="1.2.2 Signalling language"></a>1.2.2 Signalling language</h2><p><code>1.2.2信令语言</code><br>Sentence connection<br><code>句子连接</code><br>One of the most common errors in writing is failing to connect one sentence or idea to the next. Every time you end a sentence, your reader has no idea what the next sentence is going to do or say. As a result, the space between a full stop and the next capital letter is a dangerous space for you and your reader. Perhaps you stopped for ten minutes aft er a sentence, and during that time you thought about your work and your ideas developed. Perhaps you turned off your computer and went home. When you start typing again, if you don’t share the link between those sentences with your reader, you create a gap in the text which will cause problems.<br><code>写作中最常见的错误之一是未能将一个句子或想法与下一个连接起来。每次你结束一个句子，你的读者都不知道下一个句子要做什么或说什么。因此，句号和下一个大写字母之间的空格对你和你的读者来说都是危险的。也许你在说了一句话后停了十分钟，在这段时间里，你思考了你的工作，你的想法得到了发展。也许你关掉电脑回家了。当你再次开始输入时，如果你不与读者分享这些句子之间的链接，你就会在文本中创建一个间隙，这将导致问题。</code><br>One of your tasks as a writer is to make sure that gap is closed, so that your reader is carried carefully from one piece of information to the next. Connecting sentences and concepts is good for you too, as it forces you to develop your ideas logically.<br><code>作为一个作家，你的任务之一就是确保这个缺口是封闭的，这样你的读者就可以小心地从一条信息转到另一条信息。连接句子和概念对你也有好处，因为它迫使你逻辑地发展你的想法。</code><br>One way to connect sentences is to overlap, meaning to repeat something from the previous sentence:<br><code>连接句子的一种方法是重叠，意思是重复前一句的内容：</code><br><em><strong>The pattern of inflammation during an asthma attack is different from that seen in stable asthma. In stable asthma the total number of inflammatory cells does not increase.</strong></em><br><em><strong>One way to toughen polymers is to incorporate a layer of rubber particles. As a result, there has been extensive research regarding the rubber modification of PLA.</strong></em><br><code>哮喘发作期间的炎症模式与稳定期哮喘不同。在稳定型哮喘中，炎性细胞的总数不会增加。</code><br><code>使聚合物增韧的一种方法是加入一层橡胶颗粒。因此，对PLA的橡胶改性进行了广泛的研究。</code><br>Another way is to use a pronoun (it, they) or pro-form (this method, these systems) to glue the sentences together:<br><code>另一种方法是使用代词（it，they）或代词形式（this method，these systems）将句子粘合在一起：</code><br><em><strong>Many researchers have suggested ways of reducing cost without affecting the quality of the image. These methods rely on data structures built during a preprocessing step.</strong></em><br><em><strong>On the basis of these criteria it then describes the preparation of a set of polymer blends using PLA and a hydrocarbon rubber (PI). Th is combination of two mechanistically distinct polymerisations formed a novel copolymer in which the incorporation of PI significantly increased flexibility.</strong></em><br><code>许多研究人员提出了在不影响图像质量的情况下降低成本的方法。这些方法依赖于在预处理步骤期间构建的数据结构。</code><br><code>在这些标准的基础上，然后描述了使用PLA和烃橡胶（PI）的一组聚合物共混物的制备。这是两种不同的聚合机理的组合，形成了一种新的共聚物，其中PI的掺入显着增加了灵活性。</code><br>The third way is not to finish the sentence at all, but to join it to the next sentence with a semicolon or a relative clause (a ‘which’ clause). Joining sentences with a semicolon works well when two sentences are very closely related and one of them is quite short:<br><code>第三种方法是根本不结束句子，而是用一个连词或一个关系从句（一个“which”从句）将它连接到下一个句子。当两个句子非常密切相关，其中一个句子很短时，用一个小括号连接句子效果很好：</code><br><em><strong>The procedure for testing whether components are operationally safe usually takes many hours; this means that tests are rarely repeated.</strong></em><br><em><strong>It has received much attention over the past few decades due to its biodegradable properties, which offer important economic benefits.</strong></em><br><code>测试组件是否操作安全的程序通常需要数小时;这意味着测试很少重复。</code><br><code>在过去的几十年里，由于其可生物降解的特性，它受到了广泛的关注，这提供了重要的经济效益。</code></p><p>The fourth way is to use a signalling sentence connector to indicate the relationship between one sentence and the next, or one part of a sentence and the next. You know how useful sentence connectors are from your reading; when you see a word like therefore or however, you are able to process the next piece of information in the sentence correctly even if you don’t understand every word. This is because the sentence connector signals the function of the information in the sentence. The opposite is also true: when the writer does not signal the function of the information with a connector, it is harder for the reader to process the information. Even if the grammar is perfect and every word is correct, the reader still may not be sure what the information is doing (Is it a result of the previous sentence? An example? A cause?), and may interpret it differently from the way the writer intended.</p><p><code>第四种方式是使用指示句连接符来指示一个句子和下一个句子之间的关系，或者一个句子的一个部分和下一个句子之间的关系。从你的阅读中你知道句子连接符是多么有用；当你看到一个类似于so或然而的单词时，你能够正确地处理句子中的下一段信息，即使你不是每个单词都听懂了。这是因为句子连接符表示句子中信息的功能。相反的情况也是正确的：当写入者没有用连接器通知信息的功能时，阅读器就更难处理信息。即使语法是完美的，每个单词都是正确的，读者可能仍然不确定信息在做什么(它是前一句话的结果吗？举个例子？原因？)，并可能与作者的意图不同地解释它。</code></p><p>You already use words like therefore and however and one aim of this subsection is to make sure that you are using them correctly. Another aim is to expand your vocabulary of signalling words, because you can’t spend the rest of your writing life using only therefore and however! Here are some examples of signalling language arranged according to their function. It is not a long list because only those which are commonly used in science writing have been included.</p><p><code>你已经使用了像“因此”和“然而”这样的词，本小节的一个目的是确保你正确地使用它们。另一个目的是扩大你的词汇量的信号词，因为你不能花你的写作生活的其余部分只使用因此和然而！下面是一些根据功能排列的信号语言的例子。这不是一个很长的列表，因为只有那些在科学写作中常用的被包括在内。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-4.png"></p><ul><li>Be careful when you use since; it is also often used to mean ‘from that time’, so if there’s any possibility of confusion, choose a different connector.</li><li><code>使用since时要小心;它也经常用于表示“从那时起”，所以如果有任何混淆的可能性，请选择不同的连接符。</code></li><li>All these connectors can be used at the start of a sentence, even because (Because the measuring instruments were inaccurate, the experiment was unsuccessful).</li><li><code>所有这些连接词都可以用在句子的开头，即使是因为（因为测量仪器不准确，实验不成功）。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-5.png"></li><li>Don’t start sentences with so to communicate a result; it’s too informal.</li><li><code>不要用so开头来表达结果，这太不正式了。</code></li><li>You can sometimes use then, for example in sentences like ‘If x then y’, but it won’t work in every sentence, which is why it has not been included in this list.</li><li><code>有时候你可以使用then，比如在“If x then y”这样的句子中，但它并不适用于每个句子，这就是为什么它没有被包括在这个列表中。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-6.png"></li><li>on the contrary and conversely don’t fit into this category because they don’t only communicate difference; they communicate the fact that ‘exactly the opposite is true’, so you can’t use them in the sentence above (because vegetarians and meat eaters aren’t opposites, they’re just different). However, you could use them in the following sentence: Some experiments used uncalibrated instruments and succeeded; conversely, other experiments used carefully calibrated instruments and failed.</li><li><code>相反地和相反地不属于这一类，因为它们不仅传达了差异;它们传达了“恰恰相反才是真的”的事实，所以你不能在上面的句子中使用它们（因为素食者和肉食者不是对立的，他们只是不同）。然而，你可以在下面的句子中使用它们：一些实验使用未校准的仪器并成功;相反，其他实验使用仔细校准的仪器并失败。</code></li><li>Be careful when you use while; it is also often used to mean ‘at that/the same time’, so if there’s any possibility of confusion, choose a different connector.</li><li><code>使用while时要小心;它也经常用于表示“在那个/同时”，所以如果有任何混淆的可能性，请选择不同的连接符。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-7.png"></li><li>There are other connectors with the same meaning, such as still and anyway, but they are more informal.</li><li><code>还有其他具有相同含义的连接词，如still和anyway，但它们更非正式。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-8.png"></li><li>besides has more or less the same meaning as the items in the list above, but it’s more powerful and is therefore better used in more persuasive contexts.</li><li><code>besides与上面列表中的项目或多或少具有相同的含义，但它更强大，因此更好地用于更具说服力的上下文。</code><br>Now check what you have learned by looking at the way sentences are connected in the Introductions of your target articles.<br><code>现在，通过观察目标文章的引言中句子的连接方式来检查你学到了什么。</code></li></ul><h2 id="1-2-3-Passive-Active"><a href="#1-2-3-Passive-Active" class="headerlink" title="1.2.3 Passive/Active"></a>1.2.3 Passive/Active</h2><p><code>1.2.3被动/主动</code><br>Students often ask whether they can use we in their research articles. In the Introduction you usually say what you will be doing or presenting in the research article. You can use we to refer to your research group or team, but do not use it to refer to people or humanity in general. If you are referring to people in general, it’s better to use a construction with It (It is known/ thought that…) rather than We know/think that… It is also common to use the passive instead of we, especially in the central ‘report’ section (was measured, was added, etc.).<br><code>学生们经常会问，是否可以在研究文章中使用 "我们"。在引言中，您通常会说您将在研究文章中做什么或介绍什么。您可以用 we 来指您的研究小组或团队，但不要用它来指一般人或人类。如果您指的是一般人，最好用 It（众所周知/认为......），而不是 We know/think that......（我们知道/认为.......）。</code><br>In a thesis, you are writing as an individual and you don’t have a research group or team. Since you cannot write your thesis using I, you will probably write in the passive. Use words like here and in this study to let your reader know when you are referring to your own work. You can also use a ‘dummy’ subject to take the place of I or we:<br><code>在论文中，你是作为一个个体写作的，你没有一个研究小组或团队。既然你不能用I来写你的论文，你可能会用被动语态。使用像这里和本研究中这样的词，让你的读者知道你指的是你自己的工作。你也可以用“dummy”主语来代替I或we：</code><br><em><strong>This article describes an algorithm for clustering sequences into index classes.</strong></em><br><em><strong>The present paper presents a set of criteria for selecting such a component.</strong></em><br><code>这篇文章描述了一个算法聚类序列到索引类。</code><br><code>本文件提出了一套标准，选择这样的组件。</code><br>The problem with using the passive in formal writing is that the agent (the person who performed the action of the verb) is oft en not mentioned in the sentence. In other words, we say that something was done or was identified but we don’t say ‘by me’ or ‘by other researchers’, so the reader may not know who did it or who identified it. Th is can cause confusion and for that reason it is sometimes clearer to use a dummy subject (Th is article/ the present paper) in the Introduction rather than the ‘agentless’ passive (x is presented). Now look at the way the passive and dummy subject are used in the Introductions of your target articles.<br><code>在正式写作中使用被动语态的问题是，施事（动词动作的执行者）通常在句子中没有提到。换句话说，我们说某件事被做了或被确认了，但我们不说“由我”或“由其他研究者”，所以读者可能不知道是谁做的或谁确认了它。这可能会引起混淆，因此有时在引言中使用虚拟主语（Th is article/ the present paper）比“无代理人”被动语态（x is presented）更清楚。现在看看被动和虚拟主语在目标文章的介绍中的使用方式。</code><br>PARAGRAPHING<br><code>例句</code><br><em><strong>Why is paragraphing important?</strong></em><br><code>为什么分段很重要？</code><br>Paragraphs are an important visual aid to effective reading and writing. Two common errors in paragraphing are clusters of short or single-sentence paragraphs, and paragraphs that are too long. Both errors will confuse readers and are signs of poorly-organised writing.<br><code>段落是有效阅读和写作的重要视觉辅助工具。分段中两个常见的错误是短段落或单句段落的集群，以及太长的段落。这两个错误都会让读者感到困惑，也是写作组织不良的标志。</code><br>To understand how paragraphing works, imagine that you have won a 24-hour trip to Paris. You have two options. Th e first option is to fly to Paris, get off the plane and walk around the city. If you take that option, a friend may ask you later if you saw the famous Louvre art gallery; you say: ‘Well, no, I got lost and spent hours walking around the industrial area by mistake.’ You show your mother the clothes you bought in Paris and she asks if you bought them in the famous Rue de la Paix shopping street, and you say, ‘No, I bought them near my hotel. I didn’t know where the big shopping area was.’ You begin to realise that you wasted a lot of time and missed many important things.<br><code>要理解分段是如何工作的，想象一下你赢得了一次去巴黎的24小时旅行。你有两个选择第一个选择是飞到巴黎，下了飞机，在城市里四处走走。如果你选择了这个选项，一个朋友可能会问你是否看到了著名的卢浮宫艺术画廊;你说：“嗯，没有，我迷路了，错误地在工业区走了几个小时。你给你妈妈看你在巴黎买的衣服，她问你是不是在著名的芸香买的，你说：“不是，我是在我住的酒店附近买的。我不知道大型购物区在哪里。你开始意识到你浪费了很多时间，错过了很多重要的事情。</code><br>The second option is to take a short helicopter ride over Paris before you leave the airport. It’s a difficult decision because you are impatient; you only have 24 hours and you don’t want to waste time, but you do it anyway. Th e helicopter flies over Paris for half an hour in a grid pattern, after which you begin your tour of Paris. You find a well-situated hotel, which you saw from the helicopter. You buy your clothes in the Rue de la Paix — which you saw from the helicopter. You visit the Louvre and you have lunch in one of the big parks near the centre … which you saw from the helicopter.<br><code>第二种选择是在离开机场之前乘坐直升机在巴黎上空进行短暂的飞行。这是一个困难的决定，因为你没有耐心;你只有24小时，你不想浪费时间，但你还是做了。直升机在巴黎上空以网格状飞行半小时，之后你就可以开始你的巴黎之旅了。你找到了一个位置很好的酒店，你从直升机上看到的。你在芸香街买衣服--你从直升机上看到的。你参观卢浮宫，在市中心附近的一个大公园吃午饭...你从直升机上看到的。</code><br><em><strong>What is the connection between this and good paragraphing?</strong></em><br><code>这和好的分段之间有什么联系？</code><br>Let’s bring that idea to the skills of reading and writing. If you read the last page of a murder mystery before you finish the book, the rest of the story is less exciting — but you may finish the book faster. Th is is because you don’t waste time wondering who the murderer is; you know it’s the husband, so whenever his name is mentioned you concentrate and read carefully, but you don’t bother to read the details about the other suspects. This enables you to read faster by giving you the confidence to ignore things which you know are not relevant.<br><code>让我们把这个想法带到阅读和写作的技巧上。如果你在读完一本谋杀之谜之前读了最后一页，故事的其余部分就不那么令人兴奋了--但你可能会更快地读完这本书。这是因为你不会浪费时间去想凶手是谁;你知道是丈夫，所以每当提到他的名字时，你都会集中精力仔细阅读，但你不会费心去阅读其他嫌疑人的细节。这使你能够更快地阅读，让你有信心忽略那些你知道不相关的东西。</code><br>The more you know about what you are reading, the faster and more effectively you read. So how can you find out about a long article or chapter before reading it? The answer is to skim it quickly before you begin to read. Like the helicopter ride over Paris, skimming is done before reading, not instead of reading. Your aim when you skim through a text is to find out quickly what it is about and where the various pieces of information are located so that you can read it faster and more confidently.<br><code>你对你正在阅读的东西了解得越多，你就读得越快，越有效。那么，在阅读一篇长文或一章之前，你怎么能找到它呢？答案是在你开始阅读之前快速浏览一下。就像坐直升机飞越巴黎一样，略读是在阅读之前完成的，而不是代替阅读。当你浏览一篇文章时，你的目标是快速找出它是关于什么的，以及各种信息的位置，这样你就可以更快，更自信地阅读它。</code><br><em><strong>How do I skim efficiently and quickly?</strong></em><br><code>如何快速有效地浏览网页？</code><br>Most of the instructions in the box on the next page tell you just to ‘look at’ or ‘check’ something. Skimming is a pre-reading technique and should be done very fast; if it takes more than a few minutes you’re not skimming, you’re reading.<br><code>下一页方框中的大多数说明都告诉你只需要“看”或“检查”一些东西。略读是一种阅读前的技巧，应该非常快地完成;如果花了几分钟以上，你就不是在略读，你就是在阅读。</code><br><em><strong>Skimming may help me read, but how does it help me to write?</strong></em><br><code>略读可以帮助我阅读，但它如何帮助我写作呢？</code><br>Look at number 6 in the box: LOOK QUICKLY AT THE FIRST SENTENCE OF EACH PARAGRAPH. A paragraph in academic writing often starts with a topic sentence, which gives the main idea of the paragraph, and tells the reader what the paragraph is about. The other sentences are related to this idea; they discuss it, describe it, define it in more detail, argue about it, give examples of it, rephrase it, etc. When the ‘topic’ or idea moves too far away from the first sentence, the writer usually begins a new paragraph.<br><code>请看方框中的第6条：快速看每段的第一句话。学术写作中的一个段落通常以主题句开始，它给出了段落的主要思想，并告诉读者段落是关于什么的。其他的句子与这个想法有关;他们讨论它，描述它，更详细地定义它，争论它，给予例子，重新措辞，等等。</code></p><ol><li>READ THE TITLE and try to predict the type of information you expect to see<code>阅读标题，并尝试预测您期望看到的信息类型</code></li><li>LOOK AT THE NAME OF THE AUTHOR What you know about the writer will help you predict and evaluate the content.<code>看看作者的名字你对作者的了解将有助于你预测和评估文章的内容。</code></li><li>CHECK THE DATE and use it to help you assess the content.<code>标注日期并使用它来帮助您评估内容。</code></li><li>READ THE ABSTRACT to find out what the researchers did and/or what they found.<code>阅读摘要，了解研究人员做了什么和/或他们发现了什么。</code></li><li>LOOK QUICKLY AT THE FIRST PARAGRAPH without trying to understand all the words.<code>快速看第一段，不要试图理解所有的单词。</code></li><li>LOOK QUICKLY AT THE FIRST SENTENCE OF EACH PARAGRAPH without trying to understand all the words.<code>快速地看每一段的第一句话，不要试图理解所有的单词。</code></li><li>LOOK QUICKLY AT EACH FIGURE/TABLE AND READ ITS TITLE to try and fi nd out what type of visual data is included.<code>快速查看每个图/表并阅读其标题，尝试找出包含的视觉数据类型。</code></li><li>READ THE LAST PARAGRAPH especially if it has a subtitle like ‘Summary’ or ‘Conclusion’.<code>阅读最后一段，特别是如果它有一个像“总结”或“结论”的副标题</code></li></ol><p>You can therefore get a good idea of the various topics covered in an article — or in a chapter of a book — by reading the first sentence of each paragraph. And because it is a conventional way of writing paragraphs, it is a safe way for you to write paragraphs too. The more aware you are of the way other writers structure paragraphs, the easier it will be for you to do it yourself.<br><code>因此，通过阅读每一段的第一句话，你可以很好地了解一篇文章或一本书的一章所涵盖的各种主题。因为这是一种传统的写段落的方式，所以它也是一种安全的写段落的方式。你越了解其他作者构建段落的方式，你就越容易自己去做。</code>As you know, paragraphs are marked either by indentation (starting five spaces in) or by a double space between lines. Over the years, you have developed a very strong response to these visual signals. This means that each time you begin a new paragraph, this conditioned response in your brain prepares for a change or shift of some kind.<br><code>如你所知，段落是通过缩进（从五个空格开始）或行与行之间的双空格来标记的。多年来，你已经对这些视觉信号产生了非常强烈的反应。这意味着，每当你开始一个新段落时，你大脑中的这种条件反射就为某种变化或转变做好了准备。</code><br>Correct paragraphing is essential, but it is easy to get into poor paragraphing habits, either through laziness or carelessness. If you oft en write one-sentence paragraphs or your paragraphs seem to be very long or you’re not sure when to start a new paragraph, you are making writing harder for yourself. When you are planning your paper, write down each idea/concept that you want to talk about, checking that they are in a logical order and then listing what you want to say about each, using bullet points. Th is will help you create paragraphs that have a logical and coherent structure.<br><code>正确的分段是必不可少的，但很容易养成不良的分段习惯，要么是懒惰，要么是粗心。如果你经常写一句话的段落，或者你的段落看起来很长，或者你不确定什么时候开始一个新的段落，你正在为自己增加写作难度。当你在计划你的论文时，写下你想谈论的每个想法/概念，检查它们是否符合逻辑顺序，然后用要点列出你想说的每一个。这将帮助你创建具有逻辑和连贯结构的段落。</code></p><h1 id="1-3-Writing-Task-Building-a-model"><a href="#1-3-Writing-Task-Building-a-model" class="headerlink" title="1.3 Writing Task: Building a model"></a>1.3 Writing Task: Building a model</h1><p><code>1.3写作任务：建立模型</code></p><h2 id="1-3-1-Building-a-model"><a href="#1-3-1-Building-a-model" class="headerlink" title="1.3.1 Building a model"></a>1.3.1 Building a model</h2><p><code>1.3.1建立模型</code><br>You are now ready to begin building a model of Introductions by writing a short description of what the writer is doing in each sentence in the space provided. This may be hard, because it is the first time you are doing it, so read the guidelines below before you start. The Key is on the next page. Once you have tried to produce your own model you can use the Key to help you write this section of a research article when you eventually do it on your own.<br><code>你现在已经准备好开始建立一个介绍的模型，在提供的空白处写一个简短的描述，说明作者在每个句子中做了什么。这可能很难，因为这是你第一次这样做，所以在开始之前阅读下面的指导方针。钥匙在下一页。一旦你尝试创建自己的模型，你就可以使用Key来帮助你在最终独立完成时撰写研究文章的这一部分。</code><br>GUIDELINES: You should spend 30–45 minutes on this task. If you can’t think of a good description of the first sentence, choose an easier one, for example, Sentence 7, and start with that. Remember that your model is only useful if it can be transferred to other Introductions, so don’t include content words such as polymer or you won’t be able to use your model to generate Introductions in your own field.<br><code>指导原则：你应该花30-45分钟在这个任务上。如果你想不出一个好的描述第一句话，选择一个更容易的，例如，句子7，并开始。请记住，您的模型只有在可以转移到其他Introductions中时才有用，因此不要包含聚合物等内容词，否则您将无法使用您的模型在自己的领域中生成Introductions。</code><br>One way to find out what the writer is doing in a sentence — rather than what s/he is saying — is to imagine that your computer has accidentally deleted it. What is different for you as a reader when it disappears? If you press another key on the computer and the sentence comes back, how does that affect the way you respond to the information?<br><code>要想知道作者在一个句子里做了什么，而不是他/她在说什么，一个方法是想象你的电脑不小心删除了它。如果你按下电脑上的另一个键，句子又回来了，这对你对信息的反应有什么影响？</code><br>Another way to figure out what the writer is doing in a sentence is to look at the grammar and vocabulary clues. What is the tense of the main verb? What is that tense normally used for? Is it the same tense as in the previous sentence? If not, why has the writer changed the tense? What words has the writer chosen to use?<br><code>另一种弄清楚作者在句子中做了什么的方法是看语法和词汇线索。主要动词的时态是什么？这个时态通常用来做什么？这句话的时态和前一句是一样的吗？如果不是，为什么作者改变了时态？作者选用了哪些词语？</code><br>Don’t expect to produce a perfect model. You will modify your model when you look at the Key, and perhaps again when you compare it to the way Introductions work in your target articles.<br><code>不要指望创造一个完美的模型。当您查看Key时，您将修改您的模型，当您将其与目标文章中介绍的工作方式进行比较时，可能会再次修改模型。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-9.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-10.png"></p><h2 id="1-3-2-Key"><a href="#1-3-2-Key" class="headerlink" title="1.3.2 Key"></a>1.3.2 Key</h2><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-11.png"><br><code>在句子1中，“聚乳酸（PLA）由于其可生物降解的特性，近年来受到了广泛关注，这提供了重要的经济效益。”确立了本课题研究的重要性。</code><br>If you wrote ‘introduces the topic’ for Sentence 1, it won’t really help when you come to write a real research article. How exactly do you ‘introduce’ a topic? You need to be more specific.<br><code>如果你在第一句中写了“介绍主题”，那么当你要写一篇真正的研究文章时，它就不会有什么帮助了。你到底是如何“介绍”一个话题的？你得说得更具体些。</code><br>Most research articles begin by indicating that the research fi eld or topic is useful or signifi cant. Th ey may focus on the quantity of research in this area, or how useful research in this area can be, or simply how important this research fi eld is. If you look at your target articles, you will probably fi nd something in the fi rst one or two sentences that establishes the signifi cance of the research. Phrases like much study in recent years or plays a major role are common here, and you’ll fi nd a list of useful vocabulary for this in Section 1.4.<br><code>大多数研究文章的开始都表明研究领域或主题是有用的或有意义的。他们可能会关注这个领域的研究数量，或者这个领域的研究有多有用，或者只是这个研究领域有多重要。如果你看你的目标文章，你可能会在第一个或两个句子中找到一些东西，建立了研究的意义。短语如much study in recent years或plays a major role在这里很常见，你会在1.4节找到有用的词汇表。</code><br><em><strong>What if I don’t have the confi dence to say that my research is important?</strong></em><br><code>如果我没有信心说我的研究很重要呢？</code><br>Most authors of research articles begin by establishing the signifi cance of their research; if you don’t, it can look as though your research is NOT signifi cant, so don’t be shy about stating why or how your fi eld is important or useful.<br><code>大多数研究文章的作者开始通过建立他们的研究的重要性;如果你不这样做，它可以看起来好像你的研究是不重要的，所以不要羞于说明为什么或如何你的领域是重要的或有用的。</code><br><em><strong>What tense should I write in here?</strong></em><br><code>我应该在这里写什么时态？</code><br>Phrases like much study in recent years or in the past fi ve years are normally followed by the Present Perfect tense (Much study in recent years has focused on…). Other ways of establishing signifi cance may use the Present Simple tense (There are substantial benefi ts to be gained from…).<br><code>近年来的大量研究或过去五年的研究等短语通常用现在完成时（近年来的大量研究集中在......）。其他表示意义的方式可以使用现在完成时（There are substantial benefi ts to be gained from...）。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-12.png"><br><code>在句子2中，“PLA是从玉米中获得的聚合物，并且通过丙交酯的聚合产生”。作者为读者提供了一般的背景资料。</code><br>Sentence 2 is in the Present Simple tense, which is used for accepted/ established facts (see Section 1.1). Research articles oft en begin with accepted or established facts. Th is ensures that the reader shares the same level of background information as the writer, and is therefore ready to read the article.<br><code>句子2是一般现在时，用于公认的事实（见1.1节）。研究性文章通常开始于公认的或确定的事实。这确保读者与作者共享相同级别的背景信息，因此准备好阅读文章。</code><br><em><strong>So what kind of facts should I start with?</strong></em><br><code>那么我应该从什么样的事实开始呢？</code><br>Th is depends on how wide your subject — and therefore your readership — is. If the subject of your research is very specifi c, then many of your readers will have a high level of background knowledge, and you can start with fairly specifi c information. If your paper is likely to attract a wider audience, then you should start with more general background information. Remember that your background facts may come from research (see Section 1.1), so don’t forget to include the research references where necessary.<br><code>这取决于你的主题有多广泛，因此你的读者群有多广泛。如果你的研究主题非常具体，那么你的许多读者都有很高的背景知识，你可以从相当具体的信息开始。如果你的论文有可能吸引更广泛的读者，那么你应该从更一般的背景信息开始。请记住，你的背景事实可能来自研究（见1.1节），所以不要忘记在必要的地方包括研究参考。</code><br>What if there are several background facts I want to start with, not just one? How do I know which one to begin with?<br><code>如果我想从几个背景事实开始，而不仅仅是一个呢？我怎么知道从哪一个开始开始？</code><br>Start with the most general one, the one that many of your readers will already know. Th is is a ‘meeting place’ fact, a place where all your readers can start together, aft er which you can move on to more specifi c information. Always show your readers the general picture before you examine the details: show them the wall before you examine the bricks! Also, don’t forget to close the gap between these sentences (see Section 1.2.2) so that your readers can move smoothly through the information.<br><code>从最一般的开始，你的许多读者都已经知道了。这是一个“聚会场所”的事实，一个所有读者可以一起开始的地方，之后你可以继续更具体的信息。在你研究细节之前，总是先向你的读者展示一个整体：在你研究砖块之前，先让他们看看墙！另外，不要忘记缩小这些句子之间的差距（见1.2.2节），这样你的读者就可以顺利地浏览信息。</code><br>Remember that the background facts to your research are very familiar to you and the people you work with, but they won’t be as familiar to all of your readers. Th erefore, if the article is to reach a wider audience you need to state background facts which seem obvious or well-known to you.<br><code>记住，你的研究的背景事实对你和你的同事来说是非常熟悉的，但对你的所有读者来说并不熟悉。因此，如果文章要达到更广泛的受众，你需要陈述对你来说显而易见或众所周知的背景事实。</code><br><em><strong>I’m still not sure where to begin.</strong></em><br><code>我还是不知道从哪里开始开始。</code><br>If you are still stuck for a fi rst sentence, look at your title. It is helpful to your readers if you defi ne the key words in your title — perhaps you can begin with a defi nition or a fact about one of those key words.<br><code>如果你仍然被第一句话卡住了，看看你的标题。如果你在标题中定义了关键词，这对你的读者是有帮助的--也许你可以开始用一个定义或一个关于这些关键词的事实。</code><br><em><strong>Can’t I start by describing the problem I am hoping to solve?</strong></em><br><code>我不能先描述一下我希望解决的问题吗？</code><br>You can, but most authors don’t, because it’s sometimes diffi cult to say exactly what the problem is until your readers have enough background information to understand it. It’s also very hard to limit yourself to one sentence about the problem you are hoping to solve, and before you know it, you’ve written down a lot of specifi c information which your readers aren’t ready for because you haven’t given them enough background.<br><code>你可以，但大多数作者不这样做，因为在读者有足够的背景信息来理解问题之前，有时很难准确地说出问题是什么。你也很难把自己限制在一句话中来描述你希望解决的问题，在你意识到之前，你写了很多具体的信息，读者还没有准备好，因为你没有给他们足够的背景知识。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-13.png"><br><code>在句子3中，“PLA在生物医学领域有许多可能的用途1，并且也被研究为潜在的工程材料2，3”作者做了与句子1和2相同的事情，但是以更具体/详细的方式，使用研究参考来支持背景事实和重要性的声明。</code><br>Don’t the research references mean that this is part of the literature review?<br><code>难道研究参考文献不意味着这是文献综述的一部分吗？</code><br>No, it’s still part of the background to general research in this area. Th e short literature review which is generally found in the Introduction of a research article comes later, and is more likely to deal with individual studies and their methods or results. In a thesis the literature review is much longer and may be a separate chapter.<br><code>不，它仍然是这一领域一般研究的背景的一部分。简短的文献综述通常出现在研究文章的引言中，并且更有可能涉及个别研究及其方法或结果。在一篇论文中，文献综述要长得多，可能是一个单独的章节。</code><br>So why does the author include references if it’s only the background?<br><code>那么，如果只是背景，作者为什么要包括参考文献呢？</code><br>For three reasons: First, because plagiarism (failing to give others the appropriate credit for their work) is unprofessional; second, referencing gives your reader the chance to fi nd and read the study mentioned.Th e third reason is that failing to provide a reference may indicate that you are not familiar with research in your area.<br><code>原因有三：首先，因为剽窃（不给给予他人适当的赞扬）是不专业的;其次，参考给你的读者机会找到并阅读所提到的研究。第三个原因是，没有提供参考可能表明你不熟悉你所在领域的研究。</code><br>Although Sentence 3 isn’t part of the literature review (which comes later in the Introduction) it includes a citation reference. Before you write a research paper, you collect a lot of references, quotations and ideas from journals and the Internet, many of which you will mention at some point in the paper. When you are writing the Introduction, you need to ask yourself three questions:<br><code>虽然第3句并不是文献综述的一部分（在后面的介绍中），但它包含了引用参考。在你写一篇研究论文之前，你从期刊和互联网上收集了大量的参考文献、引文和想法，其中许多你会在论文的某个地方提到。当你写引言的时候，你需要问自己三个问题：</code></p><ol><li>Which of the research papers I have read should be mentioned somewhere in the Introduction? Th e selection of names and references in the Introduction is important, because they draw a research ‘map’ for the reader by indicating the key players in your field and the progress or achievements so far. These names and references give the reader a clear idea of where your research is located and how it is related to other work in the field.<br><code>我读过的哪些研究论文应该在引言中提到？在引言中选择名字和参考文献是很重要的，因为它们为读者绘制了一幅研究“地图”，指出了你所在领域的主要参与者以及迄今为止的进展或成就。这些名称和参考文献给予读者一个清晰的概念，你的研究位于何处，以及它是如何与该领域的其他工作。</code></li><li>Which ones should be part of the background to the research (as in Sentence 3 above) and which ones should go in the literature review which comes later in the Introduction? If the findings are well-known and considered reliable enough to be presented as truths, you can present them in the Present Simple as part of the factual background to your paper (as in Sentence 3) with a research reference. Th e literature review, which describes recent and current research in your field, usually mentions authors by name, and the sentences are usually in the Simple Past or Present Perfect tense.<br><code>哪些应该是研究背景的一部分（如上面的第3句），哪些应该在后面的引言中的文献综述中列出？如果研究结果是众所周知的，并且被认为足够可靠，可以作为真理呈现，你可以在现在简单中将它们作为你论文的事实背景的一部分（如第3句），并提供研究参考。文献综述描述了你所在领域最近和当前的研究，通常提到作者的名字，句子通常是一般过去时或现在完成时。</code></li><li>What order should I mention them in? Who comes first and who comes last? These questions about the literature review itself will be discussed after Sentence 6.<br><code>我应该按什么顺序提到它们？谁先来谁后来？这些关于文献综述本身的问题将在第6句之后讨论。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-14.png"><br><code>在第4句中，作者描述了一般的问题领域或该领域目前的研究热点。</code><br>Notice that the author is still not describing the specifi c problem which this research article will deal with; s/he is describing the current focus of the fi eld, a problem which many researchers in this fi eld are interested in and which leads to the specifi c problem which will be addressed in this article. Remember to keep this general description of the problem area or current research focus brief, or you will fi nd that you begin to give a specifi c description of what your research is trying to achieve, and it’s still too early in the Introduction for that.<br><code>请注意，作者仍然没有描述这篇研究文章将处理的具体问题;他/她描述的是该领域目前的焦点，这是该领域许多研究人员感兴趣的问题，并导致本文将解决的具体问题。记住，对问题领域或当前研究焦点的概括性描述要简短，否则你会发现，你开始对你的研究试图实现的目标给予了具体描述，而在引言中这样做还为时过早。</code><br>As you can see from Sentence 4, you may need a research reference when you describe the problem your paper will deal with; however, if it is a well-known problem (rather than a recent issue, as in Sentence 4), then it is not necessary to provide a reference.<br><code>正如你从第4句中看到的，当你描述你的论文将要处理的问题时，你可能需要一个研究参考;但是，如果它是一个众所周知的问题（而不是最近的问题，如第4句），那么就没有必要提供参考。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-15.png"><br><code>在第5句“一种使聚合物增韧的方法是加入一层橡胶颗粒”中，作者提供了一般问题区域和文献综述之间的过渡。</code><br>As a general rule, you should include references to previous or current research wherever it is useful, even in a sentence whose function is primarily to provide a transition. Make sure that the superscript reference number includes all and only the work referred to in the sentence (see the notes on Sentence 6 below for more about this).<br><code>一般来说，你应该在有用的地方引用以前或现在的研究，即使是在一个主要功能是提供过渡的句子中。确保上标的参考编号包括所有且仅包括句子中提到的作品（更多信息请参见下面第6句的注释）。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-16.png"><br><code>在第6句中，“例如，Penney等人表明，可以使用共混技术6制备PLA复合材料，最近，Hillier 7确定了这种复合材料的韧性。”作者简要介绍了该领域的重点研究项目。</code><br>You can’t just ‘pour’ the literature review onto the page in any order; you should arrange your references and studies so that the reader is able to process them in a logical way. Here are three common options:<br><code>你不能只是把文献综述以任何顺序“倾倒”在页面上;你应该安排你的参考文献和研究，以便读者能够以合乎逻辑的方式处理它们。以下是三种常见的选择：</code></li></ol><ul><li><em><strong>chronological</strong></em>: Deal with the research in chronological order. This may be appropriate, for example, if the development of your field is related to political decisions.<br><code>按时间顺序：按时间顺序处理研究。这可能是合适的，例如，如果你的领域的发展与政治决策有关。</code></li><li><em><strong>different approaches/theories/models</strong></em>: Group projects or studies according to their approach or methodology. Grouping similar projects together helps you avoid the ‘tennis match’ effect where you go backwards and forwards, beginning each sentence in the literature review with However or On the other hand!<br><code>不同的方法/理论/模型：根据他们的方法或方法对项目或研究进行分组。把类似的项目放在一起，可以帮助你避免“网球比赛”的效果，在这种情况下，你会前后颠倒，在文献综述的每一句话的开头都用“然而”或“另一方面”！</code></li><li><em><strong>general/specific</strong></em>: Start with general research in the field and gradually move to research that is closer to your own.<br><code>一般/具体：从该领域的一般研究开始，逐渐转向更接近你自己的研究。</code><br><em><strong>When should a research reference come in the middle of the sentence?</strong></em><br><code>什么时候应该在句子中间出现研究参考文献？</code><br>When it is necessary to avoid confusion, for example if you are referring to more than one study in a sentence or if the citation reference only refers to part of your sentence. You can see examples of this in Sentences 6 and 7.<br><code>当有必要避免混淆时，例如，如果您在一个句子中引用多项研究，或者引用参考仅指您句子的一部分。你可以在第6和第7句中看到这样的例子。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-17.png"><br><code>在句子7中，“然而，尽管橡胶颗粒对共聚物体系的机械性能的影响在两年前就得到了证明，但很少有人注意选择合适的橡胶组分。”作者描述了研究中的一个空白。</code><br>This is where you begin to introduce the purpose of your paper and the specific problem you will deal with, and in order to do this it is necessary to create a research space. You can do this either by describing a problem in the previous research or by indicating that there is a gap in the research. It is conventional to introduce it with a signalling connector such as However or Although. In professional writing it is unusual to put it in the form of a question; instead you can state it as a prediction or a hypothesis which you intend to test.<br><code>这是你开始介绍你的论文的目的和你将处理的具体问题的地方，为了做到这一点，有必要创造一个研究空间。你可以通过描述以前研究中的问题或指出研究中存在的差距来做到这一点。通常用信号连接符，如However或Although来引入它。在专业写作中，通常不以问题的形式提出问题;相反，你可以把它陈述为一个预测或假设，你打算测试。</code><br>Don’t be shy about pointing out the problems in previous research. In the first place it may be necessary in order to explain why you have done your study, and in the second place, the language used here is usually respectful and impersonal, and is therefore not considered offensive. We will look at the politeness aspect of this language in the vocabulary section at the end of the unit.<br><code>不要羞于指出以前研究中的问题。首先，它可能是必要的，以解释为什么你做了你的研究，其次，这里使用的语言通常是尊重和客观的，因此不被认为是冒犯。我们将在本单元最后的词汇部分研究这种语言的礼貌方面。</code><br>You may need more background information at this stage (for example, you may need to give details of the properties of the material which you have chosen to investigate, or describe the specific part of the device which you plan to improve). Research writing requires far more background information than you have previously given in your undergraduate writing, and it is better to offer slightly too much background information than too little.<br><code>在此阶段，您可能需要更多的背景信息（例如，您可能需要给予您选择研究的材料的特性的详细信息，或描述您计划改进的设备的特定部分）。研究性写作需要的背景信息比你以前在本科写作中提供的要多得多，提供稍微多一点的背景信息总比提供太少好。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-18.png"><br><code>在句子8中，本论文提出了一套选择这种组件的标准。作者描述了论文本身。</code><br>At this stage you move to the present work. You can describe it, say what its purpose or focus is, give its structure or a combination of these. Check Section 1.2.3 to see whether to write these sentences in the active or the passive.<br><code>在这一阶段，你进入了目前的工作。你可以描述它，说出它的目的或重点是什么，给予它的结构或这些的组合。检查1.2.3节，看看这些句子是用主动语态还是被动语态。</code><br>You normally use the Present Simple tense to describe the work itself (This paper is organised as follows or This study focuses on) and the Past Simple tense to talk about the aim of the work (The aim of this project was…), because in ‘real time’, the aim occurred before the work was carried out. It is also possible to state the aim in the Present Simple (The aim of this work is…). This is especially true in cases where the aim is only partially achieved in the paper you are submitting and the rest of the work will be done and reported on at a later stage.<br><code>你通常用一般现在时来描述工作本身（本文的组织如下或本研究的重点是），用一般过去时来谈论工作的目的（本项目的目的是.），因为在“真实的时间”中，目的发生在工作进行之前。也可以在现在简单句中陈述目的（本作品的目的是.）。如果你提交的论文只是部分实现了目标，而其余的工作将在稍后阶段完成并报告，这一点尤其如此。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-19.png"><br><code>在句子9中，“基于这些标准，然后描述了使用PLA和烃橡胶（PI）制备一组聚合物共混物。”作者详细介绍了该文件中报告的方法。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-20.png"><br><code>在句子10中，“两种不同的聚合机理的这种组合形成了一种新的共聚物，其中PI的引入显著增加了柔性”。笔者公布调查结果。</code><br>Although you can give information about your methodology or findings in the Introduction, be careful not to go into too much detail at this point or you will find that you have nothing to write about in the Methodology or Results sections.<br><code>虽然你可以在引言中给予关于你的方法或发现的信息，但注意不要在这一点上讲得太详细，否则你会发现在方法或结果部分你没有什么可写的。</code><br>Look at the way the writer begins Sentences 9 and 10. In each case the information is joined to the previous sentence with a pro-form (On the basis of these criteria in Sentence 9 and This combination in Sentence 10).<br><code>看作者如何开始第9和第10句。在每一种情况下，信息都用一个前形式与前一句相连（基于句子9中的这些标准和句子10中的这种组合）。</code></li></ul><h2 id="1-3-3-The-model"><a href="#1-3-3-The-model" class="headerlink" title="1.3.3 The model"></a>1.3.3 The model</h2><p><code>1.3.3模型</code><br>Here are the sentence descriptions we have collected:<br><code>以下是我们收集的句子描述：</code><br>In Sentence 1 the writer establishes the importance of this research topic.<br><code>在第一句中，作者确立了这个研究课题的重要性。</code><br>In Sentence 2 the writer provides general background information.<br><code>在第2句中，作者提供了一般的背景信息。</code><br>In Sentence 3 the writer does the same as in Sentences 1 and 2, but in a more specific/detailed way.<br><code>在第3句中，作者做了与第1和第2句相同的事情，但是用了更具体/详细的方式。</code><br>In Sentence 4 the writer describes the general problem area or the current research focus of the field.<br><code>在第4句中，作者描述了该领域的一般问题领域或当前的研究热点。</code><br>In Sentence 5 the writer provides a transition between the general problem area and the literature review.<br><code>在第5句中，作者提供了一般问题领域和文献综述之间的过渡。</code><br>In Sentence 6 the writer provides a brief overview of key research projects in this area.<br><code>在第6句中，作者简要概述了这一领域的主要研究项目。</code><br>In Sentence 7 the writer describes a gap in the research.<br><code>在第7句中，作者描述了研究中的一个空白。</code><br>In Sentence 8 the writer describes the paper itself.<br><code>在第8句中，作者描述了论文本身。</code><br>In Sentence 9 the writer gives details about the methodology reported in the paper.<br><code>在第9句中，作者详细介绍了论文中报告的方法。</code><br>In Sentence 10 the writer announces the findings.<br><code>在第10句中，作者宣布了调查结果。</code><br>We can streamline these so that our model has FOUR basic components:<br><code>我们可以简化这些，使我们的模型有四个基本组成部分：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-21.png"><br><code>1)确定你所在领域的重要性提供背景事实/信息（可能来自研究）定义标题/关键词中的术语存在的问题领域/当前的研究重点</code><br><code>2)以往和/或目前的研究和贡献</code><br><code>3)找出研究中的差距描述您将解决的问题提出待测试的预测</code><br><code>4)描述本文件</code></p><h2 id="1-3-4-Testing-the-Model"><a href="#1-3-4-Testing-the-Model" class="headerlink" title="1.3.4 Testing the Model"></a>1.3.4 Testing the Model</h2><p><code>1.3.4测试模型</code><br>The next step is to look at the way this model works in a real Introduction. Here are some full-length Introductions from real research articles. Read them through, and mark the model components (1, 2, 3 or 4) wherever you think you see them. For example, if you think the first sentence of the Introduction corresponds to number 1 in our model, write 1 aft er it, etc.<br><code>下一步是在一个真实的介绍中看看这个模型是如何工作的。这里有一些来自真实的研究文章的完整介绍。通读一遍，并在您认为看到的任何地方标记模型组件（1、2、3或4）。例如，如果你认为引言的第一句对应于我们模型中的数字1，那么在后面写上1，等等。</code></p><blockquote><p><em><strong>The height of biomolecules measured with the atomic force microscope depends on electrostatic interactions</strong></em><br><code>用原子力显微镜测量生物分子的高度取决于静电相互作用</code><br>INTRODUCTION<br><code>导言</code><br>Because the atomic force microscope (AFM) (Binnig et al., 1986) makes it possible to image surfaces in liquids, it has become an important tool for studying biological samples (Drake et al., 1989). Recent reports document the observation of protein assemblies under physiological conditions at nanometer resolution (Butt et al., 1990; Hoh et al., 1991; Karrasch et al., 1993, 1994; Yang et al., 1993, Schabert and Engel, 1994; Mou et al., 1995b; Muller et al., 1995b, 1996b). As demonstrated on solids under vacuum conditions (Sugawara et al., 1995) and in liquid (Ohnesorge and Binnig, 1993), the AFM also makes it possible to measure sample heights with subangstrom accuracy. However, the heights of native biological samples measured with the AFM in aqueous solution vary signifi cantly, and may diff er from values estimated with other methods (Butt et al., 1991; Apell et al., 1993; Muller et al., 1995b, 1996a; Schabert and Rabe, 1996). For example, the height reported for single purple membranes ranges from 5.1 ± 0 nm to 11.0 ± 3.4 nm (see Table 1). Height measurements on actin fi laments (Fritz et al., 1995b), bacteriophage ø29 connectors (Muller et al., 1997c), cholera toxin (Yang et al., 1994; Mou et al., 1995b), DNA (Hansma et al., 1995; Mou et al., 1995a; Wyman et al., 1995), gap junctions (Hoh et al., 1993), GroEL (Mou et al., 1996), hexagonally packed intermediate layer (HPI) (Karrasch et al., 1993; Muller et al., 1996a; Schabert and Rabe, 1996), lipid bilayers (Mou et al., 1994, 1995b; Radler et al., 1994), and microtubules (Fritz et al., 1995a) exhibit a similar variability. Height anomalies of soft surfaces have previously been studied and attributed to the mechanical properties of the sample (Weisenhorn et al., 1992; Radmacher et al., 1993, 1995; Hoh and Schoenenberger, 1994). However thin samples such as two-dimensional protein arrays or biological membranes adsorbed to a solid support are not suffi ciently compressible to explain such large height variation.<br><code>因为原子力显微镜（AFM）（Binnig等人，1986）使得对液体中的表面成像成为可能，它已经成为研究生物样品的重要工具（Drake等人，1989年）。最近的报道记录了在生理条件下以纳米分辨率观察蛋白质组装体（Butt等人，1990; Hoh等人，1991; Karrasch等人，1993，1994; Yang等人，1993，Schabert和Engel，1994; Mou等人，1995 b; Muller等人，1995 b，1996 b）。如在真空条件下对固体所证明的（Sugawara等人，1995）和在液体中（Ohnesorge和Binnig，1993），AFM还可以测量亚埃精度的样品高度。然而，在水溶液中用AFM测量的天然生物样品的高度显著变化，并且可能不同于用其他方法估计的值（Butt等人，1991; Apell等人，1993; Muller等人，1995 b，1996 a; Schabert and Rabe，1996）。例如，单个紫色膜报告的高度范围为5.1 ± 0 nm至11.0 ± 3.4 nm（见表1）。对肌动蛋白纤维的高度测量（Fritz等人，1995 b），噬菌体ø 29连接器（Muller等人，1997 c）、霍乱毒素（Yang等，1994; Mou等人，1995 b）、DNA（Hansma et al.，1995; Mou等人，1995 a; Wyman等人，1995）、间隙连接（Hoh等人，1993）、GroEL（Mou等人，1996）、六方堆积中间层（HPI）（Karrasch等人，1993; Muller等人，1996 a; Schabert和Rabe，1996）、脂质双层（Mou等人，1994，1995 b; Radler等人，1994）和微管（Fritz等人，1995 a）表现出类似的变化。先前已经研究了软表面的高度异常，并将其归因于样品的机械性能（Weisenhorn等人，1992; Radmacher等人，1993，1995; Hoh和Schoenenberger，1994）。然而，吸附到固体支持物上的薄样品如二维蛋白质阵列或生物膜不足以压缩以解释如此大的高度变化。</code><br>Here we demonstrate that electrostatic interactions between the AFM tip and the sample (Butt, 1991a, b) infl uence the measured height of a biological structure adsorbed to a solid support in buff er solution. Th e DLVO (Derjaguin, Landau, Verwey, Overbeek) theory (Israelachvili, 1991) is used to describe the electrostatic repulsion and van der Waals attraction acting between tip and sample (Butt et al., 1995). Experimental results and calculations show that the electrostatic double-layer forces can be eliminated by adjusting the electrolyte concentration (Butt, 1992a, b), providing conditions for correct height measurements with the AFM. In addition, the observed height dependence of the biological structure on electrolyte concentration allows its surface charge density to be estimated.<br><code>在这里，我们证明了AFM针尖和样品之间的静电相互作用（Butt，1991 a，b）影响吸附到缓冲溶液中的固体支持物上的生物结构的测量高度。DLVO（Derjaguin，朗道，Verwey，Overbeek）理论（Israelachvili，1991）用于描述作用在尖端和样品之间的静电排斥和货车德瓦尔斯吸引（Butt等人，1995年）。实验结果和计算表明，静电双电层力可以通过调节电解质浓度来消除（Butt，1992 a，b），为AFM正确测量高度提供了条件。此外，所观察到的高度依赖性的生物结构上的电解质浓度允许其表面电荷密度进行估计。</code></p></blockquote><blockquote><p>Optimal location discrimination of two multipartite pure states<br><code>两个多体纯态的最佳位置判别</code><br>INTRODUCTION<br><code>导言</code><br>Entanglement lies at the heart of many aspects of quantum information theory and it is therefore desirable to understand its structure as well as possible. One attempt to improve our understanding of entanglement is the study of our ability to perform information theoretic tasks locally on non-local states, such as the local implementation of non-local quantum gates [2], telecloning [3], the remote manipulation and preparation of quantum states [4] or the recently studied question of the local discrimination of non-local states by a variety of authors. In [1] it was shown that any two orthogonal pure states can be perfectly discriminated locally, whereas in [5] examples of two orthogonal mixed states were presented which cannot be distinguished perfectly locally. Another surprising development is that there exist bases of product orthogonal pure states which cannot be locally reliably discriminated, despite the fact that each state in the basis contains no entanglement [6]. Here we discuss the issue of discriminating two non-orthogonal pure states locally, and show that in this regime the optimal global procedure can be achieved.<br><code>纠缠是量子信息理论许多方面的核心，因此希望尽可能好地理解它的结构。提高我们对纠缠的理解的一个尝试是研究我们在非局域态上本地执行信息理论任务的能力，例如非局域量子门的本地实现[2]，远程克隆[3]，量子态的远程操纵和制备[4]或最近由各种作者研究的非局域态的本地鉴别问题。文献[1]证明了任意两个正交纯态都可以在局域上完全区分，而文献[5]给出了两个正交混合态的例子，它们不能在局域上完全区分。另一个令人惊讶的发展是，存在不能局部可靠地区分的乘积正交纯态的基，尽管基中的每个态不包含纠缠[6]。在这里，我们讨论的问题，区分两个非正交的纯态的本地，并表明，在这种制度的最佳整体程序可以实现。</code></p></blockquote><blockquote><p>Inert COD production in a membrane anaerobic reactor treating brewery wastewater<br><code>膜厌氧反应器处理啤酒废水的惰性COD产生</code><br>INTRODUCTION<br><code>导言</code><br>Th e chemical characterization of wastewaters is commonly undertaken to determine their biological treatability, load on an existing treatment system, or compliance with the fi nal discharge standards. In each case, one of the most important parameters to be measured is the chemical oxygen demand (COD). In general, the COD value of a wastewater mainly represents the biodegradable and non-biodegradable organic components, although inorganic compounds may be signifi cant in certain cases. In biological treatment systems, the biodegradable fraction of wastewater can be removed eff ectively, but its non-biodegradable fraction passes through the system unchanged. In addition to this, a signifi cant amount of soluble microbial products may be produced by microorganisms within the treatment systems. Some of these will be resistant to biological degradation and will appear in reactor effl uents. Th e factors that aff ect effl uent quality and overall organic matter removal in biological treatment systems are, therefore, the presence of both the inert COD fraction in the infl uent wastewater and the soluble microbial products which are produced during biological treatment. Although their concentrations may have few practical implications in the treatment of low strength wastewaters, they may have relatively greater signifi cance in the treatment of medium-high strength industrial wastewaters.<br><code>废水的化学表征通常用于确定其生物可处理性、现有处理系统的负荷或最终排放标准的符合性。在每种情况下，要测量的最重要的参数之一是化学需氧量（COD）。通常，废水的COD值主要代表可生物降解和不可生物降解的有机组分，尽管在某些情况下无机化合物可能是重要的。在生物处理系统中，废水中可生物降解的部分可以被有效地去除，但其不可生物降解的部分通过系统不变。除此之外，显著量的可溶性微生物产物可由处理系统内的微生物产生。其中一些将抵抗生物降解，并将出现在反应堆流出物中。因此，影响生物处理系统中出水质量和总有机物去除的因素是进水中惰性COD部分和生物处理过程中产生的可溶性微生物产物两者的存在。虽然它们的浓度在低浓度废水的处理中可能没有实际意义，但它们在中高浓度工业废水的处理中可能具有相对较大的意义。</code><br>There is extensive literature on the determination of inert COD fractions in industrial wastewaters under aerobic conditions (Chudoba, 1985; Ekama et al., 1986; Rittman et al., 1987; Henze et al., 1987; Orhon et al., 1989; Germirli et al., 1991). However, little has been reported under anaerobic conditions (Germili et al., 1998; Ince et al., 1998). Since medium-high strength industrial wastewaters have been treated efficiently by anaerobic treatment systems, both the inert COD fraction of wastewaters under anaerobic conditions and the soluble microbial products produced within the anaerobic treatment systems should be investigated.<br><code>有大量关于在需氧条件下测定工业废水中的惰性COD级分的文献（Chudoba，1985; Ekama等人，1986; Rittman等人，1987; Henze等人，1987; Orhon等人，1989; Germirli等人，1991年）。然而，在厌氧条件下几乎没有报道（Germili等人，1998;因斯等人，1998年）。由于中高浓度工业废水已被厌氧处理系统有效地处理，在厌氧条件下废水的惰性COD部分和厌氧处理系统内产生的可溶性微生物产物都应进行研究。</code><br>A novel anaerobic reactor system, crossflow ultrafiltration membrane anaerobic reactor (CUMAR) has previously shown great potential for retaining high biomass levels and high biological activity within a fully functioning anaerobic digester (Ince et al., 1993, 1994, 1995a). Since the CUMAR system can be operated at high organic loading rates, the quantification of its efficiency under varying loading rates would be of considerable interest, particularly with regard to the nature and quantity of soluble COD produced in the reactor effluent under various operating conditions.<br><code>一种新的厌氧反应器系统，错流超滤膜厌氧反应器（CUMAR）先前已经显示出在全功能厌氧消化器内保持高生物量水平和高生物活性的巨大潜力（因斯等人，1993年、1994年、1995年a）。由于CUMAR系统可以在高有机负荷率下操作，因此在不同负荷率下其效率的量化将是相当感兴趣的，特别是关于在各种操作条件下反应器流出物中产生的可溶性COD的性质和量。</code><br>In this study, formation of soluble microbial products within a 120:1 [is this correct? Should it be 120:1?] pilot-scale CUMAR system treating brewery wastewater will, therefore, be discussed in relation to reactor operating conditions.<br><code>在这项研究中，可溶性微生物产物的形成在120：1之内[这是正确的吗？应该是120：1吗？中试规模CUMAR系统处理啤酒废水，因此，将讨论有关反应器的操作条件。</code></p></blockquote><blockquote><p>Organic vapour phase deposition: a new method for the growth of organic thin fi lms with large optical non-linearities<br><code>有机气相沉积：一种生长大光学非线性有机薄膜的新方法</code><br>INTRODUCTION<br><code>导言</code><br>There is considerable interest in organic materials with large second-order hyperpolarizabilities for use in non-linear optical (NLO) devices such as modulators and frequency doublers [1]. To achieve a high figure of merit for such NLP devices requires a material with a non-centrosymmetric bulk structure and low dielectric constant.<br><code>具有大的二阶超极化率的有机材料用于非线性光学（NLO）器件如调制器和倍频器[1]具有相当大的兴趣。为了实现这种NLP器件的高品质因数，需要具有非中心对称体结构和低介电常数的材料。</code><br>To this end, NLP-active chromophores are traditionally incorporated into a polymer matrix and electrically poled to achieve the necessary bulk symmetry. However, such materials are limited by their low glass transition temperatures and poor stabilities at elevated temperature.<br><code>为此，NLP活性发色团传统上被并入聚合物基质中并被电极化以实现必要的体对称性。然而，此类材料受限于它们的低玻璃化转变温度和在升高的温度下差的稳定性。</code></p></blockquote><blockquote><p>Recently, single crystals of organic and organometallic salts [2–4] have been shown to possess extremely large second-order (x(2)) NLP eff ects leading to a high second harmonic generation (SHG) effi ciency. Th e naturally non-centrosymmetric crystal structures of these compounds obviates the need for external poling. Furthermore, these salts have a high optical damage threshold and suffi cient stability with respect to temperature to withstand many conventional semiconductor fabrication processes. In particular, highly pure single crystals of the salt, 4′-dimethylamino-N-methyl-4-stilbazolium tosylate (DAST) [2], have been shown to have a value of x(2) at least 103 times greater than that of urea due to dipole alignment of the cation and anion constituents of the DAST structure. To illustrate this alignment, the DAST bulk crystal structure is shown in the inset of Fig. 1.<br><code>最近，有机盐和有机金属盐的单晶[2-4]已被证明具有极大的二阶（x（2））NLP效应，导致高的二次谐波产生（SHG）效率。这些化合物的天然非中心对称晶体结构避免了外部极化的需要。此外，这些盐具有高的光学损伤阈值和相对于温度的足够稳定性，以承受许多常规的半导体制造工艺。特别是，由于DAST结构的阳离子和阴离子成分的偶极排列，高纯度的盐单晶，4′-二甲基氨基-N-甲基-4-芪苯磺酸盐（DAST）[2]，已被证明具有至少103倍于尿素的x（2）值。为了说明这种对准，DAST块体晶体结构在图1的插图中示出。</code><br>For many applications such as waveguide devices, it is desirable to grow NLO materials into optical quality thin fi lms. Although thermal evaporation in a high vacuum environment has been used to grow thin fi lms of many organic [5–7] and inorganic materials, the technique is not always applicable to highly polar molecules [8] or organic salts.<br><code>对于诸如波导器件的许多应用，期望将NLO材料生长成光学质量的薄膜。虽然在高真空环境中的热蒸发已被用于生长许多有机[5-7]和无机材料的薄膜，但该技术并不总是适用于高极性分子[8]或有机盐。</code><br>For example, when heated in vacuum, DAST decomposes before vaporization. Although in situ reactions of multicomponent organic molecules to synthesize polymer fi lms previously has been demonstrated using vacuum techniques as physical vapour deposition or vapour deposition polymerization [9], attempts in our own laboratory at double-source co-evaporation of DAST neutral precursors 4′-dimethylamino-4-stilbazole (DAS) and methyl p-toluenesulfonate (Methyltosylate, MT) to form DAST have been unsuccessful, due in part to the radically diff erent vapour pressures of DAS and MT, which leads to highly non-stoichiometric growth.<br><code>例如，当在真空中加热时，DAST在蒸发之前分解。虽然多组分有机分子合成聚合物薄膜的原位反应以前已被证明使用真空技术作为物理气相沉积或气相沉积聚合[9]，但在我们自己的实验室中，DAST中性前体4′-二甲氨基-4-芪唑（DAS）和对甲苯磺酸甲酯的双源共蒸发的尝试在一些实施方案中，用对甲苯磺酸甲酯（Methyltosylate，MT）形成DAST的方法是不成功的，部分原因是DAS和MT的蒸气压完全不同，这导致高度非化学计量的生长。</code><br>In contrast, atmospheric or low pressure (eg milliTorr) vapour phase epitaxy (VPE) has been used to grow epitaxial thin fi lms of many III-V compound semiconductors, such as InP and GaAs, where there is a large diff erence in the vapour pressures of the group III and group V atomic constituents [10]. Th is method was recently extended to allow the growth of III-V and II-VI semiconductors from volatile organic precursors [11]. Here, a high vapour pressure compound (typically a metal halide or a metallorganic) of each respective metal is carried independently, via a carrier gas, to a high temperature reaction zone. In this zone, the compounds are deposited onto a heated substrate where they thermally decompose and react to yield the desired III-V compound. Th e excess reactants and reaction products are then exhausted from the system via a scrubber.<br><code>相比之下，大气压或低压（例如毫托）气相外延（VPE）已用于生长许多III-V族化合物半导体（如InP和GaAs）的外延薄膜，其中III族和V族原子成分的蒸气压存在很大差异[10]。这种方法最近被扩展到允许从挥发性有机前体生长III-V族和II-VI族半导体[11]。在此，每种相应金属的高蒸气压化合物（通常为金属卤化物或金属有机物）经由载气独立地被运送到高温反应区。在该区域中，化合物被沉积到加热的基底上，在那里它们热分解并反应以产生所需的III-V族化合物。然后，过量的反应物和反应产物通过洗涤器从系统中排出。</code><br>In this paper we apply the techniques of VPE to grow fi lms of DAST by the reaction of two volatile organic materials in a hot-wall, atmospheric pressure reactor. By nuclear magnetic resonance (NMR) analysis, we fi nd that the stoichiometry of polycrystalline DAST fi lms is &gt;95% pure (limited by instrumental sensitivity). Using X-ray diff raction and other analytical techniques, we observe a signifi cant dependence of fi lm quality, such as ordering and crystallite size, on the substrate composition and other deposition conditions used for growth, suggesting that it may be possible to generate optical quality thin fi lms of DAST and similar organic salts and compounds by OVPD using suitable substrates. To our knowledge, this is the fi rst demonstration of the deposition of ordered thin fi lms of a highly non-linear optically active organic salt using atmospheric vapour phase techniques.<br><code>本文采用气相外延技术，在常压热壁反应器中，通过两种挥发性有机物的反应，制备了DAST薄膜。通过核磁共振（NMR）分析，我们发现多晶DAST膜的化学计量比大于95%（受仪器灵敏度的限制）。使用X射线衍射和其他分析技术，我们观察到薄膜质量，如有序性和微晶尺寸，对衬底成分和用于生长的其他沉积条件的显著依赖性，这表明使用合适的衬底通过OVPD可以生成DAST和类似有机盐和化合物的光学质量薄膜。据我们所知，这是第一次使用大气气相技术沉积高度非线性光学活性有机盐的有序薄膜。</code></p></blockquote><blockquote><p>Limitations of charge-transfer models for mixed-conducting oxygen electrodes<br><code>混合导电氧电极电荷转移模型的局限性</code><br>INTRODUCTION<br><code>导言</code><br>Traditionally, electrochemistry is concerned with charge-transfer reactions occurring across a 2-dimensional interface. Indeed, at any macroscopic two-phase boundary, the magnitude, direction and driving force for current density can be described relatively unambiguously. As early as 1933 [1], workers began introducing the concept of a ‘three-phase boundary’ (solid/liquid/gas) in order to allow for direct involvement of gas-phase species at an electrochemical interface. However, since matter cannot pass through a truly one-dimensional interface among three phases, concepts of ‘interfacial area’, ‘current density’, and ‘overpotential’ at a three-phase boundary lack clear defi nition. For example, where exactly is the current fl owing from/to, and what is the local fl ux density? Also, if we defi ne overpotential in terms of thermodynamic potentials of species outside the interfacial region, what species and region are we talking about? Although the threephase boundary concept may serve as a useful abstraction of the overall electrode reaction, it does not address these mechanistic questions.<br><code>传统上，电化学涉及发生在二维界面上的电荷转移反应。事实上，在任何宏观两相边界，电流密度的大小，方向和驱动力可以相对明确地描述。早在1933年[1]，工作人员就开始引入“三相边界”（固/液/气）的概念，以允许气相物质直接参与电化学界面。然而，由于物质不能通过三相之间的真正一维界面，因此三相边界处的“界面面积”、“电流密度”和“超电势”的概念缺乏明确的定义。例如，电流从哪里流出/流向哪里，以及局部电流密度是多少？同样，如果我们用界面区域外的热力学势来定义超势，我们讨论的是什么样的物种和区域？虽然三相边界的概念可以作为一个有用的抽象的整体电极反应，它不解决这些机械问题。</code><br>Workers studying gas-diff usion electrodes in the mid1960s recognized the limitations of the three-phase boundary concept [2, 3]. As an alternative, they began to break down the electrode reaction into individual steps, some that involve chargetransfer across a two-dimensional interface, and some that involve dissolution and diff usion of molecular species in three dimensions or across a chemical interface. Th ese and subsequent studies have demonstrated that electrodes with i-V characteristics indicative of charge-transfer limitations (eg. Tafel behaviour) can, in fact, be limited by steps that do not themselves involve chargetransfer [4]. Although the solid-state literature has held on to the three-phase boundary concept more tightly than the aqueous or polymer literature, few examples remain today or solid-state electrochemical reactions that are not partially limited by solidstate reaction and diff usion processes.<br><code>在20世纪60年代中期，研究气体微分电极的工作人员认识到三相边界概念的局限性[2，3]。作为替代方案，他们开始将电极反应分解为单个步骤，一些涉及二维界面上的电荷转移，一些涉及三维或化学界面上分子种类的溶解和差异。这些和随后的研究表明，具有指示电荷转移限制的i-V特性的电极（例如，塔菲尔行为），事实上，可以限制的步骤，本身并不涉及电荷转移[4]。尽管固态文献比水溶液或聚合物文献更紧密地坚持三相边界概念，但今天很少有不受固态反应和不同过程限制的固态电化学反应的例子。</code><br>One example is the O2-reduction reaction on a mixedconducting perovskite electrode, which defi es rational explanation in terms of interfacial impedance. In order to incorporate noncharge-transfer eff ects, workers oft en apply an empirical Butler– Volmer model (for DC characteristics) or an equivalent-circuit model (for AC impedance) that treat non-charge-transfer processes in terms of an eff ective overpotential/current relationship [5, 6]. However, this approach lacks generality and can oft en be incorrect for treating oxygen absorption and solid-state and gaseous diff usion, which contribute to the impedance in a convoluted manner [7]. Although such models may provide a useful set of parameters to ‘fi t’ data accurately, they leave the electrode reaction mechanism only vaguely or empirically defi ned, and provide little mechanistic insight.<br><code>一个例子是在混合导电钙钛矿电极上的O2还原反应，其根据界面阻抗定义了合理的解释。为了纳入非电荷转移效应，工作人员通常采用经验Butler- Volmer模型（用于DC特性）或等效电路模型（用于AC阻抗），这些模型根据有效过电位/电流关系处理非电荷转移过程[5，6]。然而，这种方法缺乏通用性，并且对于处理氧气吸收以及固态和气态差异来说往往是不正确的，这以复杂的方式对阻抗起作用[7]。虽然这样的模型可以提供一组有用的参数来准确地“拟合”数据，但是它们仅模糊地或凭经验地定义了电极反应机制，并且提供了很少的机理见解。</code><br>Th e purpose of this paper is to provide a framework for defi ning ‘charge-transfer’ and ‘non-charge-transfer’ processes, and to illustrate how they are diff erent. We investigate why charge-transfer models have diffi culty modelling non-chargetransfer eff ects, and walk through several examples including the ALS model for oxygen reduction on a porous mixed-conducting oxygen electrode. We then review a recent study of linear AC polarization of La1-x Srx CoO3-5 (LSCO) electrodes on ceria that corroborates the ALS model, and demonstrates the importance of O2 surface exchange and diff usion. Th is study shows that the electrode reaction extends up to 20 microns beyond the electrode/ electrolyte interface, implying that electrode polarization is better described by macroscopic thermodynamic gradients than as an ‘overpotential’.<br><code>本文的目的是提供一个定义“电荷转移”和“非电荷转移”过程的框架，并说明它们是如何不同的。我们研究了为什么电荷转移模型难以模拟非电荷转移效应，并通过几个例子，包括在多孔混合导电氧电极上氧还原的ALS模型。然后，我们回顾了最近的一项研究，证实了ALS模型的氧化铈上的La 1-x Srx CoO 3 -5（LSCO）电极的线性交流极化，并证明了O2表面交换和差异的重要性。这一研究表明，电极反应延伸到20微米以外的电极/电解质界面，这意味着电极极化更好地描述宏观热力学梯度比作为一个“过电位”。</code></p></blockquote><p>Now do the same for the Introductions of your target articles. You should fi nd that most Introductions begin with item 1, that the order of the model components is usually fairly reliable (although items 2 and 3 can occur more than once) and that almost all Introductions fi nish with number 4. We have, therefore, answered the three questions we set at the beginning of this unit:<br><code>现在对你的目标文章的介绍做同样的事情。您应该发现大多数介绍都是从项目1开始开始的，模型组件的顺序通常相当可靠（尽管项目2和3可能出现不止一次），并且几乎所有介绍都以数字4结束。因此，我们已经回答了本单元开始时提出的三个问题：</code><br>How do I start the Introduction? What type of sentence should I begin with?<br><code>如何开始介绍？我开始应该用什么类型的句子？</code><br>What type of information should be in my Introduction, and in what order?<br><code>我的介绍中应该包含哪些类型的信息，以及顺序如何？</code><br>How do I end the Introduction?<br><code>如何结束介绍？</code></p><h1 id="1-4-Vocabulary"><a href="#1-4-Vocabulary" class="headerlink" title="1.4 Vocabulary"></a>1.4 Vocabulary</h1><p>You now need to collect vocabulary for each part of the Introduction model. Th e vocabulary in this section is taken from over 600 research articles in different fields, all of which were written by native speakers and published in science journals. Only words/phrases which appear frequently have been included; this means that the vocabulary lists contain words and phrases which are considered normal and acceptable by both writers and editors. We will look at vocabulary for the following areas of the model:</p><p><code>现在需要为Introduction模型的每个部分收集词汇表。本节中的词汇来自不同领域的600多篇研究文章，所有这些文章都是由母语人士撰写并发表在科学期刊上的。只有经常出现的单词/短语被包括在内;这意味着词汇表包含作者和编辑认为正常和可接受的单词和短语。我们将查看模型的以下区域的词汇表：</code></p><ol><li>ESTABLISHING SIGNIFICANCE</li></ol><p><code>确立重要性</code></p><p>This includes phrases such as Much research in recent years. A good list of commonly used words and expressions will encourage you to include this in your first sentences.</p><p><code>这包括诸如近年来许多研究等短语。一个好的常用单词和表达的列表会鼓励你在你的第一句话中包括这一点。</code></p><ol start="2"><li>PREVIOUS AND/OR CURRENT RESEARCH AND CONTRIBUTIONS</li></ol><p><code>以往和/或目前的研究和贡献</code></p><p>This includes all past tense verbs describing what researchers did, i.e. calculated, monitored, etc. Instead of just using did, showed and found, you oft en need to be more specific about what a researcher actually ‘did’!</p><p><code>这包括所有描述研究人员做了什么的过去式动词，即计算，监测等，而不仅仅是使用做了，显示和发现，你经常需要更具体地说明研究人员实际上做了什么！</code></p><ol start="3"><li>GAP/PROBLEM/QUESTION/PREDICTION</li></ol><p><code>差距/问题/问题/预测</code></p><p>This includes ways to say exactly how previous and/or current research is not yet complete or has not addressed the problem your paper deals with, e.g. However, few studies have focused on…</p><p><code>这包括如何准确地说明以前和/或当前的研究尚未完成或尚未解决您的论文所涉及的问题，例如，然而，很少有研究关注......</code></p><ol start="4"><li>THE PRESENT WORK</li></ol><p><code>本工作</code></p><p>This may include your purpose, your strategy and the design of your paper, using language such as the aims of the present work are as follows:</p><p><code>这可能包括你的目的，你的策略和你的论文的设计，使用语言，如本工作的目的如下：</code></p><p>VOCABULARY TASK</p><p><code>词汇任务</code></p><p>Look through the Introductions in this unit and the Introductions of your target articles. Underline or highlight all the words and phrases that you think could be used in each of the four areas given above.</p><p><code>浏览本单元的介绍和你的目标文章的介绍。划出或突出所有你认为可以用在以上四个方面的单词和短语。</code></p><p>A full list of useful language can be found on the following pages. Th is includes all the appropriate words and phrases from the Introductions in this unit, together with some other common ones which you may have seen in your target articles. Underneath each list you will find examples of how they are used. Read through the list and check the meaning of any you don’t know in the dictionary. This list will be useful for many years.</p><p><code>有用的语言的完整列表可以在以下页面中找到。这包括了本单元介绍中所有合适的单词和短语，以及你可能在目标文章中看到的其他一些常用词。在每个列表下面，您将找到如何使用它们的示例。通读清单，并在字典中检查你不知道的意思。这份清单将在许多年内有用。</code></p><h2 id="1-4-1-Vocabulary-for-the-Introduction"><a href="#1-4-1-Vocabulary-for-the-Introduction" class="headerlink" title="1.4.1 Vocabulary for the Introduction"></a>1.4.1 Vocabulary for the Introduction</h2><p><code>1.4.1引言词汇</code><br>You now need to collect vocabulary for each part of the Introduction model. The vocabulary in this section is taken from over 600 research articles in different fields, all of which were written by native speakers and published in science journals. Only words/phrases which appear frequently have been included; this means that the vocabulary lists contain words and phrases which are considered normal and acceptable by both writers and editors. We will look at vocabulary for the following areas of the model:<br><code>现在需要为Introduction模型的每个部分收集词汇表。本节中的词汇来自不同领域的600多篇研究文章，所有这些文章都是由母语人士撰写并发表在科学期刊上的。只有经常出现的单词/短语被包括在内;这意味着词汇表包含作者和编辑认为正常和可接受的单词和短语。我们将查看模型的以下区域的词汇表：</code></p><ol><li>ESTABLISHING SIGNIFICANCE<br><code>确立重要性</code><br>This includes phrases such as Much research in recent years. A good list of commonly used words and expressions will encourage you to include this in your first sentences.<br><code>这是包括短语，如许多研究在最近几年。一个好的常用单词和表达的列表会鼓励你在你的第一句话中包括这一点。</code></li><li>PREVIOUS AND/OR CURRENT RESEARCH AND CONTRIBUTIONS<br><code>以往和/或目前的研究和贡献</code><br>This includes all past tense verbs describing what researchers did, i.e. calculated, monitored, etc. Instead of just using did, showed and found, you oft en need to be more specific about what a researcher actually ‘did’!<br><code>这包括所有描述研究人员做了什么的过去式动词，即计算，监测等，而不仅仅是使用做了，显示和发现，你经常需要更具体地说明研究人员实际上做了什么！</code></li><li>GAP/PROBLEM/QUESTION/PREDICTION<br><code>差距/问题/问题/预测</code><br>This includes ways to say exactly how previous and/or current research is not yet complete or has not addressed the problem your paper deals with, e.g. However, few studies have focused on…<br><code>这包括如何准确地说明以前和/或当前的研究尚未完成或尚未解决您的论文所涉及的问题，例如，然而，很少有研究关注......</code></li><li>THE PRESENT WORK<br><code>本工作</code><br>This may include your purpose, your strategy and the design of your paper, using language such as the aims of the present work are as follows:<br><code>这可能包括你的目的，你的策略和你的论文的设计，使用语言，如本工作的目的如下：</code><blockquote><p>VOCABULARY TASK<br><code>词汇任务</code><br>Look through the Introductions in this unit and the Introductions of your target articles. Underline or highlight all the words and phrases that you think could be used in each of the four areas given above.<br><code>浏览本单元的介绍和你的目标文章的介绍。划出或突出所有你认为可以用在以上四个方面的单词和短语。</code><br>A full list of useful language can be found on the following pages. This includes all the appropriate words and phrases from the Introductions in this unit, together with some other common ones which you may have seen in your target articles. Underneath each list you will find examples of how they are used. Read through the list and check the meaning of any you don’t know in the dictionary. This list will be useful for many years.<br><code>有用的语言的完整列表可以在以下页面中找到。这包括本单元介绍中所有合适的单词和短语，以及你可能在目标文章中看到的其他常见单词和短语。在每个列表下面，您将找到如何使用它们的示例。通读清单，并在字典中检查你不知道的意思。这份清单将在许多年内有用。</code></p></blockquote></li></ol><h2 id="1-4-1-Vocabulary-for-the-Introduction-1"><a href="#1-4-1-Vocabulary-for-the-Introduction-1" class="headerlink" title="1.4.1 Vocabulary for the Introduction"></a>1.4.1 Vocabulary for the Introduction</h2><p><code>引言词汇</code></p><ol><li>ESTABLISHING SIGNIFICANCE<br><code>确立重要性</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-22.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-23.png"><br>Here are some examples of how these are used:<br><code>下面是一些如何使用这些的例子：</code></li></ol><ul><li>A <em><strong>major current focus</strong></em> in population management is how to ensure sustainability of…<br><code>当前人口管理的一个主要重点是如何确保…</code></li><li><em><strong>Numerous experiments have established that</strong></em> ionising radiation causes…<br><code>大量的实验已经证实电离辐射会导致...</code></li><li>Low-dose responses to radiation have <em><strong>generated considerable recent research interest</strong></em>.<br><code>对辐射的低剂量反应最近引起了相当大的研究兴趣。</code></li><li>Analysis of change in the transportation sector is vital for two important reasons: …<br><code>分析交通部门的变化至关重要，原因有两个：</code></li><li>PDA accounts for <em><strong>over 95%</strong></em> of all pancreatic cancers.<br><code>PDA占所有胰腺癌的95%以上。</code></li><li><em><strong>It is generally accepted that</strong></em> joints in steel frames operate in a semirigid fashion.<br><code>一般认为钢框架中的节点是以半刚性方式工作的。</code></li><li>Nanocrystalline oxide films <em><strong>are attracting widespread interest</strong></em> in fields such as…<br><code>纳米晶体氧化物薄膜在诸如…等领域引起了广泛的兴趣…</code></li><li><em><strong>The importance of</strong></em> strength anisotropy has been demonstrated by…<br><code>强度各向异性的重要性已被证明…</code></li><li>Convection heat transfer phenomena <em><strong>play an important role in</strong></em> the development of…<br><code>对流换热现象在发展中起着重要的作用...</code><br>For <em><strong>more than 100 years</strong></em> researchers have been observing the stressstrain behaviour of…<br><code>100多年来，研究人员一直在观察...</code></li><li><em><strong>Much research in recent years has focused on</strong></em> carbon nanotubes.<br><code>近年来，许多研究都集中在碳纳米管上。</code></li></ul><ol start="2"><li>VERBS USED IN THE LITERATURE REVIEW TO PRESENT PREVIOUS AND/OR CURRENT RESEARCH AND CONTRIBUTIONS<br><code>文献综述中用于介绍既往和/或当前研究和贡献的动词</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-24.png"><br>Here are some examples of how these are used:<br><code>下面是一些如何使用这些的例子：</code></li></ol><ul><li>This phenomenon <em><strong>was demonstrated</strong></em> by…<br><code>这种现象被证明是由…</code></li><li>In their study, expanded T-cells <em><strong>were found</strong></em> in…<br><code>在他们的研究中，在...</code></li><li>Initial attempts <em><strong>focused on identifying</strong></em> the cause of…<br><code>最初的尝试集中在确定的原因.</code></li><li>Weather severity <em><strong>has been shown to</strong></em>…<br><code>恶劣的天气已经被证明...</code></li><li>Early data <em><strong>was interpreted</strong></em> in the study by…<br><code>早期的数据在研究中被解释为...</code></li><li>The algorithm <em><strong>has been proposed</strong></em> for these applications…<br><code>该算法已提出了这些应用程序.</code></li><li>The results on pair dispersion <em><strong>were reported in</strong></em>…<br><code>对色散的研究结果已在…</code></li><li>Their study <em><strong>suggested</strong></em> a possible cause for…<br><code>他们的研究提出了一个可能的原因...</code></li><li>An alternative approach <em><strong>was developed</strong></em> by…<br><code>另一种方法是由…</code></li></ul><p>Note: You can recycle these verbs at the end of the Introduction when you say what you plan to do in your paper (see 4 below)<br><code>注意：你可以在引言的末尾重复使用这些动词，当你说出你在论文中的计划时（见下面的4）</code></p><ol start="3"><li>GAP/QUESTION/PROBLEM/CRITICISM<br><code>差距/问题/问题/批评</code><br>This is often signalled by words such as however, although, while, nevertheless, despite, but.<br><code>这通常用诸如however，although，while，however，despite，but等词来表示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-25.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-26.png"><br>Here are some examples of how these are used:</li></ol><ul><li><em><strong>Few researchers have addressed the problem</strong></em> of…<br><code>很少有研究者提出了这个问题...</code></li><li><em><strong>There remains a need for</strong></em> an efficient method that can…<br><code>需要一种有效的方法，可以...</code></li><li>However, light scattering techniques have been <em><strong>largely unsuccessful</strong></em> to date.<br><code>然而，迄今为止，光散射技术在很大程度上是不成功的。</code></li><li>The high absorbance makes this <em><strong>an impractical option</strong></em> in cases where…<br><code>高吸光度使得这是一个不切实际的选择的情况下，其中...</code></li><li><em><strong>Unfortunately</strong></em>, these methods do not always guarantee…<br><code>然而，这些方法并不总是保证...</code></li><li><em><strong>An alternative approach</strong></em> is necessary.<br><code>必须采取替代办法。</code></li><li>The function of these proteins <em><strong>remains unclear</strong></em>.<br><code>这些蛋白质的功能尚不清楚。</code></li><li>These can be <em><strong>time-consuming</strong></em> and are oft en <em><strong>technically difficult</strong></em> to perform.<br><code>这些可能是耗时的，并且通常在技术上难以执行。</code></li><li><em><strong>Although</strong></em> this approach improves performance, it results in <em><strong>an unacceptable</strong></em> number of…<br><code>虽然这种方法提高了性能，但它会导致不可接受的...</code></li><li>Previous work has focused <em><strong>only</strong></em> on…<br><code>以前的工作只集中在…</code></li><li>However, the experimental confi guration was <em><strong>far from optimal</strong></em>.<br><code>然而，实验配置远非最佳。</code></li></ul><p>Note: Some of these words/phrases express very strong criticism. A useful exercise is to put an asterisk (*) next to those you think you could use if you were talking about the research of your professor or supervisor. You can also alter them to make them more polite (i.e. instead of unsuccessful, which is quite a strong criticism, you could write may not always be completely successful).<br><code>注意：其中一些词/短语表达了非常强烈的批评。一个有用的练习是，如果你谈论的是你的教授或导师的研究，在你认为可以使用的那些旁边加上星号（*）。你也可以改变它们，使它们更礼貌（即，而不是不成功，这是一个相当强烈的批评，你可以写可能并不总是完全成功）。</code></p><ol start="4"><li>THE PRESENT WORK<br><code>本工作</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/science/1-26.png"><br>Here are some examples of how these are used:<br><code>下面是一些如何使用这些的例子：</code></li></ol><ul><li><em><strong>This paper focuses on</strong></em>…<br><code>这篇论文的重点是…</code></li><li><em><strong>The purpose of this study is to describe and examine</strong></em>…<br><code>本研究的目的是描述和检查…</code></li><li><em><strong>In order to</strong></em> investigate the biological signifi cance…<br><code>为了研究其生物学意义...</code></li><li><em><strong>In this paper we present</strong></em>…<br><code>在本文中，我们提出…</code></li><li>New correlations were developed with <em><strong>excellent</strong></em> results…<br><code>新的相关性得到了很好的结果...</code></li><li><em><strong>In the present study</strong></em> we performed…<br><code>在本研究中，我们进行了...</code></li><li><em><strong>This paper introduces</strong></em> a scheme which solves these problems.<br><code>本文介绍了一种解决这些问题的方案。</code></li><li><em><strong>The approach we have used in this study</strong></em> aims to…<br><code>我们在这项研究中使用的方法旨在...</code></li><li><em><strong>This study</strong></em> investigated the use of…<br><code>这项研究调查了使用…</code></li><li><em><strong>In this report</strong></em> we test the hypothesis that…<br><code>在这份报告中，我们测试的假设，即...</code></li><li><em><strong>This paper is organised as follows</strong></em>:…<br><code>该文件的组织如下：...</code></li></ul><p>Note: In a thesis or a very long research paper, you use these to say what each chapter or section will do. Don’t rely on one-size-fi ts-all verbs such as discuss; some chapters/sections do not ‘discuss’ anything, and even if they do, their main purpose may be to compare things, analyse things or describe things rather than to discuss them.<br><code>注意：在一篇论文或一篇很长的研究论文中，你可以用这些来说明每一章或每一节将做什么。不要依赖“一刀切”的动词，如discuss;有些章节/节并不“讨论”任何事情，即使他们这样做，他们的主要目的可能是比较事物，分析事物或描述事物，而不是讨论它们。</code></p><h1 id="1-5-Writing-an-Introduction"><a href="#1-5-Writing-an-Introduction" class="headerlink" title="1.5 Writing an Introduction"></a>1.5 Writing an Introduction</h1><p><code>撰写介绍</code><br>In the next task, you will bring together and use all the information in this unit. You will write an Introduction according to the model, using the grammar and vocabulary you have learned, so make sure that you have the model (Section 1.3.3) and the vocabulary (Section 1.4) in front of you.<br><code>在下一个任务中，你将把本单元中的所有信息结合起来使用。你将根据模型，使用你学过的语法和词汇来写一篇介绍，所以确保你有模型（1.3.3节）和词汇（1.4节）在你面前。</code><br>Throughout this unit you have seen that conventional science writing is easier to learn, easier to write and easier for others to read than direct translations from your own language or more creative writing strategies. You have learned the conventional model of an Introduction and collected the vocabulary conventionally used. Your sentence patterns should also be conventional; use the sentences you have read in your target articles and in the Introductions printed here as models for the sentence patterns in your writing, and adapt them for the task.<br><code>在本单元中，你已经看到传统的科学写作比直接从你自己的语言翻译或更具创造性的写作策略更容易学习，更容易写作，更容易让别人阅读。你已经学习了介绍的常规模式，并收集了常规使用的词汇。你的句型也应该是常规的;使用你在目标文章和这里打印的介绍中读到的句子作为你写作中句型的模型，并使它们适应任务。</code><br>Follow the model exactly this time. After you have practiced it once or twice you can vary it to suit your needs. However, you should use it to check Introductions you have written so that you can be sure that the information is in an appropriate order and that you have done what your readers expect you to do in an Introduction.<br><code>这次完全按照模型来。在你练习了一两次之后，你可以改变它以适应你的需要。但是，你应该用它来检查你写的介绍，这样你就可以确保信息的顺序是正确的，你已经做了读者期望你在介绍中做的事情。</code><br>Although a model answer is provided in the Key, you should try to have your own answer checked by a native speaker of English if possible, to make sure that you are using the vocabulary correctly.<br><code>虽然答案中提供了一个标准答案，但如果可能的话，您应该尝试让母语为英语的人检查您自己的答案，以确保您正确使用词汇。</code></p><h2 id="1-5-1-Write-an-Introduction"><a href="#1-5-1-Write-an-Introduction" class="headerlink" title="1.5.1 Write an Introduction"></a>1.5.1 Write an Introduction</h2><p><code>撰写介绍</code><br>Imagine that you have just completed a research project to design a bicycle cover which can protect the cyclist from injury, pollution, or just from rain. Perhaps you provided a computer simulation of its use, or modelled the ventilation system. Perhaps you were involved in the aerodynamics, or the polymer construction of the material for the cover — or any other aspect of the project. Write the Introduction of your research paper, to be published in the Journal of Pedal-Powered Vehicles (Vol. 3). Th e title of your research paper is A COVER FOR THE SPPPV (Single-Person Pedal-Powered Vehicle) and your Introduction should be between 200– 400 words. You can lie as much as you like, and of course you will have to create fake research references. Follow the model as closely as possible; make sure your Introduction contains the four main components of the model and try out some of the new vocabulary.<br><code>想象一下，你刚刚完成了一个研究项目，设计了一个自行车盖，可以保护骑自行车的人免受伤害，污染，或只是从雨。也许你提供了一个计算机模拟它的使用，或模拟通风系统。也许你参与了空气动力学，或聚合物结构的材料的封面-或任何其他方面的项目。撰写你的研究论文的引言，将发表在《脚踏车杂志》（第三卷）上。你的研究论文的题目是一个封面的SPPPV（单人踏板动力车）和你的介绍应该在200- 400字之间。你可以尽可能多地撒谎，当然，你将不得不创建虚假的研究参考。尽可能地遵循模型;确保你的介绍包含模型的四个主要组成部分，并尝试一些新的词汇表。</code><br>If you get stuck and don’t know what to write next, use the model and the vocabulary to help you move forward. Don’t look at the key until you have fi nished writing.<br><code>如果你陷入了困境，不知道下一步该写什么，使用模型和词汇表来帮助你前进。写完后再看钥匙。</code></p><h2 id="1-5-2-Key"><a href="#1-5-2-Key" class="headerlink" title="1.5.2 Key"></a>1.5.2 Key</h2><p><code>关键</code><br>Here is a sample answer. When you read it, think about which part of the model is represented in each sentence.<br><code>下面是一个示例答案。当你读它的时候，想想模型的哪一部分在每个句子中被代表。</code></p><blockquote><p>A COVER FOR THE SPPPV (Single-Person Pedal-Powered Vehicle)<br><code>SPPPV（单人踏板动力车）的盖子</code><br>Concern about global warming and urban air pollution have become central issues in transport policy decision-making, and as a result much research in recent years has focused on the development of vehicles which are environmentally friendly. Air quality in cities is currently significantly lower than in rural areas and this has been shown to be directly linked to the level of vehicle emissions from private cars. Due to the fact that urban transport policy in the UK is designed to reduce or discourage the use of private cars, there has been an increase in the sale of non-polluting vehicles such as the SPPPV (Single-Person Pedal-Powered Vehicle). However, although the number of SPPPV users has increased, safety and comfort issues need to be addressed if the number of users is to increase to a level at which a significant effect on environmental pollution can be achieved.<br><code>对全球变暖和城市空气污染的关注已成为交通政策决策的中心问题，因此近年来的许多研究都集中在开发环保车辆上。目前，城市的空气质量明显低于农村地区，这已被证明与私人汽车的车辆排放水平直接相关。由于英国的城市交通政策旨在减少或不鼓励使用私人汽车，因此SPPPV（单人踏板动力车）等无污染车辆的销售有所增加。然而，尽管SPPPV用户的数量增加了，但如果用户的数量要增加到可以实现对环境污染的显著影响的水平，则需要解决安全性和舒适性问题。</code><br>Researchers have studied and improved many aspects of the SPPPV. In 1980, Wang et al. responded to the need for increased safety by designing an SPPPV surrounded by a ‘cage’ of safety bars, and in 2001 Martinez developed this further with the introduction of a reinforced polymer screen which could be fitted to the safety bars to protect the cyclist’s face in the event of a collision. The issue of comfort has also been addressed by many design teams; in 1998 Kohl et al. introduced an SPPPV with a built-in umbrella, which could be opened at the touch of a button, and more recently, Martinez has added a mesh filter which can be placed over the entire cage to reduce the risk of environmental pollution. However, the resulting ‘cage’ or cover is aerodynamically ineffective due to the shape of the umbrella and the weight of the mesh filter.<br><code>研究人员对SPPPV的许多方面进行了研究和改进。1980年，Wang等人通过设计一个由安全杆“笼”包围的SPPPV来满足增加安全性的需求，2001年Martinez进一步开发了这一技术，引入了一种可以安装在安全杆上的增强聚合物屏幕，以在发生碰撞时保护骑车人的面部。舒适性的问题也得到了许多设计团队的重视; 1998年，Kohl等人推出了一款内置雨伞的SPPPV，只需按一下按钮即可打开，最近，Martinez增加了一个网状过滤器，可以放置在整个笼子上，以减少环境污染的风险。然而，由于伞的形状和网状过滤器的重量，所得到的“笼”或盖在空气动力学上是无效的。</code><br>In this study, we used computer simulation to model the aerodynamic effect of the existing safety and comfort features and we present a new design which integrates these features in an optimally-effective aerodynamic shape.<br><code>在这项研究中，我们使用计算机模拟来模拟现有的安全性和舒适性功能的空气动力学效果，我们提出了一个新的设计，将这些功能集成在一个最佳的有效的空气动力学形状。</code></p></blockquote></body></html>]]></content>
      
      
      <categories>
          
          <category> sciencewriting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction: How to Use This Book</title>
      <link href="/2024/01/26/introduction-how-to-use-this-book/"/>
      <url>/2024/01/26/introduction-how-to-use-this-book/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><code>简介：如何使用这本书</code><br>Things should be made as simple as possible, but not any simpler.  — Albert Einstein<br><code>事情应该尽可能简单，但不能再简单了。——阿尔伯特·爱因斯坦</code></p><h1 id="Who-is-this-book-for"><a href="#Who-is-this-book-for" class="headerlink" title="Who is this book for?"></a>Who is this book for?</h1><p><code>这本书是给谁的？</code></p><p>This book is designed to help non-native speakers of English write science research papers for publication in English. However, it can also be used as a guide for native English speakers who would like support with their science writing, and by science students who need to write a Master’s dissertation or PhD thesis. It is a practical, rather than a theoretical book, and is intended as a fast do-it-yourself manual for researchers and scientists.<br><code>这本书是为了帮助非英语母语的人用英语写科学研究论文。然而，它也可以作为母语为英语的人谁愿意与他们的科学写作的支持，并通过科学的学生谁需要写硕士论文或博士论文的指南。这是一本实用的书，而不是一本理论书，是为研究人员和科学家提供的快速动手手册。</code></p><p>The book is aimed at those whose English language ability is at intermediate level or above. If you have taken an IELTS test, this is equivalent to a score of above 6.0; if you have taken a TOEFL test then this is approximately equivalent to a score above 550 (paper-based test) or 91 (iBT). However, if you have managed to read this far without using a dictionary, you will be able to use this book, even if you don’t understand every word.</p><p><code>这本书是针对那些英语语言能力在中级或以上的人。如果你参加过雅思考试，这相当于6.0以上的分数;如果你参加了托福考试，那么这大约相当于550分（纸质考试）或91分（iBT）。然而，如果你不使用字典就能读完这本书，即使你并不理解每一个单词，你也可以使用这本书。</code></p><h1 id="Why-do-I-need-it"><a href="#Why-do-I-need-it" class="headerlink" title="Why do I need it?"></a>Why do I need it?</h1><p><code>我为什么需要它？</code></p><p>The goal of scientific research is publication, but good scientists are not always good writers and even native speakers of English sometimes have difficulty when they write up their research. Th e aim of this book is to give you the information, vocabulary and skills you need quickly and easily so that you can write confidently using the style and structure you see in the journals you read.</p><p><code>科学研究的目标是发表，但好的科学家并不总是好的作家，甚至英语为母语的人有时在写他们的研究时也有困难。这本书的目的是给予你所需要的信息、词汇和技巧，使你能快速、轻松地使用你在阅读的期刊中看到的文体和结构进行写作。</code></p><p>As a science researcher, you are able to read and understand complex, high-level material in your field. However, you may find it difficult to produce written English which is at the same level as your reading. You may feel that your English writing does not represent the content of your work effectively or accurately. Th e aim of this book is to enable you to use your reading ability and the material you read to develop the writing skills your work requires.</p><p><code>作为一名科学研究人员，你能够阅读和理解你领域中复杂的、高层次的材料。然而，你可能会发现很难写出与你的阅读水平相同的书面英语。你可能会觉得你的英文写作不能有效或准确地表达你的作品内容。本书的目的是使你能够运用你的阅读能力和你所阅读的材料来发展你的工作所需的写作技巧。</code></p><p>Developing the skills to write up your own research is the only way to join the international science community. If you depend on English speakers to translate your writing, their translation may not represent exactly what you intended. If you depend on proofreaders to correct your English they may not notice some errors, because a sentence which is grammatically correct is still ‘wrong’ if it does not mean what you intended. Also, a proofreader may not check whether your writing fits the conventional ‘science research’ patterns. For example, you may have forgotten to justify your choice of method or explain how your results relate to your original question, and this could mean that an editor of a science journal rejects your paper as unprofessional.</p><p><code>培养撰写自己研究成果的技能是加入国际科学界的唯一途径。如果你依赖说英语的人来翻译你的文章，他们的翻译可能不能完全代表你的意图。如果你依赖校对员来纠正你的英语，他们可能不会注意到一些错误，因为一个语法正确的句子如果不是你想要的意思，仍然是“错误的”。而且，校对员可能不会检查你的写作是否符合传统的“科学研究”模式。例如，你可能忘记了证明你所选择的方法，或者解释你的结果与你最初的问题有什么关系，这可能意味着科学期刊的编辑会以不专业为由拒绝你的论文。</code></p><p>Writing and publishing a research paper is the best way to get your career off the ground. If you can turn your thesis or research project into a useful paper, your CV (Curriculum Vitae) will immediately look more professional and will be more competitive internationally. You may feel that you don’t have the time to improve your English, but you already know most of what you need from the reading you have done over the years. In order to write up your research for publication you don’t need to learn much more English than you already know. Science writing is much easier than it looks.</p><p><code>撰写和发表一篇研究论文是让你的职业生涯起步的最好方法。如果你能把你的论文或研究项目变成一份有用的论文，你的简历（简历）将立即看起来更专业，在国际上更具竞争力。你可能会觉得自己没有时间提高英语水平，但通过多年来的阅读，你已经知道了大部分你需要的东西。为了把你的研究成果写出来发表，你不需要学比你已经知道的更多的英语。科学写作比看起来容易得多。</code></p><p>Most science research is written according to a fairly conventional structure: fi rst the title, then the abstract, followed by an introduction, aft er which there is a central section which describes what was done and what was found and then a discussion and/or conclusion. At the end of the paper or research article, acknowledgements and references are added. Th is means that the structure of a research article will be quite similar for all writers.</p><p><code>大多数科学研究都是按照一种相当传统的结构来写的：首先是标题，然后是摘要，接着是引言，然后是一个中心部分，描述了所做的和所发现的，然后是讨论和/或结论。在论文或研究文章的末尾，添加致谢和参考文献。这意味着，一个研究文章的结构将是非常相似的所有作家。</code></p><p>Because science writing is so conventional, the amount of grammar and vocabulary you need to learn is quite small. For example, the nontechnical vocabulary used in scientifi c writing consists of a limited set of words such as attempt, conduct, interpret, evaluate, determine, implement, formulate, classify, correlate, enhance, which are used as a kind of ‘code’. All the vocabulary you need to get started (apart from the specialised vocabulary of your field) is in this book.</p><p><code>因为科学写作是如此的常规，你需要学习的语法和词汇量相当少。例如，在科学写作中使用的非技术词汇由有限的一组词组成，例如尝试、进行、解释、评估、确定、实施、公式化、分类、关联、增强，这些词被用作一种“代码”。所有你需要开始的词汇（除了你的领域的专业词汇）都在这本书里。</code></p><h1 id="What-will-this-book-teach-me"><a href="#What-will-this-book-teach-me" class="headerlink" title="What will this book teach me?"></a>What will this book teach me?</h1><p><code>这本书会教我什么？</code><br>The book will show you how to discover the conventions of structure, organisation, grammar and vocabulary in science writing in your field and will provide you with the tools to write in a similar way and at a similar level. It will teach you how to turn your research into a paper that can be submitted to a professional journal. You will also be able to use most of the information in the book and all of the language and vocabulary if you are writing a thesis in English.<br><code>这本书将向你展示如何在你的领域中发现科学写作中的结构，组织，语法和词汇的约定，并将为你提供以类似的方式和在类似的水平上写作的工具。它将教你如何把你的研究成果转化为可以提交给专业期刊的论文。如果你用英语写论文，你也可以使用书中的大部分信息和所有的语言和词汇。</code><br>I have been teaching English for Academic Purposes to science students for over 30 years. For the past 15 years I have been teaching research writing in the English Language Support Programme at Imperial College, London, where I also work closely with individual research students and staff who are writing a paper or thesis. Th is book is based on the most useful thing I have learned: when your language skills are not perfect, organising your information in a conventional way and using conventional language are very important. If you write according to a conventional model, the reader knows what you are trying to do because the model you are following is familiar, and language errors are therefore less signifi cant. A researcher who begins by writing according to a simple and conventional model will soon develop higher level skills for writing independently and professionally. Th e opposite is also true: researchers who do not begin by writing according to a conventional model are less likely to develop these skills.<br><code>我已经为理科学生教授学术英语30多年了。在过去的15年里，我一直在伦敦帝国理工学院的英语语言支持项目中教授研究写作，在那里我还与正在撰写论文或论文的个别研究学生和工作人员密切合作。这本书是基于我所学到的最有用的东西：当你的语言能力不完美时，用传统的方式组织信息和使用传统的语言是非常重要的。如果你按照传统的写作模式写作，读者就会知道你想做什么，因为你所遵循的模式是熟悉的，语言错误也就不那么明显了。一个研究者如果一开始就按照简单而传统的模式写作，很快就会发展出更高水平的独立写作和专业写作技能。相反的情况也是如此：研究人员如果开始不按照传统模式写作，就不太可能发展这些技能。</code></p><h1 id="How-does-the-book-work"><a href="#How-does-the-book-work" class="headerlink" title="How does the book work?"></a>How does the book work?</h1><p><code>这本书是如何运作的？</code><br>Th e strategy in this book can be summed up as follows: carefully examine good examples of the kind of writing you would like to produce, identify and master the structure, grammar and vocabulary you see in these examples and then apply them in your own writing.<br><code>本书中的策略可以总结如下：仔细检查你想写的文章的好例子，找出并掌握你在这些例子中看到的结构、语法和词汇，然后把它们运用到你自己的写作中。</code><br>Th e book is divided into fi ve units, each dealing with one section of a research article. Unit 1 deals with the Introduction, Unit 2 the Methodology, Unit 3 the Results, Unit 4 the Discussion or Conclusion and Unit 5 the Abstract and Title. Since the aim of this book is to enable you to write in a conventional way, each unit is designed to help you discover what the conventional model of that section of a research article looks like. In each unit you will also be given support on the grammar and writing skills needed to write that section of the research article and you will be guided towards the appropriate vocabulary.<br><code>这本书分为五个单元，每个单元处理一篇研究文章的一个部分。第一单元是介绍，第二单元是方法，第三单元是结果，第四单元是讨论或结论，第五单元是摘要和标题。由于本书的目的是让你能够以传统的方式写作，所以每一个单元的设计都是为了帮助你发现某篇研究文章的传统模式是什么样子的。在每个单元中，您还将获得撰写该部分研究文章所需的语法和写作技能的支持，并将指导您使用适当的词汇。</code><br>Each unit is similar. Th e unit on Introductions, for example, begins by looking at a sample research article Introduction similar to those in science journals, then there is a Grammar and Writing Skills section designed to respond to frequently asked questions. Because you are probably working hard on your research and don’t have time to do much grammar work, there are very few grammar exercises in the Grammar and Writing Skills sections. In any case, getting the answer right in a grammar exercise doesn’t automatically mean you will produce the correct grammar when you write about complex topics. Answering correctly can give you a false sense of confi dence and security.<br><code>每个单位都是相似的。例如，介绍单元首先看一篇类似于科学期刊的研究文章介绍，然后是语法和写作技巧部分，旨在回答常见问题。因为你可能正在努力研究，没有时间做太多的语法工作，语法和写作技巧部分很少有语法练习。在任何情况下，在语法练习中得到正确的答案并不意味着当你写复杂的话题时，你会产生正确的语法。正确的表达会给你给予一种虚假的自信和安全感。</code><br>After the Grammar and Writing Skills section you will create a model or template for writing Introductions using the sample Introduction, and this is followed by a detailed Key providing model descriptors, discussion and answers to questions. The unit includes extracts from real Introductions so that you can test the model and see how it works in the ‘real world’. These extracts are then used to find the vocabulary which will help you operate the model successfully. This is followed by a complete list of useful vocabulary together with examples of how the words and phrases are used.<br><code>在语法和写作技巧部分之后，您将使用示例介绍创建一个用于编写介绍的模型或模板，然后是详细的关键，提供模型描述符，讨论和问题答案。本单元包括从真实的介绍中摘录的内容，这样你就可以测试模型，看看它在“真实的世界”中是如何工作的。这些摘录然后被用来查找词汇表，这将帮助您成功地操作模型。接下来是一个完整的有用词汇表，以及单词和短语如何使用的例子。</code><br>At this stage, you will have a robust model of an Introduction, a grammar guide to deal with possible problems and a list of useful vocabulary to make the model work. Towards the end of the unit, you will be ready to test what you have learned by writing an Introduction. If you have done the tasks, you should be able to put the model, the grammar/writing skills and the vocabulary together, and a perfect Introduction will write itself almost automatically! So at the end of the unit on Introductions, you will try out what you have learned: you will write an Introduction using the model and the vocabulary list and then compare it with a sample answer in the Key.<br><code>在这个阶段，您将拥有一个强大的介绍模型，一个处理可能出现的问题的语法指南以及一个使模型工作的有用词汇表列表。在本单元的最后，你将准备好通过写一个介绍来测试你所学到的东西。如果你已经完成了任务，你应该能够把模型，语法/写作技巧和词汇放在一起，一个完美的引言几乎会自动写出来！因此，在本单元介绍的最后，您将尝试您所学的内容：你将使用模型和词汇表写一个介绍，然后将它与答案中的示例进行比较。</code><br>This pattern is repeated in the rest of the units. Ideally, you should work through the book and do each task. If you read the book without completing the tasks you will have an intellectual understanding of what to do but you may find it harder to put it into practice.<br><code>这种模式在其余单元中重复。理想情况下，你应该通读这本书并完成每一项任务。如果你没有完成任务就读了这本书，你会对该做什么有一个理智的理解，但你可能会发现把它付诸实践更难。</code></p><h1 id="Do-I-need-any-other-material-or-books"><a href="#Do-I-need-any-other-material-or-books" class="headerlink" title="Do I need any other material or books?"></a>Do I need any other material or books?</h1><p><code>我还需要其他材料或书籍吗？</code><br>No, but before you begin, you should collect three or four recent research papers in your fi eld from the journals you usually read and photocopy them. You will use these as target articles to help you adapt what you learn here to your own work, and you will refer to them while reading this book to see how the things you are learning are done in your research fi eld. Don’t use chapters from books as target articles; they are not written according to the same conventional structure as research papers and so will not help you discover how a research paper or thesis in your field is written.<br><code>不，但在你开始之前，你应该从你经常阅读的期刊上收集三到四篇你所在领域的最新研究论文，并复印下来。你将把这些文章作为目标文章来帮助你把在这里学到的东西应用到自己的工作中，你将在阅读本书时参考它们，看看你正在学习的东西是如何在你的研究领域中完成的。不要把书中的章节作为目标文章;它们不是按照与研究论文相同的传统结构来写的，所以不会帮助你发现你所在领域的研究论文或论文是如何写的。</code><br>Your target research articles should:<br><code>你的目标研究文章应该：</code></p><ul><li>be written by a researcher/research team based at an English-speaking institution, ideally a native speaker of English.<br><code>由英语机构的研究人员/研究团队撰写，最好是英语母语者。</code></li><li>be reasonably short (less than 15 A4 sides including graphs and tables).<br><code>合理的短（少于15个A4面，包括图表）。</code></li><li>deal with subject matter which is as close as possible to your own topic and the kind of research you are doing.<br><code>处理尽可能接近你自己的主题和你正在做的研究类型的主题。</code></li><li>have clearly defi ned Introduction, Methodology, Results and Discussion/ Conclusion sections. It will help you if these are subtitled so that you can locate them easily. Note that the subtitles may vary in diff erent fi elds and even in diff erent journals in each fi eld; for example the Methodology can be called ‘Procedure’, ‘Materials and Methods’, ‘Experimental’ or some other variation.<br><code>明确定义了引言、方法、结果和讨论/结论部分。它将帮助你，如果这些是副标题，以便你可以找到他们容易。请注意，不同的领域，甚至每个领域的不同期刊的副标题可能会有所不同;例如，该方法可以被称为“程序”、“材料和方法”、“实验”或一些其它变体。</code></li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> sciencewriting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>04数据操作+数据预处理</title>
      <link href="/2024/01/25/04-shu-ju-cao-zuo-shu-ju-yu-chu-li/"/>
      <url>/2024/01/25/04-shu-ju-cao-zuo-shu-ju-yu-chu-li/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h1 id="04数据操作-数据预处理"><a href="#04数据操作-数据预处理" class="headerlink" title="04数据操作+数据预处理"></a>04数据操作+数据预处理</h1><h3 id="N维数组样例"><a href="#N维数组样例" class="headerlink" title="N维数组样例"></a>N维数组样例</h3><ul><li>N维数据是机器学习和神经网络的主要数据结构。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-1.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-2.png"></li></ul><h3 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a>创建数组</h3><ul><li>创建数组需要<ul><li>形状：例如3x4矩阵</li><li>每个元素的数据类型：例如32位浮点数</li><li>每个元素的值，例如全是0，或者随机数<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-3.png"></li></ul></li></ul><h3 id="访问元素"><a href="#访问元素" class="headerlink" title="访问元素"></a>访问元素</h3><p>一个元素：[1, 2]<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-4.png"><br>一行：[1, :]<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-5.png"><br>一列：[:, 1]<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-6.png"><br>子区域：[1:3, 1:]<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-7.png"><br>子区域：[::3, ::2]<br><code>跳着访问，每三行一跳，每两列一跳</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-8.png"></p><h3 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h3><p>首先，我们导入<code>torch</code></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-9.png"><br>张量表示一个数值组成的数组，这个数组可以有多个维度</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(12)</span><br><span class="line">x</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-10.png"><br>我们可以通过张量的<code>shape</code>属性来访问张量的形状和张量中元素的总数</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-11.png"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.numel()</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-12.png"><br>要改变一个张量的形状而不改变元素数量和元素值，我们可以调用<code>reshape</code>函数</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = x.reshape(3, 4) #三行四列</span><br><span class="line">x</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-13.png"><br>使用全0、全1、其他常量或者从特定分布中随机采样的数字</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros((2, 3, 4)) #全0</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-14.png"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.ones((2, 3, 4)) #全1</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-15.png"><br>通过提供包含数值的Python列表（或嵌套列表)来为所需张量中的每一个元素赋予确定值</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])</span><br><span class="line">#二维数组</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-16.png"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]])</span><br><span class="line">#三维数组</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]]).shape</span><br></pre></td></tr></tbody></table></figure><p>深度为1，3行4列</p><p>常见的标准算术运算符（+、-、*、/ 和 **)都可以升级为按元素运算</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([1.0, 2, 4, 8])</span><br><span class="line">y = torch.tensor([2, 2, 2, 2])</span><br><span class="line">x + y, x - y, x * y, x ** y</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-17.png"><br>按元素方式应用更多的计算</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(x)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-18.png"><br>我们也可以把多个张量连结在一起</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(12, dtype=torch.float32).reshape((3, 4))</span><br><span class="line">y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])</span><br><span class="line">torch.cat((x, y), dim=0), torch.cat((x,y), dim=1)</span><br><span class="line">#按行合并                   #按列合并</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-19.png"><br>通过逻辑运算符构建二元张量</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x == y</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-20.png"><br>对张量中的所有元素进行求和会产生一个只有一个元素 的张量</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.sum()</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-21.png"><br>即使形状不同，我们仍然可以通过调用广播机制来执行按元素操作</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(3).reshape((3, 1))</span><br><span class="line">b = torch.arange(2).reshape((1, 2))</span><br><span class="line">a, b</span><br><span class="line"></span><br><span class="line">a + b</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-22.png"><br>可以用[-1]选择最后一个元素，可以用[1: 3]选择第二个和第三个元素</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[-1], x[1: 3]</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-23.png"><br>除读取之外，我们还可以通过指定索引来将元素写入矩阵</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x[1, 2] = 9</span><br><span class="line">x</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-24.png"><br>为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x[0:2, :] = 12</span><br><span class="line">x</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-25.png"><br>运行一些操作可能会导致为新的结果分配内存</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before = id(y)</span><br><span class="line">y = y + x</span><br><span class="line">id(y) == before</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-26.png"><br>执行原地操作</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = torch.zeros_like(y)</span><br><span class="line">print('id(z):', id(z))</span><br><span class="line">z[:] = x + y</span><br><span class="line">print('id(z):', id(z))</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-27.png"><br>如果在后续计算中没有重复使用x，我们也可以使用x[:] = x + y或x += y来减少操作的内存开销</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before = id(x)</span><br><span class="line">x += y</span><br><span class="line">id(x) == before</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-28.png"><br>转换为NumPy张量</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = x.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line">type(A), type(B)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-29.png"><br>将大小为1的张量转换为Python标量</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([3.5])</span><br><span class="line">a, a.item(), float(a), int(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-30.png"></p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>创建一个人工数据集，并存储在csv（逗号分隔值）文件</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join('\\', 'data'), exist_ok=True) #创建一个文件夹</span><br><span class="line">data_file = os.path.join('\\', 'data', 'house_tiny.csv') #创建一个文件名为house_tiny.csv</span><br><span class="line">with open(data_file, 'w') as f:</span><br><span class="line">    f.write('NumRooms, Alley, Price\n') #列名</span><br><span class="line">            #房间数量  #进门的路 #价格     </span><br><span class="line">    f.write('NA, Pave, 127500\n')       #每行表示一个数据样本</span><br><span class="line">           #未知  #铺路  #价格</span><br><span class="line">    f.write('2, NA, 106000\n')</span><br><span class="line">    f.write('4, NA, 178100\n')</span><br><span class="line">    f.write('NA, NA, 140000\n')</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-31.png"><br>从创建的csv文件中加载原始数据集</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(data_file) #读取文件数据</span><br><span class="line">data</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-32.png"><br>为了处理缺失的数据，典型的方法包括插值和删除，这里，我们将考虑插值</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]</span><br><span class="line">inputs = inputs.fillna(inputs.mean())</span><br><span class="line">print(inputs)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-33.png"><br>对于<code>inputs</code>中的类别值或离散值，我们将“NaN”视为一个类别</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = pd.get_dummies(inputs, dummy_na=True)</span><br><span class="line">print(inputs)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-34.png"><br>现在<code>inputs</code>和<code>outputs</code>中的所有条目都是数值类型，它们可以转换为张量格式</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)</span><br><span class="line">X, y</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-35.png"></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 动手学深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>00-03</title>
      <link href="/2024/01/25/00-03/"/>
      <url>/2024/01/25/00-03/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h1 id="00预告"><a href="#00预告" class="headerlink" title="00预告"></a>00预告</h1><p>学习深度学习关键是动手</p><ul><li>深度学习是人工智能最热的领域</li><li>核心是神经网络</li><li>神经网络是一门语言</li><li>应该像学习Python/C++一样学习深度学习<br><a href="https://zh.d2l.ai/">《动手深度学习》书籍链接</a><br><a href="https://courses.d2l.ai/zh-v2/">课程网站</a></li></ul><h1 id="01课程安排"><a href="#01课程安排" class="headerlink" title="01课程安排"></a>01课程安排</h1><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ul><li>介绍深度学习经典和最新模型<ul><li>LeNet, ResNet, LSTM, BERT, …</li></ul></li><li>机器学习<ul><li>损失函数、目标函数、 过拟合、 优化</li></ul></li><li>实践<ul><li>使用Pytorch实现介绍的知识点</li><li>在真实数据上体验算法效果</li></ul></li></ul><h3 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h3><ul><li>深度学习基础——线性神经网络，多层感知机</li><li>卷积神经网络——LeNet, AlexNet, VGG, Inception, ResNet</li><li>循环神经网络——RNN, GRU, LSTM, seq2seq</li><li>注意力机制——Attention, Transformer</li><li>优化算法——SGD, Momentum, Adam</li><li>高性能计算——并行，多GPU，分布式</li><li>计算机视觉——目标检测，语义分割</li><li>自然语言处理——词嵌入，BERT</li></ul><h3 id="你将学到什么"><a href="#你将学到什么" class="headerlink" title="你将学到什么"></a>你将学到什么</h3><ul><li>What<ul><li>深度学习里有那些技术</li></ul></li><li>How<ul><li>如何实现和调参</li></ul></li><li>Why<ul><li>背后的原因（直觉、数学）</li></ul></li></ul><h3 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h3><ul><li><a href="https://courses.d2l.ai/zh-v2/">课程主页</a></li><li><a href="https://zh.d2l.ai/">教材</a></li><li><a href="https://discuss.d2l.ai/c/16">课程论坛讨论</a></li><li><a href="https://discuss.pytorch.org/">Pytorch论坛</a></li></ul><h1 id="02深度学习介绍"><a href="#02深度学习介绍" class="headerlink" title="02深度学习介绍"></a>02深度学习介绍</h1><h1 id="03安装"><a href="#03安装" class="headerlink" title="03安装"></a>03安装</h1><p>pip install d2l -i <a href="http://pypi.douban.com/simple">http://pypi.douban.com/simple</a> –trusted-host pypi.douban.com –user</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 动手学深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文翻译】Channel State Information-Based Ranging for Underwater Acoustic Sensor Networks</title>
      <link href="/2024/01/24/lun-wen-fan-yi-channel-state-information-based-ranging-for-underwater-acoustic-sensor-networks/"/>
      <url>/2024/01/24/lun-wen-fan-yi-channel-state-information-based-ranging-for-underwater-acoustic-sensor-networks/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><code>基于信道状态信息的水声传感器网络测距</code></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Received signal strength (RSS)-based ranging is a promising distance estimation approach in underwater acoustic sensor networks (UASNs). However, the multipath-rich underwater environment complicates acoustic propagations and derails the RSS-based ranging. To address the challenges, this article provides a novel ranging method, called channel state information (CSI)-based ranging for UASNs (CRUN). Instead of RSS, the measured CSI is modeled as a set of power-loss-based equations. Then, the ranging process under multipath scenarios is transformed as a multivariate optimization problem which involves parameters of all propagation paths. This optimization problem aims to simultaneously realize distance estimation and multipath mitigation. Noticing the large number of variables makes the solution numerically unstable, a threshold-windowbased algorithm is proposed to simplify the multivariate optimization problem. In specific, the proposed algorithm extracts relative amplitude attenuations and relative time delays between the line-of-sight (LOS) path and each of the non-line-of-sight paths from CSI. The extracted parameters, being as equality constraints, simplify the multivariate optimization problem to a univariate optimization problem only concerning the desired LOS distance. Then, the simplified problem can be efficiently solved by the gradient descent algorithm. Statistical-channel-model-based simulations and lake experiments demonstrate that CRUN significantly improves the ranging accuracy and robustness compared with RSS-based approaches.<br><code>在水声传感器网络 （UASNs）中，基于接收信号强度（RSS）的测距是一种很有前途的距离估计方法。然而，多径丰富的水下环境复杂的声传播和脱轨的RSS测距。为了应对这些挑战，本文提出了一种新的测距方法，称为</code><em><strong>基于信道状态信息（CSI）的水声传感器网络（UASN）测距（CRUN）</strong></em><code>。代替RSS，测量的CSI被建模为一组基于功率损耗的方程。然后，多径场景下的测距过程转化为一个多变量的优化问题，其中涉及的所有传播路径的参数。该优化问题旨在同时实现距离估计和多径抑制。由于变量数目多会导致解的数值不稳定，提出了一种基于阈值窗口的算法来简化多变量优化问题。具体地说，该算法从CSI中提取视线（LOS）路径和每个非视线路径之间的相对幅度衰减和相对时间延迟。所提取的参数作为等式约束，将多变量优化问题简化为仅涉及期望视距的单变量优化问题。然后，简化的问题可以有效地解决梯度下降算法。基于统计信道模型的仿真和湖上实验表明，与基于RSS的方法相比，CRUN显著提高了测距精度和鲁棒性。</code><br>Index Terms—CSI, ranging, RSS, UASN.<br><code>索引词——CSI，测距，RSS，UASN。</code></p><blockquote><p>摘要部分介绍了在水下声学传感器网络（UASNs）中，基于通道状态信息（CSI）的测距方法。目前，接收信号强度（RSS）是在UASNs中常用的测距方法，但在多径环境下存在一些挑战。</p><p>为了解决这些挑战，本文提出了一种名为<em><strong>基于信道状态信息的水声传感器网络测距</strong></em>(CRUN)的新型测距方法，使用CSI作为距离估计的指示器。该方法将测得的CSI建模为一组基于功率损耗的方程，并将多路径场景下的测距过程转化为涉及所有传播路径参数的多元优化问题。文中提出了一种基于阈值窗口算法的方法来简化多元优化问题。该算法从CSI中提取相对幅度衰减和相对时间延迟参数，将其作为等式约束，将多元优化问题简化为仅涉及所需LOS距离的单变量优化问题。该简化后的问题可以通过梯度下降算法高效求解。</p><p>通过统计-通道模型为基础的仿真和湖泊实验，证明了CRUN相对于基于RSS的方法在测距精度和鲁棒性方面的显著改进。最后，文中提出了关键词，并对未来研究方向进行了讨论。</p></blockquote><h1 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h1><p>UNDERWATER wireless sensor networks (UWSNs) have attracted massive attention from academic and industrial researchers in exploring oceans and underwater environments in the latest decade [1]. Communication technologies in UWSNs can be divided into three major categories: optical communication, magnetic induction communication, and acoustic communication [2]. The optical communication provides the highest data rate and the lowest link delay among these three technologies. Nevertheless, the optical signal suffers from severe absorption and scattering introduced by underwater channels, resulting in serious inter symbol interference (ISI) and short transmission distance [3]. The magnetic induction communication is a promising alternative because of its inherent advantages in predictable channel responses, negligible propagation delay, and competitive energy consumption. However, its application is limited to a short range by the near acoustic communication [2]. The optical communication provides the highest data rate and the lowest link delay among these three technologies. Nevertheless, the optical signal suffers from severe absorption and scattering introduced by underwater channels, resulting in serious inter symbol interference (ISI) and short transmission distance [3]. The magnetic induction communication is a promising alternative because of its inherent advantages in predictable channel responses, negligible propagation delay, and competitive energy consumption. However, its application is limited to a short range by the near-field property due to the non-propagation of the magnetic wave [2]. The acoustic communication has been attractive for its less attenuation, which makes it capable of providing a long link range up to tens of kilometers, although it also has intrinsic limitations of high transmission delay and low data rate [4]. For the reasons mentioned above, acoustic technology is considered as a preferable choice for underwater wireless communication beyond tens of meters. Based on the acoustic communication, underwater acoustic sensor networks (UASNs) make applications such as environmental monitoring [5], resource exploration [6], and disaster preparation [7] both practical and effective. The performance of the applications mentioned above depends greatly on the accuracy of localization, which is one of the fundamental tasks of UASNs [8]. In general, underwater localization schemes map physical measurements derived from acoustic signals into the relative distance to the reference points, which is referred to as the ranging process [9]. This article focuses on the ranging method for UASNs.<br><code>水下无线传感器网络（Underwater wireless sensor networks，UWSNs）在近十年来引起了学术界和工业界研究人员在探索海洋和水下环境方面的大量关注[1]。UWSNs中的通信技术可以分为三大类：光通信，磁感应通信和声学通信[2]。在这三种技术中，光通信提供了最高的数据速率和最低的链路延迟。然而，光信号会受到水下信道的严重吸收和散射，导致严重的符号间干扰（ISI）和短的传输距离[3]。磁感应通信是一种很有前途的替代方案，因为它在可预测的信道响应，可忽略的传播延迟，和竞争力的能量消耗的固有优势。然而，它的应用被近距离声通信限制在短距离内[2]。在这三种技术中，光通信提供了最高的数据速率和最低的链路延迟。然而，光信号会受到水下信道的严重吸收和散射，导致严重的符号间干扰（ISI）和短的传输距离[3]。磁感应通信是一种很有前途的替代方案，因为它在可预测的信道响应，可忽略的传播延迟，和竞争力的能量消耗的固有优势。然而，由于磁波的非传播，其应用被近场特性限制在短范围内[2]。声学通信因其较少的衰减而具有吸引力，这使得它能够提供长达数十公里的长链路范围，尽管它也具有高传输延迟和低数据速率的固有限制[4]。基于上述原因，水声技术被认为是水下无线通信的首选。水声传感器网络（Underwater Acoustic Sensor Networks，UASNs）以水声通信为基础，使环境监测[5]、资源勘探[6]、防灾减灾[7]等应用变得实用而有效。上述应用的性能在很大程度上取决于定位的准确性，这是UASN的基本任务之一[8]。通常，水下定位方案将从声信号导出的物理测量映射到到参考点的相对距离，这被称为测距过程[9]。本文重点研究了无人自定位系统的测距方法。</code><br>In UASNs, physical measurements in the ranging process can be collected by one of the following methods: time of arrival (TOA), angle of arrival (AOA), and received signal strength (RSS) [10]. As the most popular underwater ranging method [11]–[14], TOA is based on the travel time of acoustic signals from source to destination. Synchronized with the source, the destination derives travel time by comparing the received timestamp with its local time. Benefiting from low propagation speed of acoustic signals, TOA methods could achieve high time resolution for impressive ranging accuracy [15]. However, the accuracy comes at the cost of extra hardware requirements, bandwidth occupation, and energy consumption. First, in order to obtain high time resolution, greater precision oscillators and higher sample rate analog to digital converters (ADCs) are required at the physical layer, which increases the manufacture and maintenance expense of sensor nodes. Second, the sensor information transmission efficiency is reduced because of extra messages exchanging for synchronization and ranging, especially in bandwidthlimited underwater acoustic channels (UACs) [1]. For instance, a typical joint synchronization and ranging protocol [16] involves at least four times of communication between sensor nodes. Moreover, the communication overhead may be further aggravated due to the high data error rate of UACs [1]. Third, the service time of UASNs is shortened by extra energy consumption due to extra messages exchanging. Since UASN nodes are usually powered by batteries, they cannot be recharged in time due to seawater corrosion and pressure [17].<br><code>在UASN中，测距过程中的物理测量可以通过以下方法之一收集：到达时间（TOA），到达角（AOA）和接收信号强度（RSS）[10]。作为最流行的水下测距方法[11]-[14]，TOA基于声信号从源到目的地的传播时间。与源同步，目的地通过将接收到的时间戳与其本地时间进行比较来得出旅行时间。得益于声学信号的低传播速度，TOA方法可以实现高时间分辨率，从而获得令人印象深刻的测距精度[15]。然而，准确性是以额外的硬件要求、带宽占用和能量消耗为代价的。首先，为了获得高时间分辨率，在物理层需要更高精度的振荡器和更高采样率的模数转换器（ADC），这增加了传感器节点的制造和维护费用。第二，传感器信息传输效率降低，因为额外的消息交换同步和测距，特别是在带宽有限的水声信道（UAC）[1]。例如，典型的联合同步和测距协议[16]涉及传感器节点之间的至少四次通信。此外，由于UAC的高数据错误率，通信开销可能进一步加剧[1]。第三，由于额外的消息交换，UASN的服务时间缩短了额外的能量消耗。由于Udash节点通常由电池供电，由于海水腐蚀和压力，它们无法及时充电[17]。</code><br>Compared with TOA-based ranging techniques, RSS-based methods require neither specific hardware support nor extra energy consumption, making it a promising approach for ranging [18]. Existing RSS-based ranging methods [19]–[23] usually transform the measured RSS into distance value by the Urick transmission loss model [24] (more information about the Urick model is given in section II). However, the performance of RSS-based ranging degrades significantly in multipath-rich underwater environments. The degradation is classified into two aspects: 1) The Urick model is incompetent in multipath scenarios, since it depicts the power loss experienced by an acoustic signal propagating along a single path; 2) As a superposition of signals arrived through multiple paths, RSS fails to distinguish each path’s characteristics. In specific, according to the Urick model, it is expected that the measured RSS decreases monotonically with the distance increases. However, this is impractical because acoustic propagations are affected by the multipath effect in underwater environments. Multipath effect is the phenomenon that signals arrive at the receiver through different paths. Those paths are introduced by reflections at the water surface and bottom as well as refractions caused by temperature and salinity [25]. Different paths contribute to diversely attenuated and phase-shifted signals, which combine at the receiver constructively or destructively depending on signal phases. Consequently, the monotonicity defined by the Urick model between measured RSS and corresponding distance is destroyed. Based on the analysis above, efforts of two aspects can be made to improve the ranging performance: 1) an indicator should be exploited to distinguish multipaths; 2) a precise model should be built to depict the complicated acoustic propagations.<br><code>与基于TOA的测距技术相比，基于RSS的测距方法既不需要特定的硬件支持，也不需要额外的能量消耗，这使得它成为一种很有前途的测距方法[18]。现有的基于RSS的测距方法[19]-[23]通常通过Urick传输损耗模型[24]将测量的RSS转换为距离值(关于Urick模型的更多信息在第二节中给出)。然而，在多径丰富的水下环境中，基于RSS的测距性能会显著下降。这种退化分为两个方面：1)Urick模型在多径情况下不适用，因为它描述了声信号沿一条路径传播时所经历的功率损失；2)作为通过多条路径到达的信号的叠加，RSS不能区分每条路径的特征。多径效应是指信号通过不同的路径到达接收端的现象。基于以上分析，可以从两个方面努力提高测距性能：1)开发一个指示器来区分多径；2)建立精确的模型来描述复杂的声传播。</code><br>In UASNs, orthogonal frequency division multiplexing (OFDM) is widely adopted to fully utilize the limited bandwidth in UACs [26]–[30]. As a fine-grained physical layer indicator, CSI is extensively applied for channel estimation and equalization to eliminate frequency selective fading caused by multipath effect in OFDM communication systems [31]. Compared with RSS, CSI depicts the amplitude-frequency response on the subcarrier level in the frequency domain and separates multipath components in the time domain [32]. Motivated by these characteristics, we propose a novel ranging method named CSI-based ranging for UASNs (CRUN). In summary, the main contributions of this article are as follows:<br><code>在UASN中，广泛采用正交频分复用（OFDM）以充分利用UAC中的有限带宽[26]-[30]。作为细粒度的物理层指示符，CSI被广泛应用于信道估计和均衡，以消除OFDM通信系统中由多径效应引起的频率选择性衰落[31]。与RSS相比，CSI在频域中描述了子载波级别上的幅频响应，并在时域中分离了多径分量[32]。受这些特点的启发，我们提出了一种新的测距方法命名为CSI为基础的测距UASN（CRUN）。概括起来，本文的主要贡献如下：</code><br>1)CSI is employed as the power indicator instead of RSS. By modeling CSI measurements of multiple subcarriers as a set of power-loss-based equations, the ranging process under multipath scenarios is defined as a multivariate optimization problem involving parameters of not only line-of-sight (LOS) path but also non-line-ofsight (NLOS) paths. This optimization problem aims to simultaneously accomplish the distance estimation and multipath mitigation tasks.<br><code>1)CSI被用作功率指示符而不是RSS。通过将多个子载波的CSI测量建模为一组基于功率损耗的方程，将多径场景下的测距过程定义为不仅涉及视线（LOS）路径而且涉及非视线（NLOS）路径的参数的多变量优化问题。该优化问题旨在同时完成距离估计和多径抑制任务。</code><br>2)A threshold-window-based algorithm is proposed to simplify the multivariate optimization problem. In specific, the proposed algorithm extracts relative amplitude attenuations and relative time delays between the LOS path and each of the NLOS paths from CSI. According to the extracted relative amplitude attenuations and time delays, NLOS parameters in the multivariate optimization can be represented by the LOS parameter, which simplifies the multivariate optimization problem to a univariate optimization problem only with respect to the desired LOS distance. Then the simplified problem can be efficiently solved by the gradient descent algorithm.<br><code>2)提出了一种基于阈值窗口的多变量优化算法。具体地说，该算法从CSI中提取LOS路径和每个NLOS路径之间的相对幅度衰减和相对时间延迟。根据提取的相对幅度衰减和时延，多变量优化中的NLOS参数可以用LOS参数表示，从而将多变量优化问题简化为仅关于期望LOS距离的单变量优化问题。然后利用梯度下降算法对简化后的问题进行求解。</code><br>To evaluate the performance of the proposed CRUN method, comprehensive simulations are performed based on a statistical channel model. Furthermore, lake experiments for CRUN on standard OFDM transceivers are conducted. According to both simulation and experiment results, the proposed CRUN method presents a great superiority in both accuracy and robustness than the traditional RSS-based approaches.<br><code>为了评估所提出的CRUN方法的性能，基于统计信道模型进行全面的仿真。此外，湖上的标准OFDM收发器的CRUN进行了实验。仿真和实验结果表明，该方法在准确性和鲁棒性方面均优于传统的基于RSS的方法。</code><br>The rest of this article is organized as follows. Section II introduces the basic information about OFDM, CSI, and the Urick model. Then the ranging process is defined as an optimization problem in Section III. Section IV elaborates methodologies for the optimization problem, including the threshold-window-based algorithm. The simulation of CRUN and experimental evaluations are presented in Section V and Section VI respectively. The related work is briefly presented in Section VII, followed by discussion in Section VIII. Finally, conclusions are presented in Section IX.<br><code>本文的其余部分组织如下。第二节介绍了OFDM、CSI和Urick模型的基本信息。然后在第三节中将测距过程定义为优化问题。第四节阐述了优化问题的方法，包括基于阈值窗口的算法。第五节和第六节分别介绍了CRUN的模拟和实验评估。第七节简要介绍了相关工作，第八节进行了讨论。最后，结论见第九节。</code></p><blockquote><p>介绍部分主要讨论了水下声学传感器网络（UASNs）中测距的重要性以及现有测距方法的局限性。其中，提到了使用接收信号强度（RSS）作为距离估计方法的优点，但也指出了在多路径丰富的水 6下环境中存在的挑战。为了克服这些挑战，本文提出了一种新的测距方法：基于通道状态信息（CSI）的测距方法，即CRUN。该方法通过将测得的CSI建模为一组基于功率损耗的方程，将多路径情况下的测距问题转化为多元优化问题，并采用阈值窗口算法来简化问题。介绍部分还提到了对CSI估计的必要性以及如何提取相对幅度衰减和相对时间延迟等参数来简化优化问题。最后，介绍部分总结了CRUN方法在测距精度和鲁棒性方面相对于RSS方法的优势，并提出了对未来研究方向的展望。</p></blockquote><h1 id="II-BASIC-INFORMATION"><a href="#II-BASIC-INFORMATION" class="headerlink" title="II. BASIC INFORMATION"></a>II. BASIC INFORMATION</h1><p><code>二.基本信息</code><br><em><strong>A. OFDM Underwater Acoustic Communication System</strong></em><br><code>A.OFDM水声通信系统</code><br>In high speed underwater acoustic communications, as a multicarrier modulation technique, OFDM is one of the mainstream transmission schemes. In OFDM systems, to fully utilize bandwidth and to realize frequency diversity reception, high speed serialized data stream is converted into low-speed parallel data, then modulated into several quadrature mixed and partially overlapped subcarriers for transmission. A scheme of typical OFDM underwater acoustic communication system [33] is shown in Fig. 1. An OFDM modulator can be implemented as an N-point inverse fast fourier transform (IFFT) on a block of N information symbols chosen from an appropriate signal constellation such as quadrature amplitude modulation (QAM) or phase shift keying (PSK), followed by digital to analog converter (DAC) on the IFFT samples. Multipath effect results in a doubly dispersive channel that exhibits dispersion in both time and frequency domains, which causes ISI and frequency selective fading respectively [34]. To mitigate ISI, cyclic prefix (CP) is introduced into the beginning of each block of N-IFFT coefficients. At the receiver, the received signal is sampled by an ADC. With discarding CP, the linear convolution of the transmitted sequence of IFFT coefficients with the discretetime channel is converted into a circular convolution, hence ISI is completely removed. However, CP fails to eliminate interference within a symbol caused by frequency selective fading, therefore a channel equalizer is supplemented. After that, the FFT operation performs baseband demodulation.<br><code>在高速水声通信中，作为一种多载波调制技术，OFDM是主流的传输方案之一。在OFDM系统中，为了充分利用带宽，实现频率分集接收，将高速串行化数据流转换成低速并行数据，然后调制成多个正交混合和部分重叠的子载波进行传输。图1示出了典型的ofdm水声通信系统的一种方案[33]。ofdm调制器可以被实现为对从适当的信号星座中选择的N个信息符号块进行N点快速逆傅立叶变换(IFFT)，然后对IFFT样本进行数模转换器(DAC)。多径效应导致双色散信道在时间域和频域都表现出色散，这分别导致码间干扰和频率选择性衰落[34]。为了减少码间干扰，循环前缀(CP)被引入到每个N-IFFT系数块的开始。在接收端，ADC对接收到的信号进行采样。通过丢弃CP，将发送的IFFT系数序列与离散时间信道的线性卷积转换为循环卷积，从而完全消除了码间干扰。然而，CP不能消除由频率选择性衰落引起的码元内的干扰，因此增加了信道均衡器。之后，FFT运算执行基带解调。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/1-1.png"></p><blockquote><p>Fig. 1. Block diagram of OFDM underwater acoustic communication system.<br><code>图1.ofdm水声通信系统框图。</code></p></blockquote><p><em><strong>B. CSI</strong></em><br><code>B. 信道状态信息</code><br>In underwater acoustic systems, CSI measurements are applied for channel equalization to improve the reliability of communication. Frequency diversity in OFDM makes CSI measurements on the subcarrier level feasible. CSI describes the attributes of the UACs, which results from a combination of scattering, refraction, reflection, and power attenuation during transmission [35]. In specific, CSI is depicted by channel frequency response (CFR) in the frequency domain and channel impulse response (CIR) in the time domain.<br><code>在水声系统中，为了提高通信的可靠性，需要对信道进行CSI测量。频率分集技术使得在子载波级进行CSI测量成为可能。CSI描述了UAC的属性，这是由于传输过程中的散射、折射、反射和功率衰减的组合造成的[35]。具体地，CSI由频域中的信道频率响应(CFR)和时域中的信道冲激响应(CIR)来描述。</code><br>In the frequency domain, the constructive and destructive phases cause frequency selective fading, which is characterized as CFR, can be modeled as<br><code>在频域中，相长相位和相消相位引起频率选择性衰落，其特征在于CFR，可以建模为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/UASN/1-2.png"><br>where X (f) and Y (f) are the transmitted and the received signal respectively. H (f) and N (f) are the CFR and the additive white Gaussian noise respectively.<br><code>其中X（f）和Y（f）分别是发送信号和接收信号。H（f）和N（f）分别是CFR和加性白色高斯噪声。</code><br>Thus, CFR can be estimated according to (1) as</p><blockquote><p>‘Basic Information’部分主要介绍了文章的基本背景信息和相关的技术背景。文章提出了在水下声学传感器网络（UASNs）中进行测距的问题，并指出接收信号强度（RSS）是一种常见的测距方法。然而，由于水下环境的多径传播问题，RSS测距方法存在一定的限制。为了解决这些限制，文章引入了基于通道状态信息（CSI）的测距方法。该方法利用CSI作为测距的指示器，通过将测得的CSI建模为一组基于功率损耗的方程，将测距问题转化为多元优化问题。文章还介绍了采用阈值窗口算法来简化优化问题的过程。此外，文章还讨论了CSI测距方法在水下声学传感器网络中的应用和挑战，并介绍了一些相关的术语和技术背景信息。此部分的目的是为读者提供所需的基础信息，以理解接下来的研究内容。</p></blockquote><h1 id="III-PROBLEM-DEFINITION"><a href="#III-PROBLEM-DEFINITION" class="headerlink" title="III. PROBLEM DEFINITION"></a>III. PROBLEM DEFINITION</h1><blockquote><p>‘Problem Definition’部分阐述了本文研究的问题定义。文章指出，虽然接收信号强度（RSS）测距方法在水下声学传感器网络（UASNs）中有潜力，但由于水下环境的多路径传播，导致RSS测距方法的准确性受到影响。因此，需要开发一种更准确的测距方法来应对这些挑战。<br>在多路径环境中，测距问题变得复杂，因为存在许多未知变量。其中的关键问题包括：路径数量未知、非直射路径（NLOS）存在多余的未知变量以及最优解取决于常数C。为了解决这些问题，该文提出了一种基于通道脉冲响应（CIR）的测距方法，并利用CIR的相对参数来简化多元优化问题。<br>该部分还讨论了如何计算CIR的幅度，以及如何提取相对幅度和时间延迟参数。通过使用CIR估计和提取这些参数，将多元优化问题简化为只涉及所需LOS距离的单变量优化问题。<br>总的来说，’Problem Definition’部分明确了该文研究的问题，即在水下声学传感器网络中，如何应对多路径传播对测距准确性的影响，并介绍了使用基于CIR的测距方法来解决这一问题的思路。</p></blockquote><h1 id="GPT解读"><a href="#GPT解读" class="headerlink" title="GPT解读"></a>GPT解读</h1><ol><li>介绍了基于信道状态信息(channel state information, CSI)的水声传感器网络测距的概念。本节解释了基于接收信号强度(received signal strength, RSS)的测距在多径丰富的水下环境中的局限性，并建议使用CSI作为电源指示器。本文描述了使用CSI测量的测距过程，并提出了涉及视距(LOS)和非视距(NLOS)路径参数的多元优化问题。为了简化优化问题，提出了一种基于阈值窗口的算法来提取LOS路径到NLOS路径的相对幅度衰减和时间延迟。这将问题简化为所需LOS距离的单变量优化问题。文章还讨论了基于csi的测距相对于传统的基于rss的测距的优势，包括提高了精度和鲁棒性。本文的其余部分将提供所提出的方法、模拟和实验评估的进一步细节。</li><li>在本节中，作者讨论了解决水声传感器网络(UASNs)优化问题背后的挑战和动机。他们确定了三个主要挑战:未知的路径数量，大量的未知变量，以及最优解的非唯一性。为了简化该问题，作者提出使用信道冲激响应(channel impulse response, CIR)来提取视距(line-of-sight, LOS)路径和每个非视距(non-line-of-sight, NLOS)路径之间的相对幅度衰减和相对时间延迟。他们定义了这些相关参数，并解释了如何从CIR测量中获得这些参数。此外，作者还讨论了未知常数C的确定及其与发射和接收增益的关系。基于这些初步结论，作者提出多变量优化问题可以简化。他们描述了估计CIR和提取相关参数的方法。最后讨论了CIR估计及其与优化问题的关系。</li><li>在本节中，作者讨论了CIR估计以及提取信道中每条路径的幅度和时间延迟所涉及的挑战。他们解释说，成形滤波器和带通滤波器的使用会引入噪声并使CIR估计失真。他们提出了一种基于阈值窗口的算法从CIR估计中提取多径参数。他们还讨论了确定优化问题中未知常数C的两种方法，消声坦克训练和在线训练。最后，将优化问题简化为关于距离参数d1的单变量优化问题，并讨论了求解单变量优化问题的一些一般信息。</li><li>本节讨论使用梯度下降算法最小化函数(24)的过程。它提到，基于观察，唯一的局部极值是全局最小值的假设通常是有效的。分析了CRUN算法的克拉美劳下界(Cramer-Rao lower bound, CRLB)，并通过仿真实验评估了CRUN算法的性能。研究了距离、噪声方差、子载波数、信噪比等因素对CRUN性能的影响。实验结果表明，CRUN比基于rss的方法具有更高的准确性和鲁棒性，并且即使在传播路径数和信噪比变化的情况下，其性能也保持稳定。本节还简要介绍了CRUN在定位中的评估，并将其与四种基于rss的定位算法进行了比较。</li><li>本节概述实验设置和研究中使用的硬件。它提到了实验的地点和所使用的设备的规格。本节还讨论了实验结果，比较了CRUN和基于rss的方法的性能。文中指出，CRUN在准确性和鲁棒性方面始终优于基于rss的方法。本节进一步讨论了室内定位研究与水下定位的相关性，因为这两种环境在拒绝gps和多路径丰富方面有相似之处。介绍了建立测量CSI与距离之间关系的各种现有方法，如基于指纹和基于模型的方法。强调了这些方法的优点和局限性。本节介绍了LiFS，一项利用CSIs在“干净”子载波上进行精确定位的相关工作。它比较了LiFS和CRUN，表明CRUN利用了所有子载波，而LiFS排除了一些子载波。还提到了两种方法用于解决优化问题的不同方法。讨论了在水下环境中实现CRUN的一些挑战和约束，如水声信道的非线性和时变特性。本节提到了研究中为应对这些挑战所采取的措施。文中还指出了CRUN中结合幅度和相位信息的潜力是未来的研究方向。最后，与传统的基于rss的方法相比，CRUN在提高测距精度和鲁棒性方面具有优势。指出细粒度物理层指示CSI和频率分集技术的能力是影响CRUN性能的因素。本节最后提到了未来研究的重点，即进一步改进CRUN。</li><li>本节讨论sinc函数第一个旁瓣振幅的计算。它解释了通过确定sinc函数的一阶导数，我们可以找到函数的极值，它表示第一个副瓣的振幅。该剖面给出了一阶导数的方程，并对其进行数值求解以获得极值。然后参照表II介绍测量向量QCSI，它代表不同频率下信道响应的观测值。描述了QCSI的方程，并提到了测量噪声，假设测量噪声是独立同分布的高斯随机变量。推导了QCSI的概率密度函数(probability density function, PDF)，并计算了其对数。为了简化导数计算，截面做了三个假设:1)只考虑视距路径，2)假设球面扩展，3)假设所有子载波的吸收系数是相同的。PDF的对数的一阶导数就是基于这些假设推导出来的。然后介绍费舍尔信息(Fisher information)，它量化了测量提供的关于被估计参数的信息量。通过计算Fisher信息，推导出估计距离参数时可达到的最小方差的克拉美罗下界(CRLB)。最后给出了包含测量噪声方差、距离参数、子载波中心频率和子载波个数的CRLB计算公式。</li></ol></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 水声传感器网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文翻译】Retentive Network A Successor to Transformer for Large Language Models</title>
      <link href="/2023/12/12/lun-wen-fan-yi-retentive-network-a-successor-to-transformer-for-large-language-models/"/>
      <url>/2023/12/12/lun-wen-fan-yi-retentive-network-a-successor-to-transformer-for-large-language-models/</url>
      
        <content type="html"><![CDATA[<html><head></head><body></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文翻译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文翻译】A Novel Noise-Aware Deep Learning Model for Underwater Acoustic Denoising</title>
      <link href="/2023/12/12/lun-wen-fan-yi-a-novel-noise-aware-deep-learning-model-for-underwater-acoustic-denoising/"/>
      <url>/2023/12/12/lun-wen-fan-yi-a-novel-noise-aware-deep-learning-model-for-underwater-acoustic-denoising/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><code>一种新的噪声感知深度学习模型用于水声降噪</code></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Underwater acoustic signal denoising technology aims to overcome the challenge of recovering valuable ship target signals from noisy audios by suppressing underwater background noise. Traditional statistical-based denoising techniques are difficult to be applied effectively in complex underwater environments, especially in the case of extremely low signal-to-noise ratios (SNRs). To address these problems, we propose a noise-aware deep learning model with fullband–subband attention network (NAFSA-Net) for underwater acoustic signal denoising. NAFSA-Net adopts an encoder to extract the feature representation of the input audio. Subsequently, the noise subnet and the target subnet are designed to estimate the noise component and the target component simultaneously. Specifically, some stacked fullband–subband attention (FSA) blocks are deployed in each subnet to capture both global dependencies and fine-grained local dependencies of features. Furthermore, we introduce an interaction module to transmit auxiliary information from the noise subnet to the target subnet. Finally, we propose an improved weight scale-invariant signal-to-noise ratio (SI-SNR) loss function to optimize the training of our model. Experimental results show that our proposed NAFSA-Net substantially outperforms traditional methods and competitive DNN-based solutions in denoising underwater noisy signals with very low SNRs. More importantly, our proposals achieve equally excellent performance on both unseen datasets, which indicates that NAFSA-Net can be a more robust choice for real-world underwater acoustic denoising systems.<br><code>水声信号去噪技术旨在通过抑制水下背景噪声，从噪声中恢复有价值的舰船目标信号。传统的基于小波变换的去噪技术难以有效地应用于复杂的水下环境，特别是在信噪比极低的情况下。为了解决这些问题，我们提出了一种具有全频带子带注意力网络（NAFSA-Net）的噪声感知深度学习模型，用于水声信号去噪。NAFSA网络采用编码器提取输入音频的特征表示。随后，设计噪声子网和目标子网，以同时估计噪声分量和目标分量。具体而言，在每个子网中部署一些堆叠的全频带子带注意（FSA）块，以捕获特征的全局依赖性和细粒度局部依赖性。此外，我们还引入了一个交互模块，将辅助信息从噪声子网传输到目标子网。最后，我们提出了一种改进的权重尺度不变信噪比（SI-SNR）损失函数来优化我们的模型的训练。实验结果表明，我们提出的NAFSA-Net在对具有非常低SNR的水下噪声信号进行去噪方面大大优于传统方法和基于竞争性DNN的解决方案。更重要的是，我们的建议在两个看不见的数据集上都实现了同样出色的性能，这表明NAFSA-Net可以成为现实世界水声去噪系统的更强大的选择。</code><br>Index Terms<br><code>索引词</code><br>Fullband–subband attention (FSA) network, noise-aware, noise-aware deep learning model with fullband–subband attention network (NAFSA-Net), underwater acoustic denoising.<br><code>全频带-子带注意力（FSA）网络，噪声感知，具有全频带-子带注意力网络的噪声感知深度学习模型（NAFSA-Net），水下声学去噪。</code></p><h1 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h1><p>ACCURATELY monitoring and discovering quiet surface or underwater targets have always been an exceedingly challenging task in a complex and always changing marine environment. After long-distance transmission, the acoustic signal radiated by the target will inevitably be corrupted by marine environmental noise, such as wind, rain, flow, and sounds from marine life [1]. Therefore, the underwater acoustic signals collected by hydrophones contain a large amount of worthless noise information, which brings serious challenges to the follow-up processing of underwater acoustic tasks, such as vessel recognition, positioning, and tracking [2], [3], [4], [5], [6]. Hence, it is an inherent challenge to exploit a reasonable denoising method to suppress ambient noise while retaining as much valuable information about the target audio as possible, which is the focus of this article.<br><code>在复杂多变的海洋环境中，精确监测和发现水面或水下安静目标一直是一项极具挑战性的任务。目标辐射的声信号经过长距离传输后，不可避免地会受到风、雨、流、海洋生物声音等海洋环境噪声的干扰[1]。因此，水听器采集到的水声信号中含有大量无用的噪声信息，这给水声任务的后续处理，如船舶识别、定位、跟踪等带来了严峻的挑战[2]，[3]，[4]，[5]，[6]。因此，利用合理的去噪方法来抑制环境噪声，同时保留尽可能多的关于目标音频的有价值的信息是一个固有的挑战，这是本文的重点。</code><br>Generally, the sounds radiated by underwater targets are mainly composed of narrowband components (line spectrum) and wideband components (continuous spectrum) [7]. The generation of the line spectrum is mainly caused by the motion of mechanical parts and propellers, as well as the resonance of the blades. Compared with the latter, the narrowband components have higher intensity and frequency stability, which contain more important target feature information. Fig. 1(a) and (b) presents the amplitude spectra and the power spectral density (PSD) of a surface vessel. Note that the red boxes in these figures mark the main frequency and harmonic components of the ship. It can be observed that the ship audio shows five dominant line spectral features. These line spectral components are usually distributed in the lowfrequency band, with little attenuation during the propagation process. Therefore, the narrowband components of underwater target signals are usually used as an important basis for target detection and feature analysis.<br><code>一般情况下，水下目标辐射的声音主要由窄带分量（线谱）和宽带分量（连续谱）组成[7]。线谱的产生主要是由机械部件和螺旋桨的运动以及叶片的共振引起的。与后者相比，窄带分量具有更高的强度和频率稳定度，包含了更重要的目标特征信息。图1（a）和（B）显示了水面船舶的振幅谱和功率谱密度（PSD）。请注意，这些图中的红框标记了船舶的主要频率和谐波分量。可以观察到，船舶音频显示出五个主导线谱特征。这些线谱分量通常分布在低频带，在传播过程中衰减很小。因此，水下目标信号的窄带分量通常被用作目标检测和特征分析的重要依据。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/1.png"></p><blockquote><p>Fig. 1. T-F domain spectrograms and PSD of clean audio, noise audio, and noisy audio (−10 dB). (a) Magnitude spectra of the clean signal. (b) PSD of the clean signal. (c) Magnitude spectra of the noise signal. (d) PSD of the noise signal. (e) Magnitude spectra of the noisy signal. (f) PSD of the noisy signal.<br><code>Fig. 1.干净音频、噪声音频和噪声音频（−10 dB）的T-F域频谱图和PSD。(a)干净信号的幅度谱。(b)干净信号的PSD。(c)噪声信号的幅度谱。(d)噪声信号的PSD。(e)噪声信号的幅度谱。(f)噪声信号的PSD。</code></p></blockquote><p>In terms of underwater acoustic signal denoising, traditional statistical prediction approaches, such as spectral subtraction [8], Wiener filtering [9], wavelet decomposition [10], empirical mode decomposition (EMD) [11], and adaptive line enhancer (ALE) [12], have been suggested, and significant progress has been made on this issue. These solutions mainly rely on the accurate analysis and modeling of the signal, as well as optimal fine-tuning of parameters. Spectral subtraction and Wiener filtering approaches are initially introduced. These methods first analyze and model the noise components in the noisy signal. Although they are simple in design and generally of low complexity, these techniques introduce additional audible spectral components after denoising, resulting in subsequent distortion of the raw audio. In addition, signal decomposition-based methods, such as wavelet decomposition and EMD, aim to decompose the noisy signal first and remove the noise components to improve the signalto-noise ratios (SNRs). The former needs to address the constraints such as optimal wavelet basis selection, threshold rule setting, and decomposition level setting, while the latter undergoes the problem of mode mixing in the EMD process. Besides, the ALE method utilizes the correlation difference between the narrowband components and the wideband components in the noisy signal to extract the line spectral features of the target signal, which has been successfully applied to passive sonar signal denoising. Unfortunately, these approaches perform poorly in quiet and remote marine scenarios. In the low SNR (−10 dB) signal, as shown in Fig. 1(e) and (f), the target signal has been completely covered by the background noise, as shown in Fig. 1(c) and (d). As the SNR continues to decrease, the performance of traditional techniques deteriorates dramatically.<br><code>在水声信号去噪方面，人们提出了传统的统计预测方法，如谱减法[8]、维纳滤波[9]、小波分解[10]、经验模式分解(EMD)[11]和自适应谱线增强器(ALE)[12]，并取得了显著的进展。这些解决方案主要依赖于对信号的准确分析和建模，以及参数的最佳微调。首先介绍了谱减法和维纳滤波法。这些方法首先对噪声信号中的噪声分量进行分析和建模。虽然这些技术设计简单，通常复杂度较低，但它们在去噪后引入了额外的可听频谱分量，导致随后的原始音频失真。此外，基于信号分解的方法，如小波分解和经验模式分解，其目的是首先对含噪信号进行分解，然后去除噪声成分，以提高信噪比。前者需要处理最优小波基选择、阈值规则设置、分解层设置等约束条件，而后者则需要解决EMD过程中的模式混合问题。此外，ALE方法利用噪声信号中窄带分量和宽带分量之间的相关性差异来提取目标信号的线谱特征，并已成功地应用于被动声纳信号去噪。不幸的是，这些方法在安静和偏远的海洋场景中表现不佳。在低信噪比(−10分贝)信号中，如图1(E)和(F)所示，目标信号已完全被背景噪声覆盖，如图1(C)和(D)所示。随着信噪比的不断降低，传统方法的性能急剧恶化。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/1.png"></p><blockquote><p>Fig. 1. T-F domain spectrograms and PSD of clean audio, noise audio, and noisy audio (−10 dB). (a) Magnitude spectra of the clean signal. (b) PSD of the clean signal. (c) Magnitude spectra of the noise signal. (d) PSD of the noise signal. (e) Magnitude spectra of the noisy signal. (f) PSD of the noisy signal.<br><code>Fig. 1.干净音频、噪声音频和噪声音频（−10 dB）的T-F域频谱图和PSD。(a)干净信号的幅度谱。(b)干净信号的PSD。(c)噪声信号的幅度谱。(d)噪声信号的PSD。(e)噪声信号的幅度谱。(f)噪声信号的PSD。</code></p></blockquote><p>Recently, data-driven methods based on deep neural networks (DNNs) have been widely used in the field of acoustic signals and have been proven to be effective and outperform conventional solutions in solving audio denoising problems. The mainstream DNN-based methods can be classified into two categories: the time–frequency (T-F) domain denoising methods [13], [14], [15], [16] and the time-domain denoising methods [17], [18], [19], [20]. The T-F domain methods operate signals on the T-F spectrogram. These schemes first convert the audio waveform to T-F spectrogram by short-time Fourier transform (STFT) and then predict a weighing mask between the noisy signal and the target clean signal [21], [22], [23] or directly estimate the spectral representations of clean signals [24], [25], [26]. Unlike the former, the time-domain denoising methods directly estimate the clean audio waveform from the raw time-domain signal in an end-to-end way. Most of the aforementioned methods have a salient feature of utilizing the clean signal as the training target. However, in the low SNR scenarios, the weak-energy target signal components are nearly buried in the noisy signal in which noise components dominate. In this case, it becomes prohibitively difficult to directly predict the clean signal from the pure-noise-like noisy signal.<br><code>近年来，基于深度神经网络(DNN)的数据驱动方法在声信号领域得到了广泛的应用，在解决音频去噪问题上被证明是有效的并且优于传统的解决方案。目前主流的基于离散神经网络的去噪方法可以分为两类：时频(T-F)域去噪方法[13]、[14]、[15]、[16]和时间域去噪方法[17]、[18]、[19]、[20]。T-F域法对T-F谱图上的信号进行运算。这些方案首先通过短时傅里叶变换(STFT)将音频波形转换为T-F谱图，然后预测噪声信号和目标清洁信号[21]、[22]、[23]之间的加权掩码或直接估计清洁信号[24]、[25]、[26]的谱表示。与前者不同的是，时域去噪方法直接从原始的时域信号中端到端地估计出干净的音频波形。大多数前述方法的显著特征是利用CLEAN信号作为训练目标。然而，在低信噪比的情况下，微弱能量的目标信号分量几乎被淹没在噪声分量占主导地位的噪声信号中。在这种情况下，从纯噪声类噪声信号中直接预测清洁信号变得非常困难。</code><br>To address the above problem, a novel noise-aware deep learning model with fullband–subband attention network (NAFSA-Net) is proposed to perform denoising of ship radiation signals, in which the noise signal and the target signal are estimated simultaneously through different subnets. In addition, we deploy an information interaction module between the two subnets, by which the noise subnet transmits complementary information to the target subnet and guides the training of the latter in obtaining additional noise-like, weak-energy target information. Taking this as an advantage, we exploit an improved weighted scale-invariant signalto-noise ratio (SI-SNR) loss function, namely, iwSI-SNR, to guide the training of the network. In addition, the spectral characteristics of ship signals inspire the assumption that valuable feature information is concentrated in the specific narrow-band regions and that ship engine sound has a distinct spatio-temporal distribution in the T-F domain. Fullband– subband attention (FSA) blocks are developed to aggregate context dependencies, in which channel attention is utilized to model fullband correlation and subband attention is used to extract the fine-grained short-term dependencies.<br><code>为了解决上述问题，提出了一种新的噪声感知深度学习模型--全频带子带注意力网络（NAFSA-Net），用于对舰船辐射信号进行去噪，该模型通过不同的阈值同时估计噪声信号和目标信号。此外，我们部署了一个信息交互模块之间的两个子网，通过该噪声子网传输补充信息的目标子网，并指导后者的训练，以获得额外的噪声类，弱能量的目标信息。以此为优势，我们利用改进的加权尺度不变信噪比（SI-SNR）损失函数，即iwSI-SNR，来指导网络的训练。此外，船舶信号的频谱特性激发了这样的假设，即有价值的特征信息集中在特定的窄带区域，并且船舶发动机声音在T-F域中具有独特的时空分布。提出了全频带子带注意（FSA）模块来聚合上下文相关性，其中信道注意用于建模全频带相关性，子带注意用于提取细粒度的短期相关性.</code><br>The rest of this article is organized as follows. We provide a detailed analysis of related work in Section II. Section III explains the overall architecture and important components of NAFSA-Net. In Section IV, we introduce the experimental procedures. Next, we extensively evaluate the performance of the proposal and discuss the results in Section V. Finally, we conclude this article in Section VI.<br><code>本文的其余部分组织如下。我们在第二节中对相关工作进行了详细的分析。第三节解释了北美渔业安全网的总体结构和重要组成部分。在第四节中，我们介绍了实验程序。接下来，我们在第五节中广泛评估了该提案的性能并讨论了结果。最后，我们在第六节中总结了这篇文章。</code></p><h1 id="II-RELATED-WORK"><a href="#II-RELATED-WORK" class="headerlink" title="II. RELATED WORK"></a>II. RELATED WORK</h1><p><em><strong>A. Deep Neural Network-Based Audio Denoising</strong></em><br><code>A.基于深度神经网络的音频去噪</code><br>In general, DNN-based denoising schemes mainly build the mapping model from mixed audio to target audio by training and learning on a large amount of data. Benefiting from powerful nonlinear modeling capabilities, such methods have shown remarkable superiority over traditional solutions. Two typical DNN-based denoising frameworks are shown in Fig. 2.<br><code>通常，基于DNN的去噪方案主要通过对大量数据的训练和学习来建立从混合音频到目标音频的映射模型。由于其强大的非线性建模能力，这些方法已经显示出明显优于传统的解决方案。两个典型的基于DNN的去噪框架如图2所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/3.png"></p><blockquote><p>Fig. 2. Illustration of different audio denoising frameworks. (a) Block diagram of the time-domain audio denoising framework. (b) Block diagram of the T-F domain audio denoising framework.<br>图二 不同音频去噪框架的插图。(a)时域音频去噪框架的框图。(b)T-F域音频去噪框架的框图。</p></blockquote><p>Given a target signal s and a noise signal n, the noisy audio can be defined as<br><code>给定目标信号s和噪声信号n，噪声音频可以被定义为：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/2.png"><br>where m ∈ 1, . . . , M, {x, s, n} ∈ RM×1, and M represents the number of samples in the signal. Fig. 2(a) shows the standard block diagram of the time-domain denoising framework. We can formulate the DNN-based denoising in the time domain as<br><code>其中m∈1，.。。，M，{x，S，n}∈Rm×1，M表示信号中的采样数。图2(A)示出了时间域去噪框架的标准框图。我们可以将基于DNN的时间域去噪表示为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/4.png"><br>where θ represents the parameter of neural network and sˆ is the denoised target signal. In the training phase, raw waveforms are directly fed into the denoising model. Timedomain methods model noisy waveforms and recover clean waveforms via a learnable encoder–decoder network.<br><code>其中，θ表示神经网络的参数，并且s θ是去噪后的目标信号。在训练阶段，原始波形被直接馈送到去噪模型中。时域方法对噪声波形进行建模，并通过可学习的编码器-解码器网络恢复干净的波形。</code><br>Different from the former, T-F domain methods first convert the audio to the T-F domain by STFT, and the corresponding spectral representations in the T-F domain can be described as<br><code>与前者不同的是，T-F域方法首先通过STFT将音频转换到T-F域，并且T-F域中相应的频谱表示可以被描述为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/5.png"><br>Note that {X, S, N} ∈ R^(F×T) denote the complex-valued T-F representation of noisy audio, target audio, and noise audio, respectively. T denotes the total number of time frames, and F is the number of frequency bins. A block diagram of the T-F domain denoising framework is shown in Fig. 2(b). T-F domain methods take spectrogram as an input feature to the network. Such methods typically predict a weighting mask to map from noisy T-F representation to clean T-F representation or directly estimate the target spectrum. Finally, inverse STFT (iSTFT) is used to reconstruct the denoised waveform.<br><code>请注意，{X，S，N}∈R^(F×T) 分别表示噪声音频、目标音频和噪声音频的复值T-F表示。T表示时间帧的总数，而F表示频率段的数目。图2(B)显示了T-F域去噪框架的框图。T-F域法将谱图作为网络的输入特征。这种方法通常预测加权掩码以从噪声的T-F表示映射到干净的T-F表示，或者直接估计目标频谱。最后，利用逆短时傅立叶变换(ISTFT)重构去噪后的波形。</code><br>Although DNN-based audio denoising techniques have shown considerable improvements compared with traditional model-based approaches, they only model the target signal, ignoring the importance of noise knowledge. What is more, these methods are mostly applied under relatively high SNR conditions and are not adapted for low SNR underwater target data.<br><code>虽然基于DNN的音频去噪技术与传统的基于模型的方法相比已经显示出相当大的改进，但它们只对目标信号进行建模，忽略了噪声知识的重要性。而且这些方法大多适用于信噪比较高的情况，不适合低信噪比的水下目标数据。</code></p><p><em><strong>B. Noise-Aware Audio Denoising</strong></em><br><code>B.噪声感知音频去噪</code><br>Traditional denoising methods usually achieve noise reduction by first estimating noise information [27], [28], [29]. For instance, spectral subtraction and Wiener filtering methods use silent segments of audio to gain noise distribution knowledge and then enhance the target audio. However, the ship-radiated audio is different from the speech signal, the former is persistent in the noisy signal, and there are no silent segments. Therefore, it is very challenging to accurately estimate the PSD of underwater nonstationary noise, which is usually assumed to be stationary. This assumption limits the upper bound on the performance of denoising methods.<br><code>传统的去噪方法通常通过首先估计噪声信息来实现降噪[27]，[28]，[29]。例如，谱减法和维纳滤波方法使用音频的静音段来获得噪声分布知识，然后增强目标音频。然而，船舶辐射音频不同于语音信号，前者在噪声信号中是持久的，并且不存在无声段。因此，它是非常具有挑战性的准确估计的功率谱的水下非平稳噪声，这通常是假定为平稳的。这种假设限制了去噪方法性能的上限。</code><br>DNN-based methods are data-driven and avoid artificial errors introduced by inaccurate assumptions. Researchers have made many beneficial attempts on incorporating noise knowledge when estimating target audio. The most straightforward solution is to add constraints on noise information to the loss function [30], [31], [32]. Such approaches do not model noise, and the estimation of the target signal acquires little gain from prior knowledge of the noise. In addition, Odelowo and Anderson [33] propose a noise prediction scheme to handle the speech enhancement task, which directly estimates noise instead of the clean target audio. This method lacks target information and even underperforms classical target-aware methods at low SNR conditions and in unseen noise scenarios. Sun et al. [34] propose a separable deep autoencoder (DAE) to estimate noise for assisting target signal enhancement. This method trains DAE networks in two stages to model target audio and noise audio separately with the constraint that the sum of the outputs of the two DAEs is equal to the noisy audio. In this article, we introduce a noise-aware network to model target and noise audio simultaneously. The noise subnet is trained to assist target audio enhancement by transmitting embedding auxiliary knowledge to the target subnet through several interaction modules. This scheme makes it suitable for low SNR conditions.<br><code>基于DNN的方法是数据驱动的，避免了由不准确的假设引入的人为错误。研究人员在估计目标音频时结合噪声知识进行了许多有益的尝试。最直接的解决方案是将对噪声信息的约束添加到损失函数[30]，[31]，[32]。这样的方法不对噪声建模，并且目标信号的估计从噪声的先验知识获得很少的增益。此外，Odelowo和安德森[33]提出了一种噪声预测方案来处理语音增强任务，该方案直接估计噪声而不是干净的目标音频。该方法缺乏目标信息，甚至在低信噪比条件下和不可见噪声场景下的性能低于经典的目标感知方法。Sun等人。[34]提出了一种可分离的深度自动编码器（DAE）来估计噪声，以辅助目标信号增强。该方法分两个阶段训练DAE网络，分别对目标音频和噪声音频进行建模，约束条件是两个DAE的输出之和等于噪声音频。在这篇文章中，我们介绍了一种噪声感知网络，可以同时对目标和噪声音频进行建模。训练噪声子网，通过多个交互模块向目标子网传输嵌入辅助知识来辅助目标音频增强。该方案使其适用于低SNR条件。</code></p><p><em><strong>C. Attention-Based Model</strong></em><br><code>C.注意力模型</code><br>The attention mechanism is first introduced to solve sequence-to-sequence tasks, such as machine translation [35] and speech recognition [36]. It was subsequently expanded into the field of computer vision [37], [38], [39], [40] and audio signal processing [19], [41], [42] with great success. Pandey and Wang [19] propose a dense convolutional network with self-attention for speech enhancement, where the attention module is designed to capture utterance level dependencies. Besides, researchers suggest using channelwise attention along the frequency dimension of the complex-valued spectrum to capture the structural features of the signal [15], [43]. In the aforementioned approaches, the attention mechanism is typically applied together to capture global context aggregation along the time dimension or frequency dimension. Different from previous solutions, we suggest using fullband attention and subband attention to capture long- and short-term features, respectively, so as to better recover the weak line spectrum part of the ship signal. It is worth noting that each attention block captures dependencies from two dimensions, time and frequency, respectively. Such attention blocks are employed in both the target subnet and noise subnet.<br><code>注意力机制首先被引入来解决序列到序列的任务，例如机器翻译[35]和语音识别[36]。它随后扩展到计算机视觉领域[37]，[38]，[39]，[40]和音频信号处理[19]，[41]，[42]并取得了巨大成功。Pandey和Wang [19]提出了一种用于语音增强的具有自注意力的密集卷积网络，其中注意力模块旨在捕获话语级别依赖性。此外，研究人员建议沿着复值频谱的频率维度使用通道注意力来捕获信号的结构特征[15]，[43]。在上述方法中，注意力机制通常被一起应用以沿着时间维度或频率维度捕获全局上下文聚合。与以往的解决方案不同，我们建议使用全带注意和子带注意，分别捕获长期和短期的功能，以便更好地恢复船舶信号的弱线谱部分。值得注意的是，每个注意力块分别从时间和频率两个维度捕获依赖关系。在目标子网和噪声子网中都采用这样的注意力块。</code></p><h1 id="III-NOISE-AWARE-SUBBAND-ATTENTION-NETWORK"><a href="#III-NOISE-AWARE-SUBBAND-ATTENTION-NETWORK" class="headerlink" title="III. NOISE-AWARE SUBBAND ATTENTION NETWORK"></a>III. NOISE-AWARE SUBBAND ATTENTION NETWORK</h1><p><code>三.噪声感知子带注意网络</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/6.png"></p><blockquote><p>Fig. 3. Overall network architecture of the proposed NAFSA-Net.<br><code>图3 拟议的NAFSA网络的总体网络架构。</code></p></blockquote><p>Fig. 3 presents the overall network architecture of the proposed NAFSA-Net. The main building blocks of NAFSA-Net are the encoder–decoder module, the FSA blocks, and the interaction module. Our model takes the STFT-transformed complex-valued spectrum as input, and the input feature can be denoted as X ∈ R^(F×T×2). Subsequently, the encoder extracts the feature representations of the original signal and feeds them into the noise and target subnets. To take full advantage of the prior feature differences between noise and target on the T-F spectrogram, several stacked FSA blocks are deployed in different subnets for exploiting the dependencies of the noise and target components, respectively. Then, the decoder receives information from the encoder and FSA blocks to reconstruct the complex-valued spectrum corresponding to each subnet. In NAFSA-Net, an interaction module is introduced to transfer supplementary information from the noise subnet to the target subnet. In this way, the target subnet is expected to obtain complementary information. Finally, the denoised spectrum is converted back to the waveform by iSTFT.<br><code>图3显示了所提出的NAFSA网络的整体网络架构。NAFSA-Net的主要构建块是编码器-解码器模块、FSA块和交互模块。我们的模型以STFT变换后的复值谱作为输入，输入特征可以表示为 X ∈ R^(F×T×2)。随后，编码器提取原始信号的特征表示，并将其馈送到噪声和目标噪声中。为了充分利用T-F谱图上噪声和目标之间的先验特征差异，将几个堆叠的FSA块部署在不同的频带中，以分别利用噪声和目标分量的依赖性。然后，解码器从编码器和FSA块接收信息以重构对应于每个子网的复值频谱。在NAFSA网络中，引入了一个交互模块，将补充信息从噪声子网传输到目标子网。以这种方式，期望目标子网获得补充信息。最后，通过iSTFT将去噪频谱转换回波形。</code></p><p><em><strong>A. Encoder and Decoder Module</strong></em><br><code>A.编码器和解码器模块</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/7.png"></p><blockquote><p>Fig. 4. Encoder–decoder module. The dashed arrow indicates the separation part with FSA blocks.<br><code>图4 编码解码器模块。虚线箭头指示具有FSA块的分离部分。</code></p></blockquote><p>In NAFSA-Net, the noise subnet and the target subnet share the encoder part. Fig. 4 illustrates the encoder–decoder-based architecture of NAFSA-Net. Three modified residual blocks are introduced in the encoder to extract latent features from spectral features. Each modified residual block consists of two 2-D convolutional layers with a kernel size of (5, 7) and a stride of (1, 1). Different from the traditional residual block, the number of channels in the convolutional layer will increase relative to their inputs. Moreover, a 1×1 convolutional layer is deployed in the skip connection part to maintain the consistency of residual inputs. All the convolutional layers in the encoder are followed by batch normalization (BN) and a rectified linear unit (ReLU). Note that we use D^(enc) ∈ R^(F×T×C) to denote the outputs of the encoder, where C is the channel number. The number of output channels for each block is 6, 12, and 24, respectively. In order to maintain sufficient resolution, no downsampling is performed in the time and frequency dimensions of the features.<br><code>在NAFSA网络中，噪声子网和目标子网共享编码器部分。图4示出了NAFSA网络的基于编码器-解码器的架构。在编码器中引入了三个改进的残差块，以从谱特征中提取潜在特征。每个修改后的残差块由两个2-D卷积层组成，其内核大小为（5，7），步幅为（1，1）。与传统的残差块不同，卷积层中的通道数量将相对于其输入增加。此外，在跳过连接部分部署了1×1卷积层，以保持剩余输入的一致性。编码器中的所有卷积层后面都是批量归一化（BN）和整流线性单元（ReLU）。注意，我们使用D^(enc) ∈ R^(F×T×C)来表示编码器的输出，其中C是通道数。每个块的输出通道数分别为6、12和24。为了保持足够的分辨率，在特征的时间和频率维度中不执行下采样。</code><br>As shown in Fig. 4, the decoder contains three consecutive convolution blocks followed by a 1 × 1 convolution layer, which learns the complex ratio mask of noise and target audio, and denoted by<br><code>如图4所示，解码器包含三个连续的卷积块，后面是1 × 1卷积层，它学习噪声和目标音频的复比掩码，并表示为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/8.png"><br>and<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/9.png"><br>respectively.<br>Each Conv block receives the output of the previous module and the output of the corresponding residual block in the encoder. All the 2-D convolution layers are followed by a ReLU activation and a BN layer. The kernel size and stride for all 2-D convolution layers are set to (1, 1) and (1, 1). The channel numbers are 12, 6, and 2, respectively. In particular, we propose to use gated convolution [44] instead of a direct skip connection to filter the feature information from the encoder to different subnets.<br><code>每个Conv块接收先前模块的输出和编码器中的对应残差块的输出。所有的二维卷积层之后都是ReLU激活和BN层。所有2-D卷积层的内核大小和步幅设置为（1，1）和（1，1）。频道号分别为12、6和2。特别是，我们建议使用门控卷积[44]而不是直接跳过连接来过滤来自编码器的特征信息到不同的编码器。</code></p><p><em><strong>B. Fullband–Subband Attention Block</strong></em><br><code>B.全频带-子频带注意阻断</code><br>NAFSA-Net captures dependencies by aggregating fullband attention and subband attention. In this section, we first introduce the implementation of fullband attention via channel adaptive strategy and subband attention via the sliding window, respectively. After that, we present the aggregating strategy of the FSA block.<br><code>NAFSA网络通过聚合全频带注意力和子带注意力来捕获依赖关系。在这一节中，我们首先介绍了通过信道自适应策略实现全带注意和通过滑动窗口实现子带注意。在此基础上，提出了FSA模块的聚合策略。</code><br>1)Fullband Attention via Channel Adaptive Strategy: Different vessels usually exhibit unique frequency band distribution characteristics in the spectrogram. In addition, valuable frequency and harmonic components of the ship tend to concentrate in the lower frequency band of mixed audio. Also, inspired by the successful application of channelwise attention in computer vision [45], [46], we propose to capture the global distribution characteristics of spectrogram by fullband attention via channel adaptive strategy. This scheme takes the frequency dimension as the channel dimension of the channel attention mechanism, which adaptively learns a weight vector corresponding to each frequency bin based on the spectral feature information. The proposed fullband attention is shown in Fig 5.<br><code>1)通过通道自适应策略实现全频带注意：不同的血管通常在频谱图中表现出独特的频带分布特征。此外，船舶的有价值的频率和谐波分量往往集中在混合音频的较低频带。此外，受通道注意力在计算机视觉中的成功应用的启发[45]，[46]，我们提出通过通道自适应策略通过全带注意力来捕获谱图的全局分布特征。该方案将频率维度作为信道关注机制的信道维度，根据频谱特征信息自适应地学习每个频率点对应的权向量。所提出的全频带注意力如图5所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/10.png"></p><blockquote><p>Fig. 5. Diagram of fullband attention.<br><code>图五 全波段注意力示意图。</code></p></blockquote><p>Given an intermediate feature D with the shape of B × C × F × T, where B denotes the batch size, first, a 2-D convolution layer is used to combine the features of multiple channels and obtain the intermediate result with the shape of B × F × T by reshape operation. Next, we aggregate the information at each frequency bin by average pooling. The reshaped features (R^(B×F)) are then fed into the MLP layer to generate the weight vector ω ∈ R^(B×F). Finally, we conduct an elementwise multiplication on the original feature D with ω after broadcasting to obtain the output (D^full ∈ R^(B×C×F×T)) of fullband attention.<br><code>给定一个形状为B × C × F × T的中间特征D，其中B表示批量大小，首先，使用二维卷积层将多个通道的特征进行组合，并通过整形操作获得形状为B × F × T的中间结果。接下来，我们通过平均池化来聚合每个频率点处的信息。然后将整形后的特征（R^(B×F)）馈送到MLP层以生成权重向量ω ∈ R^(B×F)。最后，我们在广播后对原始特征D与ω进行逐元素乘法，以获得全频带注意力的输出（D^full ∈ R^(B×C×F×T)）。</code></p><p>2)Subband Attention via Sliding Window: We use a sliding window mechanism to design subband attention, which is applied to capture fine-grained correlations within local patterns of the spectrogram. Similar strategies have been employed in [47] and [48]. Specifically, the input feature maps are first decomposed into disjoint fragments with a window length of w. Each token within a local fragment attends to all tokens within its home fragment, as well as d consecutive adjacent tokens on the left and right side, yielding a local subband of total length w + 2d tokens. d indicates the length of adjacent tokens on each side. Then, we perform self-attention on all N subbands to extract short term dependencies. After that, a 2-D convolution layer is utilized to map the subbands back to home fragments of length w and ultimately generate the output of the subband attention D^sub. For boundary tokens, the zero-padding strategy proposed in [47] is applied.<br><code>2)通过滑动窗口的子带注意：我们使用滑动窗口机制来设计子带注意，它被应用于捕获频谱图的局部模式内的细粒度相关性。在[47]和[48]中采用了类似的策略。具体地，输入特征图首先被分解成具有窗口长度w的不相交片段。本地片段内的每个令牌涉及其归属片段内的所有令牌，以及左侧和右侧的d个连续相邻令牌，从而产生总长度为w + 2d个令牌的本地子带。d表示每一侧上相邻标记的长度。然后，我们对所有N个子带执行自注意以提取短期依赖性。之后，利用2-D卷积层将子带映射回长度为w的归属片段，并最终生成子带关注D^sub的输出。对于边界令牌，应用[47]中提出的零填充策略。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/11.png"></p><blockquote><p>Fig. 6. Diagram of subband attention.<br><code>图6 子带注意图。</code></p></blockquote><p>Fig. 6 illustrates the details of subband attention. Given an input feature D ∈ R^(F×T), the subband attention can be defined as<br><code>图6示出了子带关注的细节。给定输入特征D ∈ R^(F×T)，子带注意力可以定义为：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/12.png"><br>where $D_{\text {input }}^{\text {sub }} \in \mathbb{R}^{N \times F^{\text {sub }} \times T}$(N indicates the number of local subbands, and $F^{\text {sub }}=w+2 d$ represents the width of each subband, which is equal to the length of the sliding window plus the length of the extended left and right neighbors). Segmentation(·) divides feature tensors into local fragments and reorganizes them into a new feature tensor. $\operatorname{Reshape}^1(\cdot)$ denotes a vector reshape from $\mathbb{R}^{N \times F^{\text {sub }} \times T}$ to $\mathbb{R}^{F^{\mathrm{sub}} \times N \times T}$, and $\operatorname{Reshape}^2(\cdot)$ reshapes a tensor from $\mathbb{R}^{N \times w \times T}$ back to $\mathbb{R}^{F \times T}$.<br><code>其中，</code>$D_{\text {input }}^{\text {sub }} \in \mathbb{R}^{N \times F^{\text {sub }} \times T}$<code>（N表示局部子带的数量，</code>$F^{\text {sub }}=w+2 d$<code>表示每个子带的宽度，其等于滑动窗口的长度加上扩展的左右邻居的长度）。分割（·）将特征张量划分为局部片段，并将它们重组为新的特征张量。</code>$\operatorname{Reshape}^1(\cdot)$<code>表示从</code>$\mathbb{R}^{N \times F^{\text {sub }} \times T}$<code>到</code>$\mathbb{R}^{F^{\mathrm{sub}} \times N \times T}$<code>的向量整形，</code>$\operatorname{Reshape}^2(\cdot)$<code>将张量从</code>$\mathbb{R}^{N \times w \times T}$<code>整形回</code>$\mathbb{R}^{F \times T}$<code>。</code><br>In the above equations, Conv(·) denotes the 2-D convolution layer with a kernel size of (1, 1) and a stride of (1, 1). A BN and a PReLU activation are added after the convolution layer. In addition, we set the length d of adjacent tokens within the subband to be equal to the window length w. The hyperparameter w will be optimized by comparative experiments.<br><code>在上面的等式中，Conv（·）表示具有核大小（1，1）和步幅（1，1）的2-D卷积层。在卷积层之后添加BN和PReLU激活。此外，我们将子带内的相邻令牌的长度d设置为等于窗口长度w。超参数w将通过比较实验进行优化。</code></p><p>3)FSA Block: FSA blocks are deployed between the encoder and the decoder for extracting spectral features from the noise subnet and the target subnet, respectively. Each FSA block consists of two main components: fullband attention and subband attention. The former applies channelwise attention in the frequency dimension to aggregate global dependencies of input features. Different from fullband attention, the latter performs fine-grained subband attention along the frequency and time dimensions in parallel. The fine-grained attention mechanism in the frequency dimension allows the network to pay attention to the high-power narrowband spectral components. When it comes to the time dimension, the subband attention captures dependencies within local time segments, corresponding to the temporal persistence and periodicity of ship propeller sounds.<br><code>3)FSA模块：FSA块部署在编码器和解码器之间，用于分别从噪声子网和目标子网中提取频谱特征。每个FSA块由两个主要部分组成：全带注意和子带注意。前者在频率维度上应用通道注意力来聚合输入特征的全局依赖性。与全带注意不同，后者并行地沿频率和时间维度执行细粒度子带注意沿着。频率维度的细粒度注意机制允许网络关注高功率窄带频谱分量。当涉及到时间维度时，子带注意力捕获本地时间段内的依赖性，对应于船舶螺旋桨声音的时间持久性和周期性。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/13.png"></p><blockquote><p>Fig. 7. Illustration of the FSA block. Here, each FSA block first performs fullband attention on the frequency dimension of the input features. Then, subband attention is applied in parallel along the frequency and time dimensions of the input features to produce corresponding output features D^Freq and D^Time, respectively. Finally, the FSA block combines the original features D^Res with them to get the final output features.<br><code>图7 FSA块的图示。这里，每个FSA块首先对输入特征的频率维度执行全频带关注。然后，沿着输入特征的频率和时间维度并行地应用子带注意，以分别产生对应的输出特征D^Freq和D^Time。最后，FSA块将原始特征D^Res与它们组合以获得最终输出特征。</code></p></blockquote><p>Fig. 7 shows an illustration of the FSA block. The spectral features D ∈  $\mathbb{R}^{C \times F \times T}$ are first fed into the fullband attention module to generate the intermediate features with a shape of C × F × T. To reduce computational complexity, we propose a 2-D convolution layer to halve the number of channels. The output features are then fed parallelly into a temporal subband attention and a frequencywise subband attention module. The dimensions of the input features for each of the two subband attention are $\mathbb{N}^{F}$ ×  $\mathbb{Subband}^{F}$ × (TC/2) and $\mathbb{N}^{T}$ ×  $\mathbb{Subband}^{T}$ × (FC/2), respectively. $\mathbb{N}^{k}$ , k ∈ {F, T} denotes the number of subband fragments, and  $\mathbb{Subband}^{k}$ = w + 2d, k ∈ {F, T} indicates the width of local subband. We concatenated the feature $\mathbb{D}^{Freq}$ ∈ $\mathbb{R}^{(C/2) \times F \times T}$ generated by frequencywise subband attention, the feature $\mathbb{D}^{Time}$  ∈ $\mathbb{R}^{(C/2) \times F \times T}$ produced by temporal subband attention, and the residual feature $\mathbb{D}^{Res}$ ∈ $\mathbb{R}^{(C/2) \times F \times T}$. The combined features are fed into another 2-D convolution layer. Finally, the output features of the FSA block in the target subnet are fused with the information of the noise subnet passed through the interaction module, used in the subsequent modules.<br><code>图7示出了FSA块的图示。首先将频谱特征</code>D ∈  $\mathbb{R}^{C \times F \times T}$<code>输入到全频带注意模块中，以生成具有C × F × T形状的中间特征。为了降低计算复杂度，我们提出了一个2-D卷积层，以减少一半的通道数量。然后将输出特征连续地馈送到时间子带注意和频率子带注意模块中。两个子带注意中的每一个的输入特征的维度分别为</code>$\mathbb{N}^{F}$ ×  $\mathbb{Subband}^{F}$ ×（TC/2）<code>和</code>$\mathbb{N}^{T}$ ×  $\mathbb{Subband}^{T}$ ×（FC/2）。$\mathbb{N}^{k}$，k ∈ {F，T}<code>表示子带片段的数量，</code> $\mathbb{Subband}^{k}$ = w + 2d，k ∈ {F，T}<code>表示局部子带的宽度。我们将由频率子带注意产生的特征</code>$\mathbb{D}^{Freq}$ ∈ $\mathbb{R}^{(C/2) \times F \times T}$<code>由时间子带注意产生的特征</code>$\mathbb{D}^{Time}$ ∈ $\mathbb{R}^{(C/2) \times F \times T}$<code>和残差特征</code>$\mathbb{D}^{Res}$ ∈ $\mathbb{R}^{(C/2) \times F \times T}$<code>串联起来。组合的特征被馈送到另一个2-D卷积层。最后，将目标子网FSA模块的输出特征与经过交互模块的噪声子网信息进行融合，用于后续模块。</code><br>All the convolution layers in the FSA blocks are followed by a BN and a parametric ReLU (PReLU) activation. They have the same kernel size of (1, 1) and the stride of (1, 1).<br><code>FSA块中的所有卷积层后面都是BN和参数ReLU（PReLU）激活。它们具有相同的内核大小（1，1）和步幅（1，1）。</code></p><p><em><strong>C. Interaction Module</strong></em><br><code>C.交互模块</code><br>In NAFSA-Net, the noise subnet is not independent of the target subnet, and we expect the former to enhance the training process of the latter. The noise component dominates the noisy audio with low SNR, and there are many overlapping parts of noise and ship audio in the spectrogram. In light of this, an information interaction module is introduced to transfer valuable supplementary information from the noise subnet to the target subnet. The structure of the interaction module is shown in Fig. 8.<br><code>在NAFSA网络中，噪声子网并不独立于目标子网，我们希望前者能够增强后者的训练过程。噪声分量在低信噪比的噪声音频中占主导地位，并且在频谱图中存在噪声和船舶音频的许多重叠部分。针对这一问题，引入信息交互模块，将有价值的辅助信息从噪声子网传递到目标子网。交互模块的结构如图8所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/14.png"></p><blockquote><p>Fig. 8. Structure of the interaction module.<br><code>图8 交互模块的结构。</code></p></blockquote><p>Features from the target subnet DTarget are first added to features from the noise subnet DNoise. Subsequently, we input the new features into a convolutional block consisting of a 2-D convolution layer, a BN layer, and a sigmoid activation function. A spectral mask M is produced and used to filter the information passed into the target subnet. Finally, the block adds DTarget and the filtered information to get the output features. The process of the interaction module is given by<br><code>首先将来自目标子网D目标的特征添加到来自噪声子网D噪声的特征。随后，我们将新特征输入到由2-D卷积层，BN层和sigmoid激活函数组成的卷积块中。产生频谱掩码M并将其用于过滤传递到目标子网中的信息。最后，该模块将DTTarget和过滤后的信息相加以获得输出特征。交互模块的过程由下式给出：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/15.png"><br>where ⊗ means elementwise multiplication and Mask(·) represents the shorthand for convolution, BN, and the sigmoid function. We will verify the effectiveness of the interaction module later in the ablation experiment.<br><code>其中，⊗表示逐元素乘法，Mask（·）表示卷积、BN和sigmoid函数的简写。我们将在稍后的消融实验中验证交互模块的有效性。</code></p><p><em><strong>D. Loss Function</strong></em><br><code>D.损失函数</code><br>Our proposed NAFSA-Net is designed to provide an accurate estimation of both noise and target audio at the same time. In this section, we modify the SI-SNR loss function to match the noise information and the target information from the two subnets. The weighted SI-SNR (wSI-SNR) loss function contains not only the original target estimation part SI-SNR(ˆs, s) but also the noise prediction term SI-SNR( ˆn, n)<br><code>我们提出的NAFSA网络旨在同时提供噪声和目标音频的准确估计。在本节中，我们修改SI-SNR损失函数，以匹配来自两个SNR的噪声信息和目标信息。加权SI-SNR（wSI-SNR）损失函数不仅包含原始目标估计部分SI-SNR(ˆs, s)，还包含噪声预测项SI-SNR( ˆn, n)</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/16.png"><br>Although NAFSA-Net estimates both the noise and the target audio, the training of the model should still be dominated by the target audio. Therefore, we also propose an improved wSI-SNR (iwSI-SNR) loss function to guide the training of our model. The novel loss function increases the noise-aware perception component SI-SNR(x − ˆn, s). The iwSI-SNR loss is defined as<br><code>虽然NAFSA-Net估计噪声和目标音频，但模型的训练仍然应该由目标音频主导。因此，我们还提出了一种改进的wSI-SNR（iwSI-SNR）损失函数来指导我们模型的训练。新的损失函数增加了噪声感知分量SI-SNR(x − ˆn, s)。iwSI-SNR损失定义为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/17.png"><br>In the above equations, ˆn denotes the noise estimation. α and β are the weight factor, used for balancing the contribution of each loss part. In the experiments, we compared the denoising performance of the network when different loss functions are applied.<br><code>在上述等式中，Wn表示噪声估计。α和β是权重因子，用于平衡每个损失部分的贡献。在实验中，我们比较了网络的去噪性能时，采用不同的损失函数。</code></p><h1 id="IV-EXPERIMENTAL-SETTINGS"><a href="#IV-EXPERIMENTAL-SETTINGS" class="headerlink" title="IV. EXPERIMENTAL SETTINGS"></a>IV. EXPERIMENTAL SETTINGS</h1><p><code>四.实验设置</code><br><em><strong>A. Dataset</strong></em><br><code>A.数据集</code><br>In our experiments, we trained and evaluated NAFSA-Net on ShipsEar [49], which is a public underwater acoustic signal dataset. We use two types of ship recordings (passengerboat and motorboat) and three types of underwater ambient noise signals (wind noise, flow noise, and rain noise) to generate the experimental dataset. The new dataset mixes ship signal with a randomly selected noise signal, synthesizing a total of 73 557 pieces of noisy audios with very low SNR range of [−15 dB, −5 dB], of which 70% are used for training, 20% for validation, and 10% for evaluation. For evaluation, we designed four different scenarios to perform NAFSA-Net: 1) seen ships, i.e., audios are from the same ships included in the training stage; 2) unseen ships, i.e., when signals are from the same ship types but different ships that are not included during training; 3) unseen ship types, i.e., when audios are from the unseen ship types (dredger and fishboat); and 4) unseen noise types, i.e., when audios are from the unseen noise types. The evaluation set is generated at three typical SNRs (−15, −10, and −5 dB). All noisy signals are resampled to 16 kHz and clipped to 3 s.<br><code>在我们的实验中，我们在ShipsEar [49]上训练和评估了NAFSA-Net，这是一个公共的水声信号数据集。我们使用两种类型的船舶记录（快艇和摩托艇）和三种类型的水下环境噪声信号（风噪声，流噪声和雨噪声）来生成实验数据集。新数据集将船舶信号与随机选择的噪声信号混合，合成了总共73557段噪声音频，信噪比范围非常低，为[-15 dB，-5 dB]，其中70%用于训练，20%用于验证，10%用于评估。为了进行评估，我们设计了四种不同的场景来执行NAFSA-Net：1）看到的船只，即，音频来自包括在训练阶段中的相同船只; 2）看不见的船只，即，当信号来自相同的船型但不同的船舶时，这些船舶在训练期间没有被包括在内; 3）看不见的船型，即，当音频来自看不见的船舶类型（挖泥船和渔船）时;以及4）看不见的噪声类型，即，当音频来自看不见的噪声类型时。评估集在三种典型SNR（−15、−10和−5 dB）下生成。所有噪声信号被重新采样到16 kHz并限幅到3 s。</code></p><p><em><strong>B. Experiment Details</strong></em><br><code>B.实验细节</code><br>We set the window length of 32 ms (512 samples) and the frame shift of 16 ms (256 samples) to transform the waveform audios to the complex-valued spectrums. The Adam optimizer is used for optimization with an initial learning rate of 1e−4. The batch size is set to 32. In particular, the learning rate will decay by 0.9 if the evaluation of the validation set is not improved in two consecutive epochs. An early stopping mechanism is applied with a patience of 10 epochs. Our proposal is implemented by Pytorch and trained using NVIDIA A100 40-GB GPU.<br><code>我们设置32 ms（512个样本）的窗口长度和16 ms（256个样本）的帧移位来将波形音频转换为复值频谱。Adam优化器用于初始学习率为1 e −4的优化。批量大小设置为32。特别地，如果验证集的评估在两个连续的时期中没有得到改善，则学习率将衰减0.9。一个早期停止机制是应用与耐心的10个时代。我们的建议是由Pytorch实现的，并使用NVIDIA A100 40-GB GPU进行训练。</code></p><p><em><strong>C. Evaluation Metrics</strong></em><br><code>C.评估指标</code><br>To evaluate the performance of our proposals, we report the signal-to-distortion ratio (SDR) [50], scale-invariant SNR improvement (SI-SNRi), and segment SNR (SSNR) as objective metrics. Higher values correspond to better performance.<br><code>为了评估我们的建议的性能，我们报告了信号失真比（SDR）[50]，尺度不变SNR改善（SI-SNRi）和分段SNR（SSNR）作为客观指标。较高的值对应于较好的性能。</code></p><h1 id="V-RESULTS-AND-DISCUSSIONS"><a href="#V-RESULTS-AND-DISCUSSIONS" class="headerlink" title="V. RESULTS AND DISCUSSIONS"></a>V. RESULTS AND DISCUSSIONS</h1><p><code>五.成果和建议</code><br><em><strong>A. Loss Comparisons</strong></em><br><code>A.损失比较</code><br>In this section, we focus on the analysis of two different wSI-SNR loss functions and demonstrate the superiorities of our proposed iwSI-SNR in optimizing the noise-aware denoising network. In addition, we evaluate the performance of the setting of the weight factor in the loss function. Table I shows the performance of NAFSA-Net with different loss functions and different weight factors.<br><code>在本节中，我们重点分析了两种不同的wSI-SNR损失函数，并展示了我们提出的iwSI-SNR在优化噪声感知去噪网络方面的优势。此外，我们评估的损失函数中的权重因子的设置的性能。表1显示了具有不同损失函数和不同权重因子的NAFSA网络的性能。</code><br>We first set the weight factors as the energy ratio of the target signal to the noise signal in [32] ($\alpha=|s|^2 /\left(|s|^2+|n|^2\right)$) to compare the performance of the two loss functions. It can be observed that iwSI-SNR evidently improves the SDR by 2.68 dB, the SI-SNRi by 0.88 dB, and the SSNR by 2.83 dB compared to wSI-SNR. This indicates that the iwSI-SNR loss function dominated by the target audio is more advantageous in model optimization. Next, we evaluate the effects of β in iwSI-SNR, which is set to {energy ratio, 0.3, 0.4, 0.5, 0.6, 0.7}. We can see that the performance on SI-SNR outperforms the case of β = energy ratio when β is set to a constant value. In addition, NAFSA-Net achieves optimal performance in each evaluation index when β = 0.5. These results demonstrate that the effectiveness of the proposed iwSI-SNR and the noise-aware term is equally important in improving the denoising performance compared with the target term.<br><code>我们首先将权重因子设置为[32]中目标信号与噪声信号的能量比（</code>$\alpha=|s|^2 /\left(|s|^2+|n|^2\right)$<code>）比较两种损失函数的性能。可以观察到，与wSI-SNR相比，iwSI-SNR明显地将SDR提高了2.68 dB，将SI-SNRi提高了0.88 dB，并且将SSNR提高了2.83 dB。这表明由目标音频主导的iwSI-SNR损失函数在模型优化中更有利。接下来，我们评估β在iwSI-SNR中的影响，其被设置为{能量比，0.3，0.4，0.5，0.6，0.7}。我们可以看到，当β设置为恒定值时，SI-SNR的性能优于β =能量比的情况。此外，当β = 0.5时，NAFSA-Net在各评价指标上均达到最优性能。这些结果表明，与目标项相比，所提出的iwSI-SNR和噪声感知项的有效性在改善去噪性能方面同样重要。</code><br><em><strong>B. Parameter Optimization</strong></em><br><code>B。参数优化</code><br>This section mainly optimizes the parameters w and d, which represents the window length and the number of adjacent time frames/frequency bins on each side of the subband attention module in the FSA block. In NAFSA-Net, we set d to be equal to w. To satisfy the fine-grained partitioning requirement, we set B to a small value of {2, 3, 4, 5, 6}. SDR, SI-SNRi, and SSNR scores for various models trained on the ShipsEar dataset are shown in Fig. 9. We observe that, for different w values, SI-SNRi results are similar. However, for both SDR and SSNR scores, the model achieved significantly better results than the other conditions when w = 4. Therefore, the subband width within the FSA block is set to w + 2d = 12.<br><code>本节主要对参数w和d进行优化，参数w和d表示FSA块中子带注意模块每侧的窗口长度和相邻时间帧/频率仓的数量。在NAFSA网络中，我们将d设置为等于w。为了满足细粒度分区的要求，我们将B设置为一个小值{2，3，4，5，6}。在ShipsEar数据集上训练的各种模型的SDR、SI-SNRi和SSNR得分如图9所示。我们观察到，对于不同的w值，SI-SNRi结果是相似的。然而，对于SDR和SSNR评分，当w = 4时，该模型获得的结果显著优于其他条件。因此，FSA块内的子带宽度被设置为w + 2d = 12。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/18.png"></p><blockquote><p>Fig. 9. Tuning the hyperparameter w.<br><code>见图9 调整超参数w。</code></p></blockquote><p>C. Ablation Study<br><code>C.消融研究</code><br>In this section, we demonstrate the findings of ablation experiments performed to analyze the effectiveness of different network modules in our proposed NAFSA-Net. Four sets of comparison experiments are designed to evaluate the performance of different modules. First, we keep only one target subnet as the baseline, which is not estimated for noise. After that, the noise subnet is added to the baseline model, but there is no information interaction between the two subnets. Then, an interactive module is deployed between the target subnet and the noise subnet to obtain our proposed NAFSANet. Finally, we also evaluate the NAFSA-Net using the direct skip connection module. The first model uses the SI-SNR loss function, and the next three models utilize the iwSI-SNR loss function to optimize the training process. The results of the ablation study on the ShipsEar dataset are shown in Table II.<br><code>在本节中，我们展示了消融实验的结果，以分析我们提出的NAFSA网络中不同网络模块的有效性。设计了四组对比实验来评估不同模块的性能。首先，我们只保留一个目标子网作为基线，这是没有估计的噪声。在此基础上，将噪声子网加入到基线模型中，但两个子网之间没有信息交互。然后，在目标子网和噪声子网之间部署交互模块，以获得我们提出的NAFSANet。最后，我们还评估了NAFSA网络使用直接跳过连接模块。第一个模型使用SI-SNR损失函数，接下来的三个模型使用iwSI-SNR损失函数来优化训练过程。ShipsEar数据集上的消融研究结果如表II所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/19.png"></p><blockquote><p>TABLE II ABLATION STUDY ON THE SHIPSEAR DATASET<br><code>表二：船舶数据集上的烧蚀研究</code></p></blockquote><p>We can see from Table II that the single target subnet improves the SDR by 14.22 dB, SISNR by 18.95 dB, and SSNR by 14.02 dB compared to the noisy audio. After adding the noise estimation subnet, we observe a 6.5-dB gain on SDR, 1.68-dB gain on SI-SNRi, and 6.63-dB gain on SSNR. When the information interaction module is introduced to the network, NAFSA-Net achieves further 1.94-, 1.71-, and 1.86-dB improvements in SDR, SI-SNRi, and SSNR, respectively. In addition, we further evaluate the effectiveness of gated convolution in the skip connection module. Our method achieves a gain on SDR by 3.47 dB, SI-SNRi by 2.31 dB, and SSNR by 2.99 dB, compared with the direct skip connection module. Since both subnets share the same encoder part, NAFSA-Net can filter valuable information through gated convolutional connections and transmit them to both subnets separately. These observations verify the effectiveness of the proposed noise-aware network architecture, the interaction module, and the gated convolution connection for enhancing the denoising capability in the case of low SNR.<br><code>从表II中可以看出，与噪声音频相比，单个目标子网将SDR提高了14.22 dB，将SISNR提高了18.95 dB，将SSNR提高了14.02 dB。添加噪声估计子网后，SDR上的增益为6.5 dB，SI-SNRi上的增益为1.68 dB，SSNR上的增益为6.63 dB。当将信息交互模块引入网络时，NAFSA-Net在SDR、SI-SNRi和SSNR方面分别实现了1.94、1.71和1.86 dB的改进。此外，我们进一步评估了门控卷积在跳过连接模块中的有效性。与直接跳过连接模块相比，我们的方法在SDR上实现了3.47 dB的增益，SI-SNRi为2.31 dB，SSNR为2.99 dB。由于这两个编码器共享相同的编码器部分，NAFSA-Net可以通过门控卷积连接过滤有价值的信息，并将它们分别传输到两个编码器。这些观察结果验证了所提出的噪声感知网络架构、交互模块和门控卷积连接在低SNR情况下增强去噪能力的有效性。</code></p><p><em><strong>D. Comparison With Baselines</strong></em><br><code>D.与基线的比较</code><br>In this section, we analyze the results to show the advantages of NAFSA-Net over previous techniques in terms of denoising. As a comparison, a wavelet threshold shrinking denoising method and an EMD method are used for the conventional baseline models. The level of wavelet decomposition is set to be $\left\lfloor\log _2(L) / 2\right\rfloor$, where L is the number of sample points. The EMD approach follows the setting of [11]. In addition, we further compare NAFSA-Net with other representative DNN-based approaches, which are briefly introduced as follows.<br><code>在本节中，我们将分析结果，以显示NAFSA-Net在去噪方面优于以前的技术。作为比较，小波阈值收缩去噪方法和EMD方法被用于常规基线模型。小波分解的水平被设置为</code>$\left\lfloor\log _2(L) / 2\right\rfloor$<code>，其中L是样本点的数量。EMD方法遵循[11]的设置。此外，我们还将NAFSA-Net与其他代表性的基于DNN的方法进行了比较，简要介绍如下。</code><br>1)SEGAN: A time-domain enhancement approach with the generative adversarial network (GAN).<br>2)DPRNN for Denoising: A dual-path recurrent neural network in the time domain.<br>3)DCCRN: A T-F domain complex-valued denoising model.<br>4)FullSubNet: A T-F domain method that aggregates the fullband features and the subband features to enhance the denoising performance.<br>5)NAFSA-Fullband: NAFSA-Net with only fullband attention in the FSA blocks.<br>6)NAFSA-Subband: NAFSA-Net with only subband attention module in the FSA blocks.<br><code>1)SEGAN：一种使用生成对抗网络（GAN）的时域增强方法。2)DPRNN for Denoising：时域中的双路径递归神经网络。3)DCCRN：一种T-F域复值去噪模型。4)FullSubNet：一种T-F域方法，聚合全带特征和子带特征以增强去噪性能。5)NAFSA-全波段：NAFSA-网络，仅在FSA块中关注全波段。6)NAFSA-子带：在FSA块中仅具有子带注意模块的NAFSA-网络。</code><br>All the above models are trained on the same ShipsEar dataset for 100 epochs.<br><code>所有上述模型都在相同的ShipsEar数据集上训练了100个epochs。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/20.png"></p><blockquote><p>TABLE III QUANTITATIVE COMPARISONS WITH OTHER DENOISING METHODS ON THE SHIPSEAR DATASET<br><code>表三：与船舶数据集上其他降噪方法的定量比较</code></p></blockquote><p>The overall results on SDR, SDRi, SI-SNRi, and SSNR are shown in Table III. First, we find that the traditional statistical-based techniques perform poorly for processing underwater acoustic signals with low SNRs, and the DNN-based denoising methods outperform the conventional denoising approach DNN-based denoising methods outperform the conventional denoising approaches in all evaluation metrics. Besides, our proposed NAFSA-Net (last row) achieves the best evaluation scores on the ShipsEar dataset, considerably better than other models. NAFSA-Net obtains a 22.66-dB gain on SDR, a 22.44-dB gain on SI-SNR, and a 22.51-dB gain on SSNR. In addition, we can see that the single target subnet with FSA blocks shows a very competitive denoising performance and achieves the best results among all methods based on target audio estimation. This result demonstrates that the FSA blocks proposed in this article can effectively extract the features of the target audio from the low SNR noisy signals. We have further evaluated the role of the fullband module and the subband module in the NAFSA-Net. The fine-grained subband model shows a greater advantage in denoising, and it obtains a similar SI-SNRi score as NAFS DNN-based denoising methods outperform the conventional denoising approaches in all evaluation metrics. Besides, our proposed NAFSA-Net (last row) achieves the best evaluation scores on the ShipsEar dataset, considerably better than other models. NAFSA-Net obtains a 22.66-dB gain on SDR, a 22.44-dB gain on SI-SNR, and a 22.51-dB gain on SSNR. In addition, we can see that the single target subnet with FSA blocks shows a very competitive denoising performance and achieves the best results among all methods based on target audio estimation. This result demonstrates that the FSA blocks proposed in this article can effectively extract the features of the target audio from the low SNR noisy signals. We have further evaluated the role of the fullband module and the subband module in the NAFSA-Net. The fine-grained subband model shows a greater advantage in denoising, and it obtains a similar SI-SNRi score as NAFSA-Net. It is expected that properly aggregating the long-term fullband and the fine-grained subband can further improve the denoising performance of our model. Table IV presents the experimental results of our proposed NAFSA-Net at different SNRs. One can observe that NAFSA-Net can restore the noisy signals to the same level for different SNR conditions (i.e. recover from −5, −10, and −15 dB to 12.65, 13.24, and 10.39 dB, respectively). These suggest that, even in the case of low SNRs, NAFSA-Net is an effective solution to model the characteristics of the target ships, so as to realize underwater acoustic denoising.<br><code>SDR、SDRi、SI-SNRi和SSNR的总体结果见表III。首先，我们发现传统的基于神经网络的技术在处理低信噪比的水声信号时表现不佳，基于DNN的去噪方法优于传统的去噪方法。此外，我们提出的NAFSA-Net（最后一行）在ShipsEar数据集上获得了最佳评估分数，比其他模型要好得多。NAFSA-Net在SDR上获得22.66 dB增益，在SI-SNR上获得22.44 dB增益，在SSNR上获得22.51 dB增益。此外，我们可以看到，具有FSA块的单个目标子网显示出非常有竞争力的去噪性能，并且在所有基于目标音频估计的方法中达到了最好的结果。实验结果表明，本文提出的FSA模块可以有效地从低信噪比噪声信号中提取目标音频的特征。我们进一步评估了全带模块和子带模块在NAFSA网络中的作用。细粒度子带模型在去噪方面表现出更大的优势，并且它获得了与NAFS DNN去噪方法相似的SI-SNRi得分，在所有评价指标上都优于传统的去噪方法。此外，我们提出的NAFSA-Net（最后一行）在ShipsEar数据集上获得了最佳评估分数，比其他模型要好得多。NAFSA-Net在SDR上获得22.66 dB增益，在SI-SNR上获得22.44 dB增益，在SSNR上获得22.51 dB增益。此外，我们可以看到，具有FSA块的单个目标子网显示出非常有竞争力的去噪性能，并且在所有基于目标音频估计的方法中达到了最好的结果。实验结果表明，本文提出的FSA模块可以有效地从低信噪比噪声信号中提取目标音频的特征。我们进一步评估了全带模块和子带模块在NAFSA网络中的作用。细粒度子带模型在去噪方面表现出更大的优势，并且它获得了与NAFSA-Net相似的SI-SNRi得分。可以预期，适当地聚合长期全频带和细粒度子带可以进一步提高我们的模型的去噪性能。表IV给出了我们提出的NAFSA-Net在不同SNR下的实验结果。可以观察到，NAFSA-Net可以在不同的SNR条件下将噪声信号恢复到相同的水平（即分别从−5、−10和−15 dB恢复到12.65、13.24和10.39 dB）。这表明，即使在低信噪比的情况下，NAFSA网络是一个有效的解决方案来建模的目标船舶的特性，从而实现水声降噪。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/21.png"></p><blockquote><p>TABLE IV QUANTITATIVE RESULTS OF NAFSA-NET AT DIFFERENT SNRS<br><code>表IV不同信噪比下NAFSA网络的定量结果</code></p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/22.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/23.png"></p><blockquote><p>Fig. 10. Comparison of the T-F magnitudes and PSD of motorboat audio processed using DNN-based techniques. (a) Clean. (b) Noisy. (c) SEGAN. (d) DPRNN. (e) DCCRN. (f) FullSubNet. (g) Target subnet. (h) NAFSA-Fullband. (i) NAFSA-Subband. (j) NAFSA-Net(ours).<br><code>见图10。使用基于DNN的技术处理的摩托艇音频的T-F幅度和PSD的比较。(a)干净(b)吵了.(c)塞甘(d)DPRNN。(e)DCCRN。(f)全子网(g)目标子网。(h)NAFSA-全波段。(i)NAFSA-子带。(j)NAFSA网络（我们的）。</code></p></blockquote><p>To explore the denoising performance of our proposed NAFSA-Net, we further compare the denoising differences of noisy audio of motorboats in diverse DNN-based techniques. Fig. 10 demonstrates the magnitude and PSD of the clean audio, the noisy audio, and the denoised audio processed by different DNN-based methods. It can be observed from Fig. 10(j) that our proposed NAFSA-Net shows substantial superiority in noise suppression and clean signal restoration. The clean motorboat audio consists of five dominant frequencies, which are 140.5, 359, 922, 1078, and 2031 Hz, respectively. NAFSA-Net fully recovers both low-frequency components with high fidelity and the low-energy components distributed in the high-frequency part. In comparison with SEGAN, DPRNN, DCCRN, and FullSubNet, they are unable to exactly recover the original clean signal. In these methods, the weak-energy frequency components (1078 and 2031 Hz) distributed in the high-frequency part are eliminated as noise, resulting in distortion of the denoised signals. Fig. 10(g) shows the results of a single target subnet, which aggregates advantages of both long-range dependencies and short-term correlations of spectral features through the FSA block. This method retrains a small number of noise components but is still superior in suppressing underwater ambient noise compared to the previous DNN-based methods. As for the NAFSA-fullband model and the NAFSA-subband model, the former focuses on the overall structure of the spectrogram and tends to ignore the small variations of the motorboat audio. NAFSANet combines the advantages of both fullband and subband models while training the target estimation subnet with the aid of a noise prediction subnet. Our proposals accurately separate the target signal from the noisy audio with less distortion.<br><code>为了探索我们提出的NAFSA网络的去噪性能，我们进一步比较了不同基于DNN的技术中摩托艇噪声音频的去噪差异。图10展示了通过不同的基于DNN的方法处理的干净音频、有噪音频和去噪音频的幅度和PSD。从图10（j）可以看出，我们提出的NAFSA-Net在噪声抑制和干净信号恢复方面表现出显著的优越性。干净的摩托艇音频由五个主频组成，分别为140.5、359、922、1078和2031 Hz。NAFSA-Net完全恢复具有高保真度的低频分量和分布在高频部分的低能量分量。与SEGAN、DPRNN、DCCRN和FullSubNet相比，它们无法准确地恢复原始的干净信号。在这些方法中，分布在高频部分的弱能量频率分量（1078和2031 Hz）作为噪声被消除，导致去噪信号的失真。图10（g）示出了单个目标子网的结果，其通过FSA块聚合了频谱特征的长期依赖性和短期相关性两者的优点。该方法抑制了少量的噪声分量，但与以前的基于DNN的方法相比，在抑制水下环境噪声方面仍然具有上级优势。至于NAFSA-全频带模型和NAFSA-子带模型，前者关注频谱图的整体结构，往往忽略摩托艇音频的微小变化。NAFSANet结合了全带和子带模型的优点，同时在噪声预测子网的帮助下训练目标估计子网。我们的建议准确地分离目标信号从嘈杂的音频失真较小。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/24.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/25.png"></p><blockquote><p>Fig. 11. Illustration of the reference target audio with the denoised audio by DNN-based methods in the time domain.<br><code>见图11。参考目标音频与时域中基于DNN方法的去噪音频的图示。</code></p></blockquote><p>The time-domain representations of denoised signals by different DNN-based approaches are shown in Fig. 11, in which red lines indicate the reference ship audios, the black line represents the corrupted signal by underwater background noise, and blue lines correspond to denoised vessel audios by diverse DNN-based methods. In the corrupted signals with low SNRs, the spectral characteristics of target audio are completely masked by the noise, especially the weak-energy high-frequency spectral components [see Fig. 10(b)]. Although the previous techniques, such as SEGAN, DPRNN, DCCRN, and FullSubNet, outperform the conventional methods in terms of denoising performance, they fail to follow subtle variations in the high-frequency part. As a comparison, the estimated audios by our proposed single target subnet and NAFSA-subband models are in better agreement with the clean audio waveform. They capture some of the small details of the high-frequency components in the target signals. However, even so, we also find that the audio estimated by these two methods still does not match the original audio perfectly due to the presence of noise-like components in the vessel signals. Finally, we can confirm that our proposed NAFSA-Net handles the aforementioned problems, and the denoised audio is well-aligned with the reference signal.<br><code>图11中示出了通过不同的基于DNN的方法进行去噪的信号的时域表示，其中红线表示参考船舶音频，黑线表示被水下背景噪声破坏的信号，蓝线对应于通过不同的基于DNN的方法进行去噪的船舶音频。在低信噪比的受损信号中，目标音频的频谱特征完全被噪声掩盖，特别是弱能量的高频频谱分量[见图10（b）]。虽然以前的技术，如SEGAN，DPRNN，DCCRN和FullSubNet，在去噪性能方面优于传统方法，但它们无法跟踪高频部分的细微变化。作为比较，我们提出的单目标子网和NAFSA子带模型的估计音频与干净的音频波形更好地吻合。它们捕获目标信号中高频分量的一些小细节。然而，即使如此，我们也发现，这两种方法估计的音频仍然不匹配的原始音频完美，由于在血管信号中的噪声样成分的存在。最后，我们可以确认我们提出的NAFSA网络处理了上述问题，并且去噪音频与参考信号对齐良好。</code></p><p><em><strong>E. Evaluation on Unseen Datasets</strong></em><br><code>E.对未知数据集的评估</code><br>This section further explores the generalization performance of our proposed NAFSA-Net on unseen datasets. For this purpose, we design three different unseen scenarios to test our proposals. The first is the scenario of unseen ships, in which the types of ships are the same as those in the training dataset (passengerboat and motorboat), but from different ships. The second is the unseen ship types dataset, which consists of noisy audio from new vessel types (fishboat and dredger). As for the unseen noise scenario, we generate the test dataset by mixing the noise signals collected from other seas (the South China Sea) with the ship signals from ShipsEar. Fig. 12 presents the experimental results of our proposed NAFSA-Net on three different unseen datasets.<br><code>本节进一步探讨了我们提出的NAFSA-Net在看不见的数据集上的泛化性能。为此，我们设计了三个不同的看不见的场景来测试我们的建议。第一种是看不见的船只场景，其中船只的类型与训练数据集中的船只相同（快艇和摩托艇），但来自不同的船只。第二个是看不见的船舶类型数据集，其中包括来自新船舶类型（渔船和挖泥船）的嘈杂音频。对于看不见的噪声场景，我们通过将从其他海域（南中国海）收集的噪声信号与ShipsEar的船舶信号混合来生成测试数据集。图12展示了我们提出的NAFSA网络在三个不同的看不见的数据集上的实验结果。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/26.png"></p><blockquote><p>Fig. 12. Evaluation of proposed NAFSA-Net for unseen datasets in terms of SDRi and SI-SNRi. (a) Unseen ship. (b) Unseen_ship_type. (c) Unseen_noise.<br><code>见图12 根据SDRi和SI-SNRi评估拟议的NAFSA-Net用于看不见的数据集。(a)看不见的船(b)Unseen_ship_type.(c)看不见的噪音。</code></p></blockquote><p>For the unseen ship dataset [see Fig. 12(a)], we can observe that the denoising performance of our proposed NAFSA-Net is not affected. It achieves similar SDRi and SI-SNRi scores on the new dataset as before. Besides, for the unseen ship type dataset [see Fig. 12(b)], NAFSA-Net improves SDRi by an average of 21.96 dB and SI-SNRi by an average of 21.88 dB on the output results. Different ships, especially when they are from different types, tend to demonstrate unique spectral characteristics, but the performance of NAFSA-Net only marginally degrades the SDRi by 3.1% and SI-SNRi by 2.5% on the unseen ship types dataset. The results of the spectrum analysis of the unseen ship example and unseen ship type example are shown in Figs. 13 and 14, respectively. Nonetheless, for the unseen noise dataset [see Fig. 12(c)], the results processed by the model showed a slight improvement in each metric compared to the original noisy signals. One possible reason is that the noise samples (rain, flow, and wind) in the ShipsEar dataset are collected from the sea surface, while the comparison noise samples are taken from the underwater environment. There are major differences in the spectral characteristics of the noise in the two environments. All the above results suggest that our proposed model is able to effectively deal with unseen data with similar noise features while generalizing poorly on the unseen noise dataset.<br><code>对于看不见的船舶数据集[见图12（a）]，我们可以观察到我们提出的NAFSA-Net的去噪性能不受影响。它在新数据集上实现了与以前类似的SDRi和SI-SNRi分数。此外，对于不可见的船型数据集[见图12（b）]，NAFSA-Net在输出结果上将SDRi平均提高了21.96 dB，将SI-SNRi平均提高了21.88 dB。不同的船舶，特别是来自不同类型的船舶，往往会表现出独特的光谱特征，但NAFSA-Net的性能在看不见的船舶类型数据集上仅使SDRi降低3.1%，SI-SNRi降低2.5%。未看见船舶实例和未看见船型实例的频谱分析结果示于图1和图2中。分别为13和14。尽管如此，对于看不见的噪声数据集[见图12（c）]，与原始噪声信号相比，模型处理的结果显示每个度量都略有改善。一个可能的原因是ShipsEar数据集中的噪声样本（雨、流和风）是从海面收集的，而比较噪声样本是从水下环境中采集的。在两种环境中噪声的频谱特性有很大差异。所有上述结果表明，我们提出的模型能够有效地处理具有相似噪声特征的不可见数据，而在不可见噪声数据集上的泛化能力较差。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/27.png"></p><blockquote><p>Fig. 13. T-F magnitude and PSD of clean audio, noisy audio, and denoised audio for an unseen ship (passengerboat). (a) Magnitude spectra of the clean signal. (b) PSD of the clean signal. (c) Magnitude spectra of the noisy signal. (d) PSD of the noisy signal. (e) Magnitude spectra of the denoised signal. (f) PSD of the denoised signal.<br><code>图13.一艘看不见的船（快艇）的干净音频、嘈杂音频和去噪音频的T-F幅度和PSD。(a)干净信号的幅度谱。(b)干净信号的PSD。(c)噪声信号的幅度谱。(d)噪声信号的PSD。(e)去噪信号的幅度谱。(f)去噪信号的PSD。</code></p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/28.png"></p><blockquote><p>Fig. 14. T-F magnitude and PSD of clean audio, noisy audio, and denoised audio for an unseen ship type sample (fishboat). (a) Magnitude spectra of the clean signal. (b) PSD of the clean signal. (c) Magnitude spectra of the noisy signal. (d) PSD of the noisy signal. (e) Magnitude spectra of the denoised signal. (f) PSD of the denoised signal.<br><code>见图14。对于看不见的船型样本（渔船），干净音频、嘈杂音频和去噪音频的T-F幅度和PSD。(a)干净信号的幅度谱。(b)干净信号的PSD。(c)噪声信号的幅度谱。(d)噪声信号的PSD。(e)去噪信号的幅度谱。(f)去噪信号的PSD。</code></p></blockquote><p>F. Preliminary Exploration on Unseen Noise Dataset<br><code>F.不可见噪声数据集初探</code><br>Section V-E points out that the denoising performance degrades significantly when the trained NAFSA-Net is directly applied to the new noise dataset. In light of this, we have purposefully explored some possible solutions. From the previous analysis, we know that NAFSA-Net shows good generalization for unknown target ship datasets. Therefore, a straightforward approach is to construct a new dataset using existing ship data and noise data from the target sea area and retrain the model on the new dataset. In addition, we can use a small number of samples from the target sea to fine-tune our pretrained NAFSA-Net. To verify the effectiveness of the two methods, we construct two datasets using the noise signals from the South China Sea and the ship signals from the ShipsEar dataset. These two datasets contain 21 313 noisy audios with SNRs in the range of [−15 dB, −5 dB] and [−20 dB, 10 dB], respectively. All noisy audios are resampled to 16 kHz and clipped to 3 s. Table V shows the experimental results of the two schemes on the unseen noise dataset.<br><code>第V-E节指出，当训练好的NAFSA-Net直接应用于新的噪声数据集时，去噪性能会显着下降。有鉴于此，我们有目的地探索了一些可能的解决方案。从前面的分析中，我们知道NAFSA网络对未知目标船数据集表现出良好的泛化能力。因此，一种直接的方法是使用来自目标海域的现有船舶数据和噪声数据构建新的数据集，并在新的数据集上重新训练模型。此外，我们可以使用来自目标海域的少量样本来微调预训练的NAFSA网络。为了验证这两种方法的有效性，我们使用来自南海的噪声信号和来自ShipsEar数据集的船舶信号构建了两个数据集。这两个数据集包含21313个噪声音频，SNR分别在[−15 dB，−5 dB]和[−20 dB，10 dB]范围内。所有嘈杂的音频都被重新采样到16 kHz，并被限幅到3 s。表V显示了两种方案在不可见噪声数据集上的实验结果。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/an/29.png"></p><blockquote><p>TABLE V QUANTITATIVE COMPARISONS OF NAFSA-NET ON UNSEEN DATASETS D-I ([−15 DB, −5 DB]) AND D-II ([−20 DB, −10 DB])<br><code>表V NAFSA-NET在未观测数据集D-I（[-15 DB，-5 DB]）和D-II（[-20 DB，-10 DB]）上的定量比较</code></p></blockquote><p>We first evaluate our proposed NAFSA-Net on the D-I dataset. As shown in Table V, “NAFSA-Net(retrain)” and “NAFSA-Net(finetune)” achieve similar results on each evaluation metric. Compared to the original noisy audios, these two schemes improve the SDR by an average of 20.13 dB and SI-SNR by an average of 21.53 dB. These results demonstrate that both solutions are effective for handling unseen noise datasets with an SNR range of [−15 dB, −5 dB]. When it comes to the dataset with lower SNR, we can observe that the retraining approach achieves a mere 10.13-dB gain on SDR and 25.58-dB gain on SI-SNR. As a comparison, the fine-tuning method further improves the SDR and SISNR scores by 9.78 and 9.88 dB compared to the former. These analyses point out that both the retraining method and the fine-tuning scheme can effectively address the adaptation of the model to an unseen noise dataset, while the fine-tuning scheme shows greater potential for improving the denoising performance of our model in low SNR conditions.<br><code>我们首先在D-I数据集上评估我们提出的NAFSA网络。如表V所示，“NAFSA-Net（retrain）”和“NAFSA-Net（finetune）”在每个评价指标上实现了类似的结果。与原始噪声音频相比，这两种方案平均提高了20.13 dB的SDR和21.53 dB的SI-SNR。这些结果表明，这两种解决方案都能有效处理SNR范围为[−15 dB，−5 dB]的不可见噪声数据集。当涉及到具有较低SNR的数据集时，我们可以观察到再训练方法在SDR上仅实现了10.13 dB增益，在SI-SNR上仅实现了25.58 dB增益。作为比较，与前者相比，微调方法进一步将SDR和SISNR分数提高了9.78和9.88 dB。这些分析指出，再训练方法和微调方案都可以有效地解决模型对不可见噪声数据集的适应问题，而微调方案在低信噪比条件下提高模型的去噪性能方面具有更大的潜力。</code></p><h1 id="VI-CONCLUSION"><a href="#VI-CONCLUSION" class="headerlink" title="VI. CONCLUSION"></a>VI. CONCLUSION</h1><p><code>六.结论</code><br>In this article, a novel NAFSA-Net is introduced for underwater acoustic signal denoising in the case of extremely low SNR. This solution uses different subnets to estimate both the noise and the target audio, and the noise subnet is designed to assist in the training of the target subnet. We have trained NAFSA-Net on the seen and unseen datasets generated by real ship signals and underwater ambient noise signals. Experimental results have illustrated the superiority over conventional methods and DNN-based denoising approaches.<br><code>本文提出了一种新的NAFSA网络，用于极低信噪比情况下的水声信号去噪。该解决方案使用不同的带宽来估计噪声和目标音频，并且噪声子网被设计为辅助目标子网的训练。我们已经在由真实的船舶信号和水下环境噪声信号生成的可见和不可见数据集上训练了NAFSA-Net。实验结果表明，该方法优于传统方法和基于DNN的去噪方法。</code><br>We have found that the noise-aware learning method can substantially improve the ability of handling underwater acoustic signals with very low SNRs. Compared with the single target subnet, NAFSA-Net achieves a 59.35% gain on SDRi and an 18.42% gain on SI-SNRi. Furthermore, we have evaluated different feature extraction strategies, i.e., single fullband attention, single subband attention, and FSA, and found that a combination of fullband attention and subband attention shows great advantages in extracting features of underwater acoustic audios. In addition, we have trained NAFSA-Net with two different wSI-SNR loss functions. Our proposed iwSI-SNR loss function is guided by the target audio and integrates the output information of both subnets, which shows greater potential in optimizing the training of our model. Under the same setting conditions, the iwSI-SNR loss function helps NAFSA-Net improve the SDR by 2.68 dB, the SI-SNRi by 0.88 dB, and the SSNR by 2.77 dB compared with the wSI-SNR loss function.<br><code>我们已经发现，噪声感知学习方法可以大大提高处理非常低的信噪比的水声信号的能力。与单一目标子网相比，NAFSA-Net在SDRi上获得了59.35%的增益，在SI-SNRi上获得了18.42%的增益。此外，我们还评估了不同的特征提取策略，即，单全带注意、单子带注意和FSA的研究结果表明，全带注意和子带注意相结合的方法在提取水声音频特征方面具有很大的优势。此外，我们还使用两种不同的wSI-SNR损失函数训练了NAFSA-Net。我们提出的iwSI-SNR损失函数由目标音频引导，并整合了两个音频的输出信息，这在优化我们模型的训练方面显示出更大的潜力。在相同的设置条件下，与wSI-SNR损失函数相比，iwSI-SNR损失函数帮助NAFSA-Net将SDR提高了2.68 dB，SI-SNRi提高了0.88 dB，SSNR提高了2.77 dB。</code><br>We have evaluated NAFSA-Net against three different unseen datasets. Experimental results show that our proposed NAFSA-Net has good generalization for unseen target ship audios. However, the denoising performance of the model degrades significantly in the unseen noise dataset. To this end, we further explored possible solutions to the problem of adaptation to unseen noise datasets. Therefore, a future research direction could be optimizing the generalization of the model in different noise environments and exploring the possibilities of using the same model to process different marine environmental data.<br><code>我们已经针对三个不同的看不见的数据集对NAFSA网络进行了评估。实验结果表明，我们提出的NAFSA网络具有良好的泛化能力，不可见的目标船舶音频。然而，该模型的去噪性能在不可见的噪声数据集中显着下降。为此，我们进一步探索了适应不可见噪声数据集问题的可能解决方案。因此，优化模型在不同噪声环境下的泛化能力，探索用同一模型处理不同海洋环境数据的可能性，是未来的研究方向。</code></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文翻译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文11】Swin Transformer论文逐段精读</title>
      <link href="/2023/12/06/lun-wen-11-swin-transformer-lun-wen-zhu-duan-jing-du/"/>
      <url>/2023/12/06/lun-wen-11-swin-transformer-lun-wen-zhu-duan-jing-du/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Swin Transformer是 ICCV 21的最佳论文，它之所以能有这么大的影响力，主要是因为在 ViT 之后，Swin Transformer通过在一系列视觉任务上的强大表现，进一步证明了Transformer是可以在视觉领域取得广泛应用的。</p><p>我们先来看一下, Swin Transformer这个代码库，我们可以看到这里有一系列的更新，Swin Transformer是3月份传到 arxiv上的，然后4月份这代码库就放出来了，然后紧接着5月份，5月12号他们就又放出来了，这个自监督版本的 Swin Transformer，他们管他们的方法叫 moby，moby其实就是把MoCo的前两个字母和 BYOL 的前两个字母合在了一起，从方法上和性能上其实和MoCo v3和DINO呢都是差不多的，只是换了个骨干网络，所以我们上次这个对比学习串讲也没有提这篇论文。接下来又过了一个月，Swin Transformer 就又被用到了这个视频领域，他们推出了这个 Video-Swin-Transformer，然后在一系列数据集上都取得了非常好的效果，比如说在 k-400这个数据集上就已经达到了84.9的这个准确度，然后紧接着又过了几天，就在7月初的时候，因为看到了有 MLP Mixer 这篇论文，他们又把 Swin 的这个思想用到了这个 MLP 里，推出了这个 Swin MLP，然后又过了一个月，在8月初的时候，他们又把 Swin Transformer 用到了半监督的这个目标检测里，然后取得了非常好的效果，然后10月份的时候，他们就获得了ICCV 的这个最佳论文奖，然后到12月份受到了 BEiT 和 MAE 的这个推动，他们又用 Swin Transformer基于这个掩码自监督学习的方式做了一个叫 SimMIM 的论文。</p><p>所以说在这大半年的时间里，这个原作者团队就以每个月一篇论文的速度，基本把视觉领域所有的任务都刷了个遍，而且Swin Transformer 不光是应用范围广，他的效果也非常的炸裂。</p><p>那我们现在就再回到 Paperswithcode 的这个网站上，看一下它到底在每个数据集上的表现如何，鉴于 Swin Transformer 的提出主要是用来做视觉的这个下游任务的，所以这里呢我们就看一下 COCO 和 ADE20K这个是两个数据集上它的表现，我们现在往下拉看一下这个排行榜，然后你就会发现排名第一的是一个叫 Swin V2 的模型，其实也就是作者原班人马提出的这个Version2，他们就是做了一个更大版本的这个 Swin Transformer ，有30亿参数，而且提出了一系列技术，使得这个 Swin Transformer ，可以在1536乘1536这么大的图片上去做预训练，最后这个下游任务的效果就非常的好，这里我们可以看到 COCO 都已经被刷到63.1了，而大概去年这个时候，大家用卷积神经网络的时候达到在54、55的这个准确度上挣扎。然后我们往下看排名第二的其实是一个叫 Florence 的模型，这个是一个多模态的工作，它里面负责视觉的那一部分用的是这个叫 CoSwin 的这个模型，也是 Swin Transformer 的一个变体了，然后再往下我们就可以看到GLIP 也是用的 Swin large，Soft Teacher 也是 Swin large，然后 DyHead Swin large，总之排名前十的方法全都用到了 Swin Transformer。那我们现在来看 ADE20K这个数据集，我们可以看到排名第一的还是这个 Swin V2，因为模型实在是太大了，然后接下来排名二三四的都是叫一个 SeMask 的论文，他也是基于 Swin large 的，但是第五名这个 BEiT 用的是 ViT，而不是 Swin，但是紧接着后面排名的这个6、7、8、9全都还是用的是 Swin Transformer。所以说在 Swin Transformer作者团队不懈的努力之下，Swin Transformer 在大部分视觉领域上很多数据集上都取得了最好的结果，所以这就导致 Swin Transformer成了视觉领域一个绕不开的Basler，接下来再想在这些数据集上刷分或者说再想发这些领域的论文，那多多少少都得提到Swin Transformer或者跟它比，所以说 它对这个影响力是巨大的。</p><h1 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h1><p><em><strong>Swin Transformer: Hierarchical ViT using Shifted Windows</strong></em><br>题目说 Swin Transformer是一个用了移动窗口的层级式的Vision Transformer，那这个 Swin 这个名字其实也就来自于 Shifted Windows ，就是这个 S 和 win，而这个 Shifted Window这移动窗口也是 Swin Transformer这篇论文的主要贡献。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-1.png"><br>而这个层级式 Hierarchical也可以从底下的这个图1里的 a，可以简单的看出来到底在干什么，其实Swin Transformer就是想让 Vision Transformer像卷积神经网络一样，也能够分成几个 block，也能做这种层级式的这个特征提取，从而导致提出来的特征有这个多尺度的概念。</p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>作者说这篇论文提出了一个新的 Vision Transformer，叫做 Swin Transformer，它可以被用来作为一个计算机视觉领域一个通用的骨干网络。</p><blockquote><p>他为什么这么说呢，因为我们上次在讲ViT的论文的时候，ViT在结论的部分指出，他们那篇论文呢只是做了分类这个任务，他们把下游任务比如说检测和分割留给以后的人去探索了，所以说在 ViT 出来之后，大家虽然看到了Transformer，在视觉领域的这个强大的潜力，但是大家并不确定Transformer能不能把所有视觉的任务都做掉，所以 Swin Transformer这篇论文的研究动机就是想来告诉大家用 Transformer没毛病，绝对能在方方面面上取代卷积神经网络，接下来大家都上 Transformer就好了。</p></blockquote><p>然后接下来作者说，但是你直接把Transformer从这个 NLP 用到 Vision 这边是有一些挑战的，这个挑战呢主要来自于两个方面。</p><p>一个就是这个尺度上的问题，因为比如说你现在有一张街景的图片里面有很多车和行人，里面的物体都大大小小，那这时候代表同样一个语义的词，比如说行人或者汽车，他就有非常不同的尺寸，那这种现象在 NLP 那边就没有。<br>另外一个挑战也是之前我们提到过很多次的，就是说这个图像的这个 resolution太大了，如果我们要以这个像素点作为基本单位的话，这个序列的长度就变得高不可攀。</p><p>所以说之前的工作要么就是用这个后续的特征图来当做Transformer的输入，要么就是把图片打成 patch减少这个图片的 resolution，要么就是把图片画成一个一个的小窗口然后在窗口里面去做这个自注意力。</p><p>所有的这些方法都是为了减少这个序列程度。</p><p>那基于这两个挑战，本文的作者就提出了这个 hierarchical Transformer，它的这个特征是通过一种叫做移动窗口的方式学来的。</p><p>然后作者紧接着说移动窗口的好处，不仅带来了更大的这个效率，因为跟之前的工作一样，现在这个自注意力是在这个窗口内算的，所以这个序列的长度大大的降低了，同时通过shifting就移动的这个操作能够让相邻的两个窗口之间有了交互，所以上下层之间就可以有这种cross-window connection，从而变相的达到了一种这个全局建模的能力。</p><p>然后作者说这种层级式的结构的好处，它不仅非常灵活，可以提供各个尺度的这个特征信息，同时因为它这个自注意力是在这个小窗口之内算的，所以说它的这个计算复杂度是随着这个图像大小而线性增长，而不是平方级增长，这其实也为作者之后提出 Swin V2 铺平了道路，从而让他们可以在特别大的分辨率上去预训练这个模型。<br>然后因为Swin Transformer拥有了像卷积神经网络一样这种分层的结构，有了这种多尺度的特征，所以它就很容易使用到这种下游任务里。</p><p>所以在这篇论文里，作者不光是在 ImageNet-1K 上做了实验，而且达到了非常好的准确度87.3，而且还在密集预测型的任务上，比如说就是物体检测，还有物体分割上取得了很好的成绩，比如说在 COCO 上，他们都刷到58.7的这个 AP，比之前最好的方法呢是高了2.7个点，然后在语义分割上呢, ADE上，他们也刷到了53.5的这个效果，比之前最好的方法呢高了3.2个点。</p><blockquote><p>那这些数据集其实都是大家常刷的数据集，在上面往往你只要能提升一个点，甚至可能不到一个点，只要你故事讲的好可能都能发论文，但是Swin Transformer提了大概3个点，对这个提升是相当显著的。</p></blockquote><p>所以作者这里接着说，这种基于 Transformer 的这个模型在视觉领域是非常有潜力的，然后为了凸显他们这篇文章的贡献，也就是 Shifted Windows移动窗口这个作用，他们在这个版本又加了一句话，他们说对于这种 MLP 的这种架构，他们用这种 shift window 的方法也能提升，这句话其实这个版本才加入的，他们之前第一个版本就是投稿上那篇论文其实没有这句话的，因为当时还没有MLP Mixer这篇论文。</p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-1.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-2.png"><br>这个引言还是相对比较长的，而且里面有两张图，图一作者就是大概介绍了一下Swin Transformer想干个什么事，图二作者就介绍了一下Swin Transformer主要的一个贡献，就是这个shifted window移动窗口。</p><p>引言的前两段其实跟ViT是非常一致的，都是先说在视觉领域之前卷积神经网络是主导地位，但是呢Transformer在NLP领域又用的这么好，所以我们也想把Transformer用到视觉领域里面来。</p><p>但因为ViT已经把这件事干了，所以说Swin Transformer在第三段的开始，他说他们的研究动机呢是想证明Transformer是可以用作一个通用的骨干网络，就是对所有视觉的任务，不光是分类，在检测、分割视频上也都能取得很好的效果。</p><p>那不看文字，我们先来看一下它这个图一。</p><blockquote><p>一般像写的好的论文，尤其是这种已经得了最佳论文的论文，它的图一应该是画的非常好了，就是说你只看这张图，你就大概知道这篇论文在讲什么了。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-1.png"><br><code>Figure 1. (a) The proposed Swin Transformer builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. (b) In contrast, previous vision Transformers [19] produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of self-attention globally.</code><br><code>图1.(a)所提出的Swin Transformer通过在更深层中合并图像块（以灰色示出）来构建分层特征映射，并且由于仅在每个局部窗口（以红色示出）内计算自我注意，因此具有与输入图像大小线性的计算复杂度。因此，它可以作为图像分类和密集识别任务的通用骨干。(b)相比之下，以前的视觉变换器[19]产生单个低分辨率的特征图，并且由于全局计算自我注意力而具有输入图像大小的二次计算复杂度。</code></p><p>作者这里先说了一下Vision Transformer，把它放在右边做一个对比，他说Vision Transformer 干一件什么事呢，就是说把这个图片打成patch，因为ViT里用的这个patch size是16x16的，所以说他这里的这个16 ×，也就意味着是16倍的这个下采样率，这也就意味着这里的每一个 patch，也就是每一个 token，它自始至终代表的这个尺寸都是差不多的，它每一层的这个Transformer block看到这个token的尺寸，都是这个16倍下采样率，16倍、16倍下采样率。虽然它可以通过这种全局的自注意力操作达到这个全局的建模能力，但是它对多尺寸特征的这个把握呢就会弱一些。</p><blockquote><p>那我们知道对于视觉任务，尤其是这些下游任务，比如说检测和分割来说，这个多尺寸的特征是至关重要的。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-3.png"><br>比如说对目标检测而言，运用最广的一个方法就是FPN，a feature pyramid network，它的意思就是说，当你有一个分层式的这种卷积神经网络之后，你每一个卷机层出来的那些特征，它的这个 receptive field，感受也是不一样的，能抓住物体这个不同尺寸的特征，从而能够很好的处理这个物体不同尺寸的这个问题。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-4.png"><br>那另外对于物体分割这个任务来说，那最常见的一个网络就是UNet，那UNet里为了处理物体这个不同尺寸的问题，它们就提出来一个这个skip connection这个方法，意思就是说，当你一系列这个下采样做完以后，你现在去做上采样的时候，你不光是从这个bottleneck里去拿特征，你还从之前也就是每次下采样完之后的这个东西里去拿特征，这样你就把那些高频率的这些图像细节又全都能恢复出来了。<br>当然分割里大家常用的网络结构还有 PspNet，还有 DeepLab，这些工作里也有相应的处理多尺寸的方法，比如说使用空洞卷机，比如说使用psp和aspp层。总之，对于计算机视觉的这些下游任务，尤其是这些密集预测型的任务检测、分割，有多尺寸的特征是至关重要的。</p></blockquote><p>那我们现在回到Swin Transformer，作者就说但是在 ViT 里，它处理的特征都是单一尺寸，而且是这个low resolution，也就是说自始至终都是处理的这个16倍下采样率过后的特征，所以说，它可能就不适合处理这种密集预测型的任务。同时对 ViT 而言。它的这个自注意力始终都是在这个最大的窗口上进行，就是始终都是在整图上进行的，所以它是一个全局建模，所以说它的这个复杂度是跟这个图像的尺寸进行平方倍的增长。</p><blockquote><p>那像检测和分割领域，那一般大家现在常用的这个输入尺寸，都是800乘以800或者1000乘1000了，那你之前虽然用patch size 16能处理这种24x24的图片，但是当你这个图片变到这么大的时候，即使你用patch size16，你的这个序列长度呢还是会上千，这个计算复杂度还是难以承受的。</p></blockquote><p>所以基于这些挑战，作者提出了Swin Transformer。Swin Transformer其实是借鉴了很多，卷积神经网络的这个设计理念，以及它的先验知识。比如说呢为了减少这个序列的长度，降低这个计算复杂度，所以Swin Transformer采取了在这种小窗口之内算这个自注意力，而不是像ViT一样在整图上去算自注意力。<br>这样只要你这个窗口大小是固定的，你这个自注意力的这个计算复杂度呢就是固定的，那整张图的这个计算复杂度就会跟这张图片的大小而成的线性增长关系。就是说如果你图片增大了x倍，那你的窗口数量呢也就增大了x倍，那你的计算复杂度呢也就是乘以x，而不是乘以x的平方。<br>那这个其实就算利用了卷积神经网络里的这个Locality 的这个 Inductive bias，就是利用了这个局部性的这个先验知识，就是说同一个物体的不同部位或者语义相近的不同物体还是大概率会出现在相连的地方，所以即使我是在一个 Local、一个小范围的窗口，那去算这个自注意力，那也是差不多够用的。全局去算这个自注意力对于视觉任务来说，其实是有点浪费资源的。</p><blockquote><p>那另外一个挑战就说，我们如何去生成这个多尺寸的特征呢。<br>那我们继续回想卷积神经网络，卷积神经网络为什么会有这种多尺寸的特征呢，主要是因为有Pooling池化这个操作，池化这个操作能够增大每一个这个卷积核能看到的这个感受野，从而使得每次池化过后的这个特征抓住物体的这个不同尺寸。</p></blockquote><p>所以类似的Swin Transformer这里也提出来了一个类似于池化的操作，叫做patch merging，就是把相邻的这个小patch合成一个大patch，那这样合并出来的这一个大patch，其实就能看到之前四个小patch看到的内容，它的这个感受野就增大了，同时它也能抓住这个多尺寸的特征。<br>所以也是像图一里左边画的这样，Swin Transformer刚开始的这个下采样率是4倍，然后变成了8倍、16倍，之所以刚开始是4×的，它最开始的这个patch是4乘4大小的，那一旦你有了这种多尺寸的特征信息， 你有了这种4乘8乘16乘的这个特征图，那很自然的，你就可以把这些多尺寸的特征图呢输给一个 FPN，从而你就可以去做检测了。那同样的道理，你有了这些多尺寸的特征图以后，你也可以把它扔给一个 UNET，然后它就可以去做分割了。</p><p>所以这就是作者在这篇论文里反复强调的，Swin Transformer是能够当做一个通用的骨干网络的，不光是能做这个图像分类，它还能做这种密集预测性的任务。</p><p><em><strong>第四段</strong></em><br>那按照图一讲完引言的第三段，作者在第四段主要就开始讲Swin Transformer一个关键的设计因素，也就是这个移动窗口的操作，具体的内容呢我们直接来看图二。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-2.png"><br><code>Figure 2. An illustration of the shifted window approach for computing self-attention in the proposed Swin Transformer architecture. In layer l (left), a regular window partitioning scheme is adopted, and self-attention is computed within each window. In the next layer l + 1 (right), the window partitioning is shifted, resulting in new windows. The self-attention computation in the new windows crosses the boundaries of the previous windows in layer l, providing connections among them.</code><br><code>图2.在Swin Transformer架构中计算自我注意力的移位窗口方法的说明。在层l（左）中，采用常规窗口划分方案，并且在每个窗口内计算自注意。在下一层l + 1（右）中，窗口分区被移位，从而产生新的窗口。新窗口中的自注意力计算跨越层l中的先前窗口的边界，提供它们之间的连接。</code><br>图二里它首先说，如果在这个Transformer第L层把这个输入或者说这个特征图分成这种小窗口的话，那就会有效的降低序列程度，从而减少这个计算复杂度。<br>那作者右边的这个图里也说了，这个灰色的小patch，就每一个这个小 patch是最基本的这个元素单元，也就是4x4的那个patch，然后每个红色的框是一个中型的计算单元，也就是一个窗口。</p><p>在Swin Transformer这篇论文里，一个小窗口里面默认是有七七四十九个小patch的，那在这里就是画个示意图，主要是来讲解这个 shift 的操作是怎么完成的，那如果我们现在用一个大的这个蓝色的正方形来描述这个整体的特征图呢，其实shift这个操作呢，就是往右下角的方向整体移了两个 patch，也就变成了像右图这样的格式，然后我们在新的这个特征图里去把它再次分成这个四方格，那最后shift完，我们就有这么多窗口了，这样的好处就是说窗口与窗口之间现在可以进行互动了。<br>因为如果我们按照原来的方式，就是没有shift，那这些窗口它们之间都是不重叠的，那如果每次自注意力的操作都在这个小的窗口里头进行了，那这个窗口里的patch就永远无法注意到别的窗口里的那些patch的信息，这就达不到使用Transformer的初衷了。因为Transformer的初衷就是更好的理解上下文，那现在如果你这些窗口都是不重叠的，那这个自注意力真的就变成一个孤立自注意力了，它就没有这种全局建模的能力。<br>但现在如果我们加上这个shift的操作，比如说这个patch，原来就只能跟这个窗口里的别的patch去进行交互，但是现在你shift之后，这个patch就可以跟新的窗口里的别的 patch就进行交互了，而这个新的窗口里所有的patch其实来自于上一层别的窗口里的这些patch，这也就是作者这里说的，能起到一个cross-window connection，就是窗口和窗口之间可以交互了，那在配合上之后提出了这个patch merging，那合并到这个Transformer最后几层的时候，他每一个 patch本身的感受野就已经很大了，就已经能看到大部分图片了，然后再加上这个移动窗口的操作，现在它所谓的这种窗口内的局部注意力，其实也就变相的等于是一个全局的自注意力操作了，这样就是既省内存效果也好，所以一石二鸟。</p><p><em><strong>第五段</strong></em><br>接下来第五段，作者就再次卖了一下结果，因为Swin Transformer的结果确实非常好。</p><p><em><strong>第六段</strong></em><br>然后最后一段，作者就展望了一下，作者说他们坚信一个 CV 和NLP 之间大一统的框架是能够促进两个领域共同发展的。</p><blockquote><p>这个也确实如此，因为我们人在学习的过程中也是一个多模态的学习过程，但我觉得Swin Transformer还是利用了更多的这个视觉里的这个先验知识，从而在视觉任务上呢大杀四方，但是在这个模型大一统上，也就是这个unified architecture上来说，其实 ViT 还是做的更好的，因为它真的可以什么都不改，什么先验信息都不加，就能让Transformer在两个领域都能用的很好，那这样模型不仅可以共享参数，而且我们甚至可以把所有模态的这个输入直接就拼接起来，当成一个很长的输入，直接扔给这个Transformer去做，而不用考虑每个模态的它的特性。</p></blockquote><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p><em><strong>第一段</strong></em><br>结论的第一段是非常中规中矩的，上来就说这篇论文提出了Swin Transformer，它是一个层级式的Transformer，而且它的计算复杂度是跟这个输入图像的大小呈这个线性增长的，然后作者又说了一下Swin Transformerr在COCO和ADE20K上的这个效果都非常的好，远远的超越了之前这个最好的方法，所以作者说基于此，我们希望这个 Swin Transformer能够激发出更多更好的工作，尤其是在多模态方面。</p><p><em><strong>第二段</strong></em><br>然后这个结论的第二段呢是非常有意义的，作者说因为在Swin Transformer这篇论文里，它最关键的一个贡献就是这个基于Shifted Window的这个自注意力，这个东西对很多视觉的任务，尤其是对这些下游任务，这些密集预测型的任务是非常有帮助的。</p><blockquote><p>但是也就跟我们刚才在引言最后一段说到的一样，如果这个Shifted Window这个操作不能用到NLP领域里，那其实在模型大一统上，那这个论据就不是那么强了。</p></blockquote><p>所以作者说接下来他们的这个未来工作，就是要把这个Shifted Windows用到这个NLP里面，而且如果真的能做到这一点，那Swin Transformer真的就是一个里程碑式的工作了，而且这个模型大一统的故事也就讲的圆满了。</p><h1 id="模型整体架构"><a href="#模型整体架构" class="headerlink" title="模型整体架构"></a>模型整体架构</h1><blockquote><p>那接下来我们回到正文一起读一下这个主体的方法部分，鉴于这个相关工作跟ViT的相关工作是非常相似的，所以我们这里就不再复述了。作者就是先大概讲了一下卷积神经网络，然后又讲了一下自注意力，或者Transformer是如何用来帮助卷积神经网络的，最后就是纯的Transformer用来做这个视觉里的骨干网络。</p></blockquote><p>那我们现在直接就来看一下方法部分，作者在这个章节主要分了两个大块。<br>第一个大块是3.1，作者就是大概把整体的流程讲了一下，主要就是过了一下这个前向过程，以及提出的这个patch merging这个操作是怎么做的。<br>第二个大块基于这种Shifted Window的自注意力，Swin Transformer是怎么把它变成一个transformer block，然后进行计算。<br>那我们现在直接就对着这个模型总览图来过一遍这个模型的前向过程，这个比对着文字讲还要清晰很多。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-5.png"><br><code>Figure 3. (a) The architecture of a Swin Transformer (Swin-T); (b) two successive Swin Transformer Blocks (notation presented with Eq. (3)). W-MSA and SW-MSA are multi-head self attention modules with regular and shifted windowing configurations, respectively.</code><br><code>图3.(a)Swin Transformer（Swin-T）的架构;（B）两个连续的Swin Transformer块（用等式1表示的符号）;（三））。W-MSA和SW-MSA分别是具有规则和移位窗口配置的多头自注意模块。</code></p><p>那假设说我们现在有一张输入图片，那就是ImageNet的标准尺寸，224x224x3的一张图片，那首先第一步就是像ViT那样把这个图片打成patch，那在Swin Transformer这篇论文里，它的patch size是4x4，而不是像ViT一样16x16，所以说它经过这层 patch partition，就是打成 patch 之后，得到这个图片的尺寸是56x56x48。</p><blockquote><p>56就是224/4，因为你 patchsize是4，然而这个向量的维度48是因为4x4x3，3就是图片的这个 RGB 通道。</p></blockquote><p>然后打完了patch，接下来就要做这 Linear Embedding，也就是说我们要把这个向量的维度变成一个我们预先设置好的值，就是这个Transformer能够接受的值，那在Swin Transformer的论文里，它把这个超参数呢设为c，对于Swin tiny这个网络来说，也就是这个图里画的这个网络总览图，它的c是96，所以说经历完这个Linear Embedding之后，我们这个输入的尺寸就变成了56x56x96，那前面的56x56就会拉直变成3136，变成了这个序列长度，后面这个96就变成了每一个token的这个向量的维度。</p><blockquote><p>其实这个Patch Partition和 Linear Embedding就相当于是ViT里的那个Patch Projection那一步操作，而这个在代码里也是用一次卷积操作就完成了，那这个第一部分，跟ViT其实还是没有区别的。</p></blockquote><p>但紧接着区别就来了，那首先我们可以看到，这个序列长度现在可是3136这么长，如果大家还记得的话，对于ViT来说，它用patch size 16x16，它的序列长度就只有196那么长，是相对短很多的，那这里的3136就太长了，是目前来说Transformer不能接受的这个序列长度，那怎么办呢，所以Swin Transformer就引入了这种基于窗口的自注意力计算，那每个窗口按照默认来说，它都只有七七四十九个 patch，所以说序列长度就只有49就相当小了，这样就解决了这个计算复杂度的问题。</p><p>所以也就是说，这里这个swing transformer block是基于窗口去算自注意力的，至于每一个block里具体做了什么，我们接下来马上就讲，我们现在暂时先把这个 transformer block当成是一个黑盒，我们只关注这个输入和输出的这个维度。</p><p>那大家也知道，如果你不对Transformer去做更多的约束的话，那Transformer输入的序列程度是多少，那它输出的序列长度也是多少，它的这个输入输出的尺寸是不变的，所以说在经过这两层Swin Transformer block之后，我们的这个输出还是56x56x96。</p><blockquote><p>那到这，其实Swin Transformer的第一个阶段就走完了，也就是先过一个Patch Projection层，然后再过一些 Swin Transformer block，那接下来，如果想要有多尺寸的这个特征信息，那就要构建一个层级式的transformer，也就是说我们需要一个像卷积神经网络里一样，有一个池化的操作。</p></blockquote><p>那在这篇论文里，作者就提出来这个Patch Merging的操作。</p><blockquote><p>Patch Merging其实在之前一些工作里也有用到，我个人觉得它很像之前一个操作的一个反过程，就是Pixel Shuffle的那个上采样，Pixel Shuffle是lower level任务中很常用的一个上采样的方式。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-6.png"><br>那么我们现在来简单看一下Patch Merging到底是怎么做的，假如说我们这里举个简单的例子，这有一个张量，那Patch Merging，顾名思义，就是把临近的小patch合并成一个大 patch，这样就可以起到下采样一个特征图的效果了，那这里呢因为我们是想下采样两倍，所以说我们在选点的时候，是每隔一个点选一个，也就意味着说对于这个张量来说，我每次选的点是1、1、1、1，所以其实在这里的1、2、3、4，并不是这个矩阵里有的值，而是我给它的一个序号，同样序号位置上的这个patch就会被 merge 到一起，对这个序号只是为了帮助理解。那经过我们这个隔一个点采一个样之后，我们原来的这一个张量就变成了四个张量，也就是说所有的这个1都在一起了，2也在一起，3在一起，4在一起。如果说原来那个张量的维度呢是 <code>h * w * c</code> , 当然我这里 c 没有画出来，假设说 c 是这个维度，那经过这次采样之后，我们就得到了4个张量，每个张量的大小呢是 h/2、w/2，它的这个尺寸都缩小了一倍，那现在我把这四个张量在 c 的这个维度上拼接起来，也就变成了这种形式，那这个张量的大小就变成了<code>h/2 * w/2 * 4c</code>，就是说他相当于用空间上的维度去换了更多的这个通道数。</p><blockquote><p>通过这么一个操作，我们就把原来一个大的张量就变小了，就像卷积神经网络里的这个池化操作一样，然后为了跟卷积神经网络那边保持一致，因为我们知道，不论是 VGGNet 还是 ResNet，一般在这个池化操作降维之后，它的通道数都会翻倍，从128变成256，从256再变成512，所以说这里我们也只想让他翻倍，而不是变成4倍。</p></blockquote><p>所以紧接着他又再做了一次操作，就是在 c 这个维度上，它用一个<code>1*1</code>的卷积把这个通道数降下来变成一个2 c，通过这个操作，我们就能把原来一个大小为 <code>h*w*c </code>的张量变成了 <code>h/2 * w/2 *2c</code> 的一个张量，也就是说空间大小减半，但是通道数乘2，这样就跟卷积神经网络那边完全对等起来了，整个这个过程就是Patch Merging。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-7.png"><br>所以我们回到模型总览图，那经历过这次Patch Merging操作之后，我们的这个输出的大小就从<code>56*56*96</code>变成了<code>28*28*192</code>，那同样的经过这个 Transformer block，尺寸是不变的，所以除了还是<code>28*28*192</code>，这样第二阶段呢也就完成了。<br>那第三和第四阶段都是同理，都是先进来做一次Patch Merging，然后再通过一些Swin Transformer block，所以这个维度呢就进一步降成了<code>14*14*384</code>，以及<code>7*7*768</code>。<br>这里其实我们就会发现，这个特征图的维度真的跟卷积神经网络好像，因为如果你去回想残差网络，它的这个多尺寸的特征就是经过每个残差阶段之后，它的特征图大小也是<code>56*56</code>、<code>28*28</code>、<code>14*14</code>，最后呢是<code>7*7</code>，而且其实为了和卷积神经网络保持一致，Swin Transformer这篇论文，并没有像ViT一样使用那个CLS token。<br>大家如果还记得 ViT 的话，ViT 就是给刚开始的输入序列又加了一个CLS token，所以这个长度呢就从196变成了197，然后最后拿这个CLS token的这个特征直接去做分类，但Swin Transformer这里没有用这个 token，它是像卷积神经网络一样在得到最后的这个特征图之后用了一个global average polling，就是一个全局池化的操作，直接把这个<code>7*7</code>就取平均拉直变成1了。<br>作者这个图里并没有画，因为Swin Transformer的本意并不是只做分类的，它还会去做检测和分割，所以说它只画了这个骨干网络的部分，它没有去画最后的这个分类头或者这个检测头。但是如果我们是做分类的话，那这里最后就变成了<code>1*768</code>, 然后又变成了<code>1*1,000</code>，而如果我们是做ImageNet的话，这就完成了整个一个分类网络的这个前向过程。</p><blockquote><p>所以说大家看完整个前向过程之后，就会发现Swin Transformer，它有四个这个 stage，它还有类似于池化的这个patch merging操作，然后它的自注意力还是在小窗口之内做的，以及最后它还用的是 global average polling，所以说Swin Transformer这篇论文真的是把卷积神经网络和Transformer这两系列的工作完美的结合到了一起，你也可以说它是披着Transformer皮的卷积神经网络。</p></blockquote><h1 id="窗口自注意力"><a href="#窗口自注意力" class="headerlink" title="窗口自注意力"></a>窗口自注意力</h1><blockquote><p>那说完了整个模型的前向过程，现在我们就来看一下文章的第二大块，也就是文章的主要贡献，就是基于窗口或者移动窗口的这个自注意力。</p></blockquote><p>这里作者又写了一段他们的这个研究动机，就是为什么要引入这种窗口的自注意力，其实跟之前引言里说的都是一个事情，就是说这种全局的这种自注意力的计算会导致平方倍的复杂度，同样说，当你去做视觉里的这种下游任务，尤其是这种密集预测型的任务，或者说， 当你遇到就是非常大尺寸的这种图片的时候，这种全局算自注意力的计算复杂度就非常贵了。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-8.png"><br>所以紧接着作者就说，那我们就不去全局的做这种自注意力，我们现在就用这种窗口的方式去做这种自注意力。那这里我们就举个例子，来说一下这个窗口到底是怎么划分的，作者说，原来的图片会被平均的分成一些没有重叠的窗口，那我们现在就来拿这个第一层之前的输入来举例子，它的尺寸呢就是<code>56*56*96</code>，也就说我们会有一个张量，它的这维度呢是<code>56*56</code>的，然后我们就把它切成一些不重叠的这些方格，也就是这里用橘黄色表示这些方格，这每一个橘黄色的方格就是一个窗口了，但是这个窗口并不是最小的计算单元，最小的计算单元其实还是之前的那个 patch。也就意味着说，每一个这个小窗口里其实还有 <code>m * m</code> 个 patch，那在 Swin Transformer 这篇论文里一般这个 m 是默认为7的，也就是说，这一个橘黄色的小方格里有七七四十九个小 patch，然后现在所有的这个自注意力的计算都是在这些小窗口里完成的，就说这个序列长度永远都是七七四十九。那我们原来这个大的这个整体特征图到底里面会有多少个窗口呢，那其实也就是每条边56/7就8个窗口，也就是说一共呢会有<code>8*8</code>等于64个窗口，就说我们会在这64个窗口里分别去算它们的自注意力。</p><blockquote><p>我们之前虽然也提过很多次，就是这种基于窗口的自注意力模式，那我们从来好像也没有具体的讲过它们这个计算复杂度到底如何，也就是说到底这种基于窗口的自注意力计算方式能比全局的这种自注意力方式省多少。</p></blockquote><p>在Swin Transformer这篇论文里，作者就给出了一个大概的估计，它这里写了两个公式。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-9.png"><br>第一个公式对应的就是标准的这种多头自注意力，它的计算复杂度会有多少，那在这里呢 h、w 就像它上面说的这样，就每一个图片大概会有 <code>h * w</code> 个 patch。那在我们刚才的例子里，h 和 w 就分别都是56，c 就是特征的那个维度。<br>那基于窗口的这个自注意力计算的复杂度又会是多少呢，那作者在公式2里就给出了答案，这里的 m 就是刚才的7，也就是说一个窗口的某条边上到底现在会有多少个patch。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-10.png"><br>那这个公式是怎么推算出来的呢，我们先拿这个标准的这个多头自注意力来举个例子，就说如果我们现在有一个输入那这个自注意力，首先就是说我先把它变成 q k v 三个向量，这个过程其实就是原来的向量分别乘了三个系数矩阵，然后一旦得到 query 和 k 之后，它们就会做一个相乘，最后得到这个 attention，也就是自注意力的这个矩阵，然后有了自注意力之后，它就会去和这个 value 做一次乘法，也就相当于是做了一次加权，最后因为这是多头自注意力，所以最后还会有一个这个 projection layer，这个投射层就会把向量的维度投射到我们想要的那个维度。<br>那现在如果我们给这些向量都加上它们该有的维度，也就是说刚开始这个输入是 <code>hw * c</code>，第一步q k v这个函数相当于是用一个 <code>hw * c</code> 的向量去成一个 <code>c * c</code> 的系数矩阵,然后最后得到了 <code>h w * c</code>，所以每一个计算的复杂度呢是 <code>h * w * c^2</code>，那因为你有三次操作，所以是三倍的 <code>h*w*c^2</code>，然后现在到了算自注意力，这那就是 <code>h*w*c</code>，然后乘以 k 的这个转置，也就是 <code>c*h*w</code>，所以得到了 <code>h*w*h*w</code>，那这个计算复杂度呢就是<code>(h*w)^2*c</code>。那接下来是这个自注意力矩阵和这个value的乘积，它的计算复杂度呢就还是 <code>(h*w)^2*c</code>，所以现在这块就成了<code>2*(h*w)^2*c</code>，那最后一步这个投射层也就是<code>h*w*c</code>乘以<code>c*c</code>变成了 <code>h*w*c </code>，它的计算复杂度就又是 <code>h*w*c^2</code>，所以这里再加1那就是3变成4，这个其实就是最后的公式1了。<br>那基于窗口的这种自注意力计算复杂度又是如何得到的呢，因为我们在每个窗口里算的还是多头自注意力，所以我们可以直接套用这个公式1，只不过它的这个高度和宽度变化了，那现在高度和宽度不再是 <code>h * w</code>，而是变成这个窗口有多大了，也就是 <code>m * m</code>，也就是说啊现在这个 h 变成了 m，w 也是 m，它的序列长度就只有 <code>m * m</code> 这么大，所以当你把 m 值带入到这个公式1之后，我们就得到计算复杂度是<code>4 * m^2 * c^2</code>，加上这个<code>2 * m^4 * c</code>，这个就是在一个窗口里算多头自注意力，所需要的计算复杂度，那我们现在一共有多少个窗口，其实我们现在是有 <code>h/m * w/m</code> 这么多个窗口的，那我们现在用这么多个窗口乘以每个窗口所需要的计算复杂度，就会得到接下来这个公式。我们可以看到，虽然这两个公式前面这两项是一样的，只有后面这块从 <code>(h*w)^2</code>变成了 <code>m^2 * h * w</code>，看起来好像差别不大，但其实如果你仔细带入数字进去算，你会发现这个计算复杂的差距是相当巨大的，因为这里的 h w，比如说是<code>56*56</code>的话，你这里的 m^2 其实只有49，所以是相差了几十甚至上百倍的。</p><h1 id="移动窗口自注意力"><a href="#移动窗口自注意力" class="headerlink" title="移动窗口自注意力"></a>移动窗口自注意力</h1><p>然后接下来作者说，这种基于窗口的算自注意力的方式，虽然很好的解决了这个内存和计算量的问题，但是现在我窗口和窗口之间没有通信了，这样我就达不到全局建模了，也就文章里说的，会限制他这个模型的能力，我们最好还是要有一种方式，就是能让窗口和窗口之间互相通信起来，这样效果应该会更好，因为具有上下文的信息了，所以这里作者就提出来这个移动窗口的方式。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-2.png"><br>他刚开始提出这个移动窗口，其实我们刚才已经简单的提到过了，就是在这个图2里画出来的，如果原来的窗口长这个样子，我们现在就把窗口往下移一半就变成了右边这种形式，然后如果我们Transformer是上下两层，连着做这种操作，就是先是 window，然后再是 shifted window 的话，就能起到窗口和窗口之间互相通信的目的了。<br>所以说在 Swin Transformer里，它的这个 transformer block 的安排是有讲究的，它每次都是先要做一次基于窗口的这个多头自注意力，然后再做一次基于这个移动窗口的多头自注意力，这样就达到了窗口和窗口之间的互相通信。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-5.png"><br>如果我们看图呢，也就是说每次这个输入先进来，它做一次 Layernorm，然后做这个窗口的多头自注意力，然后再过 Layernorm 过 MLP，这就是第一个 block 结束了，这个 block 结束以后，紧接着我们要做一次Shifted window，也就是基于移动窗口的这个多头自注意力，然后再过 MLP 得到输出，这两个 block 加起来，其实才算是 Swin Transformer一个基本的计算单元。<br>这也就是为什么回头我们去看这个模型的配置，也就是这里的这个2、2、6、2，就是一共有多少层Swin Transformer block的时候，我们会发现这个数字总是偶数，那是因为它始终都需要两层 block连在一起，作为一个基本单元，所以一定是2的倍数。</p><blockquote><p>那其实论文读到这里，Swin Transformer整体的这个故事和结构就已经说完了，那主要的研究动机就是想要有一个层级式的 Transformer，那为了这个层级式，所以他们介绍了这种Patch Merging的操作，从而能像卷积神经网络一样把这个Transformer分成几个阶段，然后为了减少计算复杂度，争取能做视觉里这些密集预测的任务，所以他们又提出了这种基于窗口和移动窗口的这个自注意力方式，也就是这里连在一起的两个Transformer block，最后把这些部分加在一起，就是这个Swin Transformer 的结构。</p></blockquote><p>其实作者后面还讲了两个点，一个就是怎样提高移动窗口的这个计算效率，他们采取了一种非常巧妙的这种 masking掩码的方式，另外一个点就是这篇论文里没有用绝对的位置编码，而是用相对的位置编码。</p><blockquote><p>但这两个点其实都是为了提高性能的一些技术细节，跟文章整体的故事已经没有多大关系了，鉴于移动窗口是 Swin Transformer的主要贡献，所以我们还是会讲一下这个巧妙的掩码方式，但是相对位置编码已经有很多别的视频和别的博客详细的讲解过了，这里我就不再复述了。</p></blockquote><p>那我们接下来就来说一下，目前的这种移动窗口方式到底还有哪些问题，为什么作者还要提高它的这个计算性能。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-2.png"><br>我们直接来看图二，图二就是一个基础版本的移动窗口，就是把这个窗口模式（左边）变成了这种窗口方式（右边），虽然这种方式已经能够达到窗口和窗口之间的互相通信了，但是我们会发现一个问题，就是原来你计算的时候，你的这个特征图上只有四个窗口，但是当你做完移动窗口这个操作之后，你现在得到了9个窗口，你的这个窗口的数量增加了，而且每个窗口里的元素大小不一，那比如说中间的这个窗口就还是4 4 有16个这个 pitch，但是别的这些窗口有的有4个 patch ，有的有8个 patch，这都不一样了，那如果我们想去做快速运算，就是把这些窗口全都压成一个 patch，直接去算这个自注意力，现在就做不到了，因为你窗口的大小不一样。那有一个简单粗暴的解决方式，就说我把这些小窗口周围，我再给 pad 上0 ，把它照样pad成和中间这个窗口一样大的窗口，那这样呢我们就有9个完全一样大的窗口，这样就还能把它们压成一个batch，就会快很多，但是你会发现这样的话，你无形之中，你的这个计算复杂度就提升了，因为原来你如果去算这种基于窗口的自注意力，你只用算4个窗口的，但是现在你需要去算9个窗口的，你这个复杂度呢一下提升了两倍多，所以还是相当可观的，那怎么办，怎么能让第二次这个移位完的窗口数量还是保持4个，而且每个窗口里的这个patch数量也还保持一致呢。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-11.png"><br>那作者这里就提出了一个非常巧妙的一个掩码的方式，我们现在直接来看这个图四，图四就是说，当你通过这种普通的这种移动窗口方式得到这9个窗口之后，我们现在不在这9个窗口上算这个自注意力，我们先再做一次这个循环移位，就这里这个 cyclic shift，具体的意思就是说，假如说我们把这个小窗口当成 a，这个横块当成 c，这个当成 b ，我们就先把这个 a 和 c 这一块，直接移到下面来，就是这块，最下面呢就变成了 a 和 c，然后我们再把左边这一块再搬到右边来，就变成了 b 和 a，所以说经过这次循环移位之后，原来的这个窗口，就变成了现在这个窗口的样子，那如果我们在这个大的特征图上，再去把它分成这个四宫格的话，我们现在不就又得到了四个窗口吗，意思就是说我们移位之前的窗口数呢也是4个，我移完位之后再做一次循环移位，那得到窗口数还是4个，那这样窗口的数量就固定了，也就说这个计算复杂度就固定了。<br>但是现在新的问题就来了，虽然对于这个窗口来说，它里面的元素都是互相紧挨着的，他们之间呢可以互相两两去做这个自注意力，但是对于剩下这几个窗口来说，它们里面的元素是从别的地方，很远的地方搬过来的，所以他们之间按道理来说是不应该去做这个自注意力的，就他们之间也不应该有什么太大的联系，比如说，像这里这块的元素和这块的元素，那这个 c 本来是从上面移过来的，也就意味着，假如说我们现在这个图片是上面是天空，下面是地面的话，那这个 c 原来是天空，这个块原来是地面，那你把这个 c 挪到下面来以后，难道这个天空就应该在地面之下吗，明显就是不符合常理的，那我们不应该让这种事情发生，所以就意味着，这块和这块是不应该去做这个自注意力计算的，同理那就是说，这块和这块也不应该去做自注意力，那这块这块这块这块那都是分开的，它们之间都不应该互相去做自注意力，那这个问题该怎么解决呢，那其实这里，就需要一个很常规的操作了，也就是这个掩码这个操作，这在Transformer过去的工作里是层出不穷，很多工作里，都有各式各样的这个掩码的操作，那在 Swin Transformer这篇论文里，作者也巧妙的设计了几种掩码的方式，从而能让一个窗口之中，不同的区域之间，也能用一次前项过程，就能把这个自注意力算出来，但是互相之间都不干扰，也就是它后面说的这个 masked 的Multi-head Self Attention，具体的掩码方式我们马上就讲，然后算完了这个多头自注意力之后，我们还有最后一步，也就是说我们需要把这个循环位移，再给它还原回去，也就是说我们需要把这里的a、b、c再还原到原来的位置上去，原因就是我们还需要保持，原来这个图片的这个相对位置大概是不变的，整体图片的这个语义信息也是不变的，如果我们不把这个循环位移还原的话，那我们相当于在做Transformer的这个操作之中，我们一直在把这个图片往右下角移移移，不停的再往右下角移，那这样这个图片的语义信息很有可能就被破坏掉了。<br>所以说整体而言，这个图4就是介绍一种高效的、批次的这种计算方式，比如说本来我们移动窗口之后，得到了9个窗口，而且窗口之间的这个patch数量每个都不一样，我们为了达到这个高效性，为了能够进行这个批次处理，我们先进行一次循环位移，把9个窗口变成4个窗口，然后用巧妙的这种掩码方式，让每个窗口之间能够合理的去算这个自注意力，最后再把算好的自注意力呢再还原, 就完成了这个基于移动窗口的自注意力计算。</p><blockquote><p>那现在我们就通过一个例子来大概说一下这个掩码操作是怎么做的。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-12.png"><br>首先这里这个画的图是已经经过循环位移的，也就是说这块其实是原来的那个图，这块就相当于是已经从原来这块移过来的，就是刚才说的那个 a 这块其实也是原来左边的那个b，这块其实也是原来这块的那个上面的那个 c，但总之，这个就是已经经过了循环位移之后得到的，然后我们在中间画两条线就把它打成了4个窗口，也就这个窗口0、窗口1、窗口2和窗口3，整个这个特征图的这个大小，我们暂且说他是<code>14*14</code>的，也就是说这个高和宽这两边分别都有14个 patch，之所以化成4个窗口，也是因为每个窗口里应该有7个 patch，也就是说这块是7、这块是7，然后这个图里的这些012一直到8，并不是它里面真正的内容，而是我们用的一种序号，主要就是用来区分不同区域的，因为比如说对于这一大片区域来说，区域0也就是这个窗口0，它里面的元素呢都是相邻的，所以说它是可以互相去做自注意力的，所以这一大块里所有的这个patch，我们都用序号0来代替，但是作为这个窗口1而言，那左边的区域是原图，但它右边的区域2是从原来的这片区域里移过来的，所以说这两个区域是不相同的，它们之间就不应该做这个自注意力计算，所以我们就用两个序号去代替这两个区域里的 patch，那类似的对于这个窗口2而言，那下面的区域是从上面移过来的，所以说这块，我们也用两个序号去代替它，最复杂的就是最后这个窗口3了，因为它的这块区域是原图这块区域是从左边移过来的，这块区域是从上面移过来的，这片区域是从最左上角移过来的，所以说他这四个区域都不相同，它们之间都不应该去做这个自注意力，所以说我们就用四个序号去代替它。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-13.png"></p><blockquote><p>那现在我们先以左下角的这个窗口为例，讲一下整个这个自注意力是如何算的，以及这个掩码是如何加的。</p></blockquote><p>首先我们知道在这个窗口内部，我们现在是有七七四十九个 patch，那每一个 patch 其实就是一个向量，那如果我们把这个现在这个窗口拉直，就会变成下面这个矩阵的样子，拉直就是说从这个 patch 开始，从左往右然后再往下，然后一点一点把所有的 patch全都拉直，变成一个向量，那也就意味着我们先得到的元素呢，就是3、3、3、3，都是3号位的元素，也就是都是3号序号的那个 patch，然后当这个3号循环完了之后，我们就来到了6号，所以下面呢就是6、6、6、6。那一共有多少个这个3号元素，因为你移动窗口的时候是移动窗口的一半，那在这里窗口因为是7，所以它每次移位是3，所以说，也就意味着这块呢是移动了3，那这块保留了4，那因为横边是7，所以说你一共就有<code>7*4</code> ，28个这个3号位元素，也就意味着这块是28，那所以后面呢就是<code>3*7</code> 21，21个6号元素，所以就是说，当你把这个窗口拉直以后变成的向量就是这么一个向量，这个边呢一共有49个元素，前28个呢是3号patch，后21个是6号patch，这个就指的是向量的维度 c。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-14.png"><br>那有了这个拉直的向量之后，接下来就要做自注意力了，那自注意力就是自己跟自己去算这个attention，也就说把左边这个向量转置，得到这个向量，然后它俩之间相乘就可以了。接下来就是基本的这个矩阵乘法了，那就说我这个第一行就要跟我这边这个第一列相乘，也就是说我这个3号patch跟这边的3号patch去相乘，那结果我们就简单用33来代替，就说都是同样的这个 patch，也就说它们之间是可以去算这个自注意力的，然后紧接着呢还是这个行去跟这一列算还是33，所以就是第二个元素，然后还是33、33，然后一直到这块的时候，就算成了36，所以在这个第二块区域里，就是3号区域的元素和6号区域的元素在做这自注意力了，然而事实上，我们是不想让这两个区域内的元素去做这个自注意力操作的，也就是说回头我们是需要把这整个这个区域里的元素都Masked，然后我们继续做矩阵乘法，然后当这个矩阵的行数到这个6这的时候，刚开始我们是跟这个3去算，所以说就会得到63、63，然后这个6最后还会跟6去算，就得到了66、66。同样的道理，因为6和3是两个不同的区域，所以这里面算得的自注意力，我们也不想要也是要 Masked 掉的，而这两个区域<code>（33、66）</code>里算的自注意力，才是我们想要的，因为现在我们已经知道了哪个区域我们想要，哪个区域我们不想要，所以作者就针对这个形式设计了一个掩码的模板，也就是这个模板：<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-15.png"><br>他的这个掩码这两个区域里都是0，然后这两个区域里都是-100，你其实可以理解成一个负的很大的数了，然后它让这两个矩阵呢去相加，因为原来的这个自制力矩阵里，它的值都是非常小的，所以当这两个区域的值加上这个-100之后就会变得是一个非常负的一个小数，然后再通过 softmax这个操作以后就变成0了，也就意味着说，我们把这两块区域里算得的自注意力就 masked 掉了。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-16.png"><br>其实之前，也有很多人对这个掩码的方式不是很了解，所以就去 Swin Transformer这个官方的这个代码库里去问问题，作者也给出了很详细的回复，他在issue38里就做了这么一个掩码的可视化，左边这个图就是已经经过循环位移后的这个输入了，跟我们刚才画的也一样，里面有4个窗口也分成了0、1、2一直到8的这么多个区域，然后他把掩码的这个可视化画在了右边，那对于Window0来说，它里面其实不需要掩码的，所以这里面没有什么操作，那对于我们刚才刚讲过的这个Window 2里来说，他得到的这个掩码的可视化就跟我们刚才画的也是一样的。比如说左上和右下的这两个区域呢设成0，然后剩下的这两个区域设成-100，然后用这个模板去把算得的自注意力里面不该要的值去 mask 掉。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-17.png"></p><blockquote><p>那接下来，我们再看一下这个窗口里的自注意力又是如何算的，它的掩码的这模板是如何得到的。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-18.png"><br>同样的，我们先把这个窗口里的元素都拉直，那这个拉直的过程中跟刚才那个窗口就不太一样，因为这里面你是先有四个1的元素，然后再有三个二的元素，然后又有四个一、三个二、四个一、三个二，所以就跟这个图里画了一样，就是它总是四个一、三个二、四个一、 三个二，也就是说，它的形式是一种条纹状的形式。接下来我们再去做这个自注意力，就把这个向量呢然后转置过来，然后去做矩阵乘法，那所以说最后得到自注意力，就会类似长这样，那刚开始就都是1号元素在和1号元素相乘，然后紧接就是1号和2号相乘，然后紧接就是1号跟1号，然后又是1号跟2号循环往复，那如果我们就在这一个小窗口里看的话，那左上角的这个1是我们要的，这块的这个22是我们要的，但是这里的12和21就不是我们要的了，我们回头就要想办法把它Masked。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-19.png"><br>那所以说跟刚才也一样，那我们需要的地方这个掩码就是0，那不需要的地方，我们就把它设成-100，那接下来，让这个原来的自注意力和这个掩码相加，最后经过一层 softmax 操作，我们就能把想要的自注意力值留下，不想要的值就 mask 掉了。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-20.png"><br>那我们再回过头来看作者提供的这个mask 的可视化，我们就会发现对于这个窗口而言，作者最后给出的可视化就是像我们刚才画出来的一样，这是一个横竖条纹状的模板，那最后这个区域也就是这个窗口3，我们就不多说了，他其实就是之前窗口1和窗口2的一个合体，最后的这个可视化就是这个看起来是不是相当的复杂，但是作者就通过这种巧妙的循环位移的方式，也通过这种巧妙设计的掩码模板，从而实现了只需要一次前向过程，就能把所有需要的这个自注意力值都算出来，而且只需要计算4个窗口的，就说窗口的数量没有增加计算复杂度，也没有增加非常高效的完成了这个任务。</p><p>那在方法的最后一节也就是3.3节，作者就大概介绍一下，他们提出的这个 Swin Transformer的几个变体，分别是四种就是 Swin Tiny、Swin Small、Swin Base和 Swin Large。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-21.png"><br>作者这里说这个 Swin Tiny的计算复杂度跟这个 ResNet-50 差不多，然后 Swin Small 的这个复杂度跟 ResNet-101 是差不多的，这样主要是想去做一个比较公平的对比，那这些变体之间有哪些不一样呢，其实主要不一样的就是这两个超参数，一个就是这个向量维度的大小 c，另一个就是每个 stage里到底有多少个 transform block，这里其实就跟残差网络就非常像了，残差网络也是分成了四个 stage，然后每个 stage有不同数量的这个残差块。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><blockquote><p>接下来我们就一起来看一下文章的最后一部分，叫实验部分。</p></blockquote><p>首先作者就先说一下分类上的实验，他这里一共两种预训练的方式，第一种就是在正规的这个ImageNet-1K上去做训练，也就是那个有128万张图片，有1000个类的那个数集，然后第二种方式就是在更大的这个ImageNet-22K，这个数据集上去做预训练，这个数据集就有1,400万张图片，而且里面呢有2万多个类别，当然这里面不论你是用ImageNet-1K去做预训练，还是用ImageNet-22K去做预训练，最后测试的结果都是在ImageNet-1K的那个测试集上去做的。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-22.png"><br>所有的结果作者都列在了这个表1里上半部分就是ImageNet-1K预训练的模型结果，那下半部分就是先用ImageNet-22K去预训练，然后又在ImageNet-1K上去做微调，最后得到的结果。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-23.png"><br>那在表格的上半部分，作者先是跟之前最好的那些卷积神经网络去做了一下对比，那这个RegNet是之前 facebook他们就用NASA搜出来的这种模型，然后EfficientNet是 google 用NASA搜出来的模型，这两个都算之前表现非常好的模型了，他们这个性能最高会到84.3，然后接下来作者就写了一下之前的这个 Vision Transformer会达到一个什么效果，那对于 ViT 这里来说，我们之前在讲 ViT 论文的时候也提过，因为它没有用很好的这个数据增强，而且缺少这种偏置归纳，所以说它的结果是比较差的，只有70多，然后换上 DeiT 之后，因为用了更好的数据增强和模型蒸馏，所以说 DeiT Base这个模型也能取得相当不错的结果，能到83.1，当然 Swin Transformer能更高一些，Swin Base 最高能到84.5，就是稍微比之前最好的那个卷积神经网络高那么一点点，就比84.3高了0.2，虽然之前表现最好的这个EfficientNet的模型是在<code>600*600</code>的这个图片上做的，而 Swin Base 是在<code>384*384</code>的图片上做的，所以说EfficientNet有一些优势，但是从模型的参数和这个计算的 FLOPs 上来说，EfficientNet 只有66M，而且只用了 37G 的这个 FLOPs，但是 Swin Transformer是用了88M的模型参数，而且用了47G 的这个 FLOPs，所以总体而言呢是伯仲之间。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-24.png"><br>那接下来再看表格的下半部分，就是用 ImageNet-22k 去做预训练，然后再在ImageNet-1k上微调，最后得到的结果，这里我们看到，一旦使用了更大规模的数据集，原始的这个标准的 ViT 的性能也就已经上来了，对于ViT large来说，它已经能得到85.2的准确度了，这相当高了，但是Swin Large更高，Swin Large最后能到87.3，这个是在不使用JFT-300M，就是这种特别大规模数据集上得到的结果，所以还是相当高的。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-25.png"><br>那做完了分类，接下来就是看目标检测的结果了，作者这里是在 COCO 这个数据集上训练并且进行测试的，他首先在这个表2 a 里面测试一下在不同的这个算法框架下Swin Transformer到底比卷积神经网络要好多少，因为他主要是想证明 Swin Transformer是可以当做一个通用的骨干网络来使用的，所以他这里比如说他用了 Mask R-CNN，然后ATSS、RepPointsV2 或者SparseR-CNN，这些都是表现非常好的一些算法，然后在这些算法里，过去的这个骨干网络选用的都是 ResNet-50，现在他就替换成了 Swin Tiny，那我们刚才也讲Swin Tiny 的这个参数量和 FLOPs 跟 ResNet-50是比较一致的，我们从后面的这些对比里也可以看出来，所以他们之间的比较是相对比较公平的，然后我们可以看到Swin Tiny 对ResNet-50 是全方位的碾压，就在四个算法上都超过了它，而且这个超过的幅度呢也是比较大的，比如说在这里超过4个点、4个点、4个点。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-26.png"><br>接下来，作者又换了一个方式去做这个测试，他现在是选定了一个算法，就是选定了Cascade Mask R-CNN 这个算法，然后他去换更多的不同的这个骨干网络，比如说呢他就换了 DeiT-S、ResNet-50或者 ResNet-101，这里他也分了几组，比如说这里他就选的是 Swin Tiny，因为他们之间的这个模型参数和FLOPs 是比较接近的，然后接下来他是比较了这个 Swin Small和 Swin Based结果，我们可以看出来，在相似的这个模型参数和相似的这个 Flops 之下，Swin Transformer都是比之前的这个骨干网络要表现好的。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-27.png"><br>接下来作者又做了第三种测试的方式，就是这个 table 2里的这个 c，就是系统层面的这个比较，那这个层面的比较就比较狂野了，就是现在我们追求的不是公平比较了，你什么方法都可以上，你也可以使用更多的数据，你也可以使用更多的数据增强，你甚至可以在测试的时候使用这个 test time augmentation，就是 TTA 的方式，那我们可以看到之前最好的这个方法，Copy-paste它在 COCO Validation Set上的结果是55.9，在 Test Set上结果是56，而这里如果我们跟最大的这个Swin Transformer、 Swin Large 比，它的结果分别能达到58和58.7，这都比之前高了两到三个点。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-28.png"><br>那第三个实验，作者就选择了这个语义分割里的ADE20K数据集，就在这个表3里，我们可以看到之前的这些方法，一直到这个 DeepLab V3、ResNet。</p><blockquote><p>其实都用的是卷积神经网络，然后这个 ResNet 其实也是我们组之前的一个工作，我们当时最高是能刷到48.4的这个结果，也是提升了很多，因为我们可以看到之前的这些方法，其实都在44、45左右徘徊，我们也是直接涨了两个多点。</p></blockquote><p>但是紧接着Vision Transformer 就来了，那首先就是这个 SERT 这篇论文，他们用了 ViT Large，所以就取得了50.3的这个结果，然后Swin Transformer Large也取得了这个53.5的结果，就刷的更高了，但这里我想指出的，其实作者这里也有标注，就是有两个符号的这个，他的意思是说，这些模型是在ImageNet-22K这个数据集上去做预训练，所以他们的结果才这么好。</p><blockquote><p>那最后我们就是来讲一下这个消融实验。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/11-29.png"><br>作者把消融实验都放到这个表4里了，主要就是想说一下这个移动窗口以及这个相对位置编码到底对 Swin Transformer 有多有用，那这里我们可以看到，如果光说分类任务的话，其实不论是移动窗口还是这个相对位置编码，它的提升相对于基线来说也没有特别明显，比如说这里呢从80.2到81.3提了1.1点，然后这里呢从80.几提升到81.3，也提了一个点，当然在ImageNet的这个数据集上，提升一个点也算是很显著了，但是他们更大的帮助主要是出现在这个下游任务里，就是 COCO和ADE20k这两个数据集上也就是目标检测和语义分割这两个任务上，我们可以看到，用了这个移动窗口和用了这个相对位置编码以后都会比之前大概高了3个点左右，那这个提升是非常显著的，这个想起来当然也是合理的，因为如果你现在去做这种密集型预测任务的话，你就需要你的特征对这个位置信息更敏感，而且更需要这个周围的这个上下文关系，所以说通过移动窗口提供的这种窗口和窗口之间的互相通信，以及在每个Transformer block都去做这种更准确的相对位置编码，肯定是会对这种下游任务大有帮助的。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><blockquote><p>最后我们一起来快速的点评一下这篇论文，并且讲一些我个人的看法。</p></blockquote><p>我们继续回到他的这个官方代码库，我们会发现除了作者团队他们自己在过去半年中刷了那么多的任务，比如说我们最开始讲的这个自监督的Swin Transformer，还有 Video Swin Transformer 以及Swin MLP，同时 Swin Transformer 还被别的研究者用到了不同的领域。那在这里作者就简单罗列了一下，比如说Swin Transformer 和 StyleGAN 的结合就变成了 StyleSwing，然后还有拿它去做人脸识别的，还有拿它去做这种 low level的视觉任务的，比如说图片超分、图片恢复，就有了 SwinIR 这篇论文，然后还要拿 Swin Transformer去做这个 person reID 这个任务的。<br>所以说 Swin Transformer 真的是太火，真的是在视觉领域大杀四方，感觉以后每个任务都逃不了跟 Swin 要比一比，而且因为 Swin 这么火，所以说其实很多这个开源包里都有 Swin 的这个实现，比如说作者这里也列出来，这百度的PaddlePaddle其实里面是有的，而且视觉里现在比较火的这个pytorch-image-models，就是 Timm 这个代码库，里面也是有 Swin 的实现的，同时Hugging Face 我估计也是有的。<br>虽然我前面已经说了很多 Swin Transformer的影响力已经这么巨大了，但其实他的影响力远远不止于此，他论文里这种对卷积神经网络、 对Transformer还有对 MLP这几种架构深入的理解和分析是可以给更多的研究者带来思考的，从而不仅可以在视觉领域里激发出更好的工作，而且在多模态领域里也能激发出更多更好的工作。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文精读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文10】Deepmind用机器学习指导数学直觉论文逐段精读</title>
      <link href="/2023/12/04/lun-wen-10-deepmind-yong-ji-qi-xue-xi-zhi-dao-shu-xue-zhi-jue-lun-wen-zhu-duan-jing-du/"/>
      <url>/2023/12/04/lun-wen-10-deepmind-yong-ji-qi-xue-xi-zhi-dao-shu-xue-zhi-jue-lun-wen-zhu-duan-jing-du/</url>
      
        <content type="html"><![CDATA[<html><head></head><body></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文精读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文9】MoCo论文逐段精读</title>
      <link href="/2023/12/04/lun-wen-9-moco-lun-wen-zhu-duan-jing-du/"/>
      <url>/2023/12/04/lun-wen-9-moco-lun-wen-zhu-duan-jing-du/</url>
      
        <content type="html"><![CDATA[<html><head></head><body></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文精读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文8】MAE论文逐段精读</title>
      <link href="/2023/12/04/lun-wen-8-mae-lun-wen-zhu-duan-jing-du/"/>
      <url>/2023/12/04/lun-wen-8-mae-lun-wen-zhu-duan-jing-du/</url>
      
        <content type="html"><![CDATA[<html><head></head><body></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文精读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文7】ViT论文逐段精读</title>
      <link href="/2023/12/04/lun-wen-7-vit-lun-wen-zhu-duan-jing-du/"/>
      <url>/2023/12/04/lun-wen-7-vit-lun-wen-zhu-duan-jing-du/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><em><strong>AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</strong></em></p><blockquote><p>如果说在过去的一年中在计算机视觉领域，哪个工作的影响力最大，那应该非<code>Vision Transformer</code>莫属了，因为他挑战了自从2012年 Alexnet 提出以来卷积神经网络，在计算机视觉领域里绝对统治的地位，他的结论就是说呢，如果在足够多的数据上去做预训练，那我们也可以不需要卷积神经网络，直接用一个从自然语言处理那边搬过来的标准的Transformer。也能把视觉问题解决得很好，而且Vision Transformer不光是在视觉领域挖了一个大坑，因为他打破了 cv 和 nlp在模型上的这个壁垒，所以在多模态领域也挖了一个大坑，于是在短短的一年之内，各种基于Vision Transformer的工作层出不穷，公开的论文应该已经有上百篇了，有把它扩展到别的任务的，有对模型本身进行改进的，有对它进行分析的，还有对目标函数或者训练方式进行改进的，基本上是可以说，开启了 cv 的一个新时代。</p></blockquote><p>图1里举了一些例子，在卷积神经网络cnn 上工作的不太好，但是用Vision Transformer都能处理很好的例子，比如说第一个就是遮挡，那在这么严重的遮挡情况下呢，卷积神经网络，很难看得出其中有一只鸟。第二个例子，就是数据分布上有所偏移，在这里就是对图片做了一次纹理去除的操作，所以说导致图片看起来非常魔幻，很难看出来其中到底是什么物体。第三个就是在鸟头的位置加了一个对抗性的 patch。第四个就是把图片打散了以后做排列组合，卷积神经网络也是很难去判断这到底是一个什么物体的。但所有这些例子呢，Vision Transformer都能处理得很好</p><h3 id="1-标题"><a href="#1-标题" class="headerlink" title="1.标题"></a>1.标题</h3><p>***AN IMAGE IS WORTH 16X16 WORDS: ***<br><em><strong>TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</strong></em></p><p>先来看一下题目，题目是说一张图片等价于很多16x16大小的单词，那为什么是16x16，而且为什么是单词呢，其实他是说把这个图片看作是很多很多patch，就假如说我们把这个图片打成这种方格的形式每一个方格的大小都是16x16，那这个图片其实就相当于是很多16x16大小的这个patch组成的一个整体，接下来第二句是 transformer去做大规模的图像识别</p><h3 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a>2.摘要</h3><p>虽然说现在Vision Transformer已经是 nlp 领域（自然语言处理领域）里的一个标准，我们也知道有 BERT model、GPT 3然后或者是T5模型，但是用transformer来做cv还是很有限的。在视觉里呢这个自注意力要么是跟卷积神经网络一起用，要么就是说去把某一些卷积神经网络里的卷积替换成自注意力，但是呢还是保持整体的这个结构不变。而这篇文章就证明了这种对于卷积神经网络的依赖呢是完全不必要的一个纯的<code>Vision Transformer</code>直接作用于一系列图像块的时候,他也是可以在图像分类这个任务上表现得非常好的,尤其是当在你在大规模的数据及去做预训练，然后又迁移到中小型数据及使用，这个Vision Transformer能获得跟最好的卷积神经网络相媲美的结果。</p><h3 id="3-引言"><a href="#3-引言" class="headerlink" title="3.引言"></a>3.引言</h3><p><em><strong>第一段</strong></em><br>自注意力机制的这个网络尤其是Transformer已经是自然语言处理里的必选模型了，现在比较主流的方式，就是先去一个大规模的数据集上去做预训练，然后再在一些特定领域的小数据集上去做微调，那其实这个就是要BERT这篇paper里提出来的。多亏了Transformer的计算高效性和这个可扩展性现在已经可以训练超过1,000亿参数的模型了，比如说GPT3。随着你的这个模型和数据级的增长，我们还没有看到任何性能饱和的现象。</p><blockquote><p><code>把 transformer 用到视觉问题上的一些难处</code><br>先来回顾一下 transformer ，假如说我们有一个 transformer 的encoder，我们输入一些元素，在这里假如说是在自然语言处理，这些就是一个句子里的一个一个的单词，输出了也是一些元素，Transformer里最主要的操作就是自注意力操作，自注意力操作就是每个元素都要跟每个元素去做互动，是两两互相的，然后算得一个Attention就是算得一个自注意力的图，然后用这个自注意力的图去做加权平均，最后得到这个输出。因为在做自注意力的时候，我们是两两相互的，就是说这个计算复杂度是跟这个序列的长度乘三倍的。目前一般在自然语言处理，现在硬件能支持的这个序列长度一般也就是几百或者上千，比如说在BERT里也就是512的这个序列长度，那我们现在换到视觉领域，我们就是想用Transformer的话，首先第一个要解决的问题就是<br><code>如何把一个2d的图片变成一个1d的序列或者说变成一个集合，那最直观的方式就是说我们把每个像素点当成这边的这种元素，然后直接把2d的图片拉直，然后把这扔到Transformer里，然后自己跟自己学去就好了。</code><br>但是想法很美好，实现起来复杂度太高，一般来说，在视觉里面，我们训练分类任务的时候这个图片的输入大小大概是224x224这么大，那如果把图片里的每一个像素点都直接当成这里的元素来看待呢，那其实他的序列长度就不是512了，那他的序列长度就是224x224=50176,因为我们一共就有这么多像素点，一共有50,000个像素点，那这个序列长度50,000，其实也就意味着相当是 BRET序列长度512x100倍，所以这个复杂度是相当可怕的，然后呢这还只是分类任务，他的图片大小就24x24这么大，那对于检测或者分割这些任务，现在很多模型的输入呢都已经是变成600x600或者800x800或者更大了，那这个计算复杂度就更是高不可攀了。</p></blockquote><p><em><strong>第二段</strong></em><br>在视觉邻域卷积神经网络还是占主导地位的，像Alexnet 或者 ResNet，但是transformer 在 nlp 领域又这么火，自注意力又这么香，那计算机视觉那就是想用一下自注意力怎么办？所以说很多工作就是想怎么能把自注意力用到视觉里来，一些工作把卷积神经网络和自注意力混到一起用，另外一些工作把整个卷积神经都换掉了，然后就全用自注意力。这些方法呢其实都在干一个事，不是说序列长度太长所以导致没办法把transformer 用到视觉里来吗，那我就想办法就降低这个序列长度。那比如说像这篇论文就是 CVPR 18 Local Network，他就说既然用这个像素点当输入导致序列长度太长的话，我们就不用图片当这transformer的直接输入，我们把网络中间的这个特征图当做transformer的输入。假如说我们有残差网络res50，那其实在他最后一个 stage到res4的时候，他的feature map size 其实就只有14x14那么大，你把他拉平，其实也就只有196个元素，这个序列长度就只有196，就是可以接受的范围之内了。所以他就用特征图当做 transformer 输入的方式，去降低这个序列长度。那像他说的第二个方向，他其实举了两个例子，一个叫做 Stand-Alone Attention（孤立自注意力），另外一个是 Axial Attention（轴注意力）。那对于这个孤立自注意力，他做的什么事呢，他意思是说，你这个复杂度高是来源于你用整张图，那如果我现在不用整张图了，我就用一个 local 的一个 window，我就用一个局部的小窗口，那这个复杂度就是你可以控制，你可以通过控制这个窗口的大小来让这个计算复杂度在你可接受的范围之内，那这个就有点相当于回到卷积那一套操作，因为卷积也是在一个局部的窗口里做的。对于这个轴自注意力工作，他的意思是说图片的复杂度高是因为他的序列长度是 N = H x W，他是由2d的矩阵，那我们能不能把图片这个2d矩阵拆分成两个1d的向量呢，所以他们就是先在高度这个dimension身上去做一次 self-attention，就做一个自注意力，然后他们再去在这个宽度这个dimension上再去做一次自注意力，相当于他把一个在2d矩阵上进行的自注意力操作变成了两个1d的顺序的操作，这样这个计算复杂度就大幅降低了。然后Vision Transformer的作者又说这些后一种方式完全用自注意力把卷积操作的这一类工作，他们虽然理论上是非常高效的，但事实上，因为他们的这个自注意力操作都是一些比较特殊的自注意力操作，所以没有在现在的这个硬件上去加速，所以就会导致你很难去把它训练一个大模型，所以截止到目前为止像这种Stand-Alone Attenion或者这个Axial Attention他的模型都还没有太大，就是跟这种百亿千亿级别的大Transformer模型比还是差的很远。因此啊最后一句话就说，在大规模的图像识别上，这个传统的残差网络还是效果最好的。</p><p><em><strong>第三段</strong></em></p><blockquote><p>那介绍完之前的工作，接下来就该讲Vision Transformer这篇paper到底做了什么事了，所以我们可以看到自注意力早已经在计算机视觉里有所应用，而且甚至已经有完全用自制力去取代卷积操作的工作了，所以这篇论文他就换了个角度去讲故事</p></blockquote><p>他说他们是被Transformer在NLP领域里这个可扩展性所启发，他们想做的就是直接应用于一个标准的Transformer直接作用于图片，尽量做少的修改，就是不做任何针对视觉任务的特定的改变，看看这样的Transformer能不能在视觉领域里扩展得很大很好。如果直接用Transformer，那老问题又回来了，这个这么长的序列长度该怎么办。所以Vision Transformer是怎么处理的呢，Vision Transformer是像我们刚才讲的一样，他把一个图片呢打成了很多 patch，每一个patch就是16x16，假如说这个图片的大小是224x224，sequence length是224x224等于50176。那现在如果我们换成 patch，就是这一个 patch相当于是一个元素，那有效的长宽又会是多少呢，那比如说现在这个宽度变成了24除以16，其实就相当是14了，同样的高度呢也就是14，所以说这样的话，最后的这个序列长度，就变成了14x14等于196，意思就是说现在这张图片就只有196个元素，那196对于Transformer来说，还是可以接受的这个序列程度。然后我们把这里的每一个patch当做一个元素，然后通过一个这个fc layer会得到一个linear embedding，然后这些呢就会当做输入传给Transformer，这个时候，你其实可以很容易的看出来，一个图片就已经变成一个一个的这样的图片块了，然后你可以把这些的图片块当成是像NLP里的那些单词，这一个句子有多少词那就相当是一个图片现在有多少个patch。这就是他题目的意义，一个图片等价于很多16乘16的单词。<br>然后最后一句话，他说他说我们训练这个Vision Transformer是用的有监督的训练，为什么要突出啊有监督呢，因为对于nlp那边来说的话Transformer基本上都是用无监督的方式去训练的，要么是language modeling，要么是用mask language modeling，总之都是无监督的训练方式，但是对于视觉来说，大部分的基线(baseline)网络还都是用这个有监督的训练方式去训练。</p><blockquote><p>Vision Transformer这篇paper是把视觉当成自然语言处理的任务去做，尤其是中间的这个模型，他就是用了一个Transformer encoder跟BERT是完全一样，那如果你已经听过沐神讲解BERT这篇论文，那其实Vision Transformer这篇论文对你而言没有任何技术难点，这篇论文其实就是想告诉你，用这么简洁的一套框架，Transformer也能在视觉里起到很好的效果</p></blockquote><p><em><strong>第四段</strong></em><br>当在中型大小的数据集上，比如说ImageNet去训练的时候，如果不加比较强的约束，ViT的模型其实跟同等大小的残差网络相比，其实是要弱的，那作者就得解释那他为什么弱，所以论文紧接着说，这个看起来不太好的结果，其实是可以预期的，为什么呢？因为 transformer跟卷积神经网络相比，他缺少一些卷积神经网络有的归纳偏置，这个归纳偏置的概念很有意思，他其实说的就是指一种先验知识或者一种我们提前做好的假设。比如说对于卷积神经网络来说，我们常说的就有两个inductive bias两个归纳偏置，一个叫做locality，因为卷积神经网络是以滑动窗口这种形式，是一点一点在图片上进行卷积的，所以他假设图片上相邻的区域会有相邻的特征，那这个假设当然是非常合理的，比如说桌子和椅子，就大概率他经常就会在一起，靠的越近的东西，他的相关性就会越强。另外一个归纳偏置呢叫做平移等变性，也就是我们常说的translation equivariance(平移同变性)，他的意思就是如果你把它写成公式的话，其实是这样的f(g(x)) = g(f(x))，就是不论你先做 g 这个函数还是先做 f这个函数，最后的结果是不变的。这里你可以把 f 理解成是卷积，然后 g 可以理解成是平移这个操作，那意思就是说，无论你先做平移还是先做卷积，最后的结果是一样的，因为卷积神经网络里，这个卷积核就相当于是一个模板一样，像一个 template 一样，不论你这个图片同样的物体移到哪里，那只要是同样的输入进来，然后遇到了同样的卷积核，那他的输出永远是一样的，一旦卷积神经网络有了这两个归纳偏置以后，他其实就有了很多先验信息，所以他就需要相对少的数据去学一个比较好的模型。但是对于Transformer来说，他没有这些先验信息，所以说他所有的这些能力对视觉世界的感知全都需要从这些数据里自己学。</p><p><em><strong>第五段</strong></em><br>为了验证，这个假设作者就在更大的数据集上去做了预训练这里的这个14M 1,400万的图片，就是ImageNet 22k数据集，这里的300M数据集就是google他们自己的JFT 300M数据，上了大数据以后果然这效果拔群，他们立马就发现这个大规模的预训练就比这个归纳偏置要好，Vision Transformer只要在有这个足够的数据去预训练的情况下就能在下游任务上获得很好的迁移学习效果，具体来说就是当在ImageNet 21 k上去训练或者说在这 JFT 300M上去训练的时候vit 就能获得跟现在最好的残差网络相近或者说更好的结果。最后他还罗列了一些结果，比如说ImageNet 88.5 确实相当高，ImageNet-ReaL CIFAR-100 还有VTAB这些数据集。VTAB其实也是这个作者团队他们提出来的一个数据集是融合了19个数据集，主要是用来检测一个模型稳健性好不好的，所以从侧面也可以反映出Vision Transformer的稳健性也是相当不错的。</p><blockquote><p>那引言还是写的非常简洁明了的</p><p>第一段先上来说因为Transformer在NLP那边扩展的很好，越大的数据或者越大的模型，最后的performance就会一直上升，没有饱和的现象，那自然而然就会有一个问题，那如果把Transformer用到视觉里来，是不是视觉的问题也能获得大幅度的提升呢？</p><p>第二段就开始讲前人的工作，因为这么好的方向不可能没人做过，所以一定要讲清楚自己的工作和前人的工作的区别在哪里。他说之前的工作，要么就是把卷积神经网络和自注意力合起来，要么就是用自注意力去取代卷积神经网络，但是呢从来都没有工作直接把Transformer用到视觉领域里来，而且也都没有获得很好的扩展效果。</p><p>所以第三段他就讲Vision Transformer就是用了一个标准的Transformer模型，只需要对图片做一下预处理，就是把图片打成块，然后送到Transforner里就可以了，别的什么改动都不需要，这样彻底可以把一个视觉问题理解成是一个NLP问题，这样就CV和 NLP这两个领域就大一统了。</p><p>最后两段当然是卖一下结果了，只要在有足够多数据区域训练的情况下，Vision Transformer在很多数据集上取得很好的效果。</p></blockquote><h3 id="4-结论"><a href="#4-结论" class="headerlink" title="4.结论"></a>4.结论</h3><p><em><strong>第一段</strong></em><br>结论里他上来没有任何花里胡哨，直接一句话就点明了这篇 paper 在到底在干什么，直接拿NLP领域里标准Transformer来做计算机视觉问题，然后他说跟之前的那些工作就是用自注意力的那些工作不一样的，除了在刚开始的抽图像块的时候，还有这个位置编码用了一些图像特有的这个归纳偏置，如此之外，就再也没有引入任何图像特有的归纳偏置了，这样的好处呢就是我们不需要对Vision领域有什么了解，或者有什么domain knowledge(领域知识)，我们可以直接把这个图片理解成为是一个序列的图像块，就跟一个句子里有很多单词一样，然后就可以直接用NLP里面一个标准的Transformer来做图像分类，这个简单，然后呢扩展性很好的这个策略，当你跟大规模预训练结合起来的时候，工作得出奇的好，怎么个好法，就是说Vision Transformer在很多这个图像分类的这个benchmark上超过了之前最好的方法，而且还相对便宜，就训练起来还相对便宜。</p><p><em><strong>第二段</strong></em></p><blockquote><p>然后接下来第二段呢，他又写了一些目前还没有解决的问题，或者是对未来的一个展望，为什么要写这一段呢，其实是因为Vision Transformer这篇论文属于是一个挖坑的论文，这挖坑呢你可以是挖一个新问题的坑或者是挖一个新数据集的坑，那这篇论文其实是挖了一个新模型的坑——如何能Transformer来做 cv</p></blockquote><p>那Vision Transformer不能只做分类，肯定还能去做这个分割和检测另外两个最主流的视觉任务，就是之前另外一篇很popular的工作就是这个DETR，DETR是去年目标检测的一个力作，他现在是改变了整个目标检测之前的框架，就是出框的这种方式，鉴于ViT和DETR而良好的表现，所以说拿Vision Transformer做视觉的其他问题呢应该是没有问题的。</p><blockquote><p>而事实上正是如此，就在 ViT 出来短短的一个半月之后，在2020年12月检测这块就出来了一个工作叫做ViT-FRCNN，就已经把ViT用到detection上了。seqmentation 也一样，同年12月只有一篇SETR，SETR的论文把Vision Transformer用到分割里了，而且紧接着三个月之后Swin Transformer横空出世，他把多尺度的这个设计融合到了Transformer里面，所以说更加适合做视觉的问题了，真正证明了Transformer是能够当成一个视觉领域一个通用的骨干网络的。</p></blockquote><p>另外一个未来工作方向，他就是说要去探索一下这个自监督的预训练方案，那是因为在NLP领域所有的这些大的Transformer全都是用自监督的方式训练。所以说呢Vision Transformer这篇论文，他也做了一些初始实验，他们证明了用这种自监督的训练方式也是 ok 的，但是呢跟这个有监督的训练比起来还是有不小的差距的。最后呢作者还说，继续把这个Vision Transformer变得更大，有可能会带来更好的结果。</p><blockquote><p>这个坑呢作者团队没有等别人去填，他想着反正别人可能也没有这种计算资源，那我就来把这个坑填一填吧，所以说过了半年，同样的作者团队又出了一篇论文叫Scaling Vision Transformer，就是把Transformer变得很大，提出了一个ViT-G，然后就把ImageNet分类的准确率刷到90以上了，所以说这篇论文真的是一个很好的挖坑之作，他不光是给视觉挖了一个坑，就是从此以后，你可以用传送门来做所有视觉的任务了，他同时还挖了一个更大的坑，就是在CV和NLP能够大一统之后，那是不是多模态的任务就可以用一个Transformer去解决了呢，而事实上多模态那边的工作，最近一年也是井喷式的增长，由此可以看来，Vision Transformer这篇论文的影响力还是非常巨大的。</p></blockquote><h3 id="5-相关工作"><a href="#5-相关工作" class="headerlink" title="5.相关工作"></a>5.相关工作</h3><p><em><strong>第一段</strong></em><br>Transformer在NLP领域的应用，自从2017年这个Transformer提出来去做这个机器翻译以后，基本上Transformer就是很多NLP任务里表现最好的方法，现在大规模的这个Transformer模型一般都是先在一个大规模语料库上去做预训练，然后再在这个目标任务上去做一些细小的微调，这里面有两系列比较出名的工作，一个就是BERT，一个就是GPT。BERT是用了一个 denoising自监督的方式其实就是完形填空，就是你有一个句子，然后你把其中某些词划掉，然后你先要把这些词再predict出来。GPT用的是language modeling去做自监督，language modeling 是你已经有一个句子然后我要去预测下一个词是什么，也就是next word prediction预测下一个词。因为这两个任务其实都是我们人为定的，就说语料都在哪，句子就在哪是完整的，我们只是人为的去划掉其中某些部分或者把最后的词拿掉，然后去做这种完形填空或者预测下一个词，所以这叫自监督的训练方式。</p><p><em><strong>第二段</strong></em><br>接下来开始讲自注意力在视觉里的应用，视觉里就是说如果你想简单的使用一个自注意力还用到这个图片上，最简单的方式就是你把每一个像素点当成是一个元素，你就去让他们两两去做自注意力就好了，但是呢我们在引言里也说过这个是平方复杂度，所以说是很难应用到真实的这个图片输入尺寸上的，那像现在分类任务的224x224，一个Transformer都很难处理，那更别提就我们人眼看的比较清晰的图，一般都是1 k或者4 k这种画质了，那序列长度都是上百万，直接在像素层面使用Transformer肯定是不现实的，所以说如果你想用Transformer，那就一定得做一些这个近似，这里他就罗列一些例子，比如说这些工作，他们是说你的复杂度高是因为你用了整张图，所以你的序列长度长，那我现在不用整张图，我就用这个local neighborhood，我就用一个小窗口制作这个自注意力，那序列长度不就大大降低了吗，那最后的这个计算复杂度不也就降低了吗，那另外一条路就是用Sparse Transformer，这个顾名思义，就是说我去只对一些稀疏的点去做自注意力，所以只是全局注意力的一个近似，还有一系列方法，就是说把这个自注意力用到不同大小的这种block上，或者说在极端的情况下，就是直接走轴了，也就是我们刚才讲的轴注意力，就先在横轴上去做自注意力，然后再在纵轴上做自注意力，那这个序列长度也是大大减小的，然后他说这些特制的自注意力结构其实在计算机视觉上的结果都不错，就是表现是没问题，但是他需要很复杂的工程去加速这个算子，从而在cpu或者gpu上跑的很快，或者说让训练一个大模型成为可能。</p><p><em><strong>第三段</strong></em><br>跟他们工作最相似的是一篇ICLR2020论文Vision Transformer，就是在用了更大的patch和更多的数据集去训练一个更好的transformer。</p><p><em><strong>第四段</strong></em><br>在计算机视觉领域还有很多工作是把这个卷积神经网络和这个自注意力结合起来的，而这类工作是相当多的，而且基本涵盖了视觉里的很多任务，比如说这个检测、分类、视频，还有多模态。</p><p><em><strong>第五段</strong></em><br>还有一个工作跟他们的工作很相近，叫image GPT，GPT是用在NLP里，是一个生成性的模型，那image GPT同样也是一个生成性模型，也是用无监督的方式去训练的，他跟Vision Transformer有什么相近的地方呢，是因为他也用了Transformer，最后这个 iGPT 能达到一个什么效果，如果我们拿训练好的模型去做一个微调，或者说就把他当成一个特征提取器，我们会发现他ImageNet的上最高这个分类准确率<br>也只能到72。</p><blockquote><p>那像这篇 Vision Transformer最后的结果已经有88.5了，所以说是远高于这个72的结果，但这个结果也是最近一篇论文 MAE(何凯明)<br>（Masked Autoencoders Are Scalable Vision Learners）爆火的原因，因为在BEiT（BERT Pre-Training of Image Transformer）或者MAE这类工作之前，生成是网络在视觉领域很多任务上是没法跟判别式网络比的，判别式网络往往要比生成式网络的结果要高很多，但是 MAE 就做到了，就在ImageNet-1k这个数据集上去做训练，就用一个生成式的模型，比之前判别式模型的效果要好，而且不光是在分类任务上，最近他们还研究了一下迁移学习的效果，就是在目标检测上效果也非常好。</p></blockquote><p><em><strong>第六段</strong></em><br>Vision Transformer其实还跟另外一系列工作是有关系的，就是用比ImagNet还大的数据集去做这个预训练，这种使用额外数据的方式，一般能够帮助你达到特别好的效果，比如说之前2017的这篇论文其实也就是介绍 JFT300M的这个数据集的论文，研究了卷积神经网络的效果是怎么随着数据集的增大而提高的。还有一些论文是研究了当你在更大的数据集上，比如说ImageNet-21k和 JFT300M去做预训练的时候，这个迁移学习的效果会怎么样，就是当你迁移到ImageNet或者CIFAR-100上的效果会如何。<br>在这篇论文里Vision Transformer上也是聚焦于最后两个数据集<br>ImageNet-21k和 JFT300M，但是我们并不是训一个残差网络，我们是去训Transformer。</p><blockquote><p>那其实这个相关工作写的非常彻底的，而且他列举了很多跟他们工作最相近的，比如说这篇ICLR2020的论文，还有这个iGPT，还有之前研究大数据集的，比如说BIT。其实写相关工作这个章节，就是要让读者知道在你的工作之前，别人做了哪些工作，你跟他们的区别在哪里，这个只要写清楚了，其实是对你非常有利的，并不会因此降低论文的这个创新性，反而会让整个文章变得更加简单易懂。</p></blockquote><h3 id="6-ViT模型"><a href="#6-ViT模型" class="headerlink" title="6.ViT模型"></a>6.ViT模型</h3><p>在模型的设计上最原始的Transformer来做的，这样一个好处就是我们可以直接把 NLP那边已经成功的 Transformer的架构直接拿过来用，就不用自己再去魔改模型了，而且因为Transformer已经在NLP领域火了这么多年了，他有一些写得非常高效的一些实现同样Vision Transformer可以直接把他拿过来用。</p><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/7-2.png" alt="图1"></p><blockquote><p>模型的总览图其实对很多论文来说是非常重要的，画得好的模型总览图能够让读者在不读论文的情况下，光看这张图就能大概知道你整篇论文在讲什么内容。Vision Transformer这张图其实画得就很好，因为我们也可以看到，当别人在讲解Vision Transformer这篇论文或者跟他对比的时候就是直接把这个图1直接就复制粘贴过去了，而不是说自己为了讲解的需要，还要再费尽心思再从头再画一个图。</p></blockquote><p>我们先大概过一下这个整体的流程，然后我们再具体的过一遍，整个网络的这个前向操作，这样对Vision Transformer的理解就更深刻了。</p><p>首先给定一张图，他是先把这张图呢打成了这种patch，比如说这里是把这个图打成了九宫格，然后他把这些patch变成了一个序列，每个patch会通过一个叫线性的投射层的操作得到一个特征，也就是这篇论文里说的patch embedding。但我们大家都知道啊这个自注意力是所有的元素之间两两去做这个交互，所以说他本身并不存在一个顺序问题，但是对于图片来说呢他是一个整体，这个九宫格是有自己的顺序的，比如说这图片就是1、2、3，一直到第九张图片还是有顺序的，如果这个顺序颠倒了，其实就不是原来这张图片了，那同样类似NLP那边，我们给这个patch embedding加上了一个position embedding就加上一个位置编码，一旦加上这个位置编码信息以后，这个整体的token就既包含了这个图片块原本有的图像信息，又包含了这个图像块的所在位置信息，那一旦我们得到这些一个一个的 token，那其实接下来就可能NLP那边是完全一样了，我们直接把它给这么一个Transformer encoder，然后Transformer encoder，就会同样的反馈给我们很多输出。那这里问题又来了，那这么多输出，我们该拿哪个输出去做最后的分类呢，所以说再次借鉴BERT，BERT有这个叫extra learnable embedding，也就是一个特殊字符叫cls，叫分类字符，同样的我们在这里，也加了这么一个特殊的字符，这里用新号代替，而且他也是 position embedding，他有位置信息的他永远是0，因为所有的token都在跟所有的token做交互信息，所以他们相信这个class embedding能够从别的这些embedding里头去学到有用的信息，从而我们只需要根据他的输出做一个最后的判断就可以，那最后这个MLP Head其实就是一个通用的分类头了，最后用交叉熵函数去进行模型的训练。</p><p>至于这个Transformer encoder也是一个标准transformer，这篇论文也把具体的结构列在右边了，比如说当你有这些patch的时候呢，你先进来做一次layer norm，然后再做multi-head、self-attention，然后再layer norm，然后再做一个MLP，这就是一Transformer block。然后你可以把它叠加 L 次，就得到了你的Transformer Encoder。所以说整体上来看Vision Transformer的架构还是相当简洁，它的特殊之处就在于如何把一个图片变成这里的一系列的token。</p><p>我们接下来，可以具体按照论文中的文字，再把整个模型的前向过程走一遍，假如说啊我们有个图片X，然后他的dimension是224x224x3，如果我们这里使用16x16的这个patch size 大小，那我们就会得到多少 token 呢，等于224平方除以16的平方，也就是196 就是14的平方，就我们一共会得到196个图像块，那每一个图像块的维度如何呢，其实就是16x16x3，也就是768，3就是RGB channel，所以呢我们就把原来这个图片224x224x3，变成了一个有196个patch，每个patch的维度是768，那接下来这个线性投射层是什么呢，其实这个线性投射层就是一个全连接层，文章后面呢是用大E这个符号就代表这个全连接层的，这个全连接层的维度是多少呢，其实这个全连接层的维度是768x768，这个768也就是文章中一直说的那个D，当然这个D是可以变的，就如果你的Transformer变得更大了，那这个D也可以变得相应的更大，但是前面这个768是从图像patch算来的，是16x16x3算来的，所以这个是不变的。那经过了这个线性投射，我们就得到我们的这个patching embedding，那具体来说呢就是如果XE =1，那在维度上其实就可以理解成是196x768，然后又乘了个768x768的矩阵，然后这两个维度抵消掉，所以最后你得到的还是196x768，意思就是我现在有196个token，每个 token 向量的维度是768，那到现在其实已经成功的把一个 Vision 的问题变成了一个NLP 的问题，我的输入就是一系列1d 的 token而不再是一张2d的图了。那除了图像本身带来的这些token以外，我们之前也说过，这里面要加一个额外的cls token，就是一个特殊的字符，那这个特殊字符他只有一个token，他的维度也是768，这样方便，你可以和后面图像的信息拼接起来，所以最后序列的长度就是整体进入Transformer中的这个序列，长度呢其实是196 +1，就是197乘以768了，这197就是有196个图片对应的 token和一个那个特殊字符cls token。最后呢我们还要加上这些位置编码信息，那位置编码刚才我们说1、2、3一直到9，其实这只是一个序号，而并不是我们真正使用的位置编码因为我们不可能，把这个1、2、3、4、5、6这些数字然后传给一个Transformer去学，具体的做法呢是我们有一个表，这个表的每一行其实就代表了这里面的1、2、3、4这个序号、然后每一行就是一个向量，这个向量的维度是多少呢，跟这边的D是一样的，是768 这个向量也是可以学的。然后我们把这个位置信息加到这所有的这些的token里面，而注意这里面是加而不是拼接，就不是concatenation，是直接sum了，所以加完位置编码信息以后，这个序列还是197x768。那到此，我们就做完了整个对图片的这个预处理，包括加上特殊的字符cls和加上这个位置编码信息。</p><p>也就是说我们对于Transformer输入这块的embedded patches就是197x768的一个tensor，这个tensor先过一个layer norm出来还是197x768，然后我们就要做这个多头注意力了，那多头注意力这里就变成了三份k、q、v，每一个都是197x768，那这里呢因为我们做的是多头自注意力，所以其实最后的这个维度并不是768。假设说我们现在用的是Vision Transformer的base版本，也就是说他用多头是用了12个头，那这个维度呢就变成了768除以12也就等于64的维度，也就是说这里的k、q、 v变成了197x64，但是有12个头，12个对应的k、q、v去做这个自注意力操作，最后再把这些12个头的输出直接拼接起来，这样64拼接出来以后又变成了768，所以多头注意力出来的结果经过拼接，到这块还是197x768，然后再过一层layer norm，还是197x768，然后再过一层MLP，MLP这里呢会把维度相对应的放大，一般是放大四倍，所以说就是197乘以3072，然后再把它缩小投射回去，然后再变成197乘768，就输出了这个呢，就是一个Transformer block的前向传播的过程。所以说进去呢是197x768，出来呢还是197x768，这个序列的长度和每个token对应的维度大小都是一样的。所以说你就可以在一个Transformer block上不停的往上Transformer block，想加多少加多少，那最后有L层这个Transformer block的模型就是这个Transformer Encoder，也就是这里面这个Transformer Encoder的。</p><p>标准的Transformer是需要一系列这个1D的token当做输入，那但是对于2 d 的图片怎么办，那我们就把这个图片打成这么多的小 patch，具体有多少个 patch 呢，就是 N 等于这个 HW/P^2，那其实也就是我们之前算过的24的平方除以16的平方也就是196，当然这个是对于patch size =16 而言了，如果你patch size变了就变成8或者变成14，那对应在这里的这个序列长度也会相应改变。这个Transformer从头到尾都是用了这个 D 当做向量的长度的，也就是我们刚才说的那个768，所以就是从头到尾他的向量长度都是768 这个维度是不变的，那为了和Transformer这个维度匹配上，所以说我们的这个图像的patch 的维度也得是这个 D，dimension也就是768。那具体怎么做呢，就是用一个可以训练的 linear projection，也就是一个全连接层，从全连接层出来的这个东西叫做 patch embedding，为了最后的分类我们借鉴了BERT 里的这个class token，一个特殊的cls token，这个token是一个可以学习的特征，跟图像的特征有同样的维度，都是768，但他只有一个token，就是经过很多层的这个Transformer block以后，我们把他这个输出当成是整个Transformer的模型的输出，也就是当做整个图片的这个特征，那一旦有了这个图像特征了，后面就好做了，因为你卷积神经网络到这块也是有一个图像的整体的特征，我们就在后面加一个这个MLP的这个分类图就可以了。然后对于位置编码信息，这篇文章就用的是标准的这个可以学习的1D position embedding，也就是BERT里用的位置编码，那当然作者也尝试了别的编码形式了，就比如说因为你是做图片，所以你对空间上的位置可能更敏感，你可能需要一个2D aware，就是能处理2D信息的一个位置编码，那最后发现了这个结果其实都差不多，没什么区别。其实针对这个特殊的class token，还有这个位置编码，作者还做了详细的消融实验，因为对于Vision Transformer来说，怎么对图片进行预处理，还有怎么对图片最后的输出进行后处理是很关键的，因为毕竟中间的这个模型就是一个标准的transformer，没什么好讲的，所以我们现在就来看一下这些消融实验。<br>首先我们说一下这个class token，他说因为在这篇论文中我们想跟原始的这个transformer保持尽可能的一致，所以说呢我们也用了这个class token，因为这个class token在NLP那边的分类任务里也是用的，在那边呢也是当做一个全局的对句子理解的一个特征那在这里呢<br>我们就是把它当做一个图像的整体特征<br>那拿到这个token的输出以后呢<br>我们就在后面接一个这个 mlp<br>mlp 里面呢<br>是用 tanh 当做一个非线性的激活函数<br>去做这个分类的预测<br>但是这个 class token 的设计呢<br>是完全从 nlp 里边借鉴过来的<br>之前在视觉领域呢<br>我们其实不是这么做的<br>那比如说我们现在有一个残差网络<br>一个 Res50<br>我们现在有这个前面的几个 block<br>res 2 res 3 res 4 res 5对吧<br>假如说是2345<br>那在最后这个 stage 出来的呢<br>是一个 feature map<br>这个 feature map 呢<br>是14乘14这么大<br>然后在这个 feature map 之上呢<br>我们其实是做了一步这个 gap 的操作<br>也就他这里说的 global average pulling<br>就是一个全局的平均池化<br>池化以后的特征呢<br>其实就已经拉直了 就是一个向量了<br>那这个时候<br>我们就可以把这个向量<br>理解成是一个全局的<br>对于这个图片的一个特征<br>然后我们就拿这个特征去做分类<br>那现在对于transformer来说<br>如果你有一个transformer的model<br>然后你进去呢有这么多 n 个元素<br>而你出来呢也是有这么多 n 个元素<br>我们为什么不能<br>直接在这 n 个输出上<br>去做一个这个 globel average pulling<br>然后得到一个最后的特征呢<br>我们为什么非得在这个前面呢<br>加一个 class token<br>然后最后用 class token 去做输出<br>然后去做分类呢<br>那其实通过实验呢<br>作者最后的结论是说<br>其实这两种方式都可以<br>就<br>你也可以去做这个 global average pulling<br>那得到一个全局的特征然后去做分类<br>或者你用什么一个 class token<br>去做vision transformer这篇论文<br>所有的实验<br>都是用 class token 去做的<br>他主要的目的呢<br>就是跟<br>原始的这个transformer而保持尽可能的一致<br>就像他这里说的<br>stay as close as possible<br>我不想让你觉得他这个效果好<br>有可能是因为某些 trick 带来的<br>或者是某些针对 cv 的改动而带来的<br>那就是想告诉你一个标准的transformer<br>照样能做视觉<br>那我们往下拉看这张图<br>这张图里呢<br>其实这个绿线呢<br>就代表这个全局平均池化<br>就是原来vision 领域里怎么做的<br>然后这个 class token 呢<br>就代表是 nlp 那边怎么做<br>就这条蓝线<br>那我们可以看到呢<br>其实最后呢<br>这个绿线和蓝线的效果是一样的<br>但是作者指出呢<br>这个绿线和蓝线用的这个学习率是不一样的<br>就是你得好好调参<br>如果你不好好调餐呢<br>比如说直接<br>把这个 class token<br>用的这个 learning rate<br>直接<br>拿过来去做这个全局平均池化呢<br>那他效果是非常差的<br>只有这么多可能大概低了<br>五六个点吧<br>所以说呢有的时候也不是你的 idea<br>不work<br>可能主要还是参没调好<br>炼丹技术还是要过硬 </p><h3 id="7-实验"><a href="#7-实验" class="headerlink" title="7.实验"></a>7.实验</h3><p>在这个章节主要是对比的残差网络，vit 他们这个混合模型的，这个表征学习能力，为了了解到底训练好每个模型，需要多少数据呢, 他们在不同大小的数据集上，去做预训练，然后在很多的数据集上去做测试，当考虑到预训练的这个计算代价的时候，就是预训练的时间长短的时候呢，vision transformer表现的非常好，能在大多数数据之上取得最好的结果，同时需要更少的时间去训练，最后作者呢还做了一个小小的实验，就做了一个自监督的实验，自监督说的实验的结果呢，还可以虽然没有最好，但他说这个呢还是比较有潜力的。</p><blockquote><p>而事实上确实如此 时隔一年之后，最近大火的 mae，就证明了自监督的方式去训练vit，确实效果很好。</p></blockquote><h3 id="8-评论"><a href="#8-评论" class="headerlink" title="8.评论"></a>8.评论</h3></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文精读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文6】BERT论文逐段精读</title>
      <link href="/2023/12/04/lun-wen-6-bert-lun-wen-zhu-duan-jing-du/"/>
      <url>/2023/12/04/lun-wen-6-bert-lun-wen-zhu-duan-jing-du/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><blockquote><p>如果对自然语言处理在过去三年里面，最重要的文章做排序的话，你把 BERT 排在第二的位置，那么很难有另外一篇论文能够名正言顺地排在第一的位置，今天我们的任务就是来读一下BERT这篇文章，我们知道在计算机视觉里面很早的时候，我们就可以在一个大的数据集上，比如说 ImageNet 上面训练好一个CNN的模型，然后这个模型可以用来帮助一大片的计算机视觉的任务来提升它们的性能，但是在NLP在BERT之前一直没有一个深的神经网络使得我训练好之后能够帮助一大片的NLP的任务，就导致在NLP里面，我们很多时候还是对每个人构造自己的神经网络，然后在自己上面做训练，BERT的出现使得我们终于可以在一个大的数据集上，训练好一个比较深的神经网络，然后应用在很多NLP的任务上面，既简化了这些 NLP任务的训练，又提升了它的性能，所以 BERT 和它之后的一系列工作使得自然语言处理在过去三年里面有一个质的一个飞跃。</p><p>所以今天我们的任务是对BERT这篇文章进行精读，我们跟以前一样会给大家讲一下BERT的技术上的各种细节，在我们赞美BERT这个开创性工作的同时，我们也希望通过大家读论文来发现，BERT其实也是站在巨人的肩膀上面，我们希望大家能知道BERT哪些技术，哪些思想是来自于前人的工作，哪些是它自己独创的一些东西，这样我们就能对NLP这个领域里面，做出重要贡献的研究员以合理的认可。</p></blockquote><p><em><strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong></em></p><h3 id="1-标题"><a href="#1-标题" class="headerlink" title="1.标题"></a>1.标题</h3><p>接下来我们来看一下标题，标题的第一个词是BERT，就是说名字我已经给你想好了，后面的人就不需要帮我来想名字了。然后来看下一个词Pre-training，我们知道Pre-training的意思是，我在一个数据集上训练好一个模型，然后这个模型主要的目的是用在一个别的任务上面，所以别的任务如果叫做training的话，那么在大的数据集上训练我这个任务叫做 Pre-training，就是在training之前的那一个任务。后面几个词是Deep Bidirectional by directional transformers，Deep就好好理解，我帮你做的深一点，Bidirectional双向的，我们等会再来解释，好最后一个是for Language Understanding，我们知道transformer主要是用在于机器翻译这个小任务上面，这里用的是一个更广义的词，就是对一个语言的一个理解的上面，所以总结下来就是这篇文章是关于 BERT 的模型，它是一个深的双向的transformer是用来做预训练的，然后是针对的是一般的语言的理解任务。</p><h3 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a>2.摘要</h3><p><em><strong>第一段</strong></em><br>在摘要的第一段的第一句话，它说我们介绍一个新的语言表示模型名字叫做 BERT，BERT 的名字来自于Bidirectional 的<code>B</code>，Encoder的<code>E</code>，representation的<code>r</code>和transformer的<code>t</code>，它的意思是transformer 这个模型双向的编码器表示，有意思的是这四个词跟你的标题是不一样的，所以我总觉得它 BERT 这个名字是凑出来的，因为它的想法基于一个重要的工作，叫做  ELMo，ELMo来自于芝麻街里面的人的名字，芝麻街是一个美国的老少皆智的一个少儿英语学习节目，然后所以它就凑了一个 BERT， BERT 是芝麻街里面另外一个主人公的名字，所以这篇文章和当之前的 ELMo开创了NLP 的芝麻街系列文章，基本上芝麻街里面重要的人物的名字都被用了一个遍，反正你都可以凑。</p><p>第二句话，跟最近的语言表示模型不一样，第一个引用的是ELMo这篇文章，第二个引用的是GPT这篇文章。BERT 是用来设计去训练深的双向的表示，然后使用没有标号的数据，然后再联合左右的上下文信息。</p><p>第三句话，因为我们的设计导致我们训练好的BERT，使得我们可以只用加一个额外的一个输出层，就可以得到一个不错的结果，在很多的 NLP 的任务上面，包括了问答、包括了语言推理，而且不需要对任务做很多任务特别的架构上的改动。</p><blockquote><p>所以这两句话其实分别的是讲它跟ELMo和GPT的区别，我们知道GPT它其实考虑一个单向，它用左边的上下网信息去预测未来。所以BERT这里不一样，它用了右侧和左侧信息，所以它是个双向的。Bidirectional是一个非常重要的一个字。ELMo用的是一个基于RNN的架构，而BERT用的是transformer，所以ELMo在用到一些下游的任务的时候呢，它需要对架构做一点点调整。但是 BERT 的地方相对比较简单，只需要改最上层的就行了，这个跟GPT 其实是一样的。</p><p>所以它就是分别用两句话来讲清楚了，我跟GPT和ELMo的区别，这也是一种有意思的写法，就是说在你的摘要的第一段话讲我跟哪两个工作相关，而且我跟这两个工作的区别是什么，这也多多少少表示这份工作，其实也是在基于这两篇工作之上做了一些改动。</p></blockquote><p><em><strong>第二段</strong></em><br>接下来摘要的第二段话就是卖一下BERT有什么好处了。这个模型在概念上更加简单，而且在实验上更加好，它在11个NLP的任务上得到了新的最好的结果，包括了GLUE、MultiNLI、SQuAD v1.1、SQuAD v2.0，然后它说对每个人我的绝对的精度是多少，然后呢跟之前最好的结果比是有7.7%的提升，同样道理话后面都是写的绝对的精度，然后说跟别人比它的差距在什么地方。</p><blockquote><p>所以这个地方其实有两个东西是比较有意思的，首先它说我在11个任务上比较好，那真的就是我训练一个模型，然后在一大片的任务上能做的比较好。第二个我很推重的写法就是，当你说你的东西好的时候，你要讲清两个东西, 第一个是你的绝对的精度是多少，第二个是你跟别人比它的相对的好了是多少。</p><p>为什么这两个东西都重要呢，如果你没有后面那个，只有前面那个精度的话，别人不知道8%是什么意思，93是什么意思，它没有上下文关系，如果你只说跟别人好了7.7%，那么就是说那取决于你这个数是有多大了，就是大家无法理解的7.7什么意思，你是说在30%做到了37%呢，还是80%做到87%，当然这个东西都会有不一样的解释。所以你告诉大家说我的绝对精度是多少，我的相对精度跟别人比是多少，这样子大家就很清楚你这个结果跟别人比，在我甚至不知道这个数据集大概是什么样的情况下，也能得到你这个结果是非常显著的。</p><p>所以整体上它摘要两段话，一段话是说我跟另外两篇跟我相关的工作它的区别是什么，下面一段话是说我的结果特别好。这个其实也是一个非常标准的写法，如果你做一篇论文多多少少是基于别的工作上做些改进，当然你的结果得好一些，所以你先写我跟别人比我的改进在什么地方，另外一块是说我的结果比别人好在什么地方。</p></blockquote><h3 id="3-导言"><a href="#3-导言" class="headerlink" title="3.导言"></a>3.导言</h3><p><em><strong>第一段</strong></em></p><blockquote><p>导言第一段一般是交代这篇论文关注的研究方向的一些上下文关系。</p></blockquote><p>第一句话，在语言模型里面，预训练可以用来提升很多自然语言的任务，第一篇是用词嵌入来做预训练，后面这些包括了 GPT 这篇文章。</p><p>第二句话，在这些自然语言任务里面包括两类，第一类是句子层面的任务，主要是用来建模这些句子之间的关系，比如说对个句子的情绪的一个识别，或者两个句子之间的关系。第二个任务是词源层面的任务，包括了一些实体命名的识别，就是说对每个词去识别它是不是一个实体命名，比如说它是不是一个人名、一个街道名字，这样的任务，你需要输出一些细腻度的词源层面的一些输出。</p><blockquote><p>所以说预训练在NLP里面其实已经流行了一阵子了，这个在计算机视觉里面已经用了很多年了，同样的想法用到在自然语言处理，当然肯定不会很新的。有意思的是说, 今天我们再去介绍BERT的时候，很有可能会把说在NLP上做预训练这件事情归功于BERT，大家会认BERT之前，好像这个事情不能做，然后说有了BERT之后, 终于可以这么做了，然后你读论文的时候，你会发现这个想法当然很早之前就有了，BERT不是第一个提出来这么做的，而是说BERT让这个方法出圈了，让后面的研究者都跟着你来用这个方法做在自然语言处理的任务，这当然也值得大家思考，BERT是怎么样这个东西出圈。</p></blockquote><p><em><strong>第二段</strong></em></p><blockquote><p>导言的第二段和之后一般也就是摘要的第一段的一个扩充的版本。</p></blockquote><p>在使用预训练模型做特征表示的时候，一般有两类策略，第一类策略是基于特征的，另外一个策略是基于微调的。</p><p>基于特征的的代表作是ELMo，对每一个下游的任务，构造一个跟这个任务相关的神经网络，它用的其实用的是RNN这个架构，然后在预训练好的这些表示，比如说你是一个词嵌入也好，什么别的也好，它作为一个额外的特征和你这个输入是一起输入进这个模型里面，我希望这些特征已经有了比较好的表示，所以导致你的模型训练比起来比较容易，这也是 NLP 里面使用这些预训练模型最常用的一个做法，就是把你学到这些特征跟你的输入一起放进去，作为一个很好的特征的表达。</p><p>第二类是基于微调的，比如说GPT，把预训练好的模型放在下游的任务的时候不需要改变太多，需要改一点就行了，然后这个模型它预训练好的参数会在你下游的数据上再进行微调一下，就是所有的权重在根据你的新的数据器进行微调一下。</p><blockquote><p>你介绍别人的目的通常用来铺垫你自己的方法，就是说别人哪些地方你觉得做的不好，我的方法在这一块有改进。</p></blockquote><p>这两个途径都是使用一个相同的目标函数，在预训练的时候，它都是使用一个语言模型，而且是一个单向的。</p><blockquote><p>我们知道语言模型它就是一个单向的，就我给一些词，我预测我下一个词什么东西，就是我说一句话，然后你预测我这句话下面那个词是什么东西，你是属于一个预测模型，要预测未来，它当然是单向了。你不能说，我先告诉你我这句话是什么东西，下一句话什么东西，你来猜我中间要说什么，那当然就不叫语言模型了。</p></blockquote><p><em><strong>第三段</strong></em></p><blockquote><p>接下来在第三段的时候，讲一下主要的一个想法</p></blockquote><p>现在这些技术会有局限性，特别是来做预训练的表征的时候，你的主要问题是，标准的语言模型是一个单向的，这样导致你在选架构的时候要有一些局限性，比如在 GPT 里面它用的是一个从左到右的架构，也就是说你在看一个句子的时候，你只能从左看到右，假设你的句子是从左读到右的话，然后它说这个东西不是很好，就是说它说我要如果要做句子层面的一些分析的话，比如说我要判断一个句子的情绪是不是对的话，我从左看到右和一次看所有就说从右看到左都是合法的。它另外说就算是词元上的一些任务，比如说 Q&amp;A（问答） 的时候我其实也是能够看完整个句子让我去选答案，而不是真的要去一个一个往下走，它说如果我把两个方向的信息都放进来的话，应该是能提升这些任务的性能的。</p><p><em><strong>第四段</strong></em></p><blockquote><p>在指出了相关工作的局限性和提出的想法之后，当然接下来要讲是怎么样解决这个问题的。</p></blockquote><p>提出了BERT，BERT是用来减轻之前提到过的语言模型是一个单向的一个限制，它用到的是一个叫做带掩码的language model（masked language model），就是带掩码的一个语言模型，它是受一个叫Cloze任务的一个启发，然后引用的是1953年写的一篇论文。这个带掩码的语言模型每一次随机的选一些字元，然后把它盖住，然后你的目标函数是预测那些被盖住的那些字，就等价于我给你个句子然后挖这些空，然后让你填完型填空了，跟标准的语言模型从左看到右不一样的是，带掩码的语言模型当然是允许你看左右的信息，就是你做完形填空的时候，你不能只看完形填空的左边，你也得看看右边信息才行，不然的话你也不知道怎么去填它，这样的话它允许我们训练深的、双向的一个 transformer 模型。在带掩码的语言模型之外，还训练了一个别的任务，叫做下一个句子的预测，核心思想是我给你两个句子，你给我判断说这个两个句子在原文里面是不是相邻的，还是说我其实就是随机的采样了两个句子把你放在一起，这样子的话能让你的模型去学习句子层面的一些信息。</p><p><em><strong>贡献</strong></em><br>一共列了三点贡献。</p><ol><li>展示了双向信息的重要性。GPT只用了单项, 之前有的工作只是很简单的把一个从左看到右的语言模型和一个从右看到左的语言模型简简单单的把它合并在一起，有点像双向的RNN模型，就是把它concat了在一起。而BERT这个模型在双向信息的应用上更好一点。</li><li>假设你有一个比较好的预训练的模型的话，你就不用对特定任务啊做些特定的模型的改动了，BERT是第一个基于微调的模型，在一系列的NLP任务上，包括了句子层面的和词元层面的任务上都取得了最好的成绩。</li><li>我的代码和模型全部放在这里面，大家可以随便用。</li></ol><h3 id="4-结论"><a href="#4-结论" class="headerlink" title="4.结论"></a>4.结论</h3><p>结论比较短，第一句话，最近一些实验表明是使用非监督的预训练是非常好的，这样使得资源不多的任务（比如说我的训练样本比较少的任务）也能够享受深度神经网络，主要的工序就是把前人的结果拓展到深的双向的架构上面，使得同样的一个预训练的模型能够处理大量的不一样的自然语言的任务。</p><blockquote><p>读到这里，我们基本上读了一页半的文章，就你可以看到它的故事是非常简单的东西，它说我有两个前面的工作一个叫ELMo，它用了双向的信息，但是它的网络架构呢用的比较老用的是RNN，另外一个是GPT，它用了一个新一点的transformer的架构，但是呢它只能处理单项的信息，我说我把ELMo双向的想法和GPT使用transformer的东西跟你合起来就成为了BERT，具体的改动是在做语言模型的时，我不是预测未来，而是变成完型填空。而且在写作上作者也是这么写的，它说我两个算法我把它拿过来拼在一起，而且它的主要贡献就是指出了你的双向是一个非常有用的信息。</p><p>这样它给大家很多信心, 就是很多时候我们的工作，很多时候就是a加b工作，就把两个东西给你缝在一起，或者把一个技术用来解决另外领域的问题，所以有时候说你也没必要自卑，说自己的想法特别小，不值得写出来，如果你的东西确实简单好用，别人愿意用就挺好的，你就是很朴实的把它写出来，一点问题都没有，而且说不定哪一天就出圈呢，成为了这个领域的经典之作呢。</p></blockquote><h3 id="5-相关工作"><a href="#5-相关工作" class="headerlink" title="5.相关工作"></a>5.相关工作</h3><ol><li><p>非监督的基于特征的一些工作，也就是之前我们提到过的ELMo，然后在ELMo之前是词嵌入，在之后也有很多工作，在这个地方做了一些简短的一个介绍。</p></li><li><p>非监督的基于微调的一些工作，代表作是 GPT，然后在之前的之后当然也有很多工作。</p></li><li><p>在有标号的数据上做迁移学习，在 NLP 里面有标号的数据而且比较大的数据，包括了比如说自然语言的一些推理和机器翻译，在两块都有比较大的数据集，然后你在这些有标号的数据集上训练好了模型，然后在别的任务上使用，在计算机视觉里面这一块是用的比较多了，大家经常在ImageNet上训练好模型，再去别的地方用，但是在NLP，似乎这一块不是那么的理想，它在 NLP 似乎这一块不是那么的理想，我觉得可能是一方面这两个任务确实跟别的任务差别还挺大的，第二个是说你的数据量其实还是远远的不够的，有意思的是说BERT 和它之后的一系列工作证明了在NLP上面需要没有标号的大量的数据集训练成模型，效果比你在有标号的相对说小点数据上训练模型效果更好。同样一个想法，现在也在慢慢的被计算机视觉采用，就是说我在大量的没有标号的图片上训练出了模型，也可能比你在ImageNet，这个100万数据集上训练的模型，可能效果还更好。</p></li></ol><h3 id="6-BERT模型"><a href="#6-BERT模型" class="headerlink" title="6.BERT模型"></a>6.BERT模型</h3><p>BERT里面有两个步骤，第一个叫做预训练，第二个叫做微调，接下来是对这两个概念的一个解释，在预训练里面这个模型是在一个没有标号的数据上训练的，在微调的时候我们同样适用一个BERT的模型，但是它的权重就是被初始化成我们在预训练中间得到的那个权重，然后所有的权重在微调的时候都会被参与训练，然后用的是有标号的数据，每一个下游的任务都会创建一个新的BERT模型，虽然它们都是用最早那个预训练好的BERT模型作为初始化，但是对每一个下游任务，都会根据自己的数据训练好自己的模型。</p><blockquote><p>在预训练和微调不是BERT独创，在计算机视觉里面用的很多，但是作者还是在这里做了一个简单的介绍，这是一个非常好的习惯，比如说你在写论文的时候遇到一些技术，你的方法是需要它，而且你觉得可能应该所有人都知道，你就可能一笔带过了，这个不是特别好，就说你的论文需要是自洽的，就后面人过来读过来，可能我不知道什么是预训练，我不知道什么是微调，但是这又是了解你的方法不可缺少的一部分的话，那最好是你还是给大家做个简单的说明，不要让大家去说点一下这个参考文献进去看一下，让大家的阅读带来困难。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/6-1.png" alt="图一"></p><p>接下来我们跳到上面看下图一，图一画了两块，一块是预训练，一块是微调，在预训练的时候，我们的输入是一些没有标号的句子对，我们等会再来看具体它是什么情况，你这里看到的是大概是在没有标号的数据上，训练出一个BERT的模型，把它的权重训练好，对每一个下游的任务，包括一个两个三个，对每个任务我们创建一个同样的BERT模型，但是它的权重的初始化值来自于前面训练好的权重，对每一个任务，你会有自己的有标号的数据，然后我们对着BERT继续进行训练，这样子得到对这个任务而言我BERT的版本。</p><p>接下来我们看一下模型的架构，它架构写的非常简单，BERT模型就是一个多层的双向的transformer的编码器，而且它是直接基于原始的论文和它原始的代码，没有做什么改动，因为它没有做什么改动，所以在这里说我不给大家介绍了，你们直接去看一下原始论文，或者你去看一下这篇 Blog，就能知道这个模型是怎么回事。</p><blockquote><p>你当然这么写是没问题，比如说你把人家东西直接拿过来，所以你在讲自己贡献的第三章不讲那一块是没关系的，但是我还是建议大家，你碰到这种情况的话，你基于别人的文章，你最好还是在第二章，就是在相关工作的时候，对那个文章做一定的介绍，至少你要讲清楚后面你要用到这些L、H是什么意思，不然的话你读到这里，如果你没有读过前面这篇transformer文章的话，那么你就可能读不下去了，你得会跳回去看，而且现在很多人可能第一时间是读BERT这篇文章，因为这个名字听的更多一点，那么你给这些人可能会带来阅读上的一点点不方便。</p></blockquote><p>接下来就是这个模型架构的一些细节，我调了三个参数，第一个是L，就是你的 transformer块的个数，第二个是你的隐藏层大小，第三个是你在自注意力机制里面那个多头的头的个数。我们有两个模型，一个叫做BERT base，一个叫做BERT large, base用的是12层，宽度是768，头的个数是12，所以它总共的可以学习的参数是一个亿，BERT large的话呢就是把层数翻了一倍，然后你的宽度从768变成了1024，具体1024是怎么来的，是因为你BERT你的模型的复杂度跟你的层数是一个线性的关系，跟你的宽度是一个平方的关系，因为你的深度变成了以前的两倍，那么你在宽度上面也选择一个值使的这个增加的平方，大概是之前的两倍，它的头的个数变成了16，这是因为每个头它的维度都固定在了64，因为你的宽度增加了，所以头数也增加了大的模型，它的可学习的参数是3.4个亿。下面解释说，BERT base的选取使得我们跟GPT那个模型的数量大概差不多，可以做一个稍微也还公平的比较，BERT large当然是要去用来刷榜的。</p><blockquote><p>接下来我们给大家介绍一下，你怎么样把那个超参数换算成你可学习参数的大小，也作为transformer架构的一个小回顾，我们知道这个模型里面的可学习参数，主要来自于两块，第一个是你的嵌入层，第二个就来自于你的transformer块。我们首先看一下嵌入层，嵌入层就是一个矩阵，它的输入是你字典的大小，我们字典大小这里是30,000就是30k，你的输出等于你的隐藏单元的个数就是h，它的输入会进入你的transforme块，transformer块里面有两个东西，一个是你的自注意力机制，一个是你后面的 MLP，自注意力机制本身是没有可学习参数的，但是对多头注意力的话，它会把你所有的进入的K(key)、V(value)、Q(query) 分别做一次投影，然后每一次投影它的维度是等于64的，然后因为你有各个头，头的个数 A 乘以64是等于H的，所以在这个地方其实对我们进来的话有一个key，有一个value，有一个 q，它们都会有自己的投影矩阵，这投影矩阵在每个头之间，你把它合并起来，它其实就是一个H乘以H的一个矩阵了，同样道理我拿到输出之后，我们还会做一次投影，同样道理它也是一个H乘H的东西，所以对于一个transformer模块，它的自注意力，它可学习参数是H的平方乘以4，然后再往上是你的 MLP, MLP里面需要两个全连接层，第一层的输入是H，但是它的输出是一个4乘以H的东西，另外一个权利阶层它的输入是4乘以H，但是它的输出是H，所以每一个矩阵它的大小是 H乘以4H，所以两个就是H的平方乘以8，这两个东西加起来是你一个transformer块里面的参数，然后你还要乘以 L，所以你总参数的个数应该就是30k乘以H就是你的嵌入层，再加上 L层乘以 H 的平方再乘以12， 你分别带进去啊假设你是 BERT base 的话，那么的 H 等于768，L等于12，你带进去之后, 你的总数算出来大概就是1.1个亿，后面对 BERT large 的话，你 H 代成1024，然后L代成24的话，你算出来就是3.4个亿，就大家可以去算一下，这个东西大概是不是对的。</p></blockquote><p>讲完模型之后，让我们来看一下它的输入和输出，我们之前有讲过对下游任务的话，有些任务是处理一个句子，有些任务是处理两个句子，所以为了使得BERT模型能处理，所有这些任务的话，我的输入既可以是一个句子，也可以是一个句子对，具体来说，我一个句子的意思是一段连续的文字，不一定是真正上的语义上的一段句子，我的输入叫做一个序列，所谓的序列就是可以是一个句子，也可以是两个句子。</p><blockquote><p>所以这个跟我们之前讲的transformer是有一点不一样的，transformer它训练的时候它的输入是一个序列对，因为它的编码器和解码器分别会输入一个序列，但是 BERT这个地方，我们只有一个编码器，所以我们为了使得能处理两个句子的情况，我们需要把两个句子变成一个序列。</p></blockquote><p>接下来具体看一下，我们的序列是怎么构成的，它这个地方用的切词的方法是WordPiece。</p><blockquote><p>WordPiece核心思想是，假设我按照空格切词的话，一个词作为一个token，因为我的数据量相对来说比较大，会导致我的词典大小特别大，可能是百万级别，那么根据我们之前算模型参数的方法，那么你如果是100万级别的话，就导致我的整个可学习参数都在我的嵌入层上面。WordPiece 的想法是说如果一个词在我整个里面出现了概率不大的话，那么我应该把它切开，看它的一个子序列，如果它的某一个子序列很有可能是一个词根，出现了概率比较大的话，那么就只保留这个子序列就行了，这样的话，我可以把一个相对来说比较长的词，切成很多一段一段的片段，这些片段而且是经常出现的，这样的话我可以用个相对来说比较小的，30,000的一个词典，就能够表示我一个比较大的文本了。</p></blockquote><p>切好词之后，我们看一下怎么把两个句子放在一起。<br>这个序列的第一个词永远是一个特殊的一个记号是[cls]，代表是 classification，这个词的作用是BERT最后的输出代表是整个序列的一个信息，比如说对一个整个句子层面的一个信息，因为 BERT使用的是 transformer 的编码器，所以它的自注意力层里面每一个词都会去看输入里面所有的词的关系，就算是这个词放在我的第一个的位置，它也是有办法能看到之后所有的词，所以它放在第一个是没关系的，不一定要放在最后。第二个是说他把两个句子合在一起，但是因为我要做句子层面的分类，所以我需要区分开来这两个句子，有两个办法来区分，第一个是在每个句子后面放一个特殊的词[SEP]，表示的 separate，第二个是说它学一个嵌入层来表示这个句子到底是第一个句子还是第二个句子，然后它说在图一里面画了一下，它大概长什么样子。</p><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/6-1.png" alt="图一"></p><p>让我们来看一下图一，首先可以看到这个是你输入的一个序列，然后这是你的第一个特殊的记号表示的用来分类，接下来是你的第一个句子，中间用一个separate的分开，这是你的第二个句子，然后每一个token进入BERT，得到这个token的embedding的表示，我们之后再来看这个 embedding 怎么算，对于 BERT 来讲那就是输入一个序列，得到它一个序列，最后一个transformer块它的输出，那么就表示的这个词元的BERT的表示，然后我们在后面再贴加额外的输出来得到我们要的结果。 </p><p>最后一段话说对每一个词元进入BERT的那个向量表示，它是这个词元本身的embedding，加上它的在哪一个句子的embedding，再加上你的位置的embedding，然后在图二它有给大家展示一下是怎么做出来的。</p><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/6-2.png" alt="图二"></p><p>图二演示的是BERT的嵌入层的做法，就是给一个词元的序列，然后得到一个向量的序列，这个向量的序列会进入你的transformer块，首先可以看到每一个方块是一个词元，我们这里有三个东西，第一个东西是一个词元的embedding层，那么就是说这是一个正常的embedding层对每一个词元的话，我们会输出一个它对应的向量。第二个是一个Segment Embeddings，它就表示是第一句话，还是第二句话，那就是a还是b，那么它的嵌入层，它的输入其实就是2，我每次给它的时候，是可以告诉你，这个词元到底是在句子 a 中间，还是在句子 b 中间，然后得到你相应的那一个向量，后面是一个位置的嵌入层，它的输入的大小是你这个序列最长有多长，比如说是1024，它的输入就是每一个词元在这个序列里面的位置信息，从0开始，01234，在得到我对应的位置的那个向量。所以最后的最后就是每一个词元本身的嵌入，加上你在是第几个句子的嵌入，再加上你这个序列中间位置的一个嵌入，记得在 transformer 里面我们有讲过，我们的位置信息是手动，构造出来的一个矩阵，但是在 BERT 里面，不管你是属于哪个句子，还是你的位置在哪里，它对应的向量的表示都是通过学习得来的，这样我们就介绍完了BERT对于预训练和微调都同样的部分。</p><blockquote><p>接下来我们来讲一下，在预训练和微调之间不一样的部分。</p><p>我们知道在预训练的时候，主要有两个东西比较关键，一个是说你的目标函数，第二个是说你用来做预训练的数据。</p></blockquote><p>在3.1节我们分别来看一下它们是什么，首先要介绍的就是掩码的语言模型， 前面其实就是讲一下我们之前说过的那一些东西，就是为什么双向好，带掩码的语言模型是什么样的东西。从这里就开始讲了一点新的东西在里面了，对一个输入的词元序列，如果一个词元是由WordPiece生成的话，那么它有15%的概率会随机替换成一个掩码，但是对于那些特殊的字源，就是第一个词元和中间的分割词元，那我们就不做替换了，那么如果我们的输入序列长度是1,000的话，那么就要预测150个词。接下来它说这个也有一点的问题，是因为我们在做掩码的时候，就会把词元替换成一个特殊的token，叫做[MASK]，那么在训练的时候，你大概会看到15%的词元就是对应这个mask，但是在微调的时候是没有这个东西的，因为微调的时候我不用这个目标函数，所以没有mask的东西，导致在预训练的时候，微调的时候，看到的数据会有一点点不一样，这会带来一点问题，它的一个解决方法是对于这15%的被选中去掩码的词，有80%的概率我是真的把它替换成这个特殊的掩码符号，还有10%的概率我把它替换成一个随机的词元，还有10%的概率我什么都不干，我就把它存在那里，但是用它来做预测，具体来说这个80%、10%、10%是怎么选出来的，它说我们有一个ablation study，然后给大家说我们这个东西其实是在实验上跑了，发现这个东西还不错。</p><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/6-3.png" alt="a.1"></p><p><code>它在附录的里面有给大家讲几个例子，我们来看一下，这里是附录a.1它给了三个例子，假设我的输入是一个 my dog is hairy 的话，我把最后这个词圈中了，用来做掩码的话，那么有80%的概率我把最后那个词真的就换成了 mask这个符号，还有10%的概率我就在我的字典里面随机选一个词，比如说选中的 apple 把它换掉，还有10%的概率我就什么都不变，但是就标记下这个词我用来要做预测，所以看到需要中间这个情况，是给你加入了一些噪音，最后一个情况其实是用了，你和你真的在做微调的时候，你真实看到数据是没有变化的，所以你真实看到数应该就是它了。</code></p><blockquote><p>接下来还是回到前面我们看一下，预训练中间第二个任务就是预测下一个句子。</p></blockquote><p>在 QA 和在自然语言推理里面，它们都是一个句子对，所以如果能够让它学一些句子层面的信息是不错的，具体来说我们的一个输入序列里面有两个句子，有一个 A 有个 B，然后有50%的概率b是在原文中间，真的是在 a 之后，还有50%的概率b就是一个随机从一个别的地方选取出来的一个句子，那么意味着是50%的样本是正例，50%的样本是负例。在5.1的时候，有一些结果的比较，加入这个目标函数, 能够极大的提升Q&amp;A和在语言推理的效果。</p><p><code>同样在附录的时候，它又给了一些例子我们来看一下附录，仍然是在附录 a.1，给了一个正例一个负例，正例是说这个人要去一个商店，然后他买了一加仑的牛奶，那么可以看到这个两个句子，应该就是在原文中间是一个相邻的关系，后面一个是说这个人去了商店，然后企鹅是一种不能飞的鸟，那么这两个东西当然是没有太多关系，所以这个是一个负例，而且注意到一点，这里有个</code><em><strong>##</strong></em><code>, 其实是它是在语文中间是一个词，叫做 flightless 就是一个不能飞的鸟，但是这个词出现的概率不高，所以在 WordPiece 里面把它砍成了两个词，一个叫 flight，一个 less 它都是比较常见的词，</code><em><strong>##</strong></em><code>表示的是后面那个词，其实应该是在原文中是跟着前面那个词的意思。 </code></p><p>在模型预训练这一节，最后一小段讲的是它的训练的数据，它用了两个数据集，第一个是由8个亿词的一个书本构成的数据集，第二个是Wikipedia 英文的，有着25亿个词的一个数据集，Wikipedia数据集比较好下，大家这个直接就下下来就行了，然后它又加了一句说我们应该用文本层面的一些数据集，就是我的里面是一片一片的文章，而不是一些随机打乱的一些句子，这是因为 transformer确实能够处理比较长的序列，所以给一个整个文本序列过来，当然效果会比较好一点。</p><p>第3.2章就是用BERT做微调的一个一般化的介绍，它首先讲了一下，BERT跟一些基于编码器解码器的家伙有什么不一样，我们在之前讲的 transformer 是编码器解码器，因为我们把整个句子对都放在一起进去了，所以 self-attention能够在两端之间相互能够看，但是在编码器和解码器这个架构里面，编码器其实一般是看不到解码器的东西，所以 BERT在这一块会更好一点，但实际上来说你也付出了代价，就是说你不能像transformer一样的，可以做机器翻译了。接下来我在做下游任务的时候，我会根据我的任务设计我们任务相关的输入和输出，所以的好处是我的模型其实不怎么要变，主要是怎么样把我的输入改成我要的那一个句子对，如果你真的有两个句子的话，当然就是句子 A 和 B 了，如果你就只有一个句子的话，比如说我要做一个句子的分类的话，那我B就没有了，然后根据你下游的任务的要求，我要么是拿到第一个词元对应的输出做分类，或者是拿到对应那些词元的那些输出，做你要的那些输出，不管怎么样，我都是在最后加一个输出层，然后用一个softmax得到我要的那些标号，跟预训练比微调相对来说比较便宜，所有的结果都可以使用一个TPU，然后跑一个小时就行了，使用 gpu 的话，多跑几个小时也是可以的，然后具体对每一个下游任务是怎么样，构造输入输出的，会在第四节给大家详细来介绍。</p><h3 id="7-实验"><a href="#7-实验" class="headerlink" title="7.实验"></a>7.实验</h3><blockquote><p>第四节讲的就是BERT怎么样用在各个下游任务上，特别是在摘要里面提到那几个BERT能赢下来的数据集上面。</p></blockquote><p>第一个任务叫做GLUE，它里面也包含了多个数据集，它是一个句子层面的一个任务，所以BERT就是把第一个特殊词元，就是cls这个词元，它的最后的那个向量拿出来，然后学习一个输出层就是一个 W，把它放进去之后，用softmax就得到你的标号，然后就是一个很正常的多类分类问题了。</p><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/6-4.png" alt="表一"></p><p>在表一就是在这个任务上的一些分类的结果，你可以看一下最后那个 Average，就是在所有数据集上的平均的值，当然是越高越好，这是个精度，可以看到BERT base，BERT large基本上比前面的都还要好一些，这是 GPT，然后这个是另外一个在 GPT 之前最好的结果，基本上看到是说 BERT，就算是在base跟 GPT 的可学习的参数差不多的情况下，也还是能够提升是比较大的。</p><p>接下来接下来是 SQuAD，就是斯坦福的一个 Q&amp;A 的数据集，在 Q&amp;A 这个任务里面，你的任务是我给你一段话，然后问你一个问题，需要你把我的答案找出来，这个答案已经在我给你的那一段话里面，你只需要把我这个答案对的那一个小的片段给我找出来就行了，就是这个片段的开始和结尾，所以这样子的话，它其实说白了就是对每个词元，我来判断一下你是不是答案的开头，是不是答案的结尾。具体来说它就是学两个向量S 和 E，分别对应的是这个词元，是这个答案开始的概率和答案最后的概率，具体来说它对每一个词元，也就是第二句话里面那个每一个词元，它跟它的相乘，然后再做 softmax，就会得到这个段里面每一个词元，它是答案开始那个概率，同样道理的话我会算出来它是尾的概率。然后我们就开始训练了，它这提到一点是我在做微调的时候，它用的是三个 epoch，就是扫数据三遍，然后学习利用的是5e-5，然后 bachsize 是 32，这个东西其实误导了很长一段时间，大家后来发现你用 BERT 做微调的时候，结果非常不稳定，就是同样的参数，我同样的数据集我给你训练十遍，你的Var(variance方差)特别大，最后大家发现是其实很简单，就是你这个东西不够，就是3这个东西太少了，你可能要多学习几遍会好一点，然后大家发现 BERT 用的优化器是一个 adam 的不完全版，这个东西在于你的BERT要训练很长时间的时候是没关系，但是你BERT的如果只训练那么小小的一段时间的话，那么这个东西会给你带来影响，你要把它换回到Adam的正常版会解决这个问题，表现的是BERT的它的结果，基本上可以看到是说，BERT基本上是比别人都要好很多的，还有一个是这个Q&amp;A 数据集的2.0版本，我们就不给大家仔细介绍了。</p><p>最后一个是一个叫做SWAG的一个数据集，它用来判断两个句子之间的关系，同样跟之前的训练没有太多区别，而且结果上来说，BERT 也是比别人要好很多。</p><blockquote><p>这样我们就讲完了BERT怎么做微调，基本上可以看到是说，对于还是挺不一样的这些数据集，BERT用起来还是挺方便的，你就基本上只要把它表示成你要的那一个一对句子的形式，最后的你拿到对应的输出，然后加一个输出层基本上就完事了，所以BERT对NLP整个领域的贡献还是非常大的，这样子有大量的任务可以用一个比较相对来说简单的架构，也不需要改太多东西，我就能做了。</p></blockquote><p>第五节讲的是Ablation Studies，就是说我的 BERT 设计有那么多东西，然后看一下每一块最后对我的结果的贡献是什么样的。首先假设我去掉那一个下一个句子的预测会怎么样，以及如果是我就是看一个从左看到右的语言模型会怎么样，而不是用我带掩码的语言模型，从左看到右而且没有下一个句子预测，那这个是说，我可以在上面加一个双向的 LSTM，其实是从 ELMo 那边来的想法，然后看一下结果怎么样，基本可以看到是去掉任何一个，它的效果都会有打折，特别是说对这个任务，你的影响会比较大一点。</p><p>5.2讲的是模型大小的影响，我们知道，BERT base是一个亿的可学习的参数，BERT large是三个亿的参数，相对之前transformer，其实你的提升还是挺大的。它说我们这个也跟NLP其他大家的想法是一样的， 当你把模型变得越来越大的时候，效果会越来越好，然后它说我们认为这个是第一个工作展示了，你如果把模型变得特别特别大的时候，对于你的语言模型来说有比较大的提升。</p><blockquote><p>从现在角度来看，BERT 并不大也就一个亿，而现在你 GPT3 都做到1千个亿的样子了，甚至大家在一万亿的那个程度在走，但是在三年前，确实 BERT 是开创性的工作，把一个模型能够推到那么大，引发了之后的模型大战，就是看谁的模型大。</p></blockquote><p>最后一节5.3讲的是说，假设我不用BERT做微调的时候，而是把BERT的特征作为一个静态特征输进去会怎么样，当然它的结论是，效果确实没有微调那么好，所以也是这篇文章出来的一个卖点是，用 BERT 的话，你应该用微调。</p><h3 id="8-评论"><a href="#8-评论" class="headerlink" title="8.评论"></a>8.评论</h3><p><code>沐神评价</code></p><p>最后给大家总结一下 BERT 这篇文章，从写作上来说我觉得写的还行，中规中矩吧。先写了 BERT和ELMo 和 GPT 的一个区别，然后介绍 BERT 这个模型怎么回事，接下来是说你在各个实验上，你的设置是什么样子，最后是比的结果，但结果是非常好了。</p><p>在 BERT 这篇文章的结论里面，它认为它这篇文章最大的贡献是这个双向性，我也理解说你写一个文章是最好卖一个点啊，不要说我在文章这里也好那里也好，别人都记不住，但是我觉得选用双向性这个词呢，还是有待商榷了。今天来看，BERT的贡献不仅仅是你的双向性对吧，还有很多别的东西，就算你要写我的贡献是双向性的话，我觉得你从写作上来说，至少要说你选的双向性给你带来的不好是什么，你做一个选择会得到一些，也会失去一些，你跟 GPT 比你用的是编码器，GPT 用的是解码器，当然你得到了一些好处，但是你也失去了一些，比如说你做机器翻译就不那么好做了，给一个句子翻另外一个句子，你做文本的一个摘要也不那么好做了，就是说你做那些生成类的东西可能就没那么方便了。</p><p>当然了分类问题在 NLP 里面更加常见一点，所以BERT这么做了之后啊，NLP 的那些研究者其实更喜欢BERT一些，因为把BERT用在自己的问题上更加容易一点。</p><p>另外说你如果你从现在再回过去看 BERT 的话，其实你看到的是一个完整的一个解决问题的思路，符合了大家对一个深度学习模型的一个期望，这是我训练一个很深的很宽的一个模型，在一个很大的数据集上训练好，这个模型拿出来之后，可以用在很多小的问题上，能够通过微调来全面提升这些小数据上的性能，这个在计算机视觉里面，我们用了很多年了，在 NLP 的时候，BERT 才给大家展现的是说，我现在可以训练一个很大的，三个亿的一个参数的模型，使用的是几百GB 的一个数据集，然后给大家展示了你的模型，越大的时候你的效果越好，所以这个很好的满足了深度学习研究者的一个喜好，很简单很暴力效果很好。</p><p>但我一直其实是有疑惑的, 就是BERT是在这一系列工作中间的一篇，就是让更大的数据集训练更好的模型，比前面都要好，ELMo出来的时候其实也是这样子，GPT在 BERT 之前出来，其实也相对来说训练更大的模型，它也在很多数据集上取得了很好的成绩。BERT在另外方面讲，你也就是一个更大一点的模型、更大的数据集，在一些任务上取得很好的成绩，你的好结果的唯一的结局是被后面的工作所超越，所以为什么大家记住的是 BERT，如果你把BERT和 GPT 相比，它们大概是同期的一个工作，虽然BERT确实比 GPT 的结果好那么一些，但是它的思路是非常一样的，但是从利用率上来讲，BERT是 GPT 的10倍，意味着是说，它的影响力至少是你的十倍以上。</p><h3 id="9-面试题"><a href="#9-面试题" class="headerlink" title="9.面试题"></a>9.面试题</h3><ol><li>BERT分为哪两种任务，各自的作用是什么？</li><li>在计算MLM预训练任务的损失函数的时候，参与计算的Tokens有哪些？是全部的15%的词汇还是15%词汇中真正被Mask的那些tokens？</li><li>在实现损失函数的时候，怎么确保没有被 Mask 的函数不参与到损失计算中去？</li><li>BERT的三个Embedding为什么直接相加？</li><li>BERT的优缺点分别是什么？</li><li>你知道有哪些针对BERT的缺点做优化的模型？</li><li>BERT怎么用在生成模型中？</li></ol></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文精读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文5】GAN论文逐段精读</title>
      <link href="/2023/12/04/lun-wen-5-gan-lun-wen-zhu-duan-jing-du/"/>
      <url>/2023/12/04/lun-wen-5-gan-lun-wen-zhu-duan-jing-du/</url>
      
        <content type="html"><![CDATA[<html><head></head><body></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文精读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文4】零基础多图详解图神经网络（GNN-GCN）</title>
      <link href="/2023/12/04/lun-wen-4-ling-ji-chu-duo-tu-xiang-jie-tu-shen-jing-wang-luo-gnn-gcn/"/>
      <url>/2023/12/04/lun-wen-4-ling-ji-chu-duo-tu-xiang-jie-tu-shen-jing-wang-luo-gnn-gcn/</url>
      
        <content type="html"><![CDATA[<html><head></head><body></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文精读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文3】Transformer论文逐段精读</title>
      <link href="/2023/12/04/lun-wen-3-transformer-lun-wen-zhu-duan-jing-du/"/>
      <url>/2023/12/04/lun-wen-3-transformer-lun-wen-zhu-duan-jing-du/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><blockquote><p>这篇文章可以认为是最近三年以内深度学习里面最重要的文章之一，它可以认为是开创了继MLP cl和RNN之后的第四大类模型。在上个月，斯坦福联合了100多名作者写了一篇200多页的综述文章，也就是讲Transformer模型和它之后的一些变种，他们甚至提议说将这一类模型叫做基础模型，可以见它对整个领域的影响力是有多大。</p></blockquote><h3 id="1-标题"><a href="#1-标题" class="headerlink" title="1.标题"></a>1.标题</h3><p>首先我们看一下标题，标题是<code>Attention Is All You Need</code> ，就是说你就需要注意就行了，当然在英语中，这也是一句合法的话，就是说对小孩说集中一下注意力，不要东看西看，然后这个标题成为了一个梗，就是什么什么is all you need，然后在后续出了很多很多篇文章，然后你就把这个词换成任何你想要的词，只要你换成这个词，基本上你的文章能够上头条。</p><h3 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a>2.摘要</h3><p>这个摘要的第一句话是说，在主流的序列转录模型里面，</p><blockquote><p><code>所谓的序列转录模型就是说给你一个序列，你生成另外一个序列，机器翻译说给一句英文，甚至一句中文，当然是一个序列转录模型。</code></p></blockquote><p>这样的模型主要是依赖于比较复杂的循环，或者是卷积神经网络，然后它一半是用一个叫做encoder和decoder的架构。</p><blockquote><p>当然这句话意思是说你多多少少知道这个模型的encoder、decoder的架构什么样子，CNN、RNN是什么样子。</p></blockquote><p>然后第二句话是说，在性能最好这些模型里面，通常也会在你的编码器和解码器之间使用一个叫做注意力机制的东西。</p><blockquote><p>基本上就是说这篇文章讲的是我要做序列到序列的生产，但是现在主流的模型是干什么东西。</p></blockquote><p>第三句话是说我这篇文章提出了一个新的简单的架构。</p><blockquote><p>所以现在比较有意思的是，之前我们都说我们提供一个novel，就是比较有意思的架构，现在基本上因为我们的模型其实现在都挺复杂的，如果你能做一个simple的架构，其实也挺好了，只要你的结果好，大家其实还是挺喜欢简单的架构，我们之前讲的Resnet，其实也是一个比较简单的架构，所以大家挺喜欢的，这个也是整个研究氛围的一个转变，我们说simple这个词不再是一个贬义词，而是一个褒义词，这样的结果好。 </p></blockquote><p>它说这个模型的名字叫Transformer，中文翻译叫做变形金刚。</p><blockquote><p>这个也是比较有意思的取名方法，你当然可以说我把我的模型取得一些大家很熟知的一些名词，就很容易被记住，但是你这个问题是说，如果你的文章没有出名，大家去搜你文章的时候，根本就搜不到你的文章，还是搜到的是变形金刚，取名字也是一个非常重要的事情，好的文章一般有一个比较好的名字。</p><p>我们之前讲过的Resnet，Resnet这个名字其实挺好的，叫resnet，很好记对吧，然后我们讲到第一篇文章Alexnet，其实那篇文章根本就没有提到Alexnet这个名字，他根本没有给自己的文章取个名字，估计作者也没想到自己会那么火，所以他说我们做了一个神经网络，因为这篇文章是开创性的工作，大家重复你的结果的时候，总要给你的模型取个名字，然后提到你的时候最好有个名字，所以大家给了你一个名字叫做Alexnet，Alex来自与第一作者的名字，所以对后面文章的作者来讲，如果不想让别人给你取名字的话，当然给自己取一个比较好的名字比较重要。</p></blockquote><p>然后它接着说，我这个模型仅仅依赖于注意力机制，而没有用之前的循环或者是卷积。</p><blockquote><p>这就是它的贡献了，它提出了一个新的模型，简单，然后跟之前大家表现很好的模型的架构都长得不一样。</p></blockquote><p>接下来说我做了两个机器翻译的实验，显示这个模型在性能上特别好，他说可以并行度更好，然后许用更少的时间来训练，他说我的模型达到了28.4的BLEU。</p><blockquote><p><code>BLEU是在机器翻译里面大家经常用的一个衡量标准，如果你不做机器翻译的话，你可能不一定明白BLEU是干什么事情，但是没关系。</code></p></blockquote><p>它说我这个是在英语到德语的一个翻译工作，比目前的最好的结果了两个BLEU，然后在一个英语到法语的翻译的任务上面，他们做了一个单模型，比所有的模型都要效果要好，然后它只在八个GPU上训练了3.5天，最后他说我的Transformer架构能够泛化到一些别的任务上面都很好。</p><blockquote><p>那基本上你可以看到是说，我提出了一个新的模型，主要用在是什么呢，在机器翻译这个任务上面，所以这篇文章一开始写的时候是针对机器翻译这个小任务写的，所以他整个写作的时候，他去假设你前面是知道的，然后提出一个模型，然后就主要在我的机器翻译上结果很好。这个也是比较有意思的工作，他一开始做的是机器翻译，这个比较相对来说小一点的领域上面，之所以说小，是因为你会发现机器翻译也就那么几家公司关心，你能够提供动机翻译的那些服的公司，其实全世界范围来讲也就那么多家。</p><p>但是随着之后bert、gbt把这个架构用在更多的在源处理的任务上的时候，整个这个工作要出圈了，然后最近当然大家都知道用在了图片上面，用在了video上面，几乎上什么东西都能用，所以它真正的火出圈是在这个地方。</p><p>但是你第一次读这个文章的时候, 你可能看到机器翻译这一块，你可能不那么感兴趣，当然了，现在我们知道这篇文章非常重要。</p></blockquote><h3 id="3-导言"><a href="#3-导言" class="headerlink" title="3.导言"></a>3.导言</h3><blockquote><p>这里的导言写的是比较短的，基本上可以认为是他前面摘要的前面一半的一个扩充。</p></blockquote><p><em><strong>第一段</strong></em><br>第一段话是说，在时序模型里面，当前最常用的是RNN（这是2017年），然后它包括了LSTM、GRU，然后接下来他当然是说，在这里面有两个比较主流的模型，一个叫做语言模型，另外一个是当你的输出结构化信息比较多的时候，大家会用一个叫做编码器和解码器的架构。</p><p><em><strong>第二段</strong></em><br>第二段话是讲RNN的特点是什么，同样也是它的缺点是什么。</p><blockquote><p><code>在RNN里面，给你一个序列的话，它的计算是把这个序列从左往右移一步一步往前做。假设你的一个序列是一个句子的话，它就是一个词一个词的看。</code></p></blockquote><p>对第t个词，它会计算一个输出叫做ht，也叫它的隐藏状态，然后呢它的ht是由前面一个词的隐藏状态，叫ht-1，和当前第t个词本身决定的，这样子的话它就可以把前面学到的历史信息，通过ht-1放到当下，然后和当前的词做一些计算，然后得到输出，这也是RNN如何能够有效处理时序信息的一个关键之所在，它把之前的信息全部放在隐藏状态里面，然后一个一个放下去。 </p><blockquote><p>但它的问题也来自于这里。</p><p>第一个说它是一个时序，就是一步一步计算的过程，它比较难以并行，就是说你在算第t个词的时候，算ht的那个出处的时候，你必须要保证第前面那个词的ht-1输入完成了，假设你的句子有100个词的话，那么就是说你得时序地上100步，导致说你在这个时间上你无法并行，现在在主流的GPU和那些加速器，比如说TPU一样的，大家都是成千上万个线程，你无法在这个上面并行的话，导致你的并行度比较低，使得你在计算上性能比较差。</p><p>第二个也是因为这个原因，你的历史信息是一步一步的往后传递的，如果你的时序比较长的话，那么你在很早期的那些时序信息，在后面的时候可能会丢掉，如果你不想丢掉的话，那你可能得要ht要比较大，就说你得做一个比较大的ht，但是这个的问题是说，如果你做比较大的ht，你在每一个时间不都得把它存下来，导致你的内存开销是比较大的。</p></blockquote><p>当然他也提到过，这一块其实大家在过去这些年做了非常多的改进，不管是并行的改进，以及做一些分解的方法使得我们能够提升并行度，但是本质上还是没有解决太多问题。</p><p><em><strong>第三段</strong></em><br>第三段，他其实讲attention在RNN上的应用。</p><blockquote><p>在这篇文章之前，attention已经被成功地用在编码器和解码器里面了，它主要是用在怎么样把编码器的东西很有效的传给解码器，主要是用到这一块，就是说你跟RNN是一起使用的，这个其实是讲这一段的事情。</p></blockquote><p><em><strong>第四段</strong></em><br>最后一段讲的是这篇文章提出来的Transformer，这是一个新的模型，不再使用之前被大家使用的循环神经层，而是纯基于注意力机制了，他说我这个东西是可以变形的。</p><blockquote><p>因为之前你攻击的就是时序神经网络主要是按时序地做运算，现在你做了attention之后，你可以完全做并行.。</p></blockquote><p>因为它现在纯用的attention，所以它的并行度是比较高的，这样子的话它能够在比较短的时间之内，做到一个跟之前可能更好的结果。</p><blockquote><p>这就是导言干的事情，总体来看这个导言是写的比较短的，可以认为就是摘要的前面几句话的一个稍微的扩充版本，对自己提出的，也就是一句话带过了。</p><p>这么写的原因，我觉得应该是因为这篇文章提出来的东西是比较多的，它是一个比较不一样的一个网络里面有一些核心的东西在里面。然后是发表在NeurIPS上面，NeurIPS是一个篇幅比较短的一个会议，它是一个单列的，然后也就八页吧，所以导致说你要在这么一个短的模板里面写下很多东西是很难的，那你就得压缩掉一些东西。</p></blockquote><h3 id="4-结论"><a href="#4-结论" class="headerlink" title="4.结论"></a>4.结论</h3><p>结论的第一句话是说我们介绍了Transformer这个模型，这是第一个做序列转录的模型，然后仅仅使用注意力，它把之前所有的循环程全部换成了multi-headed self-attention，基本上可以看到这篇文章主要用的是提出了是这样一个层。</p><p>第二句话是说，在机器翻译这个任务上面，Transformer能够训练的比其他的架构都要快很多，而且呢在实际的结果上确实是效果比较好。</p><p>然后第三段是说，他对于这种纯基于注意力机制的模型感到非常的激动，他想把它用在一些别的任务上面，他觉得可以用在文本以外的数据上，包括了图片、语音、video，然后他说使得生成不那么时序化也是另外一个研究的方向。</p><blockquote><p>其实现在看起来作者多多少少是预测的未来，Transformer真的在各种别的数据上以及这一块做的是比较好，虽然这些工作基本上都不失于本篇文章作者完成的，都是由别人完成的，但是本文的作者基本上是看准了大方向的。</p></blockquote><p>最后一句话是说，这篇文章所有代码放在tensor2tensor这个库里面。</p><blockquote><p>这也是比较有意思的写法，他把整个代码放在了结论的最后，但是现在我们知道如果你有代码的话，通常你会把这个代码放在你摘要的最后一句话。因为现在神经网络的文章里面细节通常是比较多的，简简单单的一篇文章很难把所有的细节写清楚，所以你最好第一时间公布你的代码，让别人能够很方便的重复你的文章，然后这样能扩大你文章的影响力。</p></blockquote><h3 id="5-相关工作"><a href="#5-相关工作" class="headerlink" title="5.相关工作"></a>5.相关工作</h3><p><em><strong>第一段</strong></em><br>首先他第一段提的是如何使用卷积神经网络来替换掉你的循环神经网络，使得减少你的时序的计算。他又提到，这些工作主要的问题是说用卷积神经网络对于比较长的序列难以建模。</p><blockquote><p><code>这是因为我们知道卷积做计算的时候，每一次它去看一个一个比较小的一个窗口，比如说看一个3x3的一个像素块，如果你两个像素隔得比较远的话，你得需要用很多层卷积，一层一层上去，才能够最后把这两个隔得远的像素给你融合起来。</code></p></blockquote><p>但是他说如果使用Transformer里面的注意力机制的话，每一次我能看到所有的像素，我一层就能够把整个序列给你看到，相对来说就没有这个问题。但是他又提到，卷积的一个比较好的地方是可以做多个输出通道，一个输出通道可以认为是它可以去识别不一样的模式，所以他说我也想要这样子的多输出通道的效果，所以他提出了一个叫做Muti-Headed Attention，就是多头的注意力机制，所以可以模拟卷积神经网络多输出通道的一个效果。</p><p><em><strong>第二段</strong></em><br>接下来第二段他讲的是自注意力机制，其实这个是Transformer里面一个关键性的点，但是他说这个工作其实之前已经有人提出来了，并不是我这个工作的创新。</p><p><em><strong>第三段</strong></em><br>另外他又提到一个叫做memory networks的东西。</p><blockquote><p>这个在17年的时候也算是一个研究的重点，如果大家不知道的话，我们可以跳过。</p></blockquote><p><em><strong>第四段</strong></em><br>在我们best knowledge里面，我们的Transformer是第一个只依赖于自注意力，来做这种encode到decode的架构的模型。</p><blockquote><p>这就是相关工作的章节，关键是说你要讲清楚，跟你论文相关的那些论文是谁，跟你的联系是什么，以及说你跟他们的区别是什么。</p></blockquote><h3 id="6-模型"><a href="#6-模型" class="headerlink" title="6.模型"></a>6.模型</h3><blockquote><p>接下来是第三章模型架构，我们知道深度神经网络的论文里面最重要的就是这一章了，那一章怎么讲你这个神经网络长什么样子。</p></blockquote><p><em><strong>第一段</strong></em><br>第一句话说这些序列模型里面，现在比较好的是一个叫做编码器和解码器的架构，然后他解释一下什么是编码器、解码器。</p><p>就对编码器来讲，它会将一个输入，就是一个长为n的一个x1一直到xn的一个东西，假设你是一个句子的话，有n个词的话，那么第xt就表示你的第t个词，他说将这个序列呢编码器会把它表示一个也是长为n，但是呢其中每一个zt，它对应的是xt的一个向量的表示，假设你是一个句子的话，那么zt就表示你第t个词的一个向量的表示，这就是你的编码器的输出，就是这样一些原始的一些输入变成一个机器学习可以理解的一系列的向量。</p><p>那对解码器来讲，我会拿到编码器的输出，然后它会生成一个长为m的一个序列。<br><code>首先注意到n和m是不一样长的, 可以一样可以不一样, 比如说你英文句子翻译中文句子的话, 那么两个句子很有可能是不一样长的。</code><br>他跟编码器的一个大的不一样，在解码器里面，你的这个词是一个一个生成的，因为对编码器来讲，你很有可能是一次性能看全整个句子，就是说做翻译的时候，我可以把整个英语的句子给你，但是你在解码的时候，只能一个一个的生成，这个东西叫做一个叫做自回归，叫做auto-regressivet的一个模型，在这个里面，你的输出又是你的输入。</p><blockquote><p><code>具体的看一下，在最开始我给定的z，那么你要去生成第一个输出，叫做y1，在拿到y1之后，我就可以去生成我的y2，然后一般来说你要生成yt的话，你可以把之前所有的y1到yt-1全部拿到，也就是说你在翻译的时候，你是一个词一个次地往外蹦，所以就是说你在过去时刻的输出，也会作为你当前时刻的输入，所以这个叫做自回归。</code></p></blockquote><p><em><strong>第二段</strong></em><br>然后它又很简单地来了一句说Transformer是使用了一个编码器解码器的架构，具体来说它是将一些自注意力和point-wise，fully connected layers，然后把一个一个堆在一起的，我们在下面的统一给大家展现这个架构。</p><blockquote><p>这个图如果你讲Transformer的话，很有可能你就是把这个图复制一下，然后放到你的ppt里面给大家讲，这就意味着说，如果你写论文的话，有一张比较漂亮的、能够把整个全局画清楚的图是非常重要的，因为很有可能别人讲你的论文的时候，就是把这个图搬过去。如果你的图画的不够好的话，别人可能还花半天来讲这些东西，如果你画的很好的话，就是一张图能够搞定所有东西。所以就是说在神经网络年代，会画图是一个很基础的技能。</p><p>但具体到这篇文章的话，这张图画的是挺好的。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-1.png"></p><p><code>首先你看到它是一个编码器和解码器的架构，这个东西（左边）是你的编码器，这一块（右边）是你的解码器，（Inputs）这是编码器的输入，就是比如说你中文翻英文的话，那么这就是你的中文的句子，然后（Outputs）这是你解码器的输入，在解码器在做预测的时候是没有输入的，实际上它就是解码器在之前时刻的一些输出作为输入在这个地方，所以这个地方写的是一个output，然后他说shifted right就是一个一个往后往右移。然后看到是你的输入进来，先进入一个嵌入层，那可是大家都要干的事情，就是说你进来是一个一个词，我要把它表述成一个向量。那这个地方加了一个叫做positional encoding，我们等会再来讲。这个地方（左上方）就是你的核心的一个编码器的架构了，这个n是说你这个层有n个，就是n个这样层摞在一起，比如说你在讲Resnet的时候，我们说的一个残差块是一个块，然后你把n个块摞在一起，最后摞成了你的东西，在这个地方（左上方）你可以认为这个叫做Transformer block，也是Transformer的一个块，具体你进去看的话，你会发现是说第一个一个叫做Multi-headed Attention，然后再有一个前馈神经网络，然后他有一个什么这个东西，这个东西大家知道这个就是一个残差的连接，然后这个norm我们等会儿再讲，基本上可以看到是说一个注意力层，再加上一个基本就是一个MLP，然后在中间有一点的残差连接，然后再有一些的normalization，然后你的编码器的输出就会作为你的解码器的一个输入, 在这个地方放进来。解码器的话跟编码就有点像，所以这一块是一样的，但是它多了一个叫做Masked的一个多头注意力机制，当然我们等会会来讲，同样道理的话，你可以基本上可以认为就是解码器其实就是这三块组成一个块，然后把它重复n次，会得到你最后的一个解码器，最后你的输出进入一个输出层，然后做一个softmax就会得到你的输出，这个是这一块就是标准的神经网络的做法。它确实是一个比较标准的编码器解码器的架构，只是说你中间的每一块跟之前是一个不一样的地方，但还有一个是说你怎么样，这个东西怎么过来，也是有一点的不一样的。</code></p><blockquote><p>接下来我们看一下每一个具体模块是怎么实现的。</p></blockquote><p>首先他给大家介绍了一下它的编码器，编码器是用一个n等于六个的一个完全一样的层，也就是之前我们画的这一块（左边），就是说，他把这个东西叫做layer，然后再用重复六个layer出来，每个layer里面会有两个sub-layers，就是一个子层，第一个sub-layer叫做multi-head self-attention，这个词已经出现很多次了，但是现在没有解释，他在之后才会解释。第二个子层名字很长，simple，position-wise fully connected feed-forward network，后面这个词是一个词，它说白了就是一个MLP，然后所以他为什么加一个simple在这里，他为了显得fancy一点，就把名字搞得特别长，我们之后再来解释。他说对每一个子层，他用了一个残差连接，我们上一期已经讲过残差连接了，他说最后我们在使用一个叫做layer normalization的东西，解释完这一些之后，他说我这个子层，其实它的公式要写出来就是长成这个样子的，<code>LayerNorm(x + Sublayer(x))</code>，就是说你的输入x进来，然后先进入你的那个子层，你是自注意力也好，mp也好，然后因为是残差连接，他就把输入和输出加在一起，最后进入他的LayerNorm。然后说他说为了简单起见，因为我的残差连接需要需要你的输入和输出是一样大小，如果不一样大小的话，你得做投影，从而为了简单起见，我就把每一个层它的输出的维度变成512，也就是说你对每一个词，你不管在哪一层，我都做了是512的这个长度的表示。这个我们之前讲的CNN是不一样的，或者我们之前做MLP的时候，经常会把维度啊往要么是往下减，要么CNN的话是空间维度往下减，但是channel维度往上拉，但是这个地方其实它就是固定长度来表示，使得这个模型相对来说是比较简单的，然后调参也就调一个参就行了，另外一个参数说你要复制多少块，所以这个简单设计影响到后面一系列网络，他说，bert怎么样GBT怎么样，实际上也就是两个操场是可以调的，你就要多少层，然后每一层里面那个维度有多大，也就是这两个参数。</p><blockquote><p>接下来给大家解释一下什么是LayerNorm，可能你不做这一块的话，可能之前是不知道LayerNorm LayerNorm也是因为Transformer这篇文章被大家广为知道。</p><p>如果你写篇文章的话，你说我用了别人的东西，最好在文章里面真的讲一下它是什么东西，你不能真的指望别人都知道所有的细节，能能够花几句话讲清楚是不错，不然的话别人还得去点开那个链接去看一下到底是什么东西，是给大家带来了困难。</p></blockquote><blockquote><p>接下来我们通过跟batch norm来对比来解释一下什么是layernorm，以及为什么我们在这些变长的应用里面不使用batchnorm。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-2.png"><br><code>所以我们考虑一个最简单的二维输入的情况，二维输入的话我就是输入是一个矩阵，然后我的每一行是一个样本，这是我的x，这个是我的batch，然后我的每一列是我的一个特征，那么这写的就是一个feature，batchnorm的时候干的事情就是每一次我去把我的每一个列，就是每一个特征，把它在一个小mini-batch里面，它的均值变成0方差变成1，你怎么把一个向量变成均值为0、方差为1呢，这是你把它的这个向量本身的均值减掉，然后再除以它的方差就行了，这个地方你算均值的时候，是在每一个小批量里面，就这条向量里面算出它的一个均值，算出它的一个方差。这个是在训练的时候，你可以做小批量，在预测的时候，你会把一个全局的一个均值给算出来，这个你认为是以整个数据扫一遍之后，在所有数据上那些平均的那个均值方差存起来，在预测的时候再使用。当然batchnorm还会去学一个能把它一个伽马出来，就是说我可以把这个向量通过学习可以放成一个任意方差为某个均值为某个值的一个东西。layernorm跟batch norm在很多时候是几乎是一样的，除了他做的方法有点不一样之外，还如果同样的是我这一个二维输入的话，layernorm干的事情就是对每个样本做了normalization，而不是对每个特征做了，就之前我是把每一个列它的均值变0方差变1，现在是我把每一个行变成均值为0、方差为1，这个行就表示的是一个样本，所以你可以认为这个layernorm就是整个把数据转置一下放到batchnorm里面出来的结果，再转置回去一下，基本上可以得到自己的东西了，这个是当你的输入是二维的时候最简单的情况，但是在我们的Transformer里面，或者说正常的RNN里面，它的输入是一个三维的东西，因为它输的是一个序列的样本，就是每一个样本其实是里面有很多个元素，它是一个序列，你给一个句子里面有n个词，所以每个词有个向量的话，还有一个batch的话，那么就是个3D的东西，我们把它画出来，就是长成这个样子的这个地方，还是你的batch，还是你的样本，列不再是我的特征了，而是我的那个序列的长度，我们写成sequence，然后对每一个sequence就是对每个词，我当然是有自己的向量，那我再画一个额外的维度画在这个地方，这个就是我的feature了，如果在之前的话，Transformer里面这个地方就是长，这个东西长的就是n，那feature就是d，d在刚刚我们设成了512，那么如果你还是用batchnormalization的话，你每次是取一根特征，然后把它的每个样本里面所有的元素，这那个序列的元素，以及它的整个batch全部搞出来，把他的均值变成0、方差变成1，就是说我这么切一下，切一块出来，把它拉成个向量，然后跟之前作一样的运算。如果是layernorm的话，那么就是对每个样本就是这么切一下我用黄色来表示，就是横着切一下，就这两种切法不一样，但说切法不一样，它是会带来不一样的结果，具体来说为什么layer norm用的多一点，一个原因是在持续的这些序列模型里面，你的每个样本的长度可能会发生变化，比如说我们这个地方，第一个样本的长度是这样长的，第二个样本可能会长一点，第三个样本可能会短一点，第四个样本是中间长，可能是长度是这样子变换的，那些没有的东西，一般我们是把它放上0进去.那我们看一下这两种切法会有不一样什么的结果，如果是用batchnorm的话，我切出来效果就是一个跟画出来结果一样，对每一个特征你切出来东西会是一个这样子的东西，剩下的东西当然是填的是0了。如果是layernorm的话，第一个样本它切出来长度是一个这样子的长度，第二个样本当然会长一点，是这样子的长度，第三个是短一点，第四个是中等长度，这里的主要的问题是在算均值和方差的上面，对于batchnorｍ来说，我算均值的时候其实是通过这样子来算的，但是我画线阴影的区域的只是有效值，别的值的话其实没什么太多用，你会发现如果你的样本长度变化比较大的时候，你每次做小批量的时候，你算出来的均值方差，它的抖动相对来说是比较大的，而且这个另外一个问题是说，因为我们记得我们在做预测的时候，我们要把这个全局的均值和方差记录下来，那么这个全局的均值方差，如果碰到一个新的预测样本，如果特别特别长怎么办，我碰到一个那么那么长的东西，那么我是不是在训练的时候没见过那么伸出去那么多，那么我在之前算的均值和方差很有可能是不那么好用的，但反过来讲，对layernorm相对来说没有太多这个问题，是因为它是每个样本自己来算我的均值和方差，我也不需要存在一个全局的一个均值方差，因为这东西是对每个样本来做的，所以相对来说你不管样本是长还是短，反正我算均值是在你自己里面算的，这样子的话相对来说它稳定一些，这也是layernorm大家去看那篇文章的时候，他是给大家这么解释的，但实际上来说我们知道一个很好用的一个东西，原文写的东西可能和之后大家的理解是不一样的，在之后又有一篇文章来解释为什么layernorm，有效是更多是从一个对梯度，对于事物的那些normalization，然后提升它的常数来解释的。</code></p></blockquote><blockquote><p>在讲完编码器的架构之后，我们来看一下解码器。</p></blockquote><p>解码器跟编码是一个很像的东西，首先它跟编码器角一样是由n等于6个同样的层构成的，每个层里面呢跟编码器有两个一样的子层，但是不一样的在于，解码器里面用了一个第三个子层，它同样是一个多头的注意力机制，跟编码器一样我们同样的用了残差连接，用的log，另外一个是我们知道在解码器的时候，他做的是一个自回归，也就是说你当前的输入集是上面一些时刻的输出，意味着是说你在做预测的时候，你当然不能看到之后的那些时刻的输出，但是我们知道在注意力机制里面, 他每一次能看到整个完整的输入，所以这个地方我们要避免这个情况发生，也就是说在解码器训练的时候，再预测第ｔ个时刻的，输出的时候你不应该看到t时刻以后的那些输入，它的做法是通过一个带掩码的注意力机制。如果你回过头来看这个图的话，你会发现这个地方是有一个masked的，你别的那些黄色的块都是多头的注意力，但是这个地方是有个masked，保证你输入进来的时候，在t时间是不会看到t时间以后的那些输入，从而保证你训练和预测的时候行为是一致的。</p><blockquote><p>在看完我们的编码器和解码器的架构之后，我们来看一下每一个子层具体是怎么定义的。</p></blockquote><p><em><strong>注意力层</strong></em><br>第一段话就是一个对注意力的一个非常一般化的介绍，属于你懂的话，你看完之后就懂了，如果你不懂的话，你开完之后可能还是不懂。但不管怎么样，我们就按照这一段话给大家来解释一遍，首先他说，注意力函数呢是一个将一个query和一些key－value对映射成一个输出的一个函数，这里面所有的query呀key value和output都是一些向量，具体来说你的output是你的valve的一个加权和，所以就导致说你的输出的维度跟你的value的维度是一样的，另外一个是说这个权重是怎么来的，对于每一个value的权重，它是这个value对应的key和你这个查询这个query的相似度算来的，这个相似度或者叫做compatibility function，不同的注意力机制有不同的算法，如果我们画一个简单示意图，可以长成这样子：<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-3.png"><br>假设我有三个value和三个对应的key，假设我们现在给一个query，这个query跟第一个第二个key比较近，就是放在这个地方，那么你的输出就是这三个v的相加，但是这个地方的权重会比较大一点，这个地方权重也可能比较大，但是这个地方的权重就会比较小一点，因为这个权重是等价于你的query和你对应的key的那个相似度。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-4.png"><br>同样道理, 我假设再给你一个query，但是他是跟最后那一个key比较像的话，那么这样子你再去算他的v的时候，就会发现它对后面的权重会比较高一点，中间权重也还不错，最后的权重是比较小一点，就会得到一个新的输出，虽然你的key value并没有变，但是随着你query的改变，因为权重分配不一样，导致你的输出会有不一样，这就是注意力机制，因为不同的相似函数导致不一样的注意力的版本。</p><p>所以接下来这一章就讲的是Transformer自己用到的这一个注意力是什么样子计算的，他取的名字叫做scaled dot-product attention，虽然名字比较长，实际上是最简单的注意力机制了。他说我这个里面query和key，它的长度是等长的，都等于dk。因为你可以不等长，不正常是有别的办法算的。然后它的value是dv，当然你的输出也一样的是dv了，他说我具体计算的是，我对我每一个query和我的key做内积，然后把它作为相似度，你可以认为两个向量作了内积的事儿，如果这两个向量的long-norm是一样的话，那么你的内积的值越大，就是它的余弦值，那么就表示这两个向量的相似度就越高，如果你的内积是0了，那就等于是两个向量正交的，就是没有相似度。然后算出来之后，他再除以根号dk，就是你这个向量的长度，然后再用一个soft max来得到你的权重，因为你给一个query，假设给n个key value pair的话，那么就会算出n个值，因为这个query会跟每个key做内积，算出来之后再放进soft max就会得到n个非负的，而且加起来和等于1的一个权重，对于权重我们觉得当然是非负加起来等于1依旧是比较好的权重，然后我们把这些权重作用在我们的value上面，就会得到我们的输出了，在实际中当然我们不能一个一个这么做运算算起来比较慢。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-5.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-6.png"><br>所以他下面给了一个在实际中的时候，我们应该怎么样算的，他说我的query可以写成一个矩阵，就是我其实可能不止一个query，我有n个query，那我们画出来就是一个假设是一个Q是这个地方，那么有n行，然后你的维度是等于dk的，同样道理的话你的k也是一个同样的东西，但你的可能会长一点或者短一点都没关系，假设你是m，就是你的query个数和你的key value的个数可能是不一样的，但是它的长度一定是一样的，这样子我才能做内积，然后给定这两个矩阵，我把它一乘就会得到一个n乘以m的一个东西，所以这个东西里面它的每一行，就这个蓝色的线就是一个query对所有key的那一个内积值，然后我们再除以这个根号dk，再做soft max，所谓的soft max就是对每一行做soft max，然后每行一行之间是独立的，这样子就会得到我的权重，然后再乘以我的v，我的v是有一个叫m行的，然后它的列数是dv的一个矩阵则是v，然后这两个矩阵乘的话就会得到一个成为ｎ乘以dv的一个东西，我们写的这个地方ｎ乘以dv，那么这个地方每一行就是我们要的一个输出了，所以这里你可以看到是说对于组key value对和你n个query的话，我可以通过两次矩阵乘法来把整个计算做掉，这些query、key value在实际中对应的就是我的序列，所以这样导致说我基本上可以并行的计算里面每个元素，因为矩阵乘法是一个非常好并行的东西。<br>接下来一段他说我提出来的注意力机制跟别的区别是什么样子，他说一般有两种比较常见的注意力机制，一种叫做加型的注意力机制，它可以处理你的，query和你的key不等长的情况，另外一个叫做点积的注意力机制，他说点积的注意力跟我的机制是一样的，除了我这里除了一个这个数（根号dk）之外，所以你可以看到它的名字叫做scaled，就是除了那个东西，然后是点积注意力机制。接下来他说这两种注意力机制其实都差不多，但是它选用的是点乘，这是因为这个实现起来比较简单，而且会比较高效，因为这就是两次矩阵乘法就能算好。</p><blockquote><p>当然你需要解释一下，你为什么不直接用最简单的点乘注意力，你为什么要这里要出一个根号dk。</p></blockquote><p>他说当你的dk不是很大的时候，其实你出不出都没关系，但是当你的dk比较大的时候，也就是说两个向量的长度比较长的时候， 那么你做点积的时候，这些值呢可能就会比较大，但也可能是比较小了，当你的值相对来说比较大的时候，你之间的相对的那些差距啊就会变大，就导致说你值最大的那一个值做出来soft max就会更加靠近于1，剩下那些值呢就会更加靠近于0，就是你的值就会更加向两端靠拢，当你出现这样子的情况的时候，你算梯度的时候，你会发现梯度比较小，因为soft max最后的结果是什么，最后的结果就是我希望我的预测值的置信的地方尽量靠近１，不置信的地方尽量靠近0，这样子的时候，我说我的收敛就差不多了，这时候你的梯度就会变得比较小，那你就会跑不动，所以他说我们在Transformer里面一般用的dk比较大，之前说过是512，所以除以一个根号dk是一个不错的选择。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-7.png"><br>整个这个注意力的计算，他在上面有张图给大家画了出来，可以看到你在里面要两个矩阵，一个是query一个是key做矩阵乘法，然后再除以根号dk，这个地方我们一会儿讲，然后再做soft max，做出来结果最后跟你的值的那个矩阵做矩阵乘法就会得到你的输出了，这个是通过计算图来展示你这个是怎么做的。</p><p>另外一个我们要讲到是怎么样做mask，mask主要是为了避免你在第ｔ时间的时候看到以后时间的东西，具体来说，我们假设我们的query和key是等长的，他们长度都为n，而且在时间上是能对应起来的，然后对第t时间课的qt，这是我的query，那么我在做计算的时候，我应该只是看k１一直到kt－１，而不应该去看了kt和他之后的东西，因为kt在当前时刻还没有，但是我们知道在注意力机制的时候，其实你会看到所有，你qt会跟所有k里面的东西全部做运算，就是kt一直算算算算到kn，那这个时候怎么办，就是说我们发现其实你算还是可以算的，就是说你把这些值全部给你算出来，然后在算出来之后，我们只要保证说在计算权重的时候，就是算输出的时候，我们不要用到后面的一些东西就行了，具体来说他就在你这个地方加了一个mask，mask的意思是说，对于qt和kt和他之后的计算那些值，我给你换成一个非常大的负数，比如说负的一一的十次方，那么这一个那么大的负数在, 进入soft max做指数的时候，它就会变成0，所以导致soft max之后出来的这些东西，它的它对应的那些权重都会变成0，而只会前面这些值出效果，这样子的话我在算我的output的时候，我只用了v对应的v１一直到vｔ－１的结果就用上了它，而后面的东西我没有看。所以这个mask效果是在我训练的时候，我让你ｔ个时间的query只看我对应的前面那一些的key value pair，使得我在做预测的时候，我跟现在这个是能够一一对应上的。</p><blockquote><p>在讲完注意力机制的计算之后，我们来看一下multi-head是在干什么事情。</p></blockquote><p>我们首先还是回到我们的文字那部分，他这里说与其我做一个单个的注意力函数，不如说我把整个query，key value投影到一个低维，投影h次，然后再做h次的注意力函数，然后每一个函数的输出我把它并在一起，然后再投影来会得到我的最终的输出。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-8.png"><br>他说我们在图二给大家演示这个效果，那我们就跳回图二看一眼它是怎么做的，然后他说这个是我们原始的value、key、query，然后在这地方我们进入一个线性层，线性层就是把你投影的比较低的维度，然后再做一个scaled dot-product attention，就是这个东西了，全部放进来，然后我们这里做h次会得到h的输出，我们再把自己这些向量全部合并在一起，最后做一次线性的投影，会回到我们的multi-head attention。</p><blockquote><p>所以为什么要做多头注意力机制呢，如果我们回过头来看这个dot-product的注意力的话，你会发现里面没有什么可以学的参数，你的具体函数就是你的内积，但有时候我为了识别不一样的那些模式，我希望你可能有一些不一样的计算像素的办法，如果你用的是加性attention的话，这里没有提到的，那里面其实还是有一个权重你来学的，你也许可以学到这些东西。</p></blockquote><p>他说那我不用那个，那我用这个的话，我的一个做法是我先让你投影到一个低维，这个投影的w是可以学的，也就是说我给你h次会，希望你能学到不一样的投影的方法，使得在那个投影进去的那个度量空间里面能够去匹配不同模式它需要的一些相似函数，然后最后把这些东西回来，最后再做一次投影，所以跟我们之前说到的有点像在卷积网络里面，你有多个输出通道的感觉。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-9.png"><br>然后我们看一下这个东西的具体的公式是怎么算的，那你会发现是在Multi-Head的情况下，你还是以前的Q、K、V，但是呢你的输出已经是你不同的头的那一个输出的做concat起来，再投影到一个WO里面的，对每一个头他就是把你的Q、K、V，然后通过一个不同的可以学习的WQ、WK、WV投影到一个低维上面，在做我们之前提到过的注意力函数，然后再出来就行了。</p><p>这个地方你可以说每一个里面当时怎么算的，他们在实际上来说，他用的h是等于8的，就是用8个头，而且我们知道你的注意力的时候，因为有残差连接的存在，使得你的输入和输出的维度至少是一样的，所以它的做法是说你投影的时候，它投影的就是你的输出的维度除以h，因为我们之前我的输出维度是512，所以除以8之后，就是每一次我们把它投影到一个64维的一个维度，然后在上面算你的注意力函数然后再并起来再投影回来。虽然这个地方你看到是非常多的小矩阵的乘法，实际上你在实现的时候也可以通过一次的矩阵乘法来实现，这个可以作为一个练习题，大家去看一下怎么样实现它。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-10.png"></p><blockquote><p>在讲完多头注意力是如何实现了之后，在3.2章的最后个小节里面讲的是在Transformer这个模型里面是如何使用注意力的，他这里讲了三种使用的情况，我们最简单的方法是回到我们之前那个架构图，看一看它到底是怎么被用的。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-1.png"><br>我们回到我们的架构图，我们看到的是黄色这个东西表示的是注意力的层，这个地方一共有三种不一样的注意力层。首先我们看一下我们的编码器的注意力是在干什么事情，然后我们分别来看一下每一个注意力层，它的输入和输出分别是什么，这个其实对应的是刚刚我们那一小节里边的三段话。 </p></blockquote><blockquote><p>首先我们来看一下我们的编码器的注意力是在干什么事情。</p></blockquote><p>我们知道编码器的输入这个地方，假设你的句子长度是n的话，它的输入其实是一个n个长为d的向量，假设我们的pn大小设成1了。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-11.png"><br>我们把它画出来就是每一个输入它的词，对应的是一个长为d的向量，然后我们这里一共有n个这样子的东西，然后我们来看一下注意力层，它有三个输入，他分别表示的是key、value和query，然后这个地方是你一根线过来，然后他复制成了三下，意思就是说同样一个东西我既作为key，也作为value，也作为query，所以这个东西叫做自注意力机制，就是说你的key、value和query其实就是一个东西，就是自己本身，然后我们知道那么这个地方我们输入了n个query，那么每个query我会拿到一个输出，那么意味着我会有n个输出，而且这个输出和value因为长度是一样的话，那么我的输出的维度其实也是那个d，就是意味着我的输入和输出的大小其实是一个东西。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-12.png"><br>但我们也可以把它画出来，画出来的话，其实你就是你的输出也是跟它长度一样长，长为n的一个东西，对于每一个query，我会计算一个这样子的输出。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-13.png"><br>因为我们知道这个输出其实就是你的value的一个加权和，权重是来自于query和key的一些东西，但它本身是一个东西，那么就意味着说，他的这个东西实际上本身就是你的输入的一个加权的一个和，然后这个绿色线代表权重的话，因为这个权重其实本身就是这一个向量，这个向量跟每一个输入的别的向量计算相似度，那么他跟自己算肯定是最大的，就是说你这根线肯定是最粗的，假设这个线跟你最这边这个向量也相似度比较高的话，那么这个权重也会稍微高一点。</p><p>假设我们不考虑多头和有投影的情况，你的输出其实就是你的输入的一个加权和，你的权重来自于你自己本身跟各个向量之间的一个相似度，但如果我们说过有多头的话, 因为有投影，其实我们在这个地方会学习h个不一样的距离空间出来，使得你出来的东西当然是会有一点点不一样了，这个就是第一个注意力层是如何用的。</p><blockquote><p>解码器</p></blockquote><p>解码器它这个地方一样的，是一个东西过来，然后复制成了三次，然后解码器的输入也是一样的，只是长度可能变成了一个长为m的样子，然后你的维度其实也是一样的，所以它跟编码器是一样的自注意力，唯一不一样的是这里有个masked这个东西。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-14.png"><br>我们之前有解释过，在解码器的时候，比如说你算这一个query它对应的输出的时候，它是不应该看后面那些东西，所以意味着是说在解码器的时候，你的这些后面的东西要设成0，我们用黄色的线表示一下，就是说后面这些东西呢这些权重你要设成零，在解码器的时候，这就是masked它的一个作用。</p><blockquote><p>第三个注意力层</p></blockquote><p>然后我们看第三个注意力层，也就是在这个地方，这个地方你看到是他不再是自注意力了，而是你的key和你的value来自于你的编码器的输出，然后你的query是来自于你解码器下一个attention的输入。</p><p>我们知道你的编码器最后一层的输出就是n个长为d的向量，我们把它画在这个地方还是那么你的解码器的masked attention就最下面那个attention它的输出是，m个也是长为d的向量，我们也把它画在这个地方，这里你的编码器的输出作为value和key进来，然后你的解码器下一层的输出呢作为query进来，意味着是说对解码器的每一个输出作为query我要算一个我要的输出，假设我用蓝色的表示的话，那么你的输出是我们知道是来自于value的一个加权和，那我就是来自于你的编码器它的输出的加权和，我如果把它划过来，就是有一个这样子的东西过来，一些权重过来，这个权重它的粗细程度就是取决于我这个query跟这个东西的相似度，假设我这个东西跟这个东西相似度比较高的话，那么我的权重这个地方就会大一点，如果相似度比较低的话，权重就会小一点，那意味着是说在这个attention干的事情其实就是去有效的把你的编码器里面的一些输出根据我想要的东西给它拎出来。</p><blockquote><p>举个具体的例子，假设你是在做英文翻译中文，我假设第一个词是<code>hello</code>对应的向量是这个东西，然后我第二个词是<code>hello world</code>的话，那么中文第一个是<code>你好</code>，所以你会知道在算<code>好</code>的时候，如果它作为query的时候，那么去看<code>hello</code>的这个向量应该是会相近一点，给它一个比较大的权重，但是<code>world</code>这个是后面的次相关，我发现到<code>word</code>这个词跟我这个query相关度没那么高，在计算<code>你</code>的相似度的时候，那么就是说在算<code>好</code>的时候，我会给它一个比较大的权重，在这一个上面，但是我在后面如果还有<code>你好，世界</code>的话，如果是这个<code>世</code>的话，那么在这个query的时候，我再去算它的输出这个东西的时候，它那么就会给第二个向量一个比较大的一个权重出来。</p></blockquote><p>意味着是说根据你在解码器的时候你的输入的不一样，那么我会去根据你的当前的那一个向量去在编码器的输出里面去挑我感兴趣的东西，也就是你注意到你感兴趣的东西，那些没有跟你不那么感兴趣的东西你就可以忽略掉它，这个也是说attention是如何在编码器和解码器之间传递信息的时候起到的一个作用。</p><blockquote><p>这样我们就讲完了Transformer里面三个attention到底是在干什么事情，那么接下来我们要去讲蓝色这个feed forward是在干什么东西。</p></blockquote><p>3.3节讲的就是这个名字很长的point-wise feed forward network，他说他其实就是一个fully connected feed-forward network，就是一个MLP了，但是他不一样的是说他是applied to each position seperately and identically。position是什么东西呢，就是你输入的那一个序列不是有很多很多个词吗，每个词它就是一个点，他就是那一个position，然后他就是把一个MLP对每一个词作用一次，然后对每个词作用的是同样一个MLP，所以这个就是point wise的意思，它说白了就是MLP只是作用在最后一个维度。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-15.png"><br>具体来看一下它是怎么写的，<code>xW1+b1</code>就是一个线性层，<code>max(0,xW1+b1)</code>就是一个Relu的激活层，然后再有一个线性层<code>max(0,xW1+b1)W2+b2</code>，我们知道在我们的注意力层，它的输入就每一个query它对应的那一个输出，它是常为512，那么就是说这个x就是一个512的一个向量，他说w1我会把512投影成2048，这个维度就等于是我把它的维度扩大了四倍，因为最后你有一个残差连接，你还得投影回去，所以W2又把2048投影回了512，所以这个东西说白了就是一个单隐藏层的MLP，然后中间隐藏层把你的输入扩大四倍，最后输出的时候也回到你输入的大小，说白了就是你用pytorch来实现的话，它其实就是把两个线性层放在一起，你都不需要改任何参数，因为pytorch去当你的输入是一个3d的时候，它默认就是在最后一个维度做计算。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-16.png"><br>为了更好的理解，我们用图把他跟你的attention这个东西给大家画一下，以及说它跟我们之前的rn它的区别在什么地方，我们这里还是考虑一个最简单的情况，就是没有残差连接，也没有lay alone，然后你的attention也是一个单头，然后没有投影，我们知道我们的输入就是一个长为n的一个一些向量，在进入attention之后，就是attention之后，我们就会得到同样长度的一些输出，在最简单的情况的attention，其实说白了就是对你的输入做一个加权的和，然后加权和之后我们进入我们的MLP，就是那个point wise的MLP，虽然我们画了几个东西, 但其实它就一个，就是说每一个红色的方块之间的权重是一样的，然后每个MLP对每一个输入的点呢做运算会得到一个输出，最后就得到了整个Transformer块的一个输出是这样子，虽然它的输入和输出都是大小都是一样的，所以这个地方你看到的是说，attention起的作用是什么东西，他就是把整个序列里面的信息抓取出来，做一次汇聚aggregation，所以这个东西已经就有了我序列中感兴趣的东西，信息已经抓取出来了，以至于我在做投影，在做MLP的时候，映射成我更想要的那个语义空间的时候，因为这个东西已经含有了我的序列信息，所以每个MLP只要在对每个点独立做就行了，因为这个地方历史信息、序列信息已经被汇聚完成，所以这个地方是可以分开做的，也就是整个Transformer是如何抽取序列信息，然后把这些信息加工成我最后要的那个语义空间那个向量的过程。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-17.png"><br>作为对比我们看一下RNN是怎么做的，我们知道RNN的输入跟你是一样，就是一些向量，然后对于第一个点呢，说白了你也就是做一个线性层，我们做一个最简单的，就是一个没有隐藏层的MLP，就是一个纯线性的层，第一个点就是直接做出去就完事了，对于下一个点，我是怎么样利用我的序列信息的呢，我还是用之前这个MLP它的权重跟之前是一样的，但是呢我的时序信息我用绿色表示，它就是把这个东西它的上一个时刻的输出放回来，作为跟输入一起并入进去，这样子我就完成了我信息的一个传递，然后用绿色的线表示的是之前的信息，蓝色的线当时表示的是我当前的信息，这样子我会得到一个当前的一个输出，历史信息就是上一次的那个输出作为历史信息进来，然后得到我当前的一个输出。<br>所以可以看到是说RNN跟Transformer是一样的，都是用一个线性层或者说一个MLP来做一个语义空间的一个转换，但是不一样的是你如何传递序列的信息，RNN是把上一个时刻的信息输出传入下一个时候做输入，但是在Transformer里面，它是通过一个attention层，然后再全局的去拉到整个序列里面信息，然后再用MLP做语义的转换，这个是两个模式之间的区别，但是它的关注点都是在你怎么有效的去使用你的序列的信息。</p><blockquote><p>这样我们就只剩最后两个层了，第一个层是叫做embedding，后面一个层叫做positional encoding。</p></blockquote><p>这embedding大家都知道，因为我的输入是一个个的词，或者一个叫词源，叫token，那我需要把它映射成一个向量，embedding就是说给任何一个词，我学习一个长为d的一个向量来表示它，这个d，当然这个地方你可以认为就是等于512了，他这里是说你的编码器要一个embedding，你的解码器的输入也要有个embedding，这货在你的soft max前面那个线性，也需要一个embedding，他说我这三个是一样的权重，这样子我训练起来会简单一点。另外一个有意思的是说他把权重乘了一个根号d，d就是512，为什么做这个事情，是因为你在学embedding的时候，多多少少会把每一个向量它的L2 Norm学成，相对来说比较小的，比如说学成1吧，就不管你的维度多大的话，最后你的值都会等于1，那就是说你的维度一大，你学的一些权重值就会变小，但是你之后我要加上这个东西（positional encoding），加这个东西的时候，它不会随着你的长度变成了它把你的L2 Norm固定住，所以它成了它之后，使得这么两个相加的时候, 再一个scale上大家都差不多，就是他做了一个hat。</p><p>下面一个东西叫做positional encoding，你为什么有这个东西，是因为你发现，attention这个东西是不会有时序信息的，你想一想你的输出是什么东西，你的输出是你的value的一个加权和，你这个权重是query和key的之间的那个距离，它跟你的序列信息是无关的，就我根本就不会去看你那个key value里面那些对在那个序列里面哪个地方，所以意味着说我给你一句话，我把顺序任何打乱之后，我attention出来结果都是一样的，顺序会变，但是值不会变，这个当然是有问题的，在处理时序数据的时候，我给你一句话，假设我把里面的词给你完全打乱，那么你语义肯定会发生变化，但是你的attention不会处理这个情况，所以我需要把时序信息加进来，RNN是怎么加呢，RNN是上一个时刻的输出作为下一个时刻的输入来传递我的历史的信息，所以它本来就是一个时序的一个东西，但是attention不是，他的做法是说我在我的输入里面加入时序信息，就是说你这一个词，他在一个位置i，i这个位置这个数字12345，加到你的输入里边，所以这个东西叫做positional encoding。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-18.png"><br>具体来说他后面给了一个公式，它是具体是怎么算的，我就不给大家完全讲个公式，给大家讲一下大概的思路是什么样子，在计算机里面我们怎么表示一个数字，假设我用一个32位的整数来表示数字的话，那就是用32个bit，每个bit上面有不同的值来表示012345678，你可以认为就是说我一个数字是用一个长为32的一个向量来表示的，现在我一个词在嵌入层会表示成一个长为512的向量，同样我用一个长为512的向量来表示一个数字，表示你这个位置012345678，具体那些值是怎么算出来的，是用周期不一样的<code>sin</code>和<code>cos</code>函数的值来算出来的，所以导致说，我任何一个值可以用一个长为512的一个向量来表示它，然后这个长为512的记录了时序信息的一个东西跟你的嵌入层相加，就会完成了我把时序信息加进我的数据的一个做法。</p><p>如果我们回到之前的架构图的话，你可以看到我的输入进来，进入embedding层之后, 那么对每个词都会拿到那个向量长为512的一个向量，然后positional encoding，就是你这个词在我这个句子中的位置告诉他，他返回给你一个长为512的一个向量表示这个位置，把这两个加起来就行了，这个东西因为是cos和sin的一个函数，它是在正1负1之间抖动的，所以这个东西成了一个根号d，使得每个数字也是在差不多的正1到负1之间这个数值区间里面，然后进去之后，那么我就完成了在输入里面加入了信息，这是因为我后面整个这一块是顺序不变，就是说我的输入的序列，不管我怎么打乱我的顺序，进去之后，我的输出那些值是不变的，最多是我的顺序发生了相应的变化，所以他就把顺序信息直接在数据里面那个值给加进去了。</p><p>这样我们就讲完了, 第三章也就是整个Transformer模型的架构。</p><blockquote><p>第四章是大概解释一下为什么要用自注意力。你发现读到这里为止， 整个这篇文章是告诉你我这个模型长什么样子，并没有告诉你说我为什么要这么做，以及我整个设计理念是怎么回事，但这个地方大家给你稍微讲了一下，我为什么要这么做，它主要说的是相对于你用循环层或者卷积层的时候，我用自注意力有多么好，但是整个来说这篇文章对整个模型的解释其实是比较欠缺的。</p></blockquote><p>然后整个这一段话他解释的其实就是我们这一个表，我们就直接给大家讲一下这个表是在干什么。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-19.png"></p><p>他说我们比较了四种不一样的层，第一个当然是他们关注的自注意力, 然后是循环层、卷积层，另外一个是它构造出来一个受限的自注意力。它有三列做比较，第一列是说我的计算复杂度当然是越低越好，第二个是说我的顺序的计算越少越好，顺序的计算就是说你下一步计算必须要等前面多少步计算完成，在算一个Layer的时候，你越不要等那么你的并行度就越高，最后一个是说一个信息从一个数据点走到另外一个数据点要走多远，这也是越短越好。</p><p><em><strong>自注意力</strong></em><br>我们分别来看一下每一个层它代表的数值是什么意思，首先我们来看自注意力，n这个地方是你序列的长度，d是你向量的长度，我们知道整个自注意力的话，其实说白了就是几个矩阵做运算，其中一个矩阵是你的query的矩阵乘以你的key的矩阵，你的query矩阵是有n行，你的n个query，然后你的列数是d，就是你的维度是d，然后你的key也是一样的，也是n乘d，所以两个矩阵一乘的话，那么算法复杂度就是n的平方乘以d，另外当然还有一些别的矩阵运算，但是它的复杂度都是一样的，所以是有个<code>O(n^2 *d)</code>的这个地方，然后你的sequential operation，因为你就是那么几个矩阵乘法，矩阵里面它可以认为是并行度比较高的，所以这个地方是一个O(1)的，最大的长度是说，你从一个点的信息想跳到另外一个点要走多少步，我们知道在attention里面，就是一个query可以跟所有的key去做运算，而且你的输出是你所有value的一个加权和，所以就是说任何query跟任何一个很远的一个key value pair，我只要一次就能过来，所以这个长度是比较短的。</p><p><em><strong>循环层</strong></em><br>然后看一下循环层，如果你的序列是乘了n的话，它就一个一个做运算，每个里面呢它的主要的计算就是一个n乘以n的一个矩阵，就是一个dense layer乘以你一个成为d的一个输入，所以它是一个n平方，然后要做n次，所以是n乘d的平方，然后你对比一下这两个东西是有一定区别的，就真的取决于是n大还是d大，如果你n大的话，当然它过一点，你d大的话是下面一个过一点，实际上来说你的d这个地方是512，你的n也差不多是几百的样子，现在当然是比较大的模型的话，d可以做到2048甚至更大，你的n相对来说也会做的比较大，也是几千的样子，所以你其实现在看起来这两个东西都差不多，就是n和d的，其实在差不多的数据上面，所以这两个都差不多，但是在循环的时候，因为你是要一步一步做运算，当前时间刻的那个词需要等待前面那个东西完成，所以导致你是一个成为n的一个序列化的操作，在并行上是比较吃亏的，我们之前提到过，另外一个是说你最初点的那个历史信息需要到最后那一个点的话，需要走过n步才能过去，所以他这个地方的最长是O(n)，所以大家会批评RNN说你对特别长的序列的时候做的不够好，因为你的信息一开始走啊走啊走，就走丢了，而不像attention一样，就可以直接一步就能过去。</p><p><em><strong>卷积层</strong></em><br>然后看一下卷积，大家也没有特别解释卷积在序列上怎么做，具体做法是它用一个1d的卷积，所以他的kernel就是个k，所以就是不是k平、就是k，n是你的长度，然后d就是你的输入的通道数和输出的通道数，所以这个地方是k乘n乘d的平方，k一般也不大，k一般就三啊五啊、几，所以这个东西你也可以认为是常数，所以导致说卷积的复杂度和RNN的复杂度其实是差不多的，但是卷积的好处是说，你就是一个卷积操作就完成了，里面的并行度很高，所以卷积做起来通常比RNN要快一点，另外一个是说卷积每一次一个点是一个成为k的一个窗口来看，所以他一次一个信息在k距离内是能够一次就能传递，如果你超过k了的话，他要传递信息的话，他要通过多层一层一层上去，但是它是一个log的一个操作，所以这个东西也不亏。</p><p><em><strong>受限的自注意力</strong></em><br>最后一个东西他是说当我做注意力的时候，我的query只跟我最近的r个邻居去做运算，这样子的话我就不用去算n平方这个的东西了，但他的问题是说，这样子的话有两个比较长的远的一个点， 需要走几步才能过来，所以他就损失了这个东西，一般来说在实际上来说，我们用attention主要是关心特别长的序列，你真的能够把整个信息揉的比较好一点，所以在实际过程中，这一块感觉上用的不是那么多，大家都是用最原始的版本，不用太做受限了，所以基本上就是考虑这三个。</p><p>所以基本上可以看起来是说就实际中，就是当你的序列的长度，和你的整个模型的宽度差不多的时候，而且大家深度都一样的话，基本上这三个模型的算法复杂都是差不多的，当然是说你的attention和卷积相对来说计算会好一点，另外一个是说attention在信息的糅合性上会好一点，所以你可能认为这个地方还是能影响一些东西了，所以你看上去是说用了self-attention之后，是不是你的感觉上对长数据处理更好，而且可能算得也不快也不会慢，但实际上其实也不是这样子的，实际上是说attention对你整个模型的假设做了更少，导致说你需要更多的数据和更大的模型才能训练出来跟你的RNN和CNN同样的效果，所以导致现在基于Transformer的模型都是特别大特别贵。</p><h3 id="7-实验"><a href="#7-实验" class="headerlink" title="7.实验"></a>7.实验</h3><p>第五章是讲你训练的一些设置是怎么样的，首先他说我的训练数据集和我的batching怎么做的，他用了两个人物，一个是英语翻德语，他用的是标准的WMT 2014的数据，它这个里面有4.5万个句子的对，他说他用的是byte-pair encoding，就是bpe。大概的思想是说，你不管是英语还是德语，其实一个词里面有很多种变化，就是什么加ing，加es，但是你如果直接把每一个词做成一个token的话，你会导致你的字典里面的东西会比较多，而且一个动词的可能有几种变化形式，你做成不一样的词的时候，他们之间的区别模型是不知道了。bpe相对来说就是把你那些词根给你提出来，这样好处是说它可以把整个字典这样的比较小，它这个地方用的是37000个token的一个字典，而且它是在英语和德语之间是共享的，就是说我们不再为英语构造一个字典，不再为德语构造一个字典，这样的好处是说我整个编码器和解码器的一个embedding，就可以用一个东西了，而且整个模型变得更加简单，也就是他之前所谓的我的编码器、解码器那个embedding它是共享权重的，另外一个英语到法语的话，他用了一个更大的一个数据集， 在接下来的硬件和schedule部分，他说我训练使用了八个P100的GPU，这个是比较有意思的，就是说在三年前，google的工作还是大量使用gpu的，但是这个之后基本上就很少见了，因为在这个之后， google让内部的员工尽量使用tpu，限制你们使用gpu，这篇文章多多少少也是推动这个进展，这是因为Transformer里面我们知道基本就是一些比较大的矩阵做乘法，tpu这种东西就是特别适合做大的矩阵乘法，以致于Google说，你们其实挪到tpu也没有太多问题，可能性能还更好一些。他说我们的base模型呢是用的一个小一点的参数，他每一个bench训练的时间是0.4秒，然后我们一共训练了10万步，一共就是在我的八个gpu上训练了12个小时，就基本上一台机器训练12个小时，也是不错的一个性能，他说我一个大的模型，这样一个batch训练需要一秒钟，然后他一共训练了30万步，最后是一台机器3.5天，那其实也是一个可承受的范围，但是这个朋友之后的那些工作基本上是属于大家几乎不能承受的时间成本了。<br>在训练器上面，他们用的是Adam，这是它的参数，β2我觉得可能不是最常用的，β2我记得应该是0.99还是0.999，所以他选了一个稍微少一点的值，然后他的学习率是用这个公式算出来的<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-20.png"><br>有意思的是他的学习率是根据你的模型的那个宽度的-0.5次方，就是说当你的模型越宽的时候，就是你学的那些向量越长的时候，你的学习率要低一点，另外一个是它有一个warmup，就是从一个小的值慢慢地爬到一个高的值，爬到之后，再根据你的步数按照0.5次方衰减，最后他说我的warmup是4000，有意思的是，基本可以看到说这个地方没有东西可以调的，就是你的学习力几乎是不用调的，取决于第一adam对学习率确实不那么敏感，第二个是说他这个地方也是把两个模型考虑进来了，这个schedule就已经也算是不错的schedule了，所以在学习率是不需要调的。</p><p>然后在5.4的时候讲的他使用的正则化，他用了三个正则化，第一个是他用的是一个叫做residual dropout，说白了就是说对每一个子层，子层就是包括了你的多头的注意力层和你之后的MLP，在每个层的输出上，在他进入残差连接之前和在进入layer alone之前，他使用了一个dropout，他的dropout率是0.1，也就是说把这些输出的10%的那些元素只乘0.1，剩下的那些只能乘以1.1。<br>另外一个他在输入加上你的词嵌入，再加上你的positional enconding的时候，在它上面也用了一个dropput，也就是把10%的元素值乘了0.1，有意思的是说你基本看到对每一个带权重的乘，他在输出上都使用的dropout，用的是比较狠的，虽然这个dropout率并不是特别高，但是它使用了大量的dropout的层来对它的模型做正则化。<br>然后另外一个他使用的是一个label smoothing，这个技术是在inception v3，意思是说我们用softmax去学一个东西的时候，我的标号是正确的是1、错误的是0，就是我用对于正确的那一个label的softmax的值去逼近于1，但我们知道softmax是很难逼近于1的，因为它里面是一个指数，它是一个很soft的一个东西，就是说它需要你的输出接近无限大的时候才能逼近于1，这个使得训练比较难，一般的做法是说你不要让搞成那么特别难的0和1，你可以把那个一的值往下降一点，比如说降成0.9，但这个地方他降得比较狠，他是降成了0.1，就是说对于正确的那个词，我只需要我的softmax的输出是到0.1就行了，叫置信度是0.1就行了，不需要做的很高，剩下的那些值就可以是0.9除以你的字典的大小。他说这里会损失你的perplexity，perplexity是你的log lost做指数，基本上可以认为是你的模型不确信度。因为你这个地方让你学说我正确答案，我也只要给个10%是对的就行了，所以当然你的不确信度会增加，所以你这个值会变高，但是他说我的模型会不那么确信会提升我的精度和我的BLUE的分数，因为精度和blue的分数才是我们关心的重点，所以这个地方他说觉得没关系。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/3-22.png"><br>那我们接下来看第二个表，第二个表表示的是不同的超参数之间的一些对比，我们可以看一下有哪些超参数，就回忆一下n是什么，n是你要堆多少层，d是你这个模型的宽度，就是一个token进来要表示成一个多长的向量，dff表示的是你拿MLP中间那个隐藏层的输出的大小，h是你的头的个数，就是你注意力层的头的个数，dk、dv分别是你一个头里面那个key和那个value那个维度，你的Pdropout是你的丢弃的率，以及这个是说你最后 label smoothing的时候你要学的那个label的真实值是等于多少，最后一个是说你要训练多少个batch。</p><p>然后可以看一下它的base模型， base模型我们知道是说他用了六个层，然后每一层的宽度是512，然后这个东西基本上就是它的四倍了，然后头的数乘以你的维度数是等于他的，就是8乘64 等于500兆，dropout率是0.1，然后这个是0.1，然后最后是训练了10万步。</p><p>另外一个是他的big模型，基本上可以看到是说，这个东西没有变，但是模型的宽度乘了两倍，然后啊这个东西也当然是跟着翻倍了，他也把这个头的个数乘了两倍，然后这个东西就不用变了，另外一个是你的模型更加复杂了，所以他用了一个比较大的丢弃率0.3，而且模型更加复杂了，收敛会慢一点，因为你的学习率是变低了的，这个地方他训练的30万个批量大小。</p><p>所以基本上可以看到是，整个模型参数相对来说还是比较简单的，基本上你能调的就是一个要多少个层，然后你的模型有多宽，以及说你要多少个头，剩下的这些东西基本上都是可以按比例算过来的， 这个也是Transformer架构的一个好处，虽然你看上去那个模型比较复杂，但是也没有太多东西可以调，这个设计上来说让后面的人更加方便一点，比如说bert它其实就是把这个价格拉过去，然后把这个把这几个参数改了一下，gpt也就是基本上就是copy过去，然后稍微调一下就行了，让后面的人的工作变得简单了很多。</p><p>最后一个表是另外一个NLP任务上，大家不熟的话，我们就可以跳过来，你放一个也挺好的，你不放其实也没关系，就是他是说我的结果还不错，就是不仅仅是在机器翻译上还行， 我又找了一个另外一个人，说效果也还挺好的，就是这个要表达的一个观点。</p><h3 id="8-评论"><a href="#8-评论" class="headerlink" title="8.评论"></a>8.评论</h3><p>最后我们来评价一下这篇文章，我们首先看一下它的写作，这篇文章的写作是非常简洁的，因为就是说，他每一句话就基本上在讲一件事情，你看那么短短的一篇文章，我们读下下来大概就花了一个半小时，也能理解是说你有那么多东西，你要塞到那么短的一个篇幅里面，你只能用这么写。<br>另外一个是说他没有用太多的写作技巧，基本上就是说你看我提出一个什么东西，这个模型长什么样子，跟CNN和RNN比是什么样子，最后的实验结果是什么东西，这个不是那么推荐的一种写法，因为对一篇文章来说，你需要在讲一个故事，让你的读者有代入感，能够convince读者。<br>但是我也能理解说，你要在一篇文章里面发现那么多东西的话，也没那么多篇幅来讲一个很好的故事，一般的建议是说，假设你写篇文章的话，你可以选择把你的东西减少一点，甚至把一些不那么重要的东西放到你的附录里面，但是在正文的时候，你还是最好讲个故事，说你为什么做这个事情，你的一些设计的理念是什么样子的，你的对整个文章的一些思考是什么样子的，这个东西让大家会觉得你的文章更加有深度一些。</p><p>接下来我们评论一下Transformer这个模型本身，我们现在当然可以看到Transformer模型不仅仅是用在机器翻译上面，它也能够用在几乎所有的NLP的任务上面，在后续的工作，bert、gpt 让大家能够训练很大的预训练模型，能够极大的提升所有NLP里面的任务的性能，这个有点像CNN在对整个计算机视觉的改变，我们能够训练一个大的CNN的模型，使得别的人物也能够从中受益。<br>另外一个是说，CNN给整个计算机视觉的研究者提供了一个同样的一个框架，使得我只要学会CNN就行了，而不需要去管以前跟任务相关的那么多的专业的知识，比如说做特征提取，对整个任务怎么建模，Transformer也是一样的，之前我们要做各种各样的数据文本的预处理，然后我要根据NLP的任务给你设计不一样的架构，现在不需要了，我们用整个transform这个架构就能够在各个任务上做得非常好的成绩，而且他预设的模型也让大家的训练变得更加简单。<br>当然我们还看到的是Transformer现在不仅仅是用在自然语言上面,，也在图片上、语音上、media上取得了很大的进展，这个是一个非常有影响力的一件事情，这是因为之前计算机视觉的研究者是用CNN，而在原处理你用RNN，然后别的人用别的模型，现在是说大家发现同样一个模型能够在所有领域都能用，就是让大家的语言变成一样了，就以前你用python，我用java，现在我说大家都用python了，那你任何一个领域的研究者做的一些突破能够很快的在别的领域被使用，能够极大的减少一个新的技术在机器学习里面各个领域被应用的那个时间。<br>另外一块的话我们知道人对世界的感知是多模态的，我们看见图片，我们读文字，我们听到语音，现在Transformer能够把这些所有的不同的数据给你融合起来，因为大家都用一个同样的架构抽取特征的话，那么他可以抽到一个同样的语义空间使得我们可以用文本、图片、语音、视频，能训练更好更大的模型，也可以看到这一块应该是现在和未来的一个研究重点。</p><p>但反过来讲，虽然Transformer这些模型取得了非常好的实验性的结果，但是我们对它的理解还在比较初级的阶段，第一个是说这篇文章的标题叫做你只需要attention就行了，但是最新的一些结果表明attention只是在Transformer里面起到一个作用，它的主要作用是把整个序列的信息给大家聚合起来，但是后面的MLP以及你的残差连接是缺一不少了，如果你把这些东西去掉的话，attention基本上什么东西的训练不出来，所以你这个模型attention也不是说你只需要它就行了。第二个是说， attention根本就不会去对你这个数据的那个顺序做建模，他为什么能够打印RNN，RNN能够显示的建模这个序列信息理论上应该比你的mlp效果更好，现在大家觉得他使用了一个更广泛的归纳偏置，使得他能处理一些更一般化的信息，这也是为什么说，而attention并没有做任何空间上的一些假设，它也能够跟CNN甚至是比CNN取得更好的一些结果。但是它的代价是说因为它的假设更加一般，所以他对数据里面抓取信息的能力变差了，以至于说你需要使用更多的数据，使用更大的模型才能训练出你要的效果，这也是为什么现在Transformer模型一个比一个大训练，一个比一个贵。</p><p>另外最后要提到一点是说，Attention也给了研究者一些鼓励说，原来在CNN和RNN之外也会有新的模型能打败他们，现在有很多工作说我就用MLP或者就用一些更简单的架构也能够在图片上面在文本上面取得很好的结果，所以我觉得在未来肯定会有更多新的架构出现让整个领域更加有意思一些。</p><h3 id="9-面试题"><a href="#9-面试题" class="headerlink" title="9.面试题"></a>9.面试题</h3><ol><li>Transformer为何使用多头注意力机制？（为什么不使用一个头）</li><li>Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？ （注意和第一个问题的区别）</li><li>Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</li><li>为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解</li><li>在计算attention score的时候如何对padding做mask操作？</li><li>为什么在进行多头注意力的时候需要对每个head进行降维？（可以参考上面一个问题）</li><li>大概讲一下Transformer的Encoder模块？</li><li>为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？</li><li>简单介绍一下Transformer的位置编码？有什么意义和优缺点？</li><li>你还了解哪些关于位置编码的技术，各自的优缺点是什么？</li><li>简单讲一下Transformer中的残差结构以及意义。</li><li>为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</li><li>简答讲一下BatchNorm技术，以及它的优缺点。</li><li>简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</li><li>Encoder端和Decoder端是如何进行交互的？（在这里可以问一下关于seq2seq的attention知识）</li><li>Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？（为什么需要decoder自注意力需要进行 sequence mask)</li><li>Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？</li><li>Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</li><li>解码端的残差结构有没有把后续未被看见的mask信息添加进来，造成信息的泄露。</li></ol></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文精读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文2】撑起计算机视觉半边天的ResNet</title>
      <link href="/2023/12/04/lun-wen-2-cheng-qi-ji-suan-ji-shi-jue-ban-bian-tian-de-resnet/"/>
      <url>/2023/12/04/lun-wen-2-cheng-qi-ji-suan-ji-shi-jue-ban-bian-tian-de-resnet/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><em><strong>Deep Residual Learning for Image Recognition</strong></em><br><em><strong>基于深度残差学习的图像识别</strong></em></p><blockquote><p>大家好，今天是我们论文精读系列的第二篇文章，我们选用的是深读学习的另外一篇经典之作——ResNet，这篇文章发表于2015年，但是假设你今天还是在用深度卷积神经网络的话，那么有一半的可能性，你是在用 ResNet或者是他的变种。<br>那么我们就来仔细读一下提出来ResNet的这篇文章，我们跟之前一样，还是采用两遍或者三遍的读法，我们首先假设自己回到过去，第一次看这篇文章的时候，如何从一个第一人称的视角给大家读一下这篇文章，另外当然我们会回到现在去看一下，论文里面的哪些结论现在看是不是已经过时了还是仍然是成立的。</p></blockquote><h1 id="第一遍"><a href="#第一遍" class="headerlink" title="第一遍"></a>第一遍</h1><h2 id="1-标题"><a href="#1-标题" class="headerlink" title="1.标题"></a>1.标题</h2><p>首先我们来看一下这篇文章的标题，标题的第一个叫做Deep Residual Learning，这个是 ResNet整个后面的一个核心的思想。第二个是图片识别也就是图片分类，当然是计算机视觉里面最大的一个应用。</p><h2 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a>2.摘要</h2><p>接下来，我们来看一下这篇文章的摘要，摘要的第一句话是说深的神经网络非常难以训练，这个就是提出了他的问题，第二句话是说我们干了什么事情，我们做了一个使用残差学习的框架来使得训练非常深的网络比之前容易很多，这个是这个文章关心的重点。接下来他来讲他们提出的方法，他说他把这些层作为一个学习残差函数相对于层输入的一个方法，而不是说跟之前一样的学习unreferenced方式。</p><blockquote><p>当然你在没有看懂这篇文章之前，也不知道他在说什么，反正知道有一个关键词是residual，毕竟这个residual也在标题里面出现了。</p></blockquote><p>接下来说我们提供了非常多的实验的证据，说我们这些残差网络非常容易训练，而且能够得到很好的精度，特别是在当你把层增加了之后，在 ImageNet 这个数据集上，他们使用了152层。</p><blockquote><p>这个层数也是非常夸张的，虽然当时候已经有 GoogLenet应该是在差不多的时候也就出来了，但是他不是用了152层深，就他是有很多并行的一些层。</p></blockquote><p>他是说我有152层的深度，这个是非常厉害的。他说我比VGG还要多了8倍，但是有更低的复杂度。</p><blockquote><p>这个东西比较有意思，就是说我比你深八倍，但是计算复杂度还要更低一点。</p></blockquote><p>然后他说用了这些残差网络做了一个ensemble之后，得到了3.57的测试精度，这个结果让他们赢下了ImageNet2015年的竞赛，然后他们还演示了怎么在CIFAR-10上训练十甚至到一千层的网络。</p><blockquote><p>我觉得这两个结果都比较有意思，第一个结果当然是说我赢下了ImageNet竞赛，我们之上一期讲过AlexNet也就是赢下了应该是第二期还是第三期的ImageNet竞赛，这个就是另外赢下的一个结果，让任何能够获得冠军甚至亚军的一个文章都值得大家关注，特别是那些提出了很不一样的架构、方法的文章，通常是会被大家追捧，所以你看到这句话的话，你通常会选择我要继续往下读一下。<br>另外一个有意思的是说，在CIFAR-10这是一个非常小的数据集，但是在计算机视觉里面也是最常见的一个数据集，他训练的100-1,000层的网络，这个是非常夸张的，之前你可能都没见过一千层的网络长什么样子。</p></blockquote><p>摘要的第二段话是说，对很多视觉的任务来说，深度是非常重要的，第二句话是说我仅仅就是把我的网络换成了我们之前学习到的残差网络，我们得到了28%的相对改进，在coco这个目标检测数据集上面，然后他也是通过这个东西，他赢下了第一名，在ImageNet的检测，然后localization，然后 coco 的detection，coco 的segmentation上面。</p><blockquote><p>所以这也是一个非常厉害的结果，就是说我就是把你背后的cnn的主干网络替换成了我们提出来的残差网络，我能够在这一系列的任务上都取得了非常好的结果，然后在赢下了竞赛第一名，如果大家做物体检测的话，大家知道 coco应该是这一块最大的也是最有名的数据集了。</p></blockquote><h2 id="3-结论、图表"><a href="#3-结论、图表" class="headerlink" title="3.结论、图表"></a>3.结论、图表</h2><blockquote><p>按照我们的办法，我们接下来要看一下他的结论，比较有意思的是这篇文章是没有结论的，这篇文章发表在CVPR上面，CVPR 是要求每篇文章的正文不能超过8页，这篇文章的结果相对来说是比较多的，因为他毕竟要放上我的ImageNet的实验和我在coco 和目标检测上的一些结果，所以导致作者没有空间去放下他的结论的部分，所以这个也是一个非常不同寻常的写法，我建议大家结论还是要有的，你没有结论，大家会觉得比较奇怪了。</p></blockquote><p>所以没有结论的话，那我们就直接看一下这里面有些公式表格是干什么事情的。<br>我们首先看一下我们的第一张图：<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/2-1.png"></p><blockquote><p>论文的第一张图出现在论文的右上角，这个是计算机视觉论文的惯例，一般我们会在第一页放上一张比较好看的图，不管是我对这个问题的一个描述也好还是我的主要结果也好，越好看越好，所以大家会看计算机视觉的文章的话，第一眼是去看一下他的图是在画什么东西。<br>当然更有意思的是，如果大家去开计算机图形学的文章的话，他也很有可能把一张图放在你的标题的上面，因为对图形学来说视觉更重要一点，所以他要把最好看的结果放在最上面，据说开创了这个风格的第一人是Randy，他是CMU的一个教授，但是他最著名的事件是他上了最后一课，这一个课非常非常的有名，这是他得了癌症之后，给CMU的同学上的最后一堂课，讲的是他小时候的梦想以及他是如何实现这些梦想的，非常的感人，建议大家一定去读一下，当然我对Randy的印象非常深刻，因为我在CMU的时候，每天路过一座桥，桥是以他的名字命名的，桥他是一个彩虹的颜色，非常的耀眼，每天晚上路过它，就是能在上面会看很久。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/2-1.png"></p><blockquote><p><code>图1 CIFAR-10上的训练错误（左）和测试错误（右），20层和56层“普通”网络。更深的网络具有更高的训练误差，从而具有更高的测试误差。ImageNet上的类似现象如图4所示。</code></p></blockquote><p>我们下面来看一下这张图， 这张图描述的是一个非常有意思的事情，他说我们的左图是我的训练误差，我的右图是我的测试误差，是在 CIFAR—10上的，他说我用了20层和56层的plain network，就是没有加你的残差的时候，你的 x 轴当然是你的轮数，你的 y 轴是你的错误率，可以看到是说，56层就是更深的更大的网络，他的误差率反而更高，他的训练误差更高，他的测试误差也会更高，他用来提出说其实你在训练更深的网络上面大家是训练不动的，不仅仅是过拟合，更多是你的训练误差，他都不能达到很好的效果，所有篇文章的摘要里面第一句话就是说训练很深的神经网络是件很难的事情，这篇论文的主要关心的是说让更深的神经网络更容易的训练，所以这张图是表示的一个观察到的现象、他要解决这个问题。</p><p>然后我们来看一下，还有没有别的有意思的图，我们可以往下翻一翻。</p><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/2-2.png"><br>第二张图当然是讲他的一个整个架构的实现，我们可以在等会在讲方法论的时候再给大家看一下这个图。</p><p>然后当然你可以往下翻，往下翻的话，可以看到一个特别特别长的一个整个架构的图：<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/2-3.png"><br>反正对于神经网络来讲，一般来说，我们会把整个网络的架构给你画出来，所以你可以选择去看一眼，如果你对这个架构相关比较少的话，如果你不熟的话之后看也没关系。</p><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/2-4.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/2-5.png"><br>那再往下当就是一些表一些图，图的话可以看一下实验结果，就这个实验结果是跟之前那个图是有一对应关系的，这个是在ImageNet 这个数据集上了，然后这个一样的，还是说没有加残差的时候，用了18层和34层的结果，这个是加了他提出的ResNet的时候，可以看到是说，在你没有加入residual这个残差连接的时候，34层的网络其实比18层的网络在训练误差上其实还高一点，也就是回应了之前的CIFAR的结果。<br>下面这个图当然是整个论文的核心图了，他说用了我们这个方法之后，使用了34层的结果的误差更低，这个地方应该是粗线是你的测试精度，这个是你的训练精度，所以看到是说，不管是训练精度还是测试精度，在加了之后跟之前没有加的比当然是区别很明显了。</p><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/2-6.png"><br>这个是他这个图的一个表的总结， 一般来说，图是给大家一个直观的表示，但是表我们可以把这些数字详细的写出来，这个东西是同样一个东西我用两种东西不同的方式来表达都有各有各的用处，图的话含有的信息更多，但是表的话这些数字对于大家是比较重要的，特别是你后面的引用者要引用一些文章跟你对比的话他可以使用这些数字，可以看到是说34层的ResNet的结果做 top 1的错误率是25.3%，但是如果你没有加的话你是28%，所以这个区别还是会挺大的。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/2-7.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/2-8.png"><br>往后面看的话可以看到更多的结果，这个是 ResNet152的结果，再往下的话就是跟一些别的结果的比较，他的152层的结果跟 GoogLenet以及 VGG在上一次竞赛的结果比，可以看到他还是会低的很多，将几乎是7.89降到了4.5，降了30%40的样子也是非常厉害了，然后这个是他赢下了15年比赛，最后的结果可以看到是做到3.57%，而且这个结果基本上是到了人类的极限了。<br>但后面还有一些在 CIFAR的结果基本上可以跟前面的模式的差不多，主要是说我的效果有多好。</p><p>这个就是这篇文章的一个非常简单的介绍，我们在接下来会给大家详详细细的把这篇文章每一段给大家过一遍。</p><h1 id="第二遍"><a href="#第二遍" class="headerlink" title="第二遍"></a>第二遍</h1><p>在第二遍里面我们就给大家详详细细每一段给大家过一遍ResNet这篇文章，我们之前已经看过摘要和其中的一些结果图了，我们接下来从导言开始。</p><h2 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h2><p>文章的第一段话当然是讲我这篇文章是关于哪一个领域了，他是说深度卷积深就网络好，为什么好是因为我们可以加很多层，这样子把网络变得特别深，然后不同程度的层他会得到不同的level的一些feature，比如说低级的一些视觉特征和高级的语义的视觉特征。</p><p>文章的第二段就提出了一个问题，就是说随着我们的网络越来越深，但是学一个好的网络就是简简单单的就把所有的网络堆在一起就行了吗？如果那么简单的话，那我就把网络做很深就行了，当我们知道这个里面有一个问题是说，当你的网络变得特别深的时候，你的梯度就会出现要么就爆炸要么就消失，解决他的一个办法是说我在我的初始化的时候要做的好一点，就是我权重在随机初始化的时候权重不要太别大也不要特别小。第二个是说我们在中间加入一些normalization，包括了BN就是batch normalization，可以使得校验每个层之间的那些输出和他的梯度的那些均值和方差，相对来说比较深的网络是可以训练的，避免有一些层特别大，有一些层特别小， 然后我们发现使用了这些技术之后是能够训练，就是说你能够收敛，虽然现在你能够收敛了，但是你的另外一个问题是当你的网络变深的时候，你的性能其实是变差的，就是你的精度会变差，这个也是之前这张图给大家讲的事情：<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/2-1.png"><br>当你的网络从20层变成30、56层的时候其实你的精度不管是训练还是验证或者测试精度都会变差，他说这个东西不是一个因为你的层数变多了然后你的模型变复杂了导致的一个过拟合，就是说他这里写了一段话就是not caused by overfitting,，因为这是为什么，是因为你的训练误差也变高了，overfitting是说你的训练误差变得很低但是你的测试误差变得很高，中间有个比较大的区别，但是现在你观察到的是说你的训练误差和你的测试误差都会变得很差，所以他不是overfitting，所以更大的像说虽然你的网络似乎是收敛，但是好像没有训练得到一个比较好的结果。</p><p>第三段就是深入地讲了一下加了更多层之后，其实精度会变差这个事情，他说我考虑一个比较浅一点的网络和他对应的比较深的版本，所谓深的版本就说我在浅的网络里面再多加一些层进去。他说如果你的浅的网络效果还不错的话，你的深的网络是不应该变差的，为什么呢，就是说你深的网络新加的那些层，我总是可以把这些层学的，他就变成一个identity mapping，就是说所谓的identity mapping就是说你的输入是 x，我的输出也是x，就等于是说我可以把一些权重学成，比如说一些简单的 n 分之一，使得你的输入和输出是一一对应的，但实际情况下是说，虽然理论上你的权重是可以学成这个样子，但实际上你做不到，就是说假设我就让SGD去优化的话，虽然这里存在一个比较好的解法，就是下面那些层学到一个跟那些比较shallow的网络那个精度比较好的网络一样的结果，但是上面那些层就是变成identity，举例到前面那个例子就是说，我20层的网络权重跟他一样，后面接加的那14层全部变成了一些identity mapping，所以如果你这样子，那就不应该精度会变差，跟之前是一样的，但是实际上他发现说你SGD他找不到这个事情，他unable to find，就是说虽然存在一个这样看上去比较优的解，但是你 SGD 找不出来，那么你怎么办，这篇文章呢就是提出了一个办法使得你显示的构造出一个identity mapping，使得你深的网络不会变得比你浅的网络更差，所以他把这个东西叫做deep residual learning framework，他这一段就详细的解释了一下，他是在干什么事情，我们来看一眼，首先他说我要学的东西叫做h(x)，那假设我现在已经有了一个浅的网络了，我换一个框在这个地方，他的输出是一个叫做 x 的东西，然后我要在上面再新加一些层，比如说我要新加上一些新的层在上面让他变得更深，之前说我新加的层那我就继续跟之前一样的学习就行了，他说我新加那些层我不要直接去学 h(x)，而是应该去学h(x) - x，就是说x是之前比较浅的网络已经学到的那个东西，但是原始的数据进来我们就叫做什么东西都行，叫做z也行，叫做图片也行，首先进来就是说以前我已经学到了 x 表的表示是 x，我新加的层的话我就不要去重新学个东西，我只是到的东西和真实的东可能是你的标号也行什么也行，他们之间的那个残差，我让这个层去学这个东西，所以最后的结果是说整个东西的输出是他的输出再加上 x，等价是这个东西，假设说你的红色的东西学的东西是 F(x)的话，那么再加上浅的网络的输出 x，那么就作为整个的输出，那么他的优化的目标呢就不再是你原始的 h(x)，而是 h(x)减去 x 就这个东西，这个其实说白了就是他的核心的思想。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/2-2.png"><br>然后我们来看一下就是说到之前这张图是干什么事情，这张图就讲了这个东西是干什么事情的，就他当然是说他的这个东西是在上面，就他的浅的网络是在这个地方，浅的网络输出的是 x，这两个假设是我新加的东西的情况下，那么他的 x 进来，进入他的一个层，进来一个relu一个激活层，再进到一个新的层，他的结果是继承 f(x)，那么他最后的输出是x直接过来加上 F(x) ，这个加号的这个事情，再做一个激活层relu做为新的输出。<br>所以他跟之前的直接加的唯一的区别就是加了这个东西identity，加了一条这样子的路, 这个东西是做一个加法，等于是你的输出不再是你自己的输出，还是说你的输出加上了你的输入，这个东西叫做 residual。</p><p>然后我们接下来会来讲说到底为什么在做residual这个事情，核心思想就是这个东西就是在神经网络要实现的话，可以通过一个叫做 shortcut connections的东西，当你数学上实现就是加一下对吧，然后你在神经网络上画的话就是多画一条线过来，这个东西不是很新叫shortcut，其实他给了几个文献，那个文献你可以去看一下，都是90年代很早很早以前就上一次神经网络时代大家早就提出过了，大家肯定是用过这个东西了，所以这个东西他其实做的是一个identity mapping，然后他说这个东西好，为什么呢，因为我就是加一个东西进来，他没有任何可以学的参数就是说我不会增加任何你要学的参数，就不会增加你的模型复杂度，也不会让你的计算变高，因为我就是一个加法而已，而且他整个网络是仍然是可以被训练的，就跟之前的东西是没有改变的，我只是在整个实现里面加了一点点东西而已，比如说他那个年代15年的时候，大家还是用Caffe用的多一点，在Caffe里面，你可以不用改他的代码直接可以实现，所以这个两段呢主要就是讲的是他提出的方法以及稍微告诉你说到底residual 在干什么事情。</p><p>接下来在introduction里面，他还说我们接下做些实验，我的非常深的residual nets非常容易去优化，但是如果不加这个残差连接的话， 他的plain的版本就是效果会很差了。第二个是说他们深的网络可以得到你越深你的精度就越高，这样子他们就赢下了比赛。然后他在摘要里面有说过 CIFAR的结果，他在这里再强调一遍说我把这个结果放在CIFAR上我能train到1000层的东西，最后当然是讲了一下我的 ImageNet上的结果，怎么样赢下了第一名。</p><blockquote><p>所以这个基本上他导论，可以认为是他摘要的一个增强版本，在结果上我说的东西更多一点，然后主要是解释了residual那个东西在干什么事情，就我解决一个什么问题，然后他的问题是什么，然后我的一些猜想，然后我具体来说我整个东西设计是什么样子的，然后给大家一个简单的一个思想，然后你读到这里的话，你大概就知道了他的核心的设计，你甚至就可以不用往下读了，你都再就知道这篇文章的核心精髓在讲什么东西了， 当然你还没有看到ResNet是怎么设计的，但是你知道他的核心就是这个residual connection，然后用了它之后效果很好，如果你不是在做这一块的话，很多时候，你就可以停在这个地方了，这也是这篇文章我觉得写的比较好的，比较标准的一个地方，就是 intro 是你的摘要的一个比较扩充的版本，也是一个比较完整的对整个工作的一个描述。</p></blockquote><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>接下来我们来看第二章，第二章一般来说就是相关的工作了。Residual Representations，就说因为我毕竟是用的 residual，他说了在计算机视觉里面是怎么做这个事情的。</p><blockquote><p>residudal 这个词其实在机器学习或统计里面用的更多一点，大家知道其实线性模型最早的解法就是不断的靠residual 来迭代的，然后另外一个在机器学习里面比较有名的叫说gradient boosting，他其实就是不断的去通过残差来学习一个网络，然后把一些弱的分类器把它叠加起来变成一个强的分类器，在20年前是曾经是也是非常火的，但是这个地方他没有去回顾这一块也能理解，这是计算机视觉的，我们发在 CVPR上面，所以他确实没有太多去考虑机器学习在干的事情。</p></blockquote><p>另外一个是说他的Shortcut Connection这个东西其实在之前也用的比较多，比如说叫做highway networks，然后这些东西他说之前其实用过的比较多，当然之前的工作相对来说比较复杂一点，他那个connection相对来说fancy一点，他在这个地方就是一个加法，就是最简单的一个做法。</p><blockquote><p>所以看到这里的话，大家其实觉得你会发现ResNet他不是第一个提出residual，或者说shortcut这种原创的文章，很多时候你看任何文章、任何经典的文章，你会发现它里面的技术不一定是原创的，你看Alexnet它里面的dropout的也好，很多别的东西也好，那些神经网络你也没有觉得他特别是原创的，但是一篇文章之所以成为经典，不见得他一定要原创性的提出了什么什么东西，他很有可能是说我把之前的一些东西然后很巧妙的放在一起能解决一个现在大家关心的比较难的问题，所以一样的能够出圈能够出名，然后大家觉得你是经典工作，然后甚至大家都不记得之前谁谁谁做过，所以对于研究者来讲也是一个好事情，就是说你会发现任何的想法呀，你随便想个什么东西，前面有那么多聪明的人基本上就把你的东西已经想过了写过文章了，你写任何一个说我做了一个什么什么东西，很有可能你会发现前面有人做过了，很多事没关系，就是说你就在文字面写清楚我前面前面谁谁谁做过了，现在我们有跟他们有什么不一样地方，比如说用同一个东西解决了一个新的问题，或者是说你反正新的问题你可能考虑上跟总会有一点点细微的区别，另外一块有可能是说你的文章太久远了，就是说可能别人一些工作30年前、40年前发表了，他的工作可能也没什么引用，就几个引用，你可能没有找到的文章，所以说你在论文里面说我提出了什么东西我觉得我是第一个，在作者来说，一般会说to our best knowledge，其实说我们真的是搜过了没有找到，所以我觉得我可能是第一个做这个事情的，但很有可能别人发现你不是的，那没关系，如果你确实很久远的文章找不到的话，对于一个review来讲，或者是你一个读者来讲，发现其实也没有太多问题，可以善意的告诉他说你这个工作很有意思，但是我这里有一个相关工作，你可以看一下就是跟你的idea有点像，就你提醒他一下，他可能会下一个版本会把它加上就行了，但也没必要特别愤怒是说你一定抄袭了什么东西，当然抄袭的是说你一个最近的工作跟他长得真的差不多，然后你也没引他，甚至你可能看上去你跟他的东西可能你在写的时候你其实看过，但是没引他当然是有问题的，所以我觉得这一块大家在对待技术的原创性来讲，其实要一个稍微客观一点，也是稍微容忍一点的态度，因为而且对于研究者来讲，你也不要觉得说什么东西都被做过了，那我没什么东西做可做了，其实也不是这样子的，你去看一下基本上深度学习的经典文章，你往前走20年前，基本上那些idea也都被用过了，只是现在可能问题还是同一个问题，但是我的数据量更大，我的计算能力更强，也是新的挑战后过来，旧的技术但是有新的应用有新的意义，也是一个非常重要的事情。</p></blockquote><p>接下来我们看一下残差连接如何处理你的输入和输出的形状是不同的情况，这里他提供了两个方案，第一个方案是说他在输入和输出上分别添加一些额外的零，使的这两个的形状能够对应起来，然后可以做相加。第二个是之前提到过全连接怎么做投影，当然如果你做到卷积上的时候，他是通过一个叫做1x1的卷积层，这个解决层的特点是他在空间维度上不做任何的东西，但是主要是在通道维度上做改变，所以他只要选取一个1乘1的卷积，使得你的输出通道是你的输入通道的两倍，这样子他就能够把你的残差连接的输入和输出跟你对比上了。另外一个是说，我们知道ResNet里面，如果我们把一个输出通道数翻了两倍，那么你的输入的高和宽通常都会被减半，那这个地方，所以你在做这个1x1的卷积的时候，你也同样会用一个步幅为2，使得你这样做出来的话在高宽和通道上都能够匹配上。</p><p>然后我们看一下实验，讲了一些他实验的一些细节，他说我们用了一些正常的practice说，他把短边随机的采样到256和480。</p><blockquote><p>这个跟我们之前的AlexNet有一点不一样，AlexNet就直接把短边放到256，这个地方是随机的放这个地方。</p></blockquote><p>随机放到比较大的地方的好处就是你在做随机的那个切割的时候，切成224乘224的时候，你的好处是你的随机性会更多一点。另外一个说他把每一个pixel的均值都减掉了里面他用了一些颜色的增强。</p><blockquote><p>在AlexNet 里面，我们用的是 PCA做一个颜色增强(PCA Color Augmentation)，现在其实用的比较简单了，就是 rgb 上的，把亮度、饱和度各种地方调一调就行了，觉得photoshop能干的事他都能干。</p></blockquote><p>另外, 他当然用了 batch normalization, 然后另外一个是说, 他说我的所有的权重啊全部是跟, 13这个 paper 里面用的是一样的, 这个其实当然是说, 你可以这么写了, 这样子的话你可以省下很多空间, 但是对读者来讲就比较尴尬了, 就是说我如果没有读过你这个论文, 我怎么去知道我还得去读你的论文, 实际上论文是13其实是, 这些作者之前的一篇文章, 他自己写的就很方便, 因为是前面这篇文章是我的, 所以我就已经说, 我们跟前面那篇文章是一样, 但实际上来说 对读者这个不是很好, 所以如果大家要写论文的话, 尽量能够, 使得别人能够不要去看里面的文献, 能够了解到你在做什么, 如果让别人去点开这个文献, 然后再去搜一下的话, 其实是不那么方便的, 另外一个是他用的, 批量大小是256, 学习率是0.1, 然后呢每一次除10, 为什么时候除10呢 当你的错误率啊, 比较平的时候他就会除10, 这个也是 AlexNet 用的方法啊, 我们之前提到过, 我们现在也不怎么用了, 因为这个东西你得守着啊, 你不守着你是谁知道什么是应该除10, 他说我的模型训练了, 60乘以10的四次方次 一个小批量 , 这个写法我有点奇怪, 我不知道这个10的4次方, 为什么有这个东西出现啊, 我的建议是说, 大家最好不要写这种iterations（迭代次数）, 为什么 是因为, 这个东西跟你的批量大小是相关的, 如果我变了一个批量大小, 那你这个东西就会变了对吧, 所以现在很多人一般会写, 我迭代了多少遍数据 , 相对来说稳定一点, 另外说, 他用了一个weight decay0.0001, 然后 momentum 0.9    都是标准的, 另外一个是说他没有用 dropout, 这是因为我没有全连接层了, 所以 dropout, 在这个地方没有太多用, 在测试的时候呢, 他用了, 标准的, 10个 crop testing, 就是你给到测试图片, 我会在里面随机的, 或者是有按照一定规则的, 去采样10个图片出来, 然后在每个子图上做预测, 就会把这个结果做平均, 这样的好处就是说, 因为你的训练的时候, 你每次是随机把图片出来, 我在测试的时候也大概模拟这个东西, 另外我做, 10次预测的话, 当然是能够降低我的一些方差了, 然后他说最后是我们把这些, 而且他是做了很多个, 分辨率啊就是在不同的分辨率上, 然后去做采样, 这样子你相对来说在测试的时候, 你做工作还是挺多的啊, 你又做了, 不同的 crop, 又做了不同的分辨率, 当然是说你要刷榜的话, 这个是很常见的一个办法, 但是在实际上我们用的比较少, 因为这个这样的话你的测试就太贵了, 一般来说, 我们不会为了那么一点点精度, 把自己的线上的性能搞的, 特别糟糕, 因为毕竟是要掏钱的   好, 这个就是我们整个模型这一块的实现, 接下来第四章讲的是实验, 实验包括来说我怎么去评估 ImagNet, 以及我的各个不同版本的ResNet,   是怎么设计的, 第一段讲的是ImageNet是怎么回事, 他这个是很标准的东西我们就不讲了, 第二个是说 他比的是一个, 没有带残差的时候, 他使用了一个18层和34层, 34层我们之前已经讲过, 就刚刚那个图, 就这个角落那个图我们已经讲过了, 我们可以看一下18层是在干什么, 这里有时候是说他在表1里面有把, 细节的架构给大家讲了, 我们来看一下表1是怎么回事啊, 表1是这一张比较大的表, 也就是大家经常截图用的一张表啊, 就是整个 ResNet不同架构之间的, 构成是什么样子的, 我们来看一下, 首先看到啊我们, 这个地方有不同的版本啊, ResNet 18,  ResNet 34、50、101然后152, 这里一共有5个版本, 然后五个版本呢 他们的第一个, 就是第一个卷积 就7<em>7, 这个卷积当然是一样的, 接下来呢 那个pooling层也是一样的, 当最后那个全连接层带是一样的, 就是啊, 最后一个全局的pooling, 然后再加一个1,000的全连接层做输出, 他的不同架构之间呢, 主要是中间不一样, 也就是那些复制的那些, 卷积层是有不一样的, 我们首先看一下34, 啊我们之前已经看过了, 这个东西表示的是一个, 残差块里面的东西, 块里面我们知道是有两个卷积层, 他用的是, 第一个模块 这个是conv2、3、4、5, 加上的讲的是四个模块, 我们之前数过的, x 就表示里面其实有很多个不同的层, 就很多个不同的块吧, 然后呢, 第一个块里面 他的组成是3</em>3, 然后通道数是64, 然后他有三个这样子的块, 如果回到前面看的话, 他对应的其就是这里对吧, 他有一个块两个块三个块, 每个块里面是这样子, 然后块之间他是通过一个, 残差连接来连接, 好的, 所以这个地方表示的是, 第一个块, 接下来是第二个块第二个块就是, 通道数是128, 这个都是一样的3乘3都是一样的, 但是通道数是64, 128，256，512, 然后中间就是说你复制多少次, 他复制的是3, 4 6 3, 统计下来那就是34层, 我们看一下为什么34层就是, 3加4加6是16, 然后以每个里面是2就是32, 再加上第一个加上最后一个, 那就是一共是34层, 18层呢, 18层跟34层其实是一样的, 主要是把这个东西变了, 他把所有这个数字啊全部变成了2, 222所以就说这一块一共是, 8乘以2一十六, 然后再加上第一个卷积层, 最后一个全连接层那就是18, 所以就是这么算出来的, 所以, 为什么你这个东西取成这个样子呢, 这也是比较好玩的, 现在论文里面, 并没有讲你为什么取成这个样子, 啊这就是超参数了是作者调出来的, 实际上来说你这些参数啊, 你可以通过一些, 网络架构的自动选取啊, 在之后的工作 有 大家有去调说, 具体你这个东西选成什么样子, 其实是可以调的, 接下来还有三个模块啊我们等会再讲 , 因为它里面有一点点不一样的地方, 他一个残差块里面是有三个层啊, 还不是之前的两个层, 我们之前之后碰到的时候再回来讲他, 好最后看一下就是你的FLOPs, 就是, 你整个网络要计算多少个浮点数运算, 这个东西是可以算出来的, 就是说卷积层的浮点运算就是等价于, 输出的高 乘以 他的宽, 乘以通道数乘以输出通道数, 再乘以你的, 和的窗口的高和宽就是一乘, 然后你在, 全连接再一层就 , 基本上就可以算出来了, 可以看到是说, 18-34基本上是翻了一倍啊, 1:8-3.6, 但是50的话其实没有比, 34高多少, 你看到就是说他做了一些别的架构, 使得, 50的时候并没有像前面样的翻倍, 而是说差不多, 之后 当然之后是翻倍的关系啊, 但是在这个地方是有一点特殊的架构, 啊我们之后再来讲, 好接下来他说他的结果, 在表2对比的18和34层, 啊有残差连接还没有残差连接的结果, 然后他的图四呢也啊, 可视化这个结果我们之前其实有讲过, 给大家再给大家重新看一下, 重新看一下就是说, 红色的线表示的是粗, 粗的那一根线表的是34的, 他的验证精度或测试精度也行, 你怎么说, 然后这个东西表示的是他的训练精度, 首先有意思的是说你的训练精度其实, 比你的测试精度要高的, 在一开始啊为什么呢, 是因为你在, 训练的时候用了大量的数据增强, 使得你的训练误差相对是比较大的, 然后你在测试的时候, 你没有做数据增强, 你的噪音比较低, 所以一开始当然是会低的, 然后他这里这个东西是干嘛的, 这个东西是你的学习率的下降, 就这个这个地方你学习率乘了0.1, 每一次乘了0.1就说本来收, 本来是SGD就是一个, 慢慢的慢慢的收敛呢突然乘了0.1, 然后整个他的步伐改变了, 就打乱了他的步子, 然后他就趴 跳到另外一个地方, 跳到另外一个地方呢, 你可以看到是, 对整个他的下降比较明显, 他这个地方是跳了两次啊, 基本上跳了两次的样子, 最后这个地方是没有跳了啊, 所以就说为什么大家也不再喜欢用啊, 乘0.1 乘0.1, 这种做法, 就是说你在什么地方跳, 其实是很尴尬, 就你在这个地方, 你说我在这个地方已经平了吧, 你为什么不在这个地方跳, 而且在后面跳, 实际上来说你不应该跳太早, 在这个地方呢你跳太早的话, 会后期收敛会无力, 就是说你最好, 其实你这个地方你还可以再往前, 训练一点, 再跳, 就是说虽然你看上去 他的没有, 做什么事情啊, 实际上他在里边做很多的微调, 但是做些微小的跳动, 但是你在, 这个地方这个宏观的数据上看不出来, 所以其实你多训练训练, 在晚一点跳其实是一个不错的选择啊, 晚一点跳的话, 你一开始就是找到方向更准一点, 到后面其实对你的后期是比较好的, 就跟你练内功一样的, 你先积累积累然后再突破对吧, 然后他主要, 其实这个图主要想说明的是说, 你这一块是没有残差连接的, 这一块是有残差连接的, 有残差连接, 当然是说, 你34的时候他会比18要好, 另外一个是说, 他的34跟这个34, 当然是会 有残差连接会好很多, 另外一个他讲到的一个事情是说, 如果有了残差连接的话呢, 他的收敛啊会快很多, 你可以比一下这个, 34和这个34, 他的收敛在这个地方你看这个地方, 大概是15乘1的4四次方的叠带的时候呢, 还在这个地方, 这个地方已经掉的很低了, 因为他的 y 轴是对应上的, 所以说他的核心思想是说, 在所有的超参数都一定的情况下, 有残差的连接收敛会快, 而且后期会好, 但你可以看到这个表啊, 就是讲的是, 最后对比说, 绝对数值上来说最后的数值啊, ResNet当然是比没有加残差的会好很多, 我们讲完这个就可以跳过一大段的实验的介绍啊, 通常论文上来说他会, 虽然你的实验结果都在你的图里面 表里面, 但是他也会在文字上重新解释一遍, 啊生怕你看不懂, 当然你能看懂的话, 你其实你也可以大量的跳过他的文字, 接下来看一下就是说他比较了不同的, 就是在你的输入输出不一样的时候, 形状然后怎么样做残差连接, 他之前有讲过两种方法啊 一个是填零, 一个是做投影, 第三个就是说, 他所有的连接都做投影, 意思是说, 就算你的输入和输出, 他的形状是一样的, 我一样的可以在那个连接的时候, 做一个1<em>1的卷积, 但是输入和输出通道数是一样, 然后做一次投影, 他对比呢这三个方案, 表3呢那, 表示的是这三个方案不同的结果啊, 就 abc 然后, 同样都是34层的 resnet, 这个是 top 1, top 1和 top5 具体是说, 你要看哪个都无所谓, 基本上你, top 1比较好的 top 5也比较好, 所以其实都没关系, 你就随便看一个都可以, 可以看到, 是说啊 top 1的时候你的 a, 就是你填0, 啊 b 就是你在不同的时候做投影, c 就是全部做投影, 基本就是说, 可以看到 b 和 c 都差不多啊, 但是他比 a 还是好一些的, 然后啊作者说我尽量不想用 c, 用 c 感觉跟他虽然好一点, 但是呢他的坏处是说, 因为你这个投影啊他相对是比较贵的, 然后给他带来了大量的计算复杂度, 所以就是说划不来, 他觉得但是 b 还不错啊 b, 他对计算, 量的增加是不多, 也毕竟你就有四次好像是会改变吧, 然后你假设, 150层的话也就4次会要做投影, 所以呢但是他结果会好一点, 所以他之后都是用的这一种方案, 也就是现在我们所谓的 resnet, 都是用的, 当你的输入输出改变的时候, 我们会用1</em>1的卷积做一次投影, 好 讲完这个之后接下来就是说, 怎么样构建更深的 resnet, 我们到现在为止, 他只讲的是 resnet34, 但是我们知道 resnet可以到50, 甚至到1,000怎么办啊, 所以他这个地方讲的是说, 我要做, 到50或者50以上的层的时候呢, 他会引入一个叫做, bottleneck的design, 就是一个瓶颈的design, 就我们具体来看一下前面这个图啊, 就他想干什么, 就是说这个是我们之前的设计, 之前呢, 是说当你的通道数是64位的时候, 他进到一个3<em>3 3</em>3, 然后都是64啊最后做加法, 但是说你通道数是不变的情况下, 他说如果你要做到比较深的时候呢, 你这个维度啊, 就比较大一点, 就是当你很深的时候, 我可以学到更多的模式, 也就是说 我可以把通道数变得更大, 这个地方, 你是从64变成了256, 当你变得更大的时候会什么问题, 因为你的计算复杂度是, 你这里乘了几啊, 乘了4, 如果你再乘4的话, 他的计算复杂都是增加16倍, 也是平方关系, 所以他就划不来了, 他说这个样太贵了, 那怎么做呢, 他的做法是说, 他虽然你这个地方是256, 但我通过一个1<em>1的卷积把他映射, 投影回到64位, 就跟他这样子是一样, 然后再做3</em>3的, 通道数不变的一个卷积, 就等价于说, 这一个操作跟这个操作是一样的, 然后再投影回256, 为什么要投影回去, 是因为你的输入的通道数是256, 所以你输出要匹配上, 所以你再投影回去等价于是说,  你先把一个等于降一次维啊, 我们这个256, 等价于是他的特征维度, 我们先对特征维度降一次维, 在降一次维的上面, 再做一个空间上的一个东西, 然后再投影回去, 这是 bottleneck的设计是怎么样的, 是怎么做的, 那他说啊虽然, 我这个地方你的通道数是之前的4倍, 但我一旦这么设置之后, 啊, 这两个东西的算法复杂都是差不多的, 就说这一块和这一块的复杂度差不多, 然后再回到我们看之前的那个表一, 我们提到过的表一, 这样子我就能看懂这个50 100 152的设计了, 基本上可以看到是说, 这个就是之前我们提到的, 东西的设计就是, 你过来之后我们先投影, 然后在这个地方再回去, 然后你看这个地方, 这个地方的输出是256, 但是到下一个的时候, 他投影回128, 然后投影然后再投影回512, 然后他的因为这一层的4啊, 所以导致说他的下一个 block 就是, 512投影回128然后投影回去, 当然你可以看到基本上不管你是啊, 层数从50 101 152, 他的基本上差不多啊, 这一块这一块这一块差不多对吧, 都是一样的, 这个这个这个都是一样, 就是说虽然你的东西不一样啊, 就是你的层数不一样, 但是里面这个设计我都是一样的, 唯一的是说, 你看到这个东西的变化的, 后面我们做的一个2048, 因为我做的是比较深啊, 比较深的话, 我可以去里面抓取更多的信息, 这样子用一个, 等于是我用一个, 2048的向量来表示我的图片, 之前用的是一个512的图片, 然后 50的时候跟34呢 , 他这个地方是一样的, 就这一块是一样的, 所以34-50主要是加入了一个, bottleneck的设计, 然后呢 这里面通道数发生变化, 但是这个地方是一样的, 因为你这个地方从2变成了3, 所以呢他从34层变成了50层, 基本上通道数, 是从64变成256, 128变成512, 512变成2048, 通道数基本上翻了四倍, 但是因为你这个 bottleneck 设计, 我的flops 数, 就是计算复杂度是差不多的, 所以导致说, 你看一下 resnet34和 resnet 50呢, 在复杂度上增加是不大的, 当然这个是理论的复杂度, 在实际跑情况下这个东西会贵一些, 因为这是理论复杂, 因为他的这些东西啊, 1乘1的卷积啊, 在计算的有效性上确实没有别的卷积高, 导致了他是说, 在实际上来说, 50还是比34要贵了不少, 110的话那么主要的就是, 你看到就是把, 6变成了23, 别的都没变, 然后152的话就是4变成了8, 23变成36, 还是这个问题, 具体你为什么要这么设计, 我觉得是, 作者可能是在做实验的时候啊, 调了一些参, 然后最后调了一个还不错的结果, 但是当然也, ResNet 的计算量那么大, 也不支撑大家说做一个特别, 大的一个搜索, 那现在我们的计算资源够了, 我们现在可以更有能力去搜索它的结构, 所以啊 现在很多时候各种 resnet 的改版啊, 大家会在这个上面慢慢的调的更好一点, 这样我们就基本上讲了, resnet 整个的架构是什么样子, 最后看一下结果, 结果我们之前大概其实也讲了, 就是当你更深的时候呢, 你会发现你的精度或者你的错误率啊 会依次下降, 从21.84一直降到了19.38, 这个东西还是挺明显的, 然后他就说我跟别的算法比, 就是我们赢下比赛的那个是3.57啊, 这个地方为什么比这个地方低那么多, 是因为啊, 他做了大量的这样子的random crop  , 最后做了融合, 所以导致效果要好一点, 他跟别人比还是很明显啊, 就3.57啊你看, 这个是别人的工作都是要低的1.6个点, 1.6个点已经挺厉害了啊, 你们想想就3.57啊, 你再往下还能够降两个1.6就基本上到顶了, 而且你是到不了底的, 因为ImageNet他的标号的错误率本来就挺高的, 估计有个1%估计是有的, 所以呢 你不应该到底, 这个就是基本上就是在ImageNet上的结果, 以及ResNet各个版本, 他到底长什么样子, 之前有说过他其实还做了一些实验, 就是说在CIFAR上面的实验, CIFAR是一个很小的数据集啊, 他之所以做这个就是跑起来容易吧, 就给大家看一下里面到底发生了什么事情, 我就不给大家特别讲CIFAR这个东西设计什么样子的, 他就是说啊 我又在CIFAR上面, CIFAR上面的ResNet和ImageNet上面的ResNet是不一样的, 因为 CIFAR它整个图片就很小, 它是一个32乘32的图片, ImageNet就是基本上300乘300以上, 所以呢 在设计上会有一点点不一样, 其实更加简单一点了, 他说啊 他主要的一个事情是说, 我在  CIFAR上面啊, 设计那么多, 然后他最后设计出了一个, 1,202层的东西, 他的参数当然不大了, 因为你的输入输出, 就输入的高宽本来就很小, 所以你当然是没ImageNet的那么大, 但是呢 比较有意思的是说, 虽然那么简单一个数据集啊,  CIFAR就是一个也就5万个, 应该是5万个样本吧, 十类的数据集, 看到是说你, 即使是在往下加你的这个地方, 基本上在101层的时候还是有个, 往下降的趋势, 当然是 最后你大概是1,000多层的时候是会往上升, 但是也还好啊, 他也没说升的特别离谱, 就是说你的, 虽然这个地方你可以看到有一点点的overfitting, 但是也不那么严重, 然后这下面一张图啊也是 CIFAR上, 跟之前其实ImageNet是图差不多了的, 就是讲一个道理, 就是你假如 什么都不加入residual的话, 那么你的56层当时比20层的, 精度要差一些, 你加了之后就是跟之前是差不多效果啊, 另外一个是说, 他主要想说的一个东西, 其实想说啊, 在整个 residual connection就是残差连接, 你要干什么事情呢, 就说在你的后面那些层啊, 新加上的层, 他说如果, 你的新加上的层啊, 不能让你的模型变好的时候, 那么呢 因为有残差连接的存在, 所以新加那些层应该是不会学到任何东西, 应该都是靠近0了, 这样子导致说等价于是说, 我就算是训练了1,000层的 ResNet, 但是呢 可能就前100层有用, 后面的900层就, 基本上因为没有什么东西可以学的, 他基本就不会动了, 所以他想讲的是这个道理, 然后他这个地方画的是说, 看一下那些最后那些层啊, 真的是在有没有用, 就是如果你没有学到东西, 那么最后那些层的他就不加输入的时候, 那些层的输出呢, 基本上是意味着0是吧, 所以他就是看了一下说, 最后那些层啊 你可以看到, 后面那些层对吧, 这是100层啊, 然后, 就看到是说如果你没有加残差的话, 其实还是大家还是比较大的, 如果你加的话大家就是, 比较小一点的, 所以他其实想说的是这个道理, 但是呢这个东西啊, 你就看一看吧, 为什么是因为, 虽然你没有加残差和加了残差, 你用的是同样的超参数, 在训练的时候同样超参数, 但是这是两个完全不一样的模型, 就加那么一点点, 就对这个模型的改变是很大的, 所以导致同样的训练超参数, 我觉得在, 没有加的时候其实收敛是不对的, 所以导致的这个东西根本就是, 没有收敛好的, 就说啊没有训练好的一个状态, 所以你在比他的话其实也很难比较, 我们有讲过啊 这篇文章他没有结论, 没有结论是因为最后这一段, 这个地方应该是用来画结论的东西, 他把它加上了一个目标检测的结果, 他说我们的结果在目标检测数据集上结果很好, 然后他的结果呢, 在这个地方就是你可以看一下啊, 就是啊 mAP就是在目标检测上最常见的一个精度的, 就是那个锚框的平均那个精度的一个东西, 然后 他是在不同的阈值下面 , 可以看到是说 跟之前比他是, 他这个东西啊 越高越好啊, 所以他是从21.2增加到了27.2, 这个东西是从70.4增加到了73.8, 然后 就是说因为他讲的这句话呀, 因为他说 details在appendix 里面, 所以你就往下看啊, 所以因为加了这个东西啊 , 所以他把结论啊讨论啊这东西都去掉了啊, 其实从我的角度来讲啊, 其实你这篇文章已经那么那么厉害了, 就前面的结果真的是啊, 很很吓人的情况下, 不写这个东西其实没关系, 甚至你可把这个东西放到下一篇文章, 写都没关系啊 你把这个东西一塞啊, 然后你就说, 后面是两页的appendix, 讲的是我这个东西是怎么做的, 我觉得这个东西啊没什么太多必要, 就是说, 我觉得一篇文章不要放太多的结果, 导致大家读起来比较难, 然后你, 这些结果其实是一个锦上添花的效果, 我觉得, 你就算不放这个结果, 你的引用数也不会变低, 你也不会被 CVPR拒掉, 可能作者觉得说这一块其实可能啊, 贡献不那么大, 毕竟没有太多新的东西, 主要是把那个CNN的主干模型换成了ResNet, 剩下都是一些实验, 所以如果你是写的是一个新的论文, 可能中的概率不高, 所以干脆放到这里, 给大家一并讲了算了, 这也我觉得这也是有一定道理啊, 但现在其实你说真话, 你就是写一个两三页的, technical report, 就是技术报告也不错了, 毕竟你在写, 因为做目标检测啊, 大家还是, 跟图片识别还是两波不一样的人, 你把这一块东西写的详细一点啊, 其实对做目标检测人是有好处的, 他们更容易复现你们的结果, 当然是我的事后的一些看法了, 就不讲那个appendix了, 大家有兴趣可以看一下, 就我想就是说给大家回顾一下就是说, 这篇文章啊, 我们读下来就是, 基本上还是挺顺利的啊, 就是从头读到尾就不需要读第三遍了, 我们好像也没有什么没有看懂东西, 有两个原因, 一个原因是这篇文章确实比较简单, 就是一个主要是一个残差连接, 在网络架构上的设计啊非常简单, 也是比 AlexNet 要简单一些啊, 然后, 所以导致说因为他东西比较简单, 写起来也不会那么复杂, 第二个是说, 我觉得作者这写作是非常厉害, 就大家可以学习一下就是说啊, 这导致说因为就算是简单的东西, 你写出来可能别人不一定看得懂啊, 有很多文章, 其实是还有一个很简单的思想, 但是写的特别烦, 然后大家看不懂啊, 这个所以是大家读起来, 这个文章是相对来说没有太多压力的, 另外一个是说, 当然是说我们从五年之后再来看这篇文章啊, 就是说他的主要贡献主要是把, residual connection就是残差连接用过来, 就是他当给了一些直观上的解释啊说, 使得一个更复杂模型能够, 如果训练, 如果新加的很多层的话效果不好的话, 我能够fallback, 能够变成一个简单模型, 使得你的模型不要给我过度的复杂化, 他其实是一个这么直观上的一个解释, 并没有做任何的分析啊, 他当然有一点点实验啊, 但我也讲过, 这个实验其实不那么特别可信, 另外一个他没有从方法论上就具体解释, 要从数学上来讲一下这是为什么, 因为这个文章也没什么公式啊, 大家从计算机视觉paper上来讲, 没什么公式挺正常的, 但是你要在, 那个年代那个年代可能还已经 ok 了, AlexNet那个年代, 你不写点公式你发NIPS是很难的, 那个年代可能一五年的, 已经可能问题不大了啊, 所以就是说, 他就是觉得啊, 给一些直观上的解释, 后来啊 事后大家其实去看, 其实大家不是特别的买原作者的账啊, 因为本来原作者的没有加太多东西, 后来其实大家觉得说一方面为什么, ResNet训练起来还比较快, 主要是因为梯度上他保持的比较好, 就如果你是正常的话, 你正常的话比如说我有一个啊, g(x)假设是你的, 原始的一个小的网络, 你在上面再加一些层, 那么等于是变成一个F(x), 再加一个 f 进去, 这个是新加进的层, 你对他求导的时候呢, 对 x, 求导的时候呢他当然是变成了啊 f(g(x)), 然后就是说对新加的这个项f, 他的求导啊, 再乘以你原来那个网络的求导, 对吧就是说, 他是一个累乘的关系, 就是说你新加一些层呢, 你的梯度啊你加的越多, 你梯度的乘法就越多, 这个是一个矩阵乘法, 让我乘东西就会 因为梯度是比较小的, 其实梯度一般来说在, 一个在0的附近的高斯一个分布吧, 就是值是比较小的, 大部分值跟零很近的, 所以你一乘一乘一乘就乘的特别小的, 就导致你很深的时候比较小, 也就是梯度的一个消失的问题, 虽然你 batch normalization, 或者什么东西让你比较好, 但是实际上来说他还是相对来说比较小, 但是如果你加了一个 ResNet 的话, ResNet的话他的好处就是说, 现在你的输出变成了是啊, f(g(x)) + g(x), 然后你对他求导的时候呢, 你就是这一块就说啊这一块是不变的, 但是你加号就是等于, 他等于是这一块, 这一块其实就这这一块下来, 就这一块下来, 但是他再加上了一个原始的一个, 就是没有, 就这个小的, 对吧 所以就是说,  我们知道, 这一块就是 这一块容易很小, 如果你加了很多东西, 但是这个浅层网络呢, 相对来说他相对来说会大一点, 所以这个东西梯度变成了一个, 你这块很小没关系, 但是我这一块能训练动, 就是一个加法, 就是一个, 这个如果你这个小数的话, 你这个数可能并不是很小, 所以是一个小数加上一个大数, 相对来说你的梯度还是会比较大的, 导致说, 就算你的不管你后面加你的 f加多少啊, 你加的特别特别深啊, 这个g本身 如果就是一个啊 , 前面的网络的话你不管加多少层, 我这个梯度在这里总是有用的, 而且是说他先把这一块给学好了, 所以这也是说啊, 从误差反传的时候角度来看, 为什么现在训练比较快啊, 另外一个比较有意思的是说, 在 CIFAR上面你加了1024啊, 就一千层以上啊, 他说我没有做任何太多的regularization（正则化）, 他其实做了 他不是没做, 他是没有说任何special的东西, 然后效果也很好, 就是说你overfitting有一点点但是不大, 这一块其实是说你在, 你就不能用这个东西来解释了, 因为这个东西是说让你的训练比较快, 然后让你训练的动, 这个东西能解释什么东西呢, 能解释说你跟你 resnet34, 你没有加那个残差连接的时候, 你为什么精度会好, 是因为你没有加的时候, 你根本就没训练动啊, 如果你回到头看那一个图啊, 比如说你看啊, 你看这张图的话, 这个是你没有加的情况下, 就是说你这个东西, 这个东西叫收敛啊, 但收敛没意义就是说, sgd 收敛就是说你, 你就他收敛, 就是说你这里地方不切, 他就这么收敛过去了, 就假设你在这个地方, 你不把他的学习率降低, 他就这么下去了, 他就收敛在这个地方了, 所以收敛是没意义的, 这 sgd 你所谓的收敛就说, train不动了, 就训练不动了, 这个东西不叫收敛啊, 收敛就是说你, 最好收敛在有比较好的地方, 所以呢这个地方是说其实是因为, 你做深的时候呢, 你用那么简单的机器训练, 根本就跑不动, 你根本就不会得到比较好的结果, 所以你只看说收敛是意义不大的, 但是你现在加的残差连接, 你的梯度比较大, 所以就没那么容易, 因为梯度一直比较大, 就没那么容易收敛, 所以导致说你一直能够往前, 所以sgd 的精髓啊, 我之前写过篇文章就是说你知道就是说你怎么, 说人生就跟SGD一样, SGD的精髓就是说, 你得一直能跑得动, 对吧 你如果哪一天你跑不动了, 你就是梯度没了, 那你就那就完了, 你就是在一个地方止步 出不去了, 就是说SGD的精髓是 你梯度很大, 一直能够跑, 反正你有噪音吗, 然后就是说, 慢慢的慢慢的他总是会收敛, 所以就说你只要保证梯度一致够大, 然后你其实就最后的结果就会比较好, 这个是大家一些经验上的总结, 所以啊 你从, 这个角度来看, 就是说你为什么加和没加效果还是很不一样的, 另外一个是说, 我们这样说 resnet 这个, 那么就是说, 在CIFAR-10上，这么小的数据集上, 为什么他的过拟合啊, 不那么明显, 这个东西其实我, 我觉得目前还是一个 open question, 就大家有一些有研究啊, 特别现在那些transformer那些模型啊, 那么大一个的东西对吧, 那么大一个东西你是怎么样, 训练的动的, 就是说, 100个billion啊, 就是1千亿的那些参数啊, 你为什么不过拟合呢, 就大家现在有很多工作啊, 最近一些年有特别有一些很有意思的工作, 但我们在这里是没有办法给大家, 今天是没有办法给大家讲一遍了, 就是说, 其实虽然你的层数很深啊, 你的参数很多啊, 但是你的模型因为是这么构造的, 使得他的 intrinsic（内在）, 就是他内在的模型复杂度其实不高了, 就是说很有可能就说你加了这个, 残差连接之后, 使得你模型的复杂度就降低了, 就是说, 你加了他 就和比他没加的时候, 他的复杂度大大的降低了, 所以他一旦模型复杂度的降低, 那么他其实过拟合就没那么严重, 所谓的模型复杂度降低, 不是说你不能表示别的东西了, 就是说你能找到一个很低的, 你能更方便的找到一个, 不那么复杂的模型去拟合你的数据, 就跟作者说的, 我不加残差连接的时候, 理论上, 我也能够学出一个有一个identity的东西, 就是后面那些层都不要, 但是实际上你做不到, 就是说因为你没有, 引导整个网络去这么走的话, 他其实这个理论上结果他根本就过不去, 所以一定是你得, 手动的把这个结果加进去, 使得他更容易能够训练出来, 所以啊 加了这个东西之后, 使得他能够, 整个resnet能够学习到一个, 相对来说更简单, 就是说如果真要做的时候, 就后面那些层都是0, 就前面那些层有东西, 就是说让你更容易的训练出一个, 简单的模型来拟合数据的情况下, 那么就是说, 等价于把你的模型复杂度都给降低了, 这个这一块有最近有很多工作啊, 有时间可以给大家讲讲, 这我觉得是, 这一块来解释这个可能是更好一点, 另外一块就是说大家如果知道, residual在机器学习是干嘛的话, 就是比如说 gradient boosting, 这个东西的话, 他的 residual 跟 gradient boosting是不一样的, gradient boosting是在标号上做residual , 然后这个地方是在 feature维度上, 但我们就不展开了, 就是说大家有兴趣可以去研究一下, 为什么这个residual  跟你的机器学习那边GBDT, 那些树上面的residual  有什么不一样, 大家可以去研究一下, 好我们这个就是对, resnet这篇文章的讲解, 基本上可以看到是说, 这篇文章提出了一个非常简单的方法, 来使得能训练更深的模型, 而且整个模型的构造是非常简单的, 虽然, 他说他的motivation为什么做的东西, 我们现在来看, 可能会觉得那个东西讲的不够深刻啊, 但是这个完全不掩盖这是一篇经典的文章啊, 你不能说我这文章实验又能飞起来, 然后还能够给一个漂亮的理论的分析, 这个是不可能的, 而且只要有一点做好了就行了, 你要么理论能飞起来, 实验根本就不做的没关系, 要么就实验能飞起来, 然后啊理论不说都没关系啊, 你只要有一个亮点, 大家认同你这个亮点, 大家会有无数人会来follow你的工作, 然后往下走, 这就是挖坑, 你把所有东西都做了, 大家怎么去跟你对吧, 然后你也给大家留口饭吃对吧, 所以你把你最大的那个肉吃了, 那么你就把剩下的饭留给大家, 所以现在说只要你的工作够厉害, 然后是很新的东西能启发的东西, 你把文章写差一点, 或者说你写很多东西没说明白, 这真的不要紧, 大家会后续的人会, 前赴后继的把你在一块往前推啊, 这个也是研究界一大魅力所在吧,</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文精读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文1】深度学习奠基作之一：AlexNet</title>
      <link href="/2023/12/04/lun-wen-1-shen-du-xue-xi-dian-ji-zuo-zhi-yi-alexnet/"/>
      <url>/2023/12/04/lun-wen-1-shen-du-xue-xi-dian-ji-zuo-zhi-yi-alexnet/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><em><strong>ImageNet Classification with Deep Convolutional Neural Networks</strong></em></p><p>AlaxNet是深度学习整个浪潮的奠基作之一，发表在2012年。</p><p>我们读这篇文章有两个目的。</p><p>第一个目的是给大家演示一下怎么用我们之前讲的<code>三步读论文法</code>读一篇文章。三步法就包括了 第一遍我们看一下标题摘要和结论，第二遍我们快速读篇文章，第三遍我们再深入读，我们可以再随时在中间停下来说不往下了, 如果你读到最后就意味着你这篇文章对你非常重要。我们假设自己回到九年前的状态去看我们怎么样去读这篇文章。</p><p>第二个目的是说我们回到现在看一下9年后，我们再来读这篇经典的文章，看一下里面哪些东西是还是成立的，哪些东西可能从现在角度来观点来看已经过时了，再经典的文章也是有时代局限性的，所以里面那些精髓我们是保留下来。</p><h1 id="第一遍"><a href="#第一遍" class="headerlink" title="第一遍"></a>第一遍</h1><h3 id="1-标题"><a href="#1-标题" class="headerlink" title="1.标题"></a>1.标题</h3><p><em><strong>ImageNet Classification with Deep Convolutional Neural Networks</strong></em></p><p>标题看到是两个词。</p><p>第一个词叫做ImageNet Classification，回到九年前，你可能也是听说过ImageNet，当时是图片分类最大的数据集，100万的图片，然后1000类，不管你说机器学习还是说计算机视觉，你可能都听说过他。</p><p>第二个词你可能就不那么理解了，就是Deep Convolutional Neural Networks，神经网络你应该是听说过，学人工智能、学机器学习，多多少少会学到过神经网络，卷积神经网络你不一定学到，那时候大家一般都不怎么讲，一般大家讲的是树、SVM讲的比较多，加一个Deep那你可能是更没听说过了，这很有可能你看到这个词之后，你可能觉得跟我的研究方向没什么关系，我就不看了，这非常有可能，这是我们现在假设是说你有师兄或师姐告诉你说这篇文章值得一读。</p><h3 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a>2.摘要</h3><p>摘要第一句话是说，我们训练的一个很大的、很深的卷积神经网络，用来对120万张图片做分类，这里面有1000个类，第一句话就告诉我干了什么事情。然后他就说我的结果在测试集上面，我的top-1和top-5的错误率是分别是37.5%和17.0%，他说我比前面的工作都要好。</p><blockquote><p>这个是比较有意思的一个写法，就是很少有人第一句话我做什么第二句话我的结果很好，这个是非常少见的一种写法。</p></blockquote><p>然后他说这个神经网络有6000万个参数和65万个神经元。</p><blockquote><p>首先你可能不一定知道神经元是什么东西，但是参数你看下还是挺厉害的，就是说你看看自己的SVM或者是你的线性模型可能参数根本就没那么多，所以看到这个数字还是比较吓人的。 </p></blockquote><p>然后他说我这个神经网络有五个卷积层，然后有一些是MaxPooling层，然后有三个全连接层，然后最后一个100层的softmax。</p><blockquote><p>你如果第一次读的话，你可能也不知道他在干干什么事情，反正你知道是讲我的模型在干嘛。</p></blockquote><p>他说为了训练快一点，我们使用了一个什么东西，然后使用了gpu的实现。</p><blockquote><p>GPU的实现在2012年已经算是比较正常了，就是在我记得2007年NVIDIA出了CUDA这个库之后，在过去的几年之内，就2012年过学几年之类的GPU在机学习界还用的还是挺多的，当时候大家主要还是用Matlab，Matlab也有很多gpu的加速包，这块用的还挺多的，所以看到这个不意外。</p></blockquote><p>另外一个是说为了减少我的过拟合，我们用了一些Regularization正则的办法，叫做Dropout，然后我们又把这个模型放到了2012年竞赛中 得到了一个15.3%的top-5的测试率。</p><blockquote><p>他这个东西就比较奇怪了，就是说你之前其实说的是17.0%，然后这里说的是15.3%，你可能第一眼会觉得比较奇怪，但我们就回来讲一下为什么。</p></blockquote><p>然后最后一句话就是说第二名拿的是26.2%，你拿的是15.3%。</p><blockquote><p>你看一下这个区别还是挺大的，就虽然我可能并不知道ImageNet绝对的错误率是多少，但是我一看你跟第二名好那么多，那还是有一定兴趣的。</p><p>这就是说你的摘要在干什么事情，所以说白了就是说我训练了一个很大的神经网络，在这个数据集上我们比第二错误率好很多，这就是整个摘要干的事情。</p><p>这个不算是一个很好的摘要，这个有点像一个技术报告干的事情，从人篇论文的角度来讲，这个比较相对来说写的， 不那么好，但是你最后一个东西就是比较强，就是说你看我就是结果好，所以在这个时候你一般来说会还是会选择往下看一眼，毕竟你好那么多，我们看一下他到底是干什么。</p></blockquote><h3 id="3-讨论"><a href="#3-讨论" class="headerlink" title="3.讨论"></a>3.讨论</h3><blockquote><p>我们接下来直接跳到最后看一下是什么样子，最后这篇文章是没有结论的，他有一个讨论，就讨论和结论会不一样，讨论更多是说我看就是吐吐槽然后看一下未来要干什么事情，结论很多时候是跟摘要的一个一一的对应，所以没有结论通常来说是比较少见的一个事情。</p></blockquote><p>我们看一下他的讨论干什么，他讨论说我们的结果显示，一个大的、很深的神经网络能够做一个特别好的结果在一个特别难的数据集上面，他说我们的网络的性能会往下降，如果你把一个一层神经网络去掉，就是说他有五层卷积神经网络，如果你去掉一层之后，他说我会降2%个点，所以深度是非常重要的，就是说你这个网络有多深是很重要的。</p><blockquote><p>这个结论倒是没有错，但是你说我把一层卷积层拿掉然后降了2%个点，不能说明说深度一定是最重要的，因为很有可能是你参数没设好，实际上说 AlexNet能去掉一些层，然后把中间参数变一变，还是有办法达到的，就是说你当时搜参搜的不够。</p><p>但反过来讲，这个结论导致现在看起来没问题，就是说你需要很深，另外一块的话，更完整的结论是说深也很重要，但是宽度也是相对来重要的，你不能特别是特别窄，你也不能特别宽特别扁都不行，就是说像你拍照一样的，你高宽比是比较重要的。</p></blockquote><p>第二段他说为了使得我们的实验更加简单 我们没有做任何Unsupervised Pre-training。</p><blockquote><p>训练神经网络时，我很有可能去来一些没有标号的一些图片，把它的权重相对来说受到比较好的范围再往下训练，主要是因为深度神经网络在当时候的训练是不容易的，所以很多时候会用一些大量的没有必要的图片让他预热。</p></blockquote><blockquote><p>所以这句话就是说我没有用他意思，其实说我不用他没关系了，这句话就是让整个深度学习在非常长的一段时间之内主要关注于有标号的数据 ，就是叫Supervised Learning。</p></blockquote><blockquote><p>在之前整个深度学习，他的目的是想说我真的能够通过训练一个非常大的神经网络，在没有标的数据上把整个东西把里面内在的结构抽取出来 。因为人类学的东西很多时候是你不一定告诉你真实答案是什么，就是你读书读百遍，其义自现，所以导致说AlexNet影响了整个深度学习，这也使得，就说当时候不管是 Hinton也好，LeCun也好，还是其他大佬也好都觉得你是走了歪的方向，因为他们追求的是无监督学习，但是在过去很多年之内，大家主要关注在有标号的数据上，然后怎么样打赢别人，有意思的是，直到最近两三年BERT这一块在自然语言界的兴起才把大家又回到了无监督的学习上面，就是给我数据但是你不要告诉我标号，我真的能去从中间理解。最近很多重要的工作是基于这一块的，以至于大家觉得是一个很信赖思想，其实在AlexNet之前整个深度学习更多是做无监督的学习，为什么，因为有监督学习打不赢别人，跟SVM效果差不多，那我们只能说我的模型够大，我可以做你不能干的事情。但是 AlexNet证明的说，我在有标号的数据项，只要够大一样的打赢别人，整个学术界、工业界都在干这一块事情。</p></blockquote><blockquote><p>那我们之后有机会给大家讲一讲 , 现在是Unsupervised的，就是没有标号这一块是怎么做，也可能很大程度未来在几年之内大家都走这一个方向。</p></blockquote><p>然后他后面说如果我们有足够多的计算资源，可以把我们的网络变得特别大， 当我有足够的资源的时候，我把网络变得很大，也没有标号的数据也没关系，然后他说只要我们的网络更大、训练更长，然后我们的结论就可以更好。但他也承认，我们现在还是跟人类的视觉还是差很远，就是说你虽然能在ImageNet上做的比较好，但是跟人的能力比还是差远，这个东西在当时是这样，但是现在来看，在图片里面找找简单的事情，深度学习已经做比人类好很多了，比如说开开无人车，也现在也问题不大了。他说最后我们想用非常大和深的神经网络在video上面，因为video视频里面有一些时序的信息，时序的信息能够有很多帮你理解在空间的图片信息。然后他说我们如果有钱、有机器、有数据的话，那我们想去训练下video数据。</p><blockquote><p>过去那么多年，大家在图片上面走的非常非常的远，在语言上面也走了非常远，就是在video上面 一直是走的比较慢，因为video确实对于图片来讲，它的计算量增加的不是一点点，而video很多时候都是有版权的。</p></blockquote><p>这是他的讨论的部分。</p><blockquote><p>然后我们第一遍里面，我们还会去看一下一些重要的图和公式，然后一些图我们来看一下是怎么回事，我们往上翻一翻。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-1.png"><br>这个图是说我的结果一些东西，这个东西当然大家是多多少少能看懂，你看一下这个图是说我是用了我们这些测试图片看一下我的分类效果怎么样。<br>第一个这是个摩托车，说你分对了，然后第二个是go-kart 然后说看上去也对，你预测的那些别的来看上去也不错，你说雪豹，第二个词也差不多是雪豹，cherry这个东西你可能没搞对，你可能是认的是后面是狗，然后你这个东西 你认的是一个敞篷车，你的预测敞篷车它的真实的标号是第二个。<br>就基本上看到是说，这图片对人来说当然是容易的，但是对一个机器学习算法来说，这个图片还真的不那么容易，毕竟它种类特别多，所以就是说, 基本上看到说效果在一些难的cass上面还是做的比较好的。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-2.png"><br>第二个图片特别有意思，第二个图片是说，他把这个神经网络最后的那些图片在倒数第二层的输出拿出来就得到一个长的向量，然后把每个图片都拿出来，然后去把说给定一张图片，看一下跟我这个向量上最近的那些图片是谁全部找出来，就基本上看到找出的东西都挺靠谱的。<br>给朵花跟我在最后一层输出那个向量的输出里面靠的很近的一些花，找出来都差不多是那个花，象也是，然后这个南瓜就是万圣节的南瓜，然后这个狗都长差不多。<br>就虽然这篇论文并没有讨论到这个东西有多重要，实际上来说，这个是整个最重要的一个结果，就是说深度神经网络的一个图片训练出来最后那个向量在语义空间里面的表示特别好，就是相似的图片真的会把它放在一起，所以导致说它是一个非常好的特征，非常适合用后面的机器学习 一个简单的分离器，就能做的特别好，这也是深度学习等一大强项。在这个地方虽然作者有把他的结果秀出来，但是对这个工 对这个东西的意义，大家其实要过了很久或者也过了几个月大家才知道这个东西确实效果太好了。</p><p>然后我们再往前看下还有什么重要的图。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-3.png"><br>这个表表示的是我的结果跟别人的结果的对比，这个是两个当前最好的结果，可以看到别人结果在top-1的错误率上面、top-5的错误率上是远远的高于这篇文章提出来结果，这个是他其实在摘要里面说过的，但是这个地方给的更详细一点，基本上可以看到是说整篇文章的卖点是我的结果特别好。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-4.png"><br>然后再往前面看 就这是结果方面再往前面看，就是说你会看到一些一些图，如果你不做神经网络，可能是不那么了解这个图，就是说当然在计算机视觉里面，大家会多多少给看到这些图，但是如果你是刚来的话其实不知道这个在干嘛。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-5.png"><br>再往前面看，另外一个主要图是这个块图，就是说如果你在这一块不了解的话，是初学者的话，其实是不怎么看得懂这个图面。</p><blockquote><p>我们假设自己回到九年前，我对这一块不那么了解的话，这张图在可能第一遍里面我是看不懂的， 第一遍你能看懂图通常是一些实验的结果图，和你对这一块方向比较了解的，然后看下这个图大概能理解这个东西跟我之前的知识一对比，大家能理解他做什么东西 ，但是这个一块相对来说比较新，这是开创性的工作，所以这一遍，我可以留到之后来看。</p></blockquote><p>第一遍读下来就知道，这篇文章结果特别好，是用神经网络做的，但是具体为什么好，然后具体怎么做的，因为你的背景知识还不够，所以在第一遍你可以放弃，然后你这时候可以决定说，我要不要往下读，我们在假设当时你应该往下走，不然的话我们就完事了，但是在当时候，你很有可能不一定往下走，你就知道效果很好用的神经网络，但是我的研究跟这一块无关，我可能就不做这一块了，但是如果你是做图片分类这一块，你是一个计算机视觉的研究者的，你很有可能还是会往下读了，因为毕竟这块工作很好，他赢了下了比赛，那么明年那肯定会有很多人用这个方法来去试一下这家伙怎么样，所以你应该多多少少是应该往下的。</p><h1 id="第二遍"><a href="#第二遍" class="headerlink" title="第二遍"></a>第二遍</h1><blockquote><p>我现在给大家读第二遍AlexNet这篇文章，第二遍里面我们需要把整个文章从头到尾读一遍，但是如果碰到了那些不懂的地方，我们可以留下来由给第三遍，但是第二遍主要的目的是知道很多细节的干什么，更多的是去了解作者是怎么想的，作者是怎么样表述东西的，每一篇文章都需要自己的观点，每个做的他都对这个世界的认识是有一定的角度的，所以通过读论文我们能够很清晰的感受到作者对整个问题的看法以及他的角度他是怎么认为的，因为写文章的时候，作者通常会把自己所有想的东西一股脑地拍在文章上面，就是一种被掏空的感觉，这个相对来说比读那些博客文章或简单来介绍什么是AlexNet才能读到的信息量更多，而且对一个人来讲，你需要读到很多文章，然后去总结很多不同的优秀的研究者对这个世界的认识，然后形成自己独特的观点，我觉得这个是一个研究者最重要的事情，每个人都需要有自己的观点，你的观点不对没关系，没有人的观点是对的，但是你一定有自己独特的观点，这样子才能够不一样，如果你跟别人的观点都一样，那么你对这个世界贡献可能就没那么多了。</p></blockquote><p>当然因为AlexNet是经典论文了，我们在这一遍还要给大家讲一下他的那些对技术的选择、对技术的描述，从现在角度来看是不是真的还是这样子的，我们等会看到会知道其实里面绝大部分的对一个技术的描述、为什么做东西在现在观点来看都是错误的，而且里面有很多大量的细节 其实从现在角度来讲是没有必要的，而且是过度的engineering，但是在当时不知道的，所以大家是说我出了一个结果，把所有东西弄在一起很好，但是在后面一些年，因为这个工作特别重要，大家会去看每一个细节到底是在干什么，然后无数的研究一起去研究这个问题，把里面东西搞得特别清楚。所以现在我们从现代角度来看里面很多的细节我们是不需要知道的，然后也加入了很多新的细节使得我们在过去一些年里面，深度的卷积神经网络在图片世界以及完全打败了人类。</p><blockquote><p>因为我们之前已经看过摘要和结论了，我们直接可以从介绍这一块往下读，我们就是一段一段的往下读，这样大家可以随时暂停一下，看一下这一段在讲什么，然后再继续播放，往下看我们是怎么讲的，但我们不可能给大家一句句的过，就是说大家可以扫一遍，我来讲一下每一段在干什么事情。</p></blockquote><p>第一段就是说一篇论文的第一段通常是讲个故事，就这样子，我们在做什么研究，哪个方向这个方向，有什么东西，然后为什么很重要。</p><p>他讲的第一句话是说，我们要做object recognition就是物体的识别或者就图片分类，然后我们为了提升它的性能 我们要收集更大数据及训练更厉害的模型，使用更好的技术来避免过拟合。</p><blockquote><p>这基本上是机器学习的正常的途径，这也是在这篇文之前，在大数据年代，大家一直在关心的，收集更大的数据 , 更大数据当然有更强的模型了，更强的模型当然是有过拟合， 其实这个过拟合这个东西是代表了深度学习的一个派别，深度学习说我可以用很大很大很大的模型，然后我通过正则来使他不要过拟合，这个是整个深度学习界在当时的一个认知，在未来几年之内大家都是这样做的，但实际上来说，你从现在观点来看，好像大家又觉得说，正则好像没那么重要，就是说叫Regularization这些东西好像并不是最关键的，最关键是你整个神经网络的设计，使得你很大很大神经网络一样的， 在没有很好的正则情况也一样样都能训练出来，但这一块大家还在做出新的方向，就是理论工作也好，实验工作也好都在推进，但是整个这一块观点代表了是整个深度学习就在很长时间之内的一个看法。</p></blockquote><p>然后后面还在讲，讲数据就是说你就有Caltech-101一样最后过渡到imageNet这个数据集。</p><blockquote><p>主要是因为这个标题是讲用ImageNet，所以我的吹一波刷ImageNet的这个特别好 因为它数据量大然后类别多。</p></blockquote><p>然后他说，为了去识别上千种不一样的类别，在百万的图片里面，我们需要一个很大的模型，就是一个Large learning capacity，然后他就直接说我们的神经网络了。</p><blockquote><p>所以这个东西是说，他们那一块人当然是一直做神经网络，所以对这块的了解比较深的。</p></blockquote><p>所以他就直接说，我们要怎么样做神经网络，主要用CNN来做，然后怎么样把CNN做的特别大。</p><blockquote><p>所以这段文章的意思是说，大家应该用CNN来做这个事情，因为CNN这个是一个很好的模型，因为做大很容易overfitting（过拟合）， 或者训练不动，所以我们要去怎么看它。<br>他这个写法是有问题的，因为写法的问题，当时候的主流大家不用CNN，当你主流的模型是用别的，所以就说你半句话不提别人的算法，然后直接只提CNN 是一个非常一个很窄的一个视角，所以你写论文的时候，千万不要只说我这个领域这个小方向大概怎么样，但是你要提到别的方向怎么样，就是做一个稍微公平一点的一个介绍。</p></blockquote><p>第三段是说，CNN虽然很好，但是很难训练，训练不动， 但是好处是说我现在有了gpu了，gpu能够算力能跟上，使得我能够训练很大的东西，而且这个图片、数据集够大我确实能够训练比较大的CNN。</p><blockquote><p>基本上前面三段是讲了一个故事，我做什么东西，现在为什么人做了。</p></blockquote><p>第四段是讲的是我这个paper的一个贡献，他第一句话当时说我们训练了一个最大的神经网络，然后取得了特别好的结果，然后我们使写了一个实现了一个GPU上性能很高的一个2D的卷积，然后我们的网络有一些新的和不常见的一些特性，然后能够提升他的性能，然后降低他的训练时间，然后他说我在第三节会给大家讲。</p><blockquote><p>第四节讲的是说这个网络很容易为很大就会过拟合我们做一些什么样过拟合的方法使得它的变好。</p></blockquote><p>最后 我们的网络有五个卷积层、三个全连接层 ，然后深度好像很重要，我们觉得我们发现移掉任何一层都不行。</p><blockquote><p>这基本上你看到是他说第三节讲的是说我们做了一个新的网络，第四节讲的是怎么样避免过拟合，跟前面的说法其实对了，就是说我需要大的数据集、大的模型，然后要避免过拟合，大数据集不是我们做的，是ImageNet，我们用它了，然后我们说我们怎么样做一个更大的网络，怎么样避免过拟合，所以这是他们主要的贡献，当然他的主要的结果是说，我训练一个很大的模型，结果特别好。<br>这个东西写的是没问题的，我觉得写的是，你要做一个东西的话，如果你说我在ImageNet上取得特别好的结果 拿到第一名，但是我是怎么做的呢，我就是拿了100个模型，把它融合起来做了拿了第一名，这个就没什么意思了，比如说你在模型上似乎没有太多创新，更多是一些工程上的技术上的创新，所以你写论文的时候，别人大家可能不会那么喜欢。<br>但是你现在说我虽然拿了第一，但是我用了一些Unusual feature，新的一些没用过的一些东西，然后使得它可以做很大，然后有一些新的一些技术，可以让我来降低过拟合，新的技术是比较有意思的，就是说你用新的技术又取得了比较好的成绩，那就是挖了一个大坑，别人能够继续往下做，你用了就说你炫技，把很多技术放在一起拿了一个最好成绩，对别人是没有太多的启发性的，别人就说这东西做的太好了，就是这东西自叹不如，就像说我做了一个世界上最大什么东西，就是你上传到b站娱乐下，大家是行的，但是你作为一个研究工作来讲就说，你这个东西过于复杂，然后过于的难以复现，我只能说你厉害我不行 我跟不了你，就说你可能能重一篇篇文章，但是你很难得到一个引用率特别高的文章，所以这篇文章当然是说我不仅做了，效果很好，毕竟是我还是有创新在里面的会在第三章和第四章来讲，所以这个就可以看到是说这篇文章最重要的就是第三章第四章，虽然我的卖点还是说我的结果很好，但是我还是有新东西的，因为反过来讲如果你就结果很好，没有新东西，大概是不会成为奠基作对吧。</p></blockquote><p>然后在后面是说我们是在GPU上训练的，但他用的是GTX580显卡，如果大家打游戏的话会知道，这个其实是一个很弱的一个gpu了，然后就3G的内存，所以他说我需要把我的整个网络切开放到我的GPU上面，这个使得我的能训练。</p><blockquote><p>虽然从他的角度来讲，从Alex一作的角度来讲，它确实是一个很大的工程量的一个东西，但是从后面大家对他的反应，大家其实这个东西不重要，虽然你可能花了你的80%时间就花在写这个东西上面，但是从一个研究工作角度来讲，还是这是技术上的东西，但是能够存下来东西很难，是那些很工程性的细节。</p></blockquote><p>这篇文章第二章讲的是数据集，并且你的标题第一句话是因为ImageNet对吧，他就讲了下，你的卖点是因为ImageNet的结果，当然你要讲一下这个数据集吧，他就大概讲了一下数据集，我们就不仔细讲了，就基本说我有1500万的数据，然后有2万类，然后他做了竞赛2010年开始他做竞赛，竞赛的话大概是有1000类，每一类有1000张图片，1000类，所以共是100万张的图片，然后他说2010年这一届是他把他的测试集给你公布出来了，所以是说你有测试集就能够报告测试精度，这是为什么在摘要里面说我的测试精度是多少，但是从2012年开始就是他们参加的是2012年这次竞赛，2012年开始他就没有公布自己的测试集了，你只能去网上提交，提交结果就要看到结果，所以就是说他说我就只能报告一些结果，所以就是为什么在摘要里面，它能够才有两个不一样的top one和five的一个错误率，这也是源自于这个地方。<br>然后后面一段其实是非常重要的一段话，但是当时要读可能就没有感觉太出来，就是说ImageNet它的图片的分辨率是很不一样的，就很多时候别的数据集的分辨率都给你做好了，就是帮你裁好了，但ImageNet是没帮你裁好了，他说我怎么做呢，我就是说把每张图片变成个256x256的图片，他具体怎么做，就是说，他首先把这张图片的短边，把你减少到256 长边是保证高宽比也往下降，但是如果你多出来怎么办呢，多出来就是说长边如果多于256的话，他以中心为界能把两个边会给你裁掉，就裁成一个256x256的图片。他说我没有做任何的预处理了，就是剪裁一下，然后他说我们的网络是在raw RGB Value上训练的。</p><blockquote><p>就这个东西可能你是要做计算机视觉人才会理解，就是说当时候大家都是一个图片过来，然后把特征抽出来，抽SIFT的也好，抽别的特征好，就算是image net的数据集他也提供了一个SIFT的版本的一个特征，但是这个工作说我不要抽特征，我直接是在原始的Pixels上做了，所以这篇文章并没有把它做一个卖点，但是在之后的工作里面基本上卖点主要是End to End(端到端)。<br><code>所谓的End to End 是说我原始的图片 原始的文本直接进去，我不要做任何特征提取，我的神经网络能帮你做出来。</code><br>虽然这篇文章讲到时候我是这么做的，但是他也没有大概是意识到这篇文章这个东西的重要性，所以为什么是说这篇文章写的一般呢，是说有可能是你从当时的历史局限性你没有看到这个东西对整个业界的影响，就是你就报告了我是怎么做的，并没有说哪些东西重要，那些东西不重要，所以就说没有把你的亮点写出来，但是这个是一个足够重要的工作 在之后的无数人去读这篇文章、复习的篇文章把它里面种东西提出来，这样慢慢的会变成整个深度学习一些核心的价值，在这个地方你很多时候你是一些原始的东西，你可能是要自己真正的去动手去关注的工作，然后去往下做研究，你大概理解到我用原始的rgp图片有多舒服。<br>所谓的SIFT的，我现在没有搞清SIFT得到有什么抽的，但是我知道AlexNet是怎么做的，所以就是说，现在我能够搞清楚整个原始图片到最后结果一步步是怎么来的，我不需要你知道那么多一些专业的知识，我也不想知道计算机视觉在过去30年之内做特征是怎么做的，其实我一我我曾经去读过，但是我其实很很早之前后我就忘了，所以说简单有效东西是能够持久。</p></blockquote><p>第三章这前面提供的就是讲我整个网络的架构，这个是它文章的主要的贡献之一，另外的贡献是怎么样避免过拟合，这是第四章。<br>他说我这个架构在图二。</p><blockquote><p>我们图二刚刚看了一眼，就是那个一个很复杂一个图，如果你是没有相关的背景的话，看到个图是有困难的，我们先看下文字是怎么讲的。</p></blockquote><p>他说首先我们最重要的一个叫做ReLU的一个非线性的东西，在标准的里面你神经网络里面的激活函数用的是tanh或者是一个Sigmoid的。</p><blockquote><p>如果你学过的话，大概也知道Sigmoid是什么东西。</p></blockquote><p>他说但是这些饱和的非线性激活函数会比那些非饱和的非线性函数要慢，他说非饱和的叫做f(x)等于是0和1输入取个最大值。</p><blockquote><p>但是其实你如果刚读的话你也不知道是什么东西，就是说我们圈一下 ，饱和的和非饱和的是什么东西呢，我们其实不知道的，我们第一第二遍没关系，就是我画出这是什么东西，我画一个问号是，这个东西我现在不理解，我们之后再理解没关系，但你先看下他怎么说的。</p></blockquote><p>然后他说你如果用的ReLU。</p><blockquote><p>这个东西叫做ReLU，深度学习的Hinton老爷子是取名界的大师，整个深度学习叫DeepLearning的名字取得多好，整个深度学习界大家都是取名的大师到最新的，BERT呀什么东西，大家的取名都Attension All you Need的，就是说取标题大家都很厉害，当然是说UP主也是取标题的大师，就是说一个东西能流行，标题是非常重要的，名字特别重要，那些搞理论的取标题不行的人都大家都被遗忘掉了。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-6.png"><br>他说我用了ReLU之后，效果特别好特别快，然后你看图一就显示了，图一是说numberEpoch就是你扫多少遍数据，然后这个是你的训练的误差，基本上看到是Solid line也就是识别度，实线是ReLU虚线是tanh。</p><blockquote><p>就是说这个图的卖点是说我用了它之后训练的特别快，但是他为什么那么快是没有说了，就是说告诉你说我用了他之后很快，为什么呢，请你看一下别的文章会怎么想，当然你可以去读一下别的文章，因为这个地方我们应该圈出来这篇文章看一下，这个东西特别告诉你特别重要东西，我们圈出来这分钟没有读过，我们只有在第三遍或者第三遍以前，我们应该去读一下这篇文章讲什么东西。</p></blockquote><p>然后他说我们也不是第一个用了这个东西了，之前有人说用了一些别的方法效果也不错，就是比如说tanh取个绝对值效果也不错，然后他说不管怎么样了能够训练更快的话当然是很重要，因为毕竟这一个深度神经网络训练起来是很贵的。</p><blockquote><p>用ImageNet在九年前训练是非常贵的一件事情，如果能够快一两倍，当然大家会去愿意会去试，但现在看起来其实ReLU也没有觉得比别的人快多少，别的技术增加之后，而且当时候觉得ReLu快的一些原因，其实现在看来都不是那么的正确，而且现在来讲换一个别的激活函数也问题不大，但是大家还是用ReLU并不是他真的比别人好太多，是因为他简单，他就是一个跟0比下最大值，那我都不需要去记他tanh、Sigmoid是什么东西，所以就是简单就是胜利了。</p></blockquote><p>第二个他觉得很重要的是说我用了多个GPU做训练，他说我的580的卡只有3Gb的内存，然后我整个神经网络需要占资源，我要训练120万的图片可以用比较大的网络，当然内存就放不下，他说我怎么样用多个GPU来做训练，他做的比较多。</p><blockquote><p>就是这里其实说句真话，你第二遍可以不看他，因为什么你发现你看不懂就是说它是一个非常的工程的一个细节，如果你读论文读到这工程细节，除非你是系统方向的论文，如果是机器学习论文的话，你可以忽略掉，就说这工程的细节你可以之后真的要复现的时候再去看，因为毕竟不是方法上的东西,   当然你可以读一下这个东西，就是说他怎么样切切切切，我们当之后会给大家讲一下是怎么切的，但是我们第二遍的时候是可以忽略掉不计的。</p></blockquote><p>第三个东西叫做local Response Normalization是一个正则化的、一个归一化的东西，他说虽然ReLU的性质很好，它不需要input normalization。</p><blockquote><p>写这句话你不是那么看得懂的，就是说看不懂的话也可以圈一下，就是说他说ReLU不需要input normalization来避免他你饱和，他之前有提到过饱和和不饱和，其实我们也是没听懂了，就是你不知道ReLU到底有什么性质，所以我可以圈一下，你就可以说第二遍的时候你可以打个问号，就说之后我们再慢慢研究。</p></blockquote><p>但是他又说其实如果你还是能够做一下Normalization，效果还是挺好的，他说这个东西怎么做</p><blockquote><p>就你可以大概扫便会发现，他能讲了很多细节的东西，但这个地方他并没有讲说你为什么一定要用它，然后他效果怎么样，他就告诉你说我是这个东西定义是什么东西是怎么来的。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-7.png"><br>我们第二遍看了这么复杂公式可以忽略掉，其实也不复杂，但是你对他的记号这一块不是那么懂的话，你在第二遍多少时候你可以忽略到，但是你知道是说他是一个Normalization的东西，然后他能够使得避免你饱和，你就知道这个东西就行了。<br>我们可以在第三遍的时候再回来看，但是我们如果回到现在可以看到这个东西不重要，这个东西在之后几乎上没有别人用到它，所以这个东西在现在看来是一个没有太多必要东西，而且现在我们有更好的我们有Normalization的技术，如果你第三遍没有读他也不要紧，你不知道他其实也没关系。</p></blockquote><p>3.4讲的是overlapping Pooling，就是说Pooling这个层是把一些输入给你总结一下，一般来说两个Pooling东西是不会overlap的，就不重叠的，就是说我现在要overlap。</p><blockquote><p>这东西在你第一遍读的时候，其实你可能也不算理解他要干什么事情，就说你都不知道Pooling，首先你都不一定知道Pooling是干什么东西的，我们先圈出来，这东西我们第一遍都没有看懂 东西都圈出来，然后Pooling是干什么的东西的，我们得去回过头来搜一下Pooling是干嘛，然后再说Overlapping的Pooling是在干什么东西，但是你大概理解上来说，就是说他对一个传统的用Pooling的方法做了一定的改动，改动不大，但是他说效果会很好，就是你知道他改的东西就行了。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-5.png"><br>最后 3.5是一个overall的一个架构，他讲的是说在图二里面显示了我们有八个层，然后前面五个层是卷积，后面三个层是全连接，然后最后是一个softmax，然后他说但是因为他使用多个GPU，所以他整个架构是比较麻烦的。当然大概读一下，然后然后再往下的是说我们第一个卷积层是224x224x3的一个输入图片，你看到是一个原始的图片进去了，他跟别的方法用预先抽Feature是不一样的。</p><blockquote><p>所以他并没有说给大家卖一下说你看我们跟别人是不一样的，不需要预处理图片，所以这篇文章真的就是一个技术报告，就讲我做了什么东西，也不讲跟别人的区别是什么，也不讲我们这个东西到底为什么是这样子，为什么重要，但是只要你工作重要，技术报告也能够成为奠基作。</p></blockquote><blockquote><p>当我们现在回到现在的角度来看一下他在干什么事情，就可以看下这个图，这个图是大家讲神经网络的时候，经常会把这个图截屏给大家看一下整个网络架构。但是这个图我觉得能看懂他人不多，因为这个图根本就是一个特别复杂的一个OverEngineer就是过度的扣细节扣出来东西。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-5.png"><br>首先可以看下在框框表示什么东西，如果你第一次看的话 可能不一定理解，这框框是表示每一层的输入和输出这个数据的一个大小，他进来的时候是一张图片，是一个高宽分别是24x24的图片，它的宽度rgb的那个通道数是三，它有三个图片，就RGB它的高宽是24这个东西进来，然后第一层卷积，卷积他的一个窗口是11x11，就大家回顾一下卷积是在干什么，他是11x11然后进来，他第一层卷积有48个输出的通道，他这里有Stride等于4，但卷积的Stride的就是每次往前后往往左后往下跳四下，这里比较有意思是说他有两个GPU，就Alex小哥有两个GPU，所以他为了把这个东西塞进去，他把整个网络给你切开，横的切了一刀，这个东西是放在就是说这个地方我画一下：<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-8.png"><br>这个地方换一个颜色，这地方是有个卷积的一个层在这个地方，这个地方也有个卷积层，这个东西放在gpu0上的，这个东西是放在gpu1上的，所以gpu1和gpu0都有自己的卷积的那个核的参数，他们拿到同一张图片各做各的东西， GPU0做出来东西写来这个地方，GPU1做东西写在这个地方，所以这个这一块是在gpu1上，这一块是在gpu0上，这是他的输出的结果，那GPU1的东西然后继续往下 第二个卷积层，就是说这是第一个卷积层，就是这第一个这第二个、第三个、第四个、第五个这个是表示12345的卷积层的输出，卷积层是在前面中间那个绿色的蓝色的框框里面，可以看到是说第一个卷积层在两个gpu上各有一个，然后第二个卷积层他是在每个gpu把当前的那个卷积层结果拿过来，就是说第二个卷积层读的是gpu0的第一个卷积上输出直接进来，中间是没有任何通讯的。然后这有个奇怪的设定，到第三个的时候，小哥觉得说如果你们个搞个的不是办法，那么第三个的时候还是每个gpu上有自己的卷积核，但是他每个人会去看别人，你看一下这个东西，就是说第三个他的话他会把第二个卷积层的输出在gpu0上gpu1的都拿到，然后再看一眼，对这个也是一样 他会都拿的都看一眼，所以在这个地方他两个GPU之间会通讯一次。然后继续往下做，第四也是各搞各的，就中间是没有任何通讯的，第五个也是各搞各的，只是说每个都不一样，就是你能看到是通道数的有增加，通道数从48变到128变到192变到192，那他的高宽是有在变化的，就是说你看到它的这个地方本来是224x224现在你的变成了高宽变成了55x55，然后变成27x27、13x13，就基本上你看就是说你的输入来是一个很瘪的很宽的一个图片，然后把它高宽慢慢的变小，但是深度的慢慢的增加，你随着你的网络增加，我慢慢的把空间信息压缩，24x24高宽，我慢慢的压缩我的空间信息压到最后是13x13，就是你认为这个里面的每一个像素能够代表前面一大块的像素，然后我再把我的通道数慢慢的增加，就是说你可认为每个通道数是去看一种特定的一些模式，就192个你可以简单认为是说我能够识别图片中间的192种不同的模式，就每一个通道去识别一个是猫腿还是一个边，还是一个什么东西，所以就是说他在慢慢的压缩你的信息，把空间信息压缩，但是这个是个语义的一个空间慢慢的增加，到最后卷积完之后进入全连接层，全连接层你看到这个地方又来了一个在机器之间的就两个卡之间的通讯，全连接层的输入是每个gpu第五个卷积的输出合并起来，合并成一个大的做成全连接，全连接虽然还是各搞各的，就每个人做一个2048的一个全连接，但是最后的结果是要拼回成一个4096的，就是实际上来说一个24x24x3的图片最后再进入你最后的分类层在这个地方的时候，它是表示成了一个4096长的一个向量，包括了说每一块是来自两块卡一片是2048一片是2048最后拼起来，所以说说一张图片会表示成一个4096的这个维度，然后最后用一个线性分类去做链接，这是他整个架构的事情，所以他没写，但是整个这一个东西是一个奠基性的工作，所以他来我们在第一遍的时候有讲过，这个长为4096的向量其实是很好的能够抓住你的语义信息，如果你两个图片它的4096的向量特别相近的话，这两个图片很有可能是同一个物体的图片，所以在深度学习一个主要的用处是说一张图片通过前面这些东西最后把它压缩成一个长为4096的一个向量，这个向量能够把中间的语义信息都能表示起来，就说他变成了一个机器能懂的东西，这是一个人能看懂的像素，通过这一块做特征提取之后变成了一个长为一个4096的机器能看到东西，这个东西第一可以用它来做 各种搜索也好，做分类也好，做很多事情都可以，所以整个机器学习你都可以认为是一个知识的压缩的过程，你前面的原始的数据不管是图片文字还是语音还是视频，它通过中间一个模型最后压缩成一个向量，这个向量机器能够去识别，然后机械的识别之后，他就能够在上面做各种各样的事情，这才是整个深度神经网络的一个精髓之所在，这个地方就是说已经这个雏形已经很明确了，所以说是怎么做了，但是它的重要性在后面的研究者把它们做出来。</p><blockquote><p>反过来讲这个模型我是说大家都其实很多事都没仔细看过这个模型长什么样子，是因为他太复杂了，可能有两个原因。第一是Alex小哥为了能在他的那两块卡上训练这个东西，他强行把你切开，切成了一个上下都有的，他觉得是一个贡献，因为他可能花了很多时间去写代码去把它做出来，但是在现代角度来去看这个东西是一个过于复杂的技术细节，因为其实你就算是3Gb的内存你一样的能训练，你把你的代码实现好一点也是能训练的，我记得caffe是能够训练的。第二个是说这个东西你把它做到了两个gpu上，如果我有三个gpu我有四个gpu怎么办，我怎么切它，所以是说你真的就是一个数据集在你自己的机器上是这么训练的，但是他没有一定的通用性，所以导致说在未来大家基本上是忽略掉整个这一块的工作了，就是说基本上大家是就算我要做多gpu卡的训练我也不那么切模型。</p></blockquote><blockquote><p>但是这个世界就是30年河东 三十年河西，虽然在AlexNet出来之后在过去的起码6、7年之内大家都没有这么做过这个，其实你可以认为叫做模型并行叫model parallel，但是到最近过去两三年，就是更大的模型出来了GBDT、BERT出来了大家又发现又训练不动了，现在大家又回到了 我要把这个模型切成几块的部分，虽然这个技术再出来之后在未来几年这大家都觉得这是一个没用的东西，但是现在大家又发现这东西又很有用了（真香），在计算机视觉里面用的不多，但是在自然语言处理里面现在又成为一个主流的一个办法，把模型切开是能训练100亿的1000亿的或者甚至10000亿的模型，所以这个是整个模型，你可以认为是整个第三章它的主要的贡献就是在这一个图上面了，这个地方已经结束了，大家可以去看一下里面一些细节。</p></blockquote><p>第四节讲的是如何降低过拟合，就是说我训练了一个很大的神经网络，现在说我要避免我过于的过拟合，我应该怎么办，但如果不理解过拟合的话，可以去看一下我们之前的课程，就是说过拟合就是说给你一些题你就把它背下来，你根本就没有理解题是在干什么，所以考试的时候肯定考不好，这就是过拟合，他提到几个方法，一个叫数据增强叫Dat Augmentation，Data Augmentation就是说 我们去把一些图片人工的把它变大，就他使用了其实也是别人的工作，别人已经提出的东西，他使用了两种方法，第一种方法是说因为你的图片是256x256，他随机的在里面抠一块224x224的区域出来，就随机面扣一片，拿着一做一张新的图片，那他说你这样做的话，那么我的图的大小就变成了以前的2048倍的大小，因为你有2048种抠法在这个里面，所以这个东西大家之后不会这么算，就是说你不能直接这么算，因为你的抠下东西都长得差不多，所以  你不能说自己就变成2048倍了。 第一个是空间上的抠，第二个是说我把整个RGB的channel就是那个颜色通道上做一些改变，他用的是一个PCA（主成分分析）的方法，他后面有讲过他的PCA是怎么做的，大家可以去读一下，但是第你第二遍你可以忽略的，如果你不是很清楚PCA在干什么或者是他怎么做的话，你可以选择掉忽略的东西，但是你知道它是在通道上做的一些变换使得它颜色会有不一样，所以就是这样子的话，每次图片跟原始图片是有一定的不一样性在里面。</p><p>然后他又前面有说到说我这个东西用Python写的在GPU上跑的，然后在CPU跑，然后我的模型是在GPU训练，所以说我这个东西是free的就免费的，因为我在CPU跑的很好，他当时要这么看是因为我觉得gpu相对来说不强，然后他的cpu相对来说比较好一点，但是这个东西在过去很多就是算现在，你发现GPU的发展远远的超过cpu的速度，所以导致说你如果把这个东西用python来写在cpu跑你基本上做数据增强可能是你最花时间的东西，模型训练比他快多了，所以这个结论在当时是成立的，现在看起来是说你的Date Augmentation很容易成为你的性能的瓶颈，你很有可能要搬到gpu上获得用很好的c++来实现，不过就是他觉得最重要的一个东西。</p><p>第二个东西叫做Dropou，他第一句话是说很多个模型把你放起来是很有用的，叫做Model ensemble，大家做比赛都是搞好几十个模型，然后把它做融合，但是对深度学习来讲太贵了，就是说你神经网络本来就很贵了，你还要做模型融合当然更贵，所以他怎么办，他用了一个叫Dropout的技术，Dropout的是另外篇文章，虽然也是这帮人做的，他说他随机的把一些隐藏层的输出变成用50%的概率设成零，这样子就好比是说每一次我都是把一些东西设成零就等于是说这个模型就变了，就每一次得到一个新的模型，但是这些模型之间他的权重是共享的，就除了那些设成0的、非0的东西大家都是一样的，但是他觉得这个东西这样子的话，我就每一次能得到一个新的模型，然后等价也是最后变成了很多模型做融合，这是他的思路。</p><blockquote><p>但是后来大家发现是说Dropout的其实好像也不是在做模型的融合，更多的是Dropout就是一个正则项，后来他们在几年抽又重新写了一篇JMLR的文章说Dropout的在现行模型上是等价一个L2正则项，但在更复杂上面说大概是一个正则的效果，但是无法构造出一个跟他相等的一个正则的东西，但是大家现在觉得Dropout的是一个正则的东西，但当时候大家是不怎么觉得的。，</p></blockquote><p>然后他说我们把Dropout的放在了前面的两个全连接上面，然后如果没有Dropout的话，就是说overfitting非常严重，如果有Dropout的话，但是他就是说我的训练会比人家慢两倍，所以AlexNet这个设计用了三个全连接，最后一个是有的，因为人家输出中间了两个很大的4096全连接是他的一大瓶颈，这是他当时设计的一个缺陷，所以导致说整个模型特别大，你根本就放不进你的gpu里面，第二个是说你要用Dropout来过避免过拟合， 现在这CNN的设计通常不会使用那么大的全连接，所以导致说Dropout的也不那么重要，而且gpu内存也没那么的吃紧了，所以就导致你的模型设计的一个决定导致你的实现来好，你加了很多新的东西，但反过来讲 Dropout的在全连接上还是非常有用的，在RNN那一块、在Attension那一块Dropout用的非常多。</p><p>这下我们来看一下第五章，第五章讲的是它的模型是怎么样训练的，他说我们用的是SGD来训练，SGD（随机梯度下降）当然现在大家都知道是训练深度学习最常用的算法，但是在当年大家并不是这么觉得，因为SGD调参相对来说比较难调，大家可能更希望用一些更稳定的算法，比如说L-BFGS或者是甚至是Gradient Descent这样子算法来计算，相对是调参更容易些，但是后来大家发现sgd它里面的噪音对你的模型泛化性其实是有好处的，所以现在深度学习大家都是用这个了，也是说在这个文章之后，SGD基本上在机器学习界成为了最主流的一个优化算法。但是我用的批量大小是128  momentum是0.9然后weight decay是0.0005，然后他们说我们发现用了一点点的weight decay是非常重要的，然后他在后面实现了这个weight decay是怎么实现了，大家就可以看一下，如果你不知道SGD的话，你应该去查一下SGD的定义是什么，如果你知道的话，你大概也知道他是在干什么。<br>weight decay当时候在机器学习界主流上应该叫做L2 regularization就是L2的正则项，但是因为这篇文章以及整个神经网络里面，他们喜欢weight decay这个词来用，这样子他的这个东西不是加在模型上，而是加在你的优化算法上，虽然他两个是等价的关系，但是因为深度学习的学习，所以大家现在基本上把这个东西叫做weight decay了，然后当然是momentum这个东西也是因为这篇文章之后才用的特别多，虽然在2010年的时候有大量的加速的算法里面有很Fancy的各种加速SGD算法，但是现在看起来似乎你用一个简单的momentum也是不错，他直接上来就说当你的优化的表面非常的不平滑的时候，这个冲量使得你不要被当下的这个梯度太多的去误导，你可以保持一个冲量从过去那个方向沿着一个比较平缓的方向往前走，这样子就不容易陷入到，因为你的优化的表面不那么平滑掉到也坑里面去了，让大家可以看下这个是怎么定义的：<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-9.png"><br>就是你的momentum项，它是等于0.9 x 过去的momentum减去你的weight decay再减去你的梯度，他的ε 这个地方其实是你的学习率一般，我们用η但是他用的是这个。</p><p>然后他说接下来我们的权重是用的一个均值为零，方差为0.01的高斯随机变量来初始化了。</p><blockquote><p>我们当时要在d2l直播的时候也经常用这一个均值为0.01方差的来给大家初始化些权重，均值为你0.01是怎么选出来的，0.01也就是大家发现这个值还不错，也不大也不小，但对很多网络都是可以的，但是你如果特别深的时候，你需要更多的优化，但是对一些相对而简单。<br>AlexNet的从现在角度来看是一个比较简单的神经网络，0.01是一个不错的选项，你像现在就算是比较大的那些BERT 他也就是用了0.02，就大家都用0.02作为随机的初始值的方差。</p></blockquote><p>我在第二层 第四层和第五层的卷积层把我的偏移量初始化成1，剩下的全部初始化成0。就是说他全连接层也初始化1了，就这个东西就比较奇怪了，其实偏移本质上来说它如果你的数据比较平衡的话，你应该初始化是要零的，但是他这里说我发现初始化1效果不错，后来其实大家也没有去跟进这一个细节，大家觉得这个东西你到底是把2还是第几层数据化的1，这个东西调参怎么调，所以大家其实觉得全部初始化为零效果也不差，而且不需要调整，所以这一个技术在后面用的是比较少的。<br>然后还有一个是说我用每一个层用一个同样的学习率，但是他的学习率是从0.01开始，然后他发现如果你的Validation Error就是你的验证误差不往下降了，它就手动的把它乘以0.1就是降低十倍。</p><blockquote><p>这个东西当然是假设你有很多时间的话，而且你的计算比较贵的，那你就盯着你的训练，然后发现它不动了，就停下来手动调一下，这个其实在很长一段时间当时大家都是这么做的，但是后来大家觉得其实也没那么复杂，比如说ResNet他的结果就是说每我训练120轮，就120个Epoch 然后每30轮我就下降0.1，然后这也是一种主流做法。<br>另外一种主流做法是说你前面可以做了更长一点必须能够60轮或者是100轮，然后再在后面再下降，也是另外一种主流做法。<br>但是在AlexNet之后的很多训练里面，大家都是做规则性的把学习率往下下降十倍这个是一个非常主要的做法。<br>但是现在我们很少用了，因为毕竟你为什么降十倍，什么是要下降是比较难的，现在我们用更平滑一点的曲线来下降我的学习率，比如说我会用一个cos的一个函数比较平缓的往下降，然后当然一开始的话 0.1是怎么选的也比较尴尬，就是说一开始你不能选太大，因为你太大你就容易炸掉了，但是你如果许的太小的训练也不动，所以现在的一个主流做法就是说你学习率从零开始，然后再慢慢的上去 慢慢的下去。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-10.png"><br>所以如果画出来的话，如果这是你的Epoch轴的话，你的学习率是从一个很小的值开始然后线性的上去，然后到一个比较大的数据值，然后再用一个比如一个cos的函数这么下来，相对来说是一个平话的，你如果是用着AlexNet的话，他其实就是一个这样子的一个值（蓝色），它是一个这样子的下降每次下降十倍，但是具体什么说下降是根据手动来选的，现在大家用红色这个线更加平滑，然后调参更少也是更常见了。</p></blockquote><p>最后他还说我训练了90个Cycle，用现代话也就是我训练的90个Epoch给我扫了数据90遍，然后在每一遍用的是ImageNet完整了120万张图片，需要5-6天在两个NVIDIA GTX上面训练。</p><blockquote><p>这个时间上对当时候的人来说是相对来说比较长的，我调次参我得等个5-6天才能知道我的结果是什么，然后我调的五次参，那我基本上一个月就没了，当时候大家基本上能够一个小时，或者几分钟能跑出来是比较好的，这样子调参你的效率会高一点。<br>在ALexNet这个文章出来的很长一段时间之内，大家花了很多时间去等待你的gpu把你的模型训练出来，一等可能就是几天或者甚至是一个星期，这个导致了大家特别的想去购买最新的gpu，也是导致了Nvidia的股价在过去的一些年暴涨的一个原因，当然在Image这一块领域在过去的一些年我们的训练确实得到了显著提升，但是现在在文本领域又是要训练一次要很多天，而且是在用几百甚至几千块卡的情况下还要很多天这个也是会迎来下一次的迭代，看看我们怎么在文本里把整个计算开销，或者说绝对的时间下降到一个合理的范围，我觉得一个合理范围是几个小时是比较合理的， 或者说半天也是可以，我晚上提交的任务早上起来能看到结果，这也是比较舒服的，但是一个运任务要训练个几天，我觉得是相对来说不能忍的一件事情了。</p></blockquote><p>下面一章是实验，当然实验是这篇论文最重要的部分，我就再去反过来讲，他也没有太多东西，主要还是我们之前讲过的这张图：<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-3.png"><br>他说我比别人相比是要好很多，但是别人的算法我比别人基本上好了一大截，里面当然是说这些东西我是具体怎么做的，现在看起来都是很标准的一些东西了。</p><blockquote><p>就是我就不再给大家重复细节了，很多是我们读论文的时候，实验部分相对来说是不那么重要的，你关心的是实验的一个效果，但是具体实验是怎么做的呢，很多时候除非你是这个领域的专家，你扫眼大概也能听懂，如果你是刚来这个领域的话，你不用太关心这些细节，除非你要去重复他的实验，只有在你要重复他的实验或者审论文或者是你是这一块的专家的时候，你会大概去看一下他的实验。</p></blockquote><p>比较有意思是说，最后他报告了他在完整的imagenet上数据的时间，就完整的ImageNet的图片是有890万，比我们之前说的120万要多个7、8倍的样子，而且它有1万类，这个是一个比较厉害的结果。</p><blockquote><p>其实当时候这个结果我其实不是很清楚为什么大家没有特别去关心这个东西，可能毕竟对计算机视觉来说，能在竞赛中拿到第一次相对是比较重要，但是在一个完整的imageNet上训练出来的模型的预训练效果很有可能比你在120万张图片上预训练的效果在别地方用的时候会好很多，但是比较有意思是我并没有看到后面的很多论文用的是他在完整数据上训练的模型，当我在几年之后也训练过这些模型，发现它效果确实是好一些，但是就是很奇怪大家对imageNet的印象总是拿120万的图片，但是不知道是他的完整其实是更大，而且上面的模型相对来说质量更好一点，这个也是我一直不是很理解的一件事情。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-3.png"><br>到最后是说他们之前有提过ImageNet只有2010年是提供了测试数据，2012年的时候你是得去网上提交他的数据才行，所以他只报告了五层CNN和七层CNN的相对结果，这个两个是空掉了，所以这个就是他把这个东西叫做测试集，这把2010的叫做验证集。</p><blockquote><p>我觉得这个写字和文章做了很对的地方，其实在机器学习里面，很多时候大家是搞混测试集和训练集的，还有验证集的，所谓的验证集就说你可以一直测用来调参，但是你的测试集相对来说你应该就测那么几次，当然ImageNet本身的测试集也是允许你每天提交个几次，所以导致说你也不是真正意义上的测试集。<br>曾经有团队国内的团队去不断的注册很多小号去提交，然后把根据测试集调参，然后被主官方抓出来，也曾经成为一个新闻，但是确实在机器学习界，我们对测试集和验证集的区别，我觉得不管是工业界好也是学术界的好，意思是存在很多的误解的。</p></blockquote><p>第6.1讲的是说，我来看一下我这个网络会是什么样子，他说他发现了一个比较奇怪的一个现象，因为他是在两个gpu上训练的，他的两个gpu上他卷积的话，如果你是输出通道是256个的话，其中128通道是在一个GPU上算，另外128的通道是在另外一个GPU上算，大家知道每个通道你可认为去识别了一些图片里面一些特定的一些模式，他比较有意思是说他发现在gpu1上的通道基本上是跟颜色无关的，就是但是gpu2上学习到的东西，就是那些模式跟颜色是相关的，而且他说我重复了很多次实验发现总发现了一个这样子的事情，所以他也不是很理解。<br>其实从现在角度来讲，你其实也不是那么理解到底你发现了什么东西，很有可能你跟gpu也没有太多关系，很多时候，毕竟这个是一个随机的东西，所以这个东西他提出一个疑问了我觉得这个地方，但是在很多年之后，大家好像也忽略掉这个事情，好像大家也没有因为大家不再这么做了，而且都是在一个gpu上把所有通道上面训练，所以大家也没仔细去看每个通道在干嘛。</p><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/ai/1-2.png"><br>图四的话就是在一些比较难的图片的分类效果和用神经网络最后倒数第二层的输出那4096位的向量去对比，别的图片时候能够把相似图片找出来，就是干的事情，他说主要是用神经网络一直被人诟病的是你到底学的是什么东西，你那么多层的东西里面到底发生了什么事情得到一个好的结果，所以这个地方他做了一些工作，但是我觉得还是对后面还有挺有启发性的，他说你可以去看一下我的每一个Feature activations，就是每一个卷积层或者全连接层他的输出那些东西在干什么，然后他写了一些结果，</p><p>这一段就是讲看看那个图片怎么来的，在后面确实很多工作是沿着这一块往下走了，大家现在大概知道说虽然很多时候你并不知道在学什么，但是有一些神经元还是很有对应性的，他在底层的神经元学到了是一些比较局部的信息 比如说纹理方向偏上的话，他学到更多是一些比较全局点的，比如说这是一个洞这一个人的头这是个手这是个动物  或者是很多信息在里面，也很多有意思的工作去看到底神经网络是学什么，他到底是学一个东西的形状 还是去学一个东西的纹理，大家其实在这一块是要非常有意思的一些发现。</p><blockquote><p>但是反过来讲，神经网络现在仍然是大家不知道到底在学什么，相对于别的相对来说比较简单一点的机器学习模型来讲，它的可解释性一直是一个大家诟病的地方，但最近些年大家慢慢的开始去研究他可所谓的公平性，神经网络的偏移也是大家乐意的一个重点，因为你如果想用一个机器学习模型来真的做决策的话，这个决策能影响到人的话，那 我们当下知道说你到底是用什么来做决策，你是不是决策是公平的。</p></blockquote><blockquote><p>第二遍我们当然留下了很多东西 ，留下了一些我们没有特别高的东西，但是我们整个读完一遍之后对里面的一些细节做什么想有一些比较知道的了解，而且你读到这的时候 相对说你比读一篇别人给你，总结好的 一篇博客一篇文章相对来说你能看到的细节更多。<br>接下来的话，你可以选择说，我继续往下走，继续往下走的话，你可以说我有些东西不是很懂，那么也不是很懂怎么办呢，去看一下他们引用的那些文章，比如说我们说ReLU也好，说SGD也好，他都有相对来说它的引用文献，你可以去引用文献看一下，那里面的技术到底是怎么描述的，然后再回个头来理解它到底是怎么用的。<br>但是你也可以说，我就不用看了，我大概知道他讲什么东西了，除非我确实要在这一块做研究的话，想了解每个细节的话，我读到这个地方也就差不多了，这也是另外个可能性。<br>所以我们来这个地方就不给大家讲第三遍了，为什么呢，是因为我们提到的一些，所以我觉得这个东西看上去很麻烦，没有给大家自己想很多说，从现在角度来讲你也不需要知道了。<br>另外一块的话，当然有一些卷积是怎么工作的，池化层是怎么工作的，大家也不需要去看真正的原始文章,  你随便在网上找一下，大家跟你讲的明明白白，这也是AlexNet读经典文章的时候我们的一个便利，如果是读最新的文章的话，那我们肯定要给大家讲第三遍，把所有的细节给大家讲清楚，不然的话，你去搜还真不一定能搜到这些新的技术是在干什么。<br>所以我觉得作为研究者来说，读论文绝对是获取信息最好的手段，比你去网上搜看Wikipedia，还是看看blog来的更加好一点，而且里面的东西更加详细一点，这次我们就给大家讲到这里，谢谢大家！</p></blockquote></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文精读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文翻译】Swin Transformer: Hierarchical ViT using Shifted Windows</title>
      <link href="/2023/12/02/lun-wen-fan-yi-swin-transformer-hierarchical-vit-using-shifted-windows/"/>
      <url>/2023/12/02/lun-wen-fan-yi-swin-transformer-hierarchical-vit-using-shifted-windows/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><code>Swin Transformer：使用移位窗口的分层视觉Transformer</code></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO testdev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-theart by a large margin of+2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at <a href="https://github.com/microsoft/Swin-Transformer">https://github.com/microsoft/Swin-Transformer</a>.<br><code>本文提出了一种新的视觉Transformer，称为Swin Transformer，它能够作为计算机视觉的通用骨干。将Transformer从语言适应到视觉的挑战来自于两个领域之间的差异，例如视觉实体的规模变化很大，图像中的像素与文本中的单词相比分辨率很高。为了解决这些差异，我们提出了一个分层的Transformer，其表示是用移位窗口计算的。移位加窗方案通过将自注意力计算限制在非重叠的局部窗口同时还允许跨窗口连接来带来更高的效率。这种分层架构具有在各种尺度下建模的灵活性，并且具有相对于图像大小的线性计算复杂度。Swin Transformer的这些特性使其与广泛的视觉任务兼容，包括图像分类（ImageNet-1 K上的87.3 top-1精度）和密集预测任务，如对象检测（COCO testdev上的58.7 box AP和51.1 mask AP）和语义分割（ADE 20 K上的53.5 mIoU瓦尔）。它的性能超过了之前的最先进水平，在COCO上为+2.7框AP和+2.6掩模AP，在ADE 20 K上为+3.2 mIoU，证明了基于Transformer的模型作为视觉骨干的潜力。分层设计和移动窗口的方法也证明有利于全MLP架构。代码和模型在https://github.com/microsoft/Swin-Transformer上公开。</code></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Beginning with AlexNet [35] and its revolutionary performance on the ImageNet image classification challenge, CNN architectures have evolved to become increasingly powerful through greater scale [27, 69], more extensive connections [31], and more sophisticated forms of convolution [64, 17, 75]. With CNNs serving as backbone networks for a variety of vision tasks, these architectural advances have led to performance improvements that have broadly lifted the entire field.<br><code>计算机视觉建模长期以来一直由卷积神经网络（CNN）主导。从AlexNet [35]及其在ImageNet图像分类挑战中的革命性性能开始，CNN架构已经发展到通过更大的规模[27，69]，更广泛的连接[31]和更复杂的卷积形式[64，17，75]变得越来越强大。随着CNN作为各种视觉任务的骨干网络，这些架构上的进步导致了性能的提高，从而广泛地提升了整个领域。</code><br>On the other hand, the evolution of network architectures in natural language processing (NLP) has taken a different path, where the prevalent architecture today is instead the Transformer [58]. Designed for sequence modeling and transduction tasks, the Transformer is notable for its use of attention to model long-range dependencies in the data. Its tremendous success in the language domain has led researchers to investigate its adaptation to computer vision, where it has recently demonstrated promising results on certain tasks, specifically image classification [19] and joint vision-language modeling [43].<br><code>另一方面，自然语言处理（NLP）中的网络架构的发展已经走上了一条不同的道路，今天流行的架构是Transformer [58]。Transformer专为序列建模和转换任务而设计，它以关注数据中的长距离依赖性建模而闻名。它在语言领域的巨大成功促使研究人员研究它对计算机视觉的适应性，最近它在某些任务上表现出了有希望的结果，特别是图像分类[19]和联合视觉语言建模[43]。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/swin/1.png"></p><blockquote><p>Figure 1. (a) The proposed Swin Transformer builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. (b) In contrast, previous vision Transformers [19] produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of selfattention globally.</p></blockquote><p><code>图1.(a)所提出的Swin Transformer通过在更深层中合并图像块（以灰色示出）来构建分层特征映射，并且由于仅在每个局部窗口（以红色示出）内计算自我注意，因此具有与输入图像大小线性的计算复杂度。因此，它可以作为图像分类和密集识别任务的通用骨干。(b)相比之下，以前的视觉变换器[19]产生单个低分辨率的特征图，并且由于全局自注意力的计算而具有输入图像大小的二次计算复杂度。</code></p><p>In this paper, we seek to expand the applicability of Transformer such that it can serve as a general-purpose backbone for computer vision, as it does for NLP and as CNNs do in vision. We observe that significant challenges in transferring its high performance in the language domain to the visual domain can be explained by differences between the two modalities. One of these differences involves scale. Unlike the word tokens that serve as the basic elements of processing in language Transformers, visual elements can vary substantially in scale, a problem that receives attention in tasks such as object detection [38, 49, 50]. In existing Transformer-based models [58, 19], tokens are all of a fixed scale, a property unsuitable for these vision applications. Another difference is the much higher resolution of pixels in images compared to words in passages of text. There exist many vision tasks such as semantic segmentation that require dense prediction at the pixel level, and this would be intractable for Transformer on high-resolution images, as the computational complexity of its self-attention is quadratic to image size. To overcome these issues, we propose a generalpurpose Transformer backbone, called Swin Transformer, which constructs hierarchical feature maps and has linear computational complexity to image size. As illustrated in Figure 1(a), Swin Transformer constructs a hierarchical representation by starting from small-sized patches (outlined in gray) and gradually merging neighboring patches in deeper Transformer layers. With these hierarchical feature maps, the Swin Transformer model can conveniently leverage advanced techniques for dense prediction such as feature pyramid networks (FPN) [38] or U-Net [47]. The linear computational complexity is achieved by computing self-attention locally within non-overlapping windows that partition an image (outlined in red). The number of patches in each window is fixed, and thus the complexity becomes linear to image size. These merits make Swin Transformer suitable as a general-purpose backbone for various vision tasks, in contrast to previous Transformer based architectures [19] which produce feature maps of a single resolution and have quadratic complexity.<br><code>在本文中，我们试图扩展Transformer的适用性，使其可以作为计算机视觉的通用骨干，就像它在NLP和CNN在视觉中所做的那样。我们观察到，在将其在语言领域的高性能转移到视觉领域方面的重大挑战可以通过两种模式之间的差异来解释。这些差异之一涉及规模。与作为语言变形金刚中处理的基本元素的单词标记不同，视觉元素在尺度上可以有很大的变化，这是一个在物体检测等任务中受到关注的问题[38，49，50]。在现有的基于transformer的模型中[58，19]，令牌都是固定规模的，这是一个不适合这些视觉应用的属性。另一个区别是图像中像素的分辨率比文本段落中的单词高得多。存在许多视觉任务，如语义分割，需要在像素级进行密集预测，这对于Transformer在高分辨率图像上是难以处理的，因为其自我注意力的计算复杂度是图像大小的二次方。为了克服这些问题，我们提出了一个通用的Transformer骨干，称为Swin Transformer，它构造分层特征映射，并具有线性计算复杂度的图像大小。如图1（a）所示，Swin Transformer从小尺寸的补丁（以灰色轮廓显示）开始，逐渐合并更深Transformer层中的相邻补丁，从而构建了一个分层表示。通过这些分层特征图，Swin Transformer模型可以方便地利用高级技术进行密集预测，例如特征金字塔网络（FPN）[38]或U-Net [47]。线性计算复杂度是通过在划分图像的非重叠窗口（红色轮廓）内局部计算自注意力来实现的。每个窗口中的补丁的数量是固定的，因此复杂度与图像大小成线性关系。这些优点使Swin Transformer适合作为各种视觉任务的通用骨干，与之前基于Transformer的架构[19]形成对比，后者产生单一分辨率的特征图并具有二次复杂性。</code></p><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/swin/2.png"></p><blockquote><p>Figure 2. An illustration of the shifted window approach for computing self-attention in the proposed Swin Transformer architecture. In layer l (left), a regular window partitioning scheme is adopted, and self-attention is computed within each window. In the next layer l + 1 (right), the window partitioning is shifted, resulting in new windows. The self-attention computation in the new windows crosses the boundaries of the previous windows in layer l, providing connections among them.</p></blockquote><p><code>图2.在Swin Transformer架构中计算自我注意力的移位窗口方法的说明。在层l（左）中，采用常规窗口划分方案，并且在每个窗口内计算自注意。在下一层l + 1（右）中，窗口分区被移位，从而产生新的窗口。新窗口中的自注意力计算跨越层l中的先前窗口的边界，提供它们之间的连接。</code></p><p>A key design element of Swin Transformer is its shift of the window partition between consecutive self-attention layers, as illustrated in Figure 2. The shifted windows bridge the windows of the preceding layer, providing connections among them that significantly enhance modeling power (see Table 4). This strategy is also efficient in regards to real-world latency: all query patches within a window share the same key set, which facilitates memory access in hardware. In contrast, earlier sliding window based self-attention approaches [30, 46] suffer from low latency on general hardware due to different key sets for different query pixels. Our experiments show that the proposed shifted window approach has much lower latency than the sliding window method, yet is similar in modeling power (see Tables 5 and 6). The shifted window approach also proves beneficial for all-MLP architectures [56].<br><code>Swin Transformer的一个关键设计元素是在连续的自我关注层之间移动窗口分区，如图2所示。移动的窗口桥接了前一层的窗口，提供了它们之间的连接，从而显著增强了建模能力（参见表4）。这种策略在现实世界的延迟方面也很有效：窗口中的所有查询补丁共享相同的密钥集，这有助于硬件中的内存访问。相比之下，早期的基于滑动窗口的自注意方法[30，46]由于不同查询像素的不同密钥集而在一般硬件上具有低延迟。我们的实验表明，所提出的移位窗口方法具有比滑动窗口方法低得多的延迟，但在建模能力方面相似（参见表5和表6）。移位窗口方法也证明对全MLP架构有益[56]。</code><br>The proposed Swin Transformer achieves strong performance on the recognition tasks of image classification, object detection and semantic segmentation. It outperforms the ViT / DeiT [19, 57] and ResNe(X)t models [27, 64] significantly with similar latency on the three tasks. Its 58.7 box AP and 51.1 mask AP on the COCO test-dev set surpass the previous state-of-the-art results by +2.7 box AP (Copy-paste [23] without external data) and +2.6 mask AP (DetectoRS [42]). On ADE20K semantic segmentation, it obtains 53.5 mIoU on the val set, an improvement of +3.2 mIoU over the previous state-of-the-art (SETR [73]). It also achieves a top-1 accuracy of 87.3% on ImageNet-1K image classification.<br><code>所提出的Swin Transformer在图像分类、目标检测和语义分割等识别任务上取得了很好的性能。它在三个任务上的延迟相似，显著优于ViT / DeiT [19，57]和ResNe（X）t模型[27，64]。其在COCO测试开发集上的58.7框AP和51.1掩模AP超过了之前的最先进结果，分别为+2.7框AP（无外部数据的复制-粘贴[23]）和+2.6掩模AP（DetectoRS [42]）。在ADE 20 K语义分割上，它在瓦尔集上获得了53.5 mIoU，比之前的最新技术水平（SETR [73]）提高了+3.2 mIoU。它还在ImageNet-1 K图像分类中达到了87.3%的前1准确率。</code><br>It is our belief that a unified architecture across computer vision and natural language processing could benefit both fields, since it would facilitate joint modeling of visual and textual signals and the modeling knowledge from both domains can be more deeply shared. We hope that Swin Transformer’s strong performance on various vision problems can drive this belief deeper in the community and encourage unified modeling of vision and language signals.<br><code>我们相信，跨计算机视觉和自然语言处理的统一架构可以使这两个领域受益，因为它将促进视觉和文本信号的联合建模，并且可以更深入地共享这两个领域的建模知识。我们希望Swin Transformer在各种视觉问题上的出色表现能够在社区中加深这种信念，并鼓励视觉和语言信号的统一建模。</code></p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h1><p><em><strong>CNN and variants</strong></em><br><code>CNN及其变体</code><br>CNNs serve as the standard network model throughout computer vision. While the CNN has existed for several decades [36], it was not until the introduction of AlexNet [35] that the CNN took off and became mainstream. Since then, deeper and more effective convolutional neural architectures have been proposed to further propel the deep learning wave in computer vision, e.g., VGG [48], GoogleNet [53], ResNet [27], DenseNet [31], HRNet [59], and EfficientNet [54]. In addition to these architectural advances, there has also been much work on improving individual convolution layers, such as depth-wise convolution [64] and deformable convolution [17, 75]. While the CNN and its variants are still the primary backbone architectures for computer vision applications, we highlight the strong potential of Transformer-like architectures for unified modeling between vision and language. Our work achieves strong performance on several basic visual recognition tasks, and we hope it will contribute to a modeling shift.<br><code>CNN是整个计算机视觉的标准网络模型。虽然CNN已经存在了几十年[36]，但直到AlexNet的引入[35]，CNN才起飞并成为主流。从那时起，人们提出了更深入、更有效的卷积神经架构，以进一步推动计算机视觉中的深度学习浪潮，例如，[48]、VGG [48]、GoogleNet [53]、ResNet [27]、DenseNet [31]、HRNet [59]和EfficientNet [54]。除了这些架构上的进步，还有很多关于改进单个卷积层的工作，例如深度卷积[64]和可变形卷积[17，75]。虽然CNN及其变体仍然是计算机视觉应用的主要骨干架构，但我们强调了类transformer架构在视觉和语言之间统一建模的强大潜力。我们的工作在几个基本的视觉识别任务上实现了强大的性能，我们希望它将有助于建模的转变。</code><br><em><strong>Self-attention based backbone architectures</strong></em><br><code>基于自注意力的骨干网架构</code><br>Also inspired by the success of self-attention layers and Transformer architectures in the NLP field, some works employ self-attention layers to replace some or all of the spatial convolution layers in the popular ResNet [30, 46, 72]. In these works, the self-attention is computed within a local window of each pixel to expedite optimization [30], and they achieve slightly better accuracy/FLOPs trade-offs than the counterpart ResNet architecture. However, their costly memory access causes their actual latency to be significantly larger than that of the convolutional networks [30]. Instead of using sliding windows, we propose to shift windows between consecutive layers, which allows for a more efficient implementation in general hardware.<br><code>同样受到自注意层和Transformer架构在NLP领域的成功启发，一些作品采用自注意层来取代流行的ResNet中的部分或全部空间卷积层[30，46，72]。在这些工作中，自注意力在每个像素的局部窗口内计算以加速优化[30]，并且它们实现了比对应的ResNet架构稍好的精度/FLOPs权衡。然而，它们昂贵的内存访问导致它们的实际延迟明显大于卷积网络[30]。而不是使用滑动窗口，我们建议在连续层之间移动窗口，这允许在一般硬件中更有效地实现。</code><br><em><strong>Self-attention/Transformers to complement CNNs</strong></em><br><code>Self-attention/Transformers补充CNN</code><br>Another line of work is to augment a standard CNN architecture with self-attention layers or Transformers. The self-attention layers can complement backbones [61, 7, 3, 65, 21, 68, 51] or head networks [29, 24] by providing the capability to encode distant dependencies or heterogeneous interactions. More recently, the encoder-decoder design in Transformer has been applied for the object detection and instance segmentation tasks [8, 13, 76, 52]. Our work explores the adaptation of Transformers for basic visual feature extraction and is complementary to these works.<br><code>另一项工作是用自注意层或变形金刚来增强标准CNN架构。自注意层可以通过提供编码远程依赖或异构交互的能力来补充骨干[61，7，3，65，21，68，51]或头部网络[29，24]。最近，Transformer中的编码器-解码器设计已被应用于对象检测和实例分割任务[8，13，76，52]。我们的工作探讨了变形金刚的适应基本的视觉特征提取，是这些作品的补充。</code><br><em><strong>Transformer based vision backbones</strong></em><br>Most related to our work is the Vision Transformer (ViT) [19] and its follow-ups [57, 66, 15, 25, 60]. The pioneering work of ViT directly applies a Transformer architecture on non-overlapping medium-sized image patches for image classification. It achieves an impressive speed-accuracy trade-off on image classification compared to convolutional networks. While ViT requires large-scale training datasets (i.e., JFT-300M) to perform well, DeiT [57] introduces several training strategies that allow ViT to also be effective using the smaller ImageNet-1K dataset. The results of ViT on image classification are encouraging, but its architecture is unsuitable for use as a general-purpose backbone network on dense vision tasks or when the input image resolution is high, due to its low-resolution feature maps and the quadratic increase in complexity with image size. There are a few works applying ViT models to the dense vision tasks of object detection and semantic segmentation by direct upsampling or deconvolution but with relatively lower performance [2, 73]. Concurrent to our work are some that modify the ViT architecture [66, 15, 25] for better image classification. Empirically, we find our Swin Transformer architecture to achieve the best speed-accuracy trade-off among these methods on image classification, even though our work focuses on general-purpose performance rather than specifically on classification. Another concurrent work [60] explores a similar line of thinking to build multi-resolution feature maps on Transformers. Its complexity is still quadratic to image size, while ours is linear and also operates locally which has proven beneficial in modeling the high correlation in visual signals [32, 22, 37]. Our approach is both efficient and effective, achieving state-of-the-art accuracy on both COCO object detection and ADE20K semantic segmentation.<br><code>与我们的工作最相关的是Vision Transformer（ViT）[19]及其后续产品[57，66，15，25，60]。ViT的开创性工作直接将Transformer架构应用于非重叠的中等大小图像块，用于图像分类。与卷积网络相比，它在图像分类方面实现了令人印象深刻的速度-准确性权衡。虽然ViT需要大规模的训练数据集（即，JFT-300 M），DeiT [57]引入了几种训练策略，允许ViT使用较小的ImageNet-1 K数据集也是有效的。ViT在图像分类上的结果是令人鼓舞的，但其架构不适合用作密集视觉任务的通用骨干网络，或者当输入图像分辨率很高时，由于其低分辨率特征图和复杂性随图像大小的二次增加。有一些作品通过直接上采样或反卷积将ViT模型应用于对象检测和语义分割的密集视觉任务，但性能相对较低[2，73]。与我们的工作同时进行的是一些修改ViT架构[66，15，25]的工作，以实现更好的图像分类。从经验上讲，我们发现我们的Swin Transformer架构在图像分类的这些方法中实现了最佳的速度-精度权衡，即使我们的工作重点是通用性能，而不是专门的分类。另一个并行工作[60]探索了类似的思路，在变形金刚上构建多分辨率特征图。它的复杂性仍然是图像大小的二次方，而我们的是线性的，并且也在局部操作，这已经证明有利于模拟视觉信号中的高相关性[32，22，37]。我们的方法既高效又有效，在COCO对象检测和ADE 20 K语义分割方面都达到了最先进的准确性。</code></p><h1 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h1><h3 id="3-1-Overall-Architecture"><a href="#3-1-Overall-Architecture" class="headerlink" title="3.1. Overall Architecture"></a>3.1. Overall Architecture</h3><p><code>3.1.整体架构</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/swin/3.png"></p><blockquote><p>Figure 3. (a) The architecture of a Swin Transformer (Swin-T); (b) two successive Swin Transformer Blocks (notation presented with Eq. (3)). W-MSA and SW-MSA are multi-head self attention modules with regular and shifted windowing configurations, respectively.</p></blockquote><p><code>图3.(a)Swin Transformer（Swin-T）的架构;（B）两个连续的Swin Transformer块（用等式1表示的符号）;（三））。W-MSA和SW-MSA分别是具有规则和移位窗口配置的多头自注意模块。</code></p><p>An overview of the Swin Transformer architecture is presented in Figure 3, which illustrates the tiny version (SwinT). It first splits an input RGB image into non-overlapping patches by a patch splitting module, like ViT. Each patch is treated as a “token” and its feature is set as a concatenation of the raw pixel RGB values. In our implementation, we use a patch size of 4×4 and thus the feature dimension of each patch is 4 × 4 × 3 = 48. A linear embedding layer is applied on this raw-valued feature to project it to an arbitrary dimension (denoted as C).<br><code>Swin Transformer体系结构的概述如图3所示，其中说明了微型版本（SwinT）。它首先将输入的RGB图像分割成不重叠的补丁由补丁分裂模块，如ViT。每个补丁都被视为一个“token”，其特征被设置为原始像素RGB值的串联。在我们的实现中，我们使用4×4的补丁大小，因此每个补丁的特征维度为4 × 4 × 3 = 48。线性嵌入层被应用于这个原始值特征，以将其投影到任意维度（表示为C）。</code><br>Several Transformer blocks with modified self-attention computation (Swin Transformer blocks) are applied on these patch tokens. The Transformer blocks maintain the number of tokens (H/4 × W/4 ), and together with the linear embedding are referred to as “Stage 1”.<br><code>在这些补丁令牌上应用具有修改的自注意力计算的几个Transformer块（Swin Transformer块）。Transformer块保持令牌的数量（H/4 × W/4），并且与线性嵌入一起被称为“阶段1”。</code><br>To produce a hierarchical representation, the number of tokens is reduced by patch merging layers as the network gets deeper. The first patch merging layer concatenates the features of each group of 2 × 2 neighboring patches, and applies a linear layer on the 4C-dimensional concatenated features. This reduces the number of tokens by a multiple of 2×2 = 4 (2× downsampling of resolution), and the output dimension is set to 2C. Swin Transformer blocks are applied afterwards for feature transformation, with the resolution kept at H/8 × W/8 . This first block of patch merging and feature transformation is denoted as “Stage 2”. The procedure is repeated twice, as “Stage 3” and “Stage 4”, with output resolutions of H/16 × W/16 and H/32 × W/32 , respectively. These stages jointly produce a hierarchical representation, with the same feature map resolutions as those of typical convolutional networks, e.g., VGG [48] and ResNet [27]. As a result, the proposed architecture can conveniently replace the backbone networks in existing methods for various vision tasks.<br><code>为了产生分层表示，随着网络的深入，通过补丁合并层来减少令牌的数量。第一个面片合并层连接每组2 × 2相邻面片的特征，并在4C维连接特征上应用线性层。这将令牌的数量减少了2×2 = 4的倍数（分辨率的2倍下采样），并且输出维度设置为2C。之后应用Swin Transformer块进行特征转换，分辨率保持在H/8 × W/8。将片合并和特征变换的该第一块表示为“阶段2”。该过程重复两次，作为“阶段3”和“阶段4”，输出分辨率分别为H/16 × W/16和H/32 × W/32。这些阶段共同产生分层表示，具有与典型卷积网络相同的特征图分辨率，例如，[27]第48话，因此，所提出的架构可以方便地取代现有方法中的骨干网络，用于各种视觉任务。</code><br><em><strong>Swin Transformer block</strong></em><br><code>Swin Transformer block</code><br>Swin Transformer is built by replacing the standard multi-head self attention (MSA) module in a Transformer block by a module based on shifted windows (described in Section 3.2), with other layers kept the same. As illustrated in Figure 3(b), a Swin Transformer block consists of a shifted window based MSA module, followed by a 2-layer MLP with GELU nonlinearity in between. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module.<br><code>Swin Transformer is built by replacing the standard multi-head self attention (MSA) module in a Transformer block by a module based on shifted windows (described in Section 3.2), with other layers kept the same. As illustrated in Figure 3(b), a Swin Transformer block consists of a shifted window based MSA module, followed by a 2-layer MLP with GELU nonlinearity in between. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module.</code></p><h3 id="3-2-Shifted-Window-based-Self-Attention"><a href="#3-2-Shifted-Window-based-Self-Attention" class="headerlink" title="3.2. Shifted Window based Self-Attention"></a>3.2. Shifted Window based Self-Attention</h3><p><code>3.2.基于移位窗口的自注意</code><br>The standard Transformer architecture [58] and its adaptation for image classification [19] both conduct global self-attention, where the relationships between a token and all other tokens are computed. The global computation leads to quadratic complexity with respect to the number of tokens, making it unsuitable for many vision problems requiring an immense set of tokens for dense prediction or to represent a high-resolution image.<br><code>标准Transformer架构[58]及其对图像分类的适应[19]都进行全局自注意，其中计算令牌和所有其他令牌之间的关系。全局计算导致二次复杂度相对于令牌的数量，使其不适合许多视觉问题，需要一个巨大的令牌集的密集预测或代表一个高分辨率的图像。</code><br><em><strong>Self-attention in non-overlapped windows</strong></em><br><code>非重叠窗口中的自我注意</code><br>For efficient modeling, we propose to compute self-attention within local windows. The windows are arranged to evenly partition the image in a non-overlapping manner. Supposing each window contains M×M patches, the computational complexity of a global MSA module and a window based one on an image of h ×w patches are:<br><code>为了有效建模，我们建议在局部窗口内计算自我注意力。窗口被布置为以非重叠的方式均匀地划分图像。假设每个窗口包含M×M块，全局MSA模块和基于h ×w块图像的窗口的计算复杂度为：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/swin/4.png"><br>where the former is quadratic to patch number hw, and the latter is linear when M is fixed (set to 7 by default). Global self-attention computation is generally unaffordable for a large hw, while the window based self-attention is scalable.<br><code>其中，前者是补丁数hw的二次函数，而后者在M固定（默认设置为7）时是线性的。全局自注意计算通常对于大硬件是负担不起的，而基于窗口的自注意是可扩展的。</code><br>Shifted window partitioning in successive blocks<br><code>连续块中的移位窗口划分</code><br>The window-based self-attention module lacks connections across windows, which limits its modeling power. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks.<br><code>基于窗口的自我注意模块缺乏跨窗口的连接，这限制了其建模能力。为了在保持非重叠窗口的有效计算的同时引入跨窗口连接，我们提出了一种移位窗口分区方法，该方法在连续的Swin Transformer块中的两个分区配置之间交替。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/swin/2.png"></p><blockquote><p>Figure 2. An illustration of the shifted window approach for computing self-attention in the proposed Swin Transformer architecture. In layer l (left), a regular window partitioning scheme is adopted, and self-attention is computed within each window. In the next layer l + 1 (right), the window partitioning is shifted, resulting in new windows. The self-attention computation in the new windows crosses the boundaries of the previous windows in layer l, providing connections among them.</p></blockquote><p><code>图2.在Swin Transformer架构中计算自我注意力的移位窗口方法的说明。在层l（左）中，采用常规窗口划分方案，并且在每个窗口内计算自注意。在下一层l + 1（右）中，窗口分区被移位，从而产生新的窗口。新窗口中的自注意力计算跨越层l中的先前窗口的边界，提供它们之间的连接。</code><br>As illustrated in Figure 2, the first module uses a regular window partitioning strategy which starts from the top-left pixel, and the 8 × 8 feature map is evenly partitioned into 2 × 2 windows of size 4 × 4 (M = 4). Then, the next module adopts a windowing configuration that is shifted from that of the preceding layer, by displacing the windows by (|M/2| , |M/2|) pixels from the regularly partitioned windows.<br><code>如图2所示，第一个模块使用从左上角像素开始的常规窗口划分策略，将8 × 8特征图均匀划分为大小为4 × 4（M = 4）的2 × 2窗口。然后，下一个模块采用从前一层移位的窗口配置，通过将窗口移位（|M/2|、|M/2|）来自规则划分的窗口的像素。</code><br>With the shifted window partitioning approach, consecutive Swin Transformer blocks are computed as<br><code>使用移位窗口分区方法，连续Swin Transformer块计算为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/swin/5.png"><br>where $\hat{z}^l$ and $z^l$ denote the output features of the (S)WMSA module and the MLP module for block l, respectively; W-MSA and SW-MSA denote window based multi-head self-attention using regular and shifted window partitioning configurations, respectively.<br><code>其中，</code>$\hat{z}^l$ <code>和</code> $z^l$ <code>分别表示块1的（S）WMSA模块和MLP模块的输出特征; W-MSA和SW-MSA分别表示使用常规和移位窗口划分配置的基于窗口的多头自注意。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/swin/6.png"><br>The shifted window partitioning approach introduces connections between neighboring non-overlapping windows in the previous layer and is found to be effective in image classification, object detection, and semantic segmentation, as shown in Table 4.<br><code>移位窗口划分方法引入了前一层中相邻非重叠窗口之间的连接，并且被发现在图像分类、对象检测和语义分割中是有效的，如表4所示。</code><br><em><strong>Efficient batch computation for shifted configuration</strong></em><br><code>移位配置的高效批处理计算</code><br>An issue with shifted window partitioning is that it will result in more windows, from $\lceil\frac {h} {M} \rceil$ × $\lceil\frac {w} {M} \rceil$ to ($\lceil\frac {h} {M} \rceil$+ 1) × ($\lceil\frac {w} {M} \rceil$+1) in the shifted configuration, and some of the windows will be smaller than M ×M. A naive solution is to pad the smaller windows to a size of M × M and mask out the padded values when computing attention. When the number of windows in regular partitioning is small, e.g. 2 × 2, the increased computation with this naive solution is considerable (2 × 2 → 3 × 3, which is 2.25 times greater). Here, we propose a more efficient batch computation approach by cyclic-shifting toward the top-left direction, as illustrated in Figure 4. After this shift, a batched window may be composed of several sub-windows that are not adjacent in the feature map, so a masking mechanism is employed to limit self-attention computation to within each sub-window. With the cyclic-shift, the number of batched windows remains the same as that of regular window partitioning, and thus is also efficient. The low latency of this approach is shown in Table 5.<br><code>移位窗口划分的一个问题是，它将导致更多的窗口，从移位配置中的</code>$\lceil\frac {h} {M} \rceil$ <code>× </code>$\lceil\frac {w} {M} \rceil$<code>到（</code>$\lceil\frac {w} {M} \rceil$<code> + 1）×（</code>$\lceil\frac {h} {M} \rceil$ <code>+1），并且一些窗口将小于M ×M。一个简单的解决方案是将较小的窗口填充到M × M的大小，并在计算注意力时屏蔽填充值。当常规分区中的窗口数量很小时，例如2 × 2，使用这种简单的解决方案增加的计算量是相当大的（2 × 2 → 3 × 3，这是2.25倍)。在这里，我们提出了一种更有效的批处理计算方法，即向左上方循环移位，如图4所示。在该移位之后，批处理窗口可以由在特征图中不相邻的若干子窗口组成，因此采用掩蔽机制来将自注意力计算限制在每个子窗口内。通过循环移位，批量窗口的数量与常规窗口划分的数量保持相同，因此也是有效的。这种方法的低延迟如表5所示。</code><br><em><strong>Relative position bias</strong></em><br><code>相对位置偏差</code><br>In computing self-attention, we follow [45, 1, 29, 30] by including a relative position bias B ∈ $R^(M^2<em>M^2)]$ to each head in computing similarity:<br><code>在计算自我注意力时，我们遵循[45，1，29，30]，在计算相似度时包括每个头部的相对位置偏差B ∈ R^(M^2×M^2)：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/swin/7.png"><br>where Q,K, V ∈ $R^(M^2</em>d)$ are the query, key and value matrices; d is the query/key dimension, and M2 is the number of patches in a window. Since the relative position along each axis lies in the range [−M+1,M−1], we parameterize a smaller-sized bias matrix ˆB ∈ $R^(2M−1)*(2M−1)$, and values in B are taken from ˆB.<br><code>其中Q、K、V ∈ RM 2 ×d是查询、键和值矩阵; d是查询/键维度，M2是窗口中的补丁数量。由于沿沿着每个轴的相对位置位于范围[−M+1，M−1]内，我们参数化一个较小的偏置矩阵B ∈ R^[(2M−1)*(2M−1)]，B中的值取自ˆB。</code></p><p>We observe significant improvements over counterparts without this bias term or that use absolute position embedding, as shown in Table 4. Further adding absolute position embedding to the input as in [19] drops performance slightly, thus it is not adopted in our implementation.<br><code>如表4所示，我们观察到与没有此偏置项或使用绝对位置嵌入的同行相比的显着改进。如[19]中进一步向输入添加绝对位置嵌入会略微降低性能，因此在我们的实现中不采用它。</code><br>The learnt relative position bias in pre-training can be also used to initialize a model for fine-tuning with a different window size through bi-cubic interpolation [19, 57].<br><code>在预训练中学习的相对位置偏差也可以用于初始化模型，以通过双三次插值[19，57]使用不同的窗口大小进行微调。</code></p><h3 id="3-3-Architecture-Variants"><a href="#3-3-Architecture-Variants" class="headerlink" title="3.3. Architecture Variants"></a>3.3. Architecture Variants</h3><p><code>3.3.架构变体</code><br>We build our base model, called Swin-B, to have of model size and computation complexity similar to ViTB/DeiT-B. We also introduce Swin-T, Swin-S and Swin-L, which are versions of about 0.25×, 0.5× and 2× the model size and computational complexity, respectively. Note that the complexity of Swin-T and Swin-S are similar to those of ResNet-50 (DeiT-S) and ResNet-101, respectively. The window size is set to M = 7 by default. The query dimension of each head is d = 32, and the expansion layer of each MLP is α = 4, for all experiments. The architecture hyper-parameters of these model variants are:<br><code>我们构建了我们的基础模型，称为Swin-B，具有类似于ViTB/DeiT-B的模型大小和计算复杂性。我们还介绍了Swin-T，Swin-S和Swin-L，它们分别是模型大小和计算复杂度的0.25倍，0.5倍和2倍。请注意，Swin-T和Swin-S的复杂度分别与ResNet-50（DeiT-S）和ResNet-101相似。窗口大小默认设置为M = 7。对于所有实验，每个头的查询维度为d = 32，并且每个MLP的扩展层为α = 4。这些模型变体的架构超参数是：</code><br>Swin-T: C = 96, layer numbers = {2, 2, 6, 2}<br>Swin-S: C = 96, layer numbers ={2, 2, 18, 2}<br>Swin-B: C = 128, layer numbers ={2, 2, 18, 2}<br>Swin-L: C = 192, layer numbers ={2, 2, 18, 2}<br>where C is the channel number of the hidden layers in the first stage. The model size, theoretical computational complexity (FLOPs), and throughput of the model variants for ImageNet image classification are listed in Table 1.<br><code>其中C是第一阶段中隐藏层的通道编号。ImageNet图像分类的模型大小、理论计算复杂度（FLOPs）和模型变量的吞吐量列于表1中。</code></p><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><p><code>4.实验</code><br>We conduct experiments on ImageNet-1K image classification [18], COCO object detection [39], and ADE20K semantic segmentation [74]. In the following, we first compare the proposed Swin Transformer architecture with the previous state-of-the-arts on the three tasks. Then, we ablate the important design elements of Swin Transformer.<br><code>我们对ImageNet-1 K图像分类[18]，COCO对象检测[39]和ADE 20 K语义分割[74]进行了实验。在下文中，我们首先将所提出的Swin Transformer架构与之前的三个任务的最新技术进行比较。然后，详细介绍了Swin Transformer的设计要点。</code></p><h3 id="4-1-Image-Classification-on-ImageNet-1K"><a href="#4-1-Image-Classification-on-ImageNet-1K" class="headerlink" title="4.1. Image Classification on ImageNet-1K"></a>4.1. Image Classification on ImageNet-1K</h3><p><code>4.1.基于ImageNet-1 K的图像分类</code><br><em><strong>Settings</strong></em><br>For image classification, we benchmark the proposed Swin Transformer on ImageNet-1K [18], which contains 1.28M training images and 50K validation images from 1,000 classes. The top-1 accuracy on a single crop is reported. We consider two training settings:<br><code>对于图像分类，我们在ImageNet-1 K [18]上对所提出的Swin Transformer进行了基准测试，ImageNet-1 K包含来自1，000个类的1.28 M训练图像和50 K验证图像。报告单次裁剪的前1精度。我们考虑两种训练设置：</code></p><ul><li>Regular ImageNet-1K training. This setting mostly follows [57]. We employ an AdamW [33] optimizer for 300 epochs using a cosine decay learning rate scheduler and 20 epochs of linear warm-up. A batch size of 1024, an initial learning rate of 0.001, and a weight decay of 0.05 are used. We include most of the augmentation and regularization strategies of [57] in training, except for repeated augmentation [28] and EMA [41], which do not enhance performance. Note that this is contrary to [57] where repeated augmentation is crucial to stabilize the training of ViT.<br><code>ImageNet-1 K训练这一点，大多数人都有[57]。我们采用AdamW [33]优化器，使用余弦衰减学习率调度器和20个线性预热时期进行300个时期。批量大小为1024，初始学习率为0.001，权重衰减为0.05。我们在训练中包括了[57]中的大多数增强和正则化策略，除了重复增强[28]和EMA [41]，它们不会提高性能。请注意，这与[57]相反，其中重复增强对于稳定ViT的训练至关重要。</code></li><li>Pre-training on ImageNet-22K and fine-tuning on ImageNet-1K. We also pre-train on the ImageNet-22K dataset, which contains 14.2 million images and 22K classes. We employ an AdamW optimizer for 90 epochs using a cosine learning rate scheduler with a 5-epoch linear warm-up. A batch size of 4096, an initial learning rate of 0.001, and a weight decay of 0.01 are used. In ImageNet-1K fine-tuning, we train for 30 epochs with a batch size of 1024, a constant learning rate of 10^−5, and a weight decay of 10^−8.<br><code>在ImageNet-22 K上进行预训练，并在ImageNet-1 K上进行微调。我们还在ImageNet-22 K数据集上进行了预训练，该数据集包含1420万张图像和22 K个类。我们采用AdamW优化器为90个历元使用余弦学习率调度与5历元线性热身。批量大小为4096，初始学习率为0.001，权重衰减为0.01。在ImageNet-1 K微调中，我们训练了30个epoch，批量大小为1024，恒定学习率为10^−5，权重衰减为10^−8。</code></li></ul><p><em><strong>Results with regular ImageNet-1K training</strong></em><br><code>常规ImageNet-1 K训练的结果</code><br>Table 1(a) presents comparisons to other backbones, including both Transformer-based and ConvNet-based, using regular ImageNet-1K training.<br><code>Table 1(a) presents comparisons to other backbones, including both Transformer-based and ConvNet-based, using regular ImageNet-1K training.</code><br>Compared to the previous state-of-the-art Transformerbased architecture, i.e. DeiT [57], Swin Transformers noticeably surpass the counterpart DeiT architectures with similar complexities: +1.5% for Swin-T (81.3%) over DeiT-S (79.8%) using 224^2 input, and +1.5%/1.4% for Swin-B (83.3%/84.5%) over DeiT-B (81.8%/83.1%) using 224^2/384^2 input, respectively.<br><code>与之前最先进的基于变压器的架构（即DeiT [57]）相比，Swin变压器明显优于具有类似复杂性的DeiT架构：使用224^2输入，Swin-T（81.3%）比DeiT-S（79.8%）分别增加1.5%，使用224^2/384^2输入，Swin-B（83.3%/84.5%）比DeiT-B（81.8%/83.1%）分别增加1.5%/1.4%。</code><br>Compared with the state-of-the-art ConvNets, i.e. RegNet [44], the Swin Transformer achieves a slightly better speed-accuracy trade-off. Noting that while RegNet [44] are obtained via a thorough architecture search, the Swin Transformer is manually adapted from a standard Transformer and has potential for further improvement.<br><code>与最先进的ConvNets（即RegNet [44]）相比，Swin Transformer实现了略好的速度-精度权衡。注意，虽然RegNet [44]是通过彻底的架构搜索获得的，但Swin Transformer是从标准Transformer手动改编的，并且具有进一步改进的潜力。</code></p><p><em><strong>Results with ImageNet-22K pre-training</strong></em><br><code>ImageNet-22 K预训练结果</code><br>We also pretrain the larger-capacity Swin-B and Swin-L on ImageNet22K. Results fine-tuned on ImageNet-1K image classification are shown in Table 1(b). For Swin-B, the ImageNet22K pre-training brings 1.8%∼1.9% gains over training on ImageNet-1K from scratch. Compared with the previous best results for ImageNet-22K pre-training, our models achieve significantly better speed-accuracy trade-offs: Swin-B obtains 86.4% top-1 accuracy, which is 2.4% higher than that of ViT with similar inference throughput (84.7 vs. 85.9 images/sec) and slightly lower FLOPs (47.0G vs. 55.4G). The larger Swin-L model achieves 87.3% top-1 accuracy, +0.9% better than that of the Swin-B model.<br><code>我们还在ImageNet 22 K上预训练了更大容量的Swin-B和Swin-L。ImageNet-1 K图像分类的微调结果如表1（b）所示。对于Swin-B，ImageNet 22 K预训练比ImageNet-1 K从头开始的训练带来了1.8%到1.9%的增益。与之前ImageNet-22 K预训练的最佳结果相比，我们的模型实现了更好的速度-准确性权衡：Swin-B获得了86.4%的top-1准确率，比ViT高2.4%，具有相似的推理吞吐量（84.7 vs. 85.9图像/秒）和略低的FLOP（47.0G vs. 55.4G）。较大的Swin-L模型达到了87.3%的top-1准确率，比Swin-B模型高出+0.9%。</code></p><h3 id="4-2-Object-Detection-on-COCO"><a href="#4-2-Object-Detection-on-COCO" class="headerlink" title="4.2. Object Detection on COCO"></a>4.2. Object Detection on COCO</h3><p><code>4.2.COCO上的目标检测</code><br><em><strong>Settings</strong></em><br>Object detection and instance segmentation experiments are conducted on COCO 2017, which contains 118K training, 5K validation and 20K test-dev images. An ablation study is performed using the validation set, and a system-level comparison is reported on test-dev. For the ablation study, we consider four typical object detection frameworks: Cascade Mask R-CNN [26, 6], ATSS [71], RepPoints v2 [12], and Sparse RCNN [52] in mmdetection [10]. For these four frameworks, we utilize the same settings: multi-scale training [8, 52] (resizing the input such that the shorter side is between 480 and 800 while the longer side is at most 1333), AdamW [40] optimizer (initial learning rate of 0.0001, weight decay of 0.05, and batch size of 16), and 3x schedule (36 epochs). For system-level comparison, we adopt an improved HTC [9] (denoted as HTC++) with instaboost [20], stronger multi-scale training [7], 6x schedule (72 epochs), soft-NMS [5], and ImageNet-22K pre-trained model as initialization.<br><code>在COCO 2017上进行了目标检测和实例分割实验，其中包含118 K训练，5 K验证和20 K测试开发图像。使用确认集进行消融研究，并在测试开发中报告系统级比较。对于消融研究，我们考虑了四个典型的对象检测框架：级联掩码R-CNN [26，6]，ATSS [71]，RepPoints v2 [12]和稀疏RCNN [52]在mm检测[10]中。对于这四个框架，我们使用相同的设置：多尺度训练[8，52]（对输入进行优化，使短边在480到800之间，而长边最多为1333），AdamW [40]优化器（初始学习率为0.0001，权重衰减为0.05，批量大小为16）和3x调度（36个epoch）。为了进行系统级比较，我们采用了改进的HTC [9]（表示为HTC++），其中包括instaboost [20]，更强的多尺度训练[7]，6x时间表（72 epochs），soft-NMS [5]和ImageNet-22 K预训练模型作为初始化。</code><br>We compare our Swin Transformer to standard ConvNets, i.e. ResNe(X)t, and previous Transformer networks, e.g. DeiT. The comparisons are conducted by changing only the backbones with other settings unchanged. Note that while Swin Transformer and ResNe(X)t are directly applicable to all the above frameworks because of their hierarchical feature maps, DeiT only produces a single resolution of feature maps and cannot be directly applied. For fair comparison, we follow [73] to construct hierarchical feature maps for DeiT using deconvolution layers.<br><code>我们将Swin Transformer与标准ConvNets（即ResNe（X）t）和以前的Transformer网络（例如DeiT）进行比较。通过仅改变主干而其他设置不变来进行比较。请注意，虽然Swin Transformer和ResNe（X）t由于其分层特征映射而直接适用于所有上述框架，但DeiT仅产生单一分辨率的特征映射，不能直接应用。为了公平比较，我们遵循[73]使用反卷积层构建DeiT的分层特征图。</code></p><p><em><strong>Comparison to ResNe(X)t</strong></em><br><code>与ResNe（X）t的比较</code><br>Table 2(a) lists the results of Swin-T and ResNet-50 on the four object detection frameworks. Our Swin-T architecture brings consistent +3.4∼4.2 box AP gains over ResNet-50, with slightly larger model size, FLOPs and latency.<br><code>表2（a）列出了Swin-T和ResNet-50在四个对象检测框架上的结果。我们的Swin-T架构带来了比ResNet-50一致的+3.4 4. 2盒AP增益，模型大小、FLOP和延迟略大。</code><br>Table 2(b) compares Swin Transformer and ResNe(X)t under different model capacity using Cascade Mask RCNN. Swin Transformer achieves a high detection accuracy of 51.9 box AP and 45.0 mask AP, which are significant gains of +3.6 box AP and +3.3 mask AP over ResNeXt10164x4d, which has similar model size, FLOPs and latency. On a higher baseline of 52.3 box AP and 46.0 mask AP using an improved HTC framework, the gains by Swin Transformer are also high, at +4.1 box AP and +3.1 mask AP (see Table 2(c)). Regarding inference speed, while ResNe(X)t is built by highly optimized Cudnn functions, our architecture is implemented with built-in PyTorch functions that are not all well-optimized. A thorough kernel optimization is beyond the scope of this paper.<br><code>表2（b）使用级联掩码RCNN比较了不同模型容量下的Swin Transformer和ResNe（X）t。Swin Transformer实现了51.9 box AP和45.0 mask AP的高检测准确度，这比ResNeXt10164x4d显著提高了+3.6 box AP和+3.3 mask AP，ResNeXt10164x4d具有相似的模型大小，FLOPs和延迟。在使用改进的HTC框架的52.3盒AP和46.0掩模AP的较高基线上，Swin Transformer的增益也很高，为+4.1盒AP和+3.1掩模AP（参见表2（c））。关于推理速度，虽然ResNe（X）t是由高度优化的Cudnn函数构建的，但我们的架构是用内置的PyTorch函数实现的，这些函数并没有得到很好的优化。彻底的内核优化超出了本文的范围。</code></p><p><em><strong>Comparison to DeiT</strong></em><br><code>与DeiT比较</code><br>The performance of DeiT-S using the Cascade Mask R-CNN framework is shown in Table 2(b). The results of Swin-T are +2.5 box AP and +2.3 mask AP higher than DeiT-S with similar model size (86M vs. 80M) and significantly higher inference speed (15.3 FPS vs. 10.4 FPS). The lower inference speed of DeiT is mainly due to its quadratic complexity to input image size.<br><code>使用级联掩码R-CNN框架的DeiT-S的性能如表2（b）所示。Swin-T的结果比DeiT-S高+2.5框AP和+2.3掩模AP，具有相似的模型大小（86 M与80 M）和显著更高的推理速度（15.3 FPS与10.4 FPS）。DeiT的推理速度较低，主要是由于其复杂度是输入图像大小的二次方。</code></p><p><em><strong>Comparison to previous state-of-the-art</strong></em><br><code>与先前最先进技术的比较</code><br>Table 2(c) compares our best results with those of previous state-ofthe-art models. Our best model achieves 58.7 box AP and 51.1 mask AP on COCO test-dev, surpassing the previous best results by +2.7 box AP (Copy-paste [23] without external data) and +2.6 mask AP (DetectoRS [42]).<br><code>表2（c）将我们的最佳结果与以前最先进的模型进行了比较。我们的最佳模型在COCO测试开发中实现了58.7框AP和51.1掩模AP，超过了之前的最佳结果+2.7框AP（没有外部数据的复制粘贴[23]）和+2.6掩模AP（DetectoRS [42]）。</code></p><h3 id="4-3-Semantic-Segmentation-on-ADE20K"><a href="#4-3-Semantic-Segmentation-on-ADE20K" class="headerlink" title="4.3. Semantic Segmentation on ADE20K"></a>4.3. Semantic Segmentation on ADE20K</h3><p><code>4.3.基于ADE20K的语义分割</code><br><em><strong>Settings</strong></em><br>ADE20K [74] is a widely-used semantic segmentation dataset, covering a broad range of 150 semantic categories. It has 25K images in total, with 20K for training, 2K for validation, and another 3K for testing. We utilize UperNet [63] in mmseg [16] as our base framework for its high efficiency. More details are presented in the Appendix.<br><code>ADE20K [74]是一个广泛使用的语义分割数据集，涵盖了150个语义类别。它总共有25K张图像，其中20K用于训练，2K用于验证，另外3K用于测试。我们利用mmseg [16]中的UperNet [63]作为我们的基础框架，以提高其效率。更多详情见附录。</code><br><em><strong>Results</strong></em><br>Table 3 lists the mIoU, model size (#param), FLOPs and FPS for different method/backbone pairs. From these results, it can be seen that Swin-S is +5.3 mIoU higher (49.3 vs. 44.0) than DeiT-S with similar computation cost. It is also +4.4 mIoU higher than ResNet-101, and +2.4 mIoU higher than ResNeSt-101 [70]. Our Swin-L model with ImageNet-22K pre-training achieves 53.5 mIoU on the val set, surpassing the previous best model by +3.2 mIoU (50.3 mIoU by SETR [73] which has a larger model size).<br><code>表3列出了不同方法/主干对的mIoU、模型大小（#param）、FLOP和FPS。从这些结果可以看出，Swin-S比DeiT-S高+5.3 mIoU（49.3 vs. 44.0），具有相似的计算成本。它也比ResNet-101高+4.4 mIoU，比ResNeSt-101高+2.4 mIoU [70]。我们使用ImageNet-22 K预训练的Swin-L模型在瓦尔集上实现了53.5 mIoU，超过了之前的最佳模型+3.2 mIoU（SETR [73]的50.3 mIoU，模型大小更大）。</code></p><h3 id="4-4-Ablation-Study"><a href="#4-4-Ablation-Study" class="headerlink" title="4.4. Ablation Study"></a>4.4. Ablation Study</h3><p><code>4.4.消融研究</code><br>In this section, we ablate important design elements in the proposed Swin Transformer, using ImageNet-1K image classification, Cascade Mask R-CNN on COCO object detection, and UperNet on ADE20K semantic segmentation.<br><code>在本节中，我们使用ImageNet-1 K图像分类、Cascade Mask R-CNN进行COCO对象检测，以及UperNet进行ADE 20 K语义分割，消除了Swin Transformer中的重要设计元素。</code></p><p><em><strong>Shifted windows</strong></em><br>Ablations of the shifted window approach on the three tasks are reported in Table 4. Swin-T with the shifted window partitioning outperforms the counterpart built on a single window partitioning at each stage by +1.1% top-1 accuracy on ImageNet-1K, +2.8 box AP/+2.2 mask AP on COCO, and +2.8 mIoU on ADE20K. The results indicate the effectiveness of using shifted windows to build connections among windows in the preceding layers. The latency overhead by shifted window is also small, as shown in Table 5.<br><code>表4报告了移位窗口法在三项任务上的消融。在ImageNet-1 K上，具有移位窗口分区的Swin-T在每个阶段的性能都优于基于单窗口分区的Swin-T，其top-1准确率为+1.1%，在COCO上为+2.8 box AP/+2.2 mask AP，在ADE 20 K上为+2.8 mIoU。结果表明，使用移动窗口在前几层的窗口之间建立连接的有效性。移位窗口的延迟开销也很小，如表5所示。</code></p><p><em><strong>Relative position bias</strong></em><br>Table 4 shows comparisons of different position embedding approaches. Swin-T with relative position bias yields +1.2%/+0.8% top-1 accuracy on ImageNet-1K, +1.3/+1.5 box AP and +1.1/+1.3 mask AP on COCO, and +2.3/+2.9 mIoU on ADE20K in relation to those without position encoding and with absolute position embedding, respectively, indicating the effectiveness of the relative position bias. Also note that while the inclusion of absolute position embedding improves image classification accuracy (+0.4%), it harms object detection and semantic segmentation (-0.2 box/mask AP on COCO and -0.6 mIoU on ADE20K).<br><code>表4显示了不同位置嵌入方法的比较。相对位置偏差的Swin-T在ImageNet-1 K上产生+1.2%/+0.8%的top-1准确度，在COCO上产生+1.3/+1.5框AP和+1.1/+1.3掩模AP，在ADE 20 K上产生+2.3/+2.9 mIoU，分别与没有位置编码和绝对位置嵌入的情况相比，表明相对位置偏差的有效性。还请注意，虽然包含绝对位置嵌入提高了图像分类准确性（+0.4%），但它损害了对象检测和语义分割（COCO上为-0.2框/掩码AP，ADE 20 K上为-0.6 mIoU）。</code></p><p><em><strong>Different self-attention methods</strong></em><br><code>不同的自我注意方法</code><br>The real speed of different self-attention computation methods and implementations are compared in Table 5. Our cyclic implementation is more hardware efficient than naive padding, particularly for deeper stages. Overall, it brings a 13%, 18% and 18% speed-up on Swin-T, Swin-S and Swin-B, respectively.<br><code>表5比较了不同自注意力计算方法和实现的真实的速度。我们的循环实现比朴素填充更有硬件效率，特别是对于更深的阶段。总体而言，它分别为Swin-T，Swin-S和Swin-B带来了13%，18%和18%的速度提升。</code><br>The self-attention modules built on the proposed shifted window approach are 40.8×/2.5×, 20.2×/2.5×, 9.3×/2.1×, and 7.6×/1.8× more efficient than those of sliding windows in naive/kernel implementations on four network stages, respectively. Overall, the Swin Transformer architectures built on shifted windows are 4.1/1.5, 4.0/1.5, 3.6/1.5 times faster than variants built on sliding windows for Swin-T, Swin-S, and Swin-B, respectively. Table 6 compares their accuracy on the three tasks, showing that they are similarly accurate in visual modeling.<br><code>在四个网络阶段上，基于所提出的移动窗口方法构建的自注意模块的效率分别比朴素/内核实现中的滑动窗口高40.8×/2.5 ×、20.2×/2.5×、9.3×/2.1×和7.6×/1.8×。总体而言，构建在移位窗口上的Swin Transformer架构分别比构建在Swin-T、Swin-S和Swin-B滑动窗口上的变体快4.1/1.5、4.0/1.5、3.6/1.5倍。表6比较了它们在三个任务上的准确性，表明它们在视觉建模中的准确性相似。</code><br>Compared to Performer [14], which is one of the fastest Transformer architectures (see [55]), the proposed shifted window based self-attention computation and the overall Swin Transformer architectures are slightly faster (see Table 5), while achieving +2.3% top-1 accuracy compared to Performer on ImageNet-1K using Swin-T (see Table 6).<br><code>与Performer [14]相比，Performer是最快的Transformer架构之一（见[55]），提出的基于移位窗口的自注意力计算和整体Swin Transformer架构略快（见表5），同时与使用Swin-T的ImageNet-1 K上的Performer相比，实现了+2.3%的top-1准确度（见表6）。</code></p><h1 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h1><p>This paper presents Swin Transformer, a new vision Transformer which produces a hierarchical feature representation and has linear computational complexity with respect to input image size. Swin Transformer achieves the state-of-the-art performance on COCO object detection and ADE20K semantic segmentation, significantly surpassing previous best methods. We hope that Swin Transformer’s strong performance on various vision problems will encourage unified modeling of vision and language signals.<br><code>本文介绍了Swin Transformer，一个新的视觉Transformer，它产生一个层次化的特征表示，并具有线性计算复杂度相对于输入图像的大小。Swin Transformer在COCO对象检测和ADE 20K语义分割方面实现了最先进的性能，大大超过了以前的最佳方法。我们希望Swin Transformer在各种视觉问题上的强大性能将鼓励视觉和语言信号的统一建模。</code></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文翻译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文翻译】Spectrum Sensing for Underwater Cognitive Radio With Limited Sensing Time</title>
      <link href="/2023/11/29/lun-wen-fan-yi-spectrum-sensing-for-underwater-cognitive-radio-with-limited-sensing-time/"/>
      <url>/2023/11/29/lun-wen-fan-yi-spectrum-sensing-for-underwater-cognitive-radio-with-limited-sensing-time/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><code>有限感知时间的水下认知无线电频谱感知研究</code></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Underwater acoustic (UWA) communications suffer from the following factors: many multi-paths, slow propagation speed, rapid time-varying channels, and various noise such as the sound of marine animals and artificial acoustic systems. Moreover, since there are no strict standards or specifications for UWA communications, UWA communications generally employ cognitive radio (CR)-based ad-hoc networks, and recently, orthogonal frequency division multiple access (OFDMA) has been adopted to improve CR-based communication performance by maximizing multiplexing gain. However, due to the CR protocol, the performance of the UWA communication is significantly affected by sensing techniques. Therefore, this letter proposes a deep-learning-based spectrum sensing scheme in an OFDMAbased UWA-CR network. Compared to the existing schemes, the proposed scheme has a limited sensing time even shorter than one symbol duration, which is effective in a UWA environment where a long symbol duration is essential. In addition, by learning animal noise and interference caused by the broken orthogonality of OFDMA, the proposed scheme increases the detection accuracy of idle channels and recognizes animal sounds to prevent damage to animal. The simulation results confirm the superiority of the proposed scheme.<br><code>水声通信（UWA）面临着多径、传播速度慢、信道时变快以及海洋动物和人工声系统等各种噪声的影响。此外，由于对于UWA通信没有严格的标准或规范，所以UWA通信通常采用基于认知无线电（CR）的自组织网络，并且最近，已经采用正交频分多址（OFDMA）来通过最大化复用增益来改善基于CR的通信性能。然而，由于CR协议，UWA通信的性能受到传感技术的显著影响。因此，这篇文章提出了一种基于OFDMA的UWA-CR网络中基于深度学习的频谱感知方案。与现有方案相比，所提出的方案具有有限的感测时间，甚至短于一个符号持续时间，这是有效的，在水声环境中，长符号持续时间是必不可少的。此外，该方案通过学习动物噪声和OFDMA正交性破坏引起的干扰，提高了空闲信道的检测精度，并识别动物声音，防止对动物造成伤害。仿真结果证实了该方案的优越性。</code><br>Index Terms<br><code>索引词</code><br>Spectrum sensing, underwater acoustic communication, cognitive radio, deep learning, limited sensing time.<br><code>频谱感知，水声通信，认知无线电，深度学习，有限感知时间。</code></p><h1 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h1><p>For underwater communication, acoustic waveform has been used to exploit the moderate attenuation owing to low frequency unlike high-frequency electromagnetic waves [1]. However, the UWA communications have insufficient frequency resources due to extremely limited bandwidth caused by the low-frequency wave [2]. To solve this problem, there are many efforts applying the orthogonal frequency division multiplexing (OFDM) waveform, orthogonal frequency division multiple access (OFDMA), and cognitive radio (CR) to UWA communications [3], [4].<br><code>对于水下通信，声波波形已被用于利用与高频电磁波不同的低频的适度衰减[1]。然而，由于低频波造成的带宽极其有限，UWA通信的频率资源不足[2]。为了解决这个问题，存在将正交频分复用（OFDM）波形、正交频分多址（OFDMA）和认知无线电（CR）应用于UWA通信的许多努力[3]、[4]。</code><br>As a result, an OFDMA-based CR protocol employs cyclic prefix (CP) to compensate for multi-path and achieves high spectral efficiency compared to single-carrier-based approaches [4]. Moreover, the protocol enables a terminal to detect spectrum holes and transmit packets without requiring a grant, which effectively enhances resource efficiency by eliminating the process for exchanging control signals. However, the OFDMA-based CR protocol identifies and utilizes idle resources solely through spectrum sensing, and thus it inevitably makes the resource efficiency highly dependent on the sensing accuracy. Consequently, numerous previous studies have been conducted to enhance the accuracy of spectrum sensing [5].<br><code>因此，与基于单载波的方法相比，基于OFDMA的CR协议采用循环前缀（CP）来补偿多径并实现高频谱效率[4]。此外，该协议使得终端能够检测频谱空洞并发送分组而不需要授权，这通过消除用于交换控制信号的过程而有效地提高了资源效率。然而，基于OFDMA的CR协议仅通过频谱感知来识别和利用空闲资源，因此不可避免地使得资源效率高度依赖于感知精度。因此，已经进行了许多先前的研究以提高频谱感测的准确性[5]。</code><br>To improve sensing accuracy in terrestrial wireless communications, there are various sensing schemes based on a covariance matrix of the received signals. Specifically, most of them utilize a sample-covariance matrix (SCM), which is an empirical mean of the covariance matrix derived from multiple received OFDM symbols [6]. However, an OFDM waveform in UWA communication has long CP and long symbol duration due to the high delay spread of UWA multi-path channels. Hence, the SCM-based schemes undergo seriously long sensing delay, which wastes energy and causes a change in the communication environment during the sensing time.<br><code>为了提高地面无线通信中的感测精度，存在基于接收信号的协方差矩阵的各种感测方案。具体地，它们中的大多数利用样本协方差矩阵（SCM），其是从多个接收的OFDM符号导出的协方差矩阵的经验平均值[6]。然而，由于水声多径信道的高延迟扩展，水声通信中的OFDM波形具有长CP和长符号持续时间。因此，基于SCM的方案经历非常长的感测延迟，这浪费了能量并且在感测时间期间引起通信环境的变化。</code><br>As another scheme, there is an energy detection (ED)-based sensing scheme that should not require a long sensing time because multiple symbols are not needed [7]. Although the ED-based scheme can flexibly change the sensing time, the sensing accuracy is low due to its too simple abstraction of information. In particular, it shows extremely low performance in an environment if synchronization between terminals is not guaranteed or where the sensing time is shorter even than a single OFDM symbol. In addition, the performance of ED-based sensing is significantly degraded even in the presence of interfering sources such as dolphins affecting UWA channels.<br><code>作为另一种方案，存在基于能量检测（艾德）的感测方案，其不应要求长的感测时间，因为不需要多个符号[7]。虽然基于ED的方案可以灵活地改变感测时间，但由于其过于简单的信息抽象，感测精度较低。特别地，在如果终端之间的同步没有得到保证或者感测时间甚至比单个OFDM符号更短的环境中，其显示出极低的性能。此外，基于ED的传感的性能显着下降，即使在干扰源，如海豚影响UWA信道的存在。</code><br>This letter addresses a sensing scheme that can achieve high sensing accuracy with a short sensing time in an OFDMA-based CR protocol. Specifically, the proposed scheme improves signal detection accuracy by about 10 % compared to the ED-based scheme in scenarios where the sensing time is shorter than a single OFDM symbol duration. In addition, the proposed scheme can protect dolphins by learning dolphin sounds in the neural network (NN) training process.<br><code>这封信介绍了一种在基于OFDMA的CR协议中能够以较短的感测时间实现高感测精度的感测方案。具体地说，在感知时间小于单个ofdm符号持续时间的情况下，所提出的方案比基于ED的方案提高了约10%的信号检测精度。此外，该方案通过在神经网络(NN)训练过程中学习海豚的声音来保护海豚。</code><br>Numerical results show that the proposed scheme outperforms the existing ED-based sensing scheme for various signal-to-noise ratio (SNR) regimes and sensing times through receiver operating characteristic (ROC) curves. Furthermore, we have compared the proposed NN structure to a simple NN structure which has the same computational complexity as the proposed NN. The results demonstrate that the proposed scheme has a well-suited structure for UWA spectrum sensing.<br><code>数值结果表明，该方案优于现有的ED为基础的传感方案的各种信号噪声比（SNR）制度和传感时间通过接收机工作特性（ROC）曲线。此外，我们已经比较了建议的NN结构，一个简单的NN结构，具有相同的计算复杂度建议的NN。结果表明，该方案具有良好的适应结构的水声频谱感知。</code></p><h1 id="II-SYSTEM-MODEL"><a href="#II-SYSTEM-MODEL" class="headerlink" title="II. SYSTEM MODEL"></a>II. SYSTEM MODEL</h1><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/1.png" alt="图一"><br><code>采用基于OFDMA的CR协议的水声网络系统模型。</code><br>Fig. 1 depicts a UWA network with an OFDMA-based CR protocol. The UWA network consists of a sink node, sensor nodes, and noise sources, where each sensor node communicates with the sink node as in [8]. The sets of sensor node indices and noise source indices are denoted as J = {1, 2, . . . , J} and M = {1, 2, . . . ,M}, respectively. We assume that OFDMA is employed in the communication between a sensor node and the sink node. The minimum set of subcarriers allocated to a node is called resource block (RB), and the time consumed for the spectrum sensing is denoted as Tsens.<br><code>图1描绘了具有基于OFDMA的CR协议的UWA网络。UWA网络由汇聚节点、传感器节点和噪声源组成，其中每个传感器节点与汇聚节点通信，如[8]中所述。传感器节点索引和噪声源索引的集合被表示为J = {1，2，...，J}且M = {1，2，...，M}。我们假设OFDMA被用于传感器节点和汇聚节点之间的通信。分配给节点的子载波的最小集合被称为资源块（RB），并且频谱感测所消耗的时间被表示为Tsens。</code><br>In the OFDMA-based CR protocol, node j detects idle resources through spectrum sensing when a packet to transmit occurs, and then the node transmits the packet to the idle resource. Therefore, if node j performs spectrum sensing, signals transmitted by other nodes are received by node j through the sensing link. The sensing link is physically identical to the acoustic communication channel, but it is used only for sensing purposes without decoding in the system under consideration.<br><code>在基于OFDMA的CR协议中，节点j在出现要发送的分组时通过频谱感知来检测空闲资源，然后节点将分组发送到空闲资源。因此，如果节点j进行频谱感知，则其他节点发送的信号通过感知链路被节点j接收。感测链路在物理上与声学通信信道相同，但它仅用于感测目的，而不在所考虑的系统中进行解码。</code><br>A. UWA Channel Generation<br><code>A.UWA信道生成</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/2.png" alt="图二"><br><code>采用BELLHOP模拟器的拟议频谱感测方案的框图。</code><br>Fig. 2 depicts the block diagram of the proposed spectrum sensing scheme. In the figure, a UWA channel between sensor node i and sensor node J is denoted by hi,J, and the UWA channel is obtained by a UWA channel generating simulator called BELLHOP [9], [10]. The BELLHOP simulator can generate channels by deriving intensity field and performing ray tracing based on underwater information such as a sound speed profile (SSP) and geographical features, where the details of the channel model were discussed in [9]. Therefore, these channels are more realistic than the channels generated by a simple transmission loss model [11].<br><code>图2描绘了所提出的频谱感测方案的框图。在图中，传感器节点i和传感器节点J之间的UWA信道由hi，J表示，并且UWA信道由称为BELLHOP的UWA信道生成模拟器获得[9]，[10]。BELLHOP模拟器可以通过导出强度场并基于水下信息（例如声速剖面（SSP）和地理特征）执行射线跟踪来生成通道，其中通道模型的详细信息在[9]中进行了讨论。因此，这些信道比由简单传输损耗模型生成的信道更真实[11]。</code></p><h1 id="III-LEARNING-BASED-SPECTRUM-SENSING"><a href="#III-LEARNING-BASED-SPECTRUM-SENSING" class="headerlink" title="III. LEARNING-BASED SPECTRUM SENSING"></a>III. LEARNING-BASED SPECTRUM SENSING</h1><p><code>三.基于学习的频谱感知</code><br>A. Transmitted Signal<br><code>A.发射信号</code><br>As shown in Fig. 2, the continuous time signal detected by sensor node i is denoted as yi(t). To express yi(t), we first define the signal transmitted by the other sensor node j as follows.<br><code>如图2所示，传感器节点i检测到的连续时间信号表示为yi（t）。为了表示yi（t），我们首先如下定义由另一传感器节点j发送的信号。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/3.png"><br>where U is the number of OFDM symbols in a packet, fc is the center frequency, Ttot is the duration of total OFDM symbol including CP, and ˆx^(u)_j is defined as<br><code>其中，U是分组中的OFDM符号的数量，fc是中心频率，Ttot是包括CP的总OFDM符号的持续时间，并且ˆx^(u)_j被定义为：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/4.png"></p><p>In (2), ˆx^(u)_j = 0 for t ≤ −TCP and t ≥ Tsym, Kj is the set of subcarrier indices allocated to node j, Tsym is the duration of a single OFDM symbol, TCP is the duration of CP, and d(u) j,k is the transmitted data of node j carried on the k-th subcarrier in the u-th OFDM symbol.<br><code>在（2）中，ˆx^(u)_j对于t ≤-TCP和t ≥ Tsym，Kj = 0，Kj是分配给节点j的子载波索引的集合，Tsym是单个OFDM符号的持续时间，TCP是CP的持续时间，d（u）j，k是在第u个OFDM符号中的第k个子载波上承载的节点j的发送数据。</code></p><p>B. Animal and Artificial Noise<br><code>B.动物和人工噪声</code><br>In addition to the transmitted signal, we generate animal and artificial noise based on the sound source files to implement realistic UWA communication environments, where the public data in [12] and [13] is used as the source files. Specifically, the dolphin sounds with grade 3 called high-quality whistles in [13] are extracted and used as the animal noise.<br><code>除了传输的信号之外，我们还根据声源文件生成动物和人工噪声，以实现逼真的UWA通信环境，其中[12]和[13]中的公共数据用作源文件。具体地，提取[13]中称为高质量哨声的3级海豚声音并用作动物噪声。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/7.png" alt="Fig. 3. Analysis of time/frequency signals from underwater sources of interference such as bottlenose dolphins, snapping shrimps, and vessels."><br><code>分析来自水下干扰源的时间/频率信号，如海豚、虾类和船只。</code><br>Fig. 3 shows the results of analyzing time/frequency signals of bottlenose dolphins, snapping shrimps, and vessels, which are representative sources of underwater interference. Since a UWA communication typically uses the center frequency of around 10 kHz, it is hardly affected by vessel sounds. Snapping shrimp can affect the UWA communication, but the effect is not critical. On the other hand, bottlenose dolphins have effective signals up to 20 kHz, making it a significant obstacle to finding idle resources.<br><code>图3显示了对海豚、虾蛄和船只的时间/频率信号进行分析的结果，这些信号是水下干扰的代表性来源。由于UWA通信通常使用10 kHz左右的中心频率，因此几乎不受船只声音的影响。捕虾对水声通信有一定的影响，但影响并不严重。另一方面，海豚的有效信号高达20 kHz，这是寻找闲置资源的一个重大障碍。</code></p><p>C. Detected Signal<br><code>C.检测信号</code><br>Let us denote a sound of the noise source m as sm(t), and then a detected signal in the sensor node i is represented as<br><code>让我们将噪声源m的声音表示为sm（t），然后将传感器节点i中的检测信号表示为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/5.png"><br>In (3), the operator ∗ represents a convolution product, z(t) is additive white gaussian noise, and the operating gap gj,i is defined as gj,i = ri − lj,i. As shown in Fig. 4, lj,i is the time when the signal of node j arrives at node i, and ri is the time at which node i starts sensing.<br><code>在（3）中，运算符n表示卷积积，z（t）是加性白色高斯噪声，并且操作间隙gj，i被定义为gj，i = ri-lj，i。如图4所示，lj，i是节点j的信号到达节点i的时间，ri是节点i开始感测的时间。</code><br>Finally, the detected signal yi(t) is discretized by sampling, and it is expressed as<br><code>最后，通过采样对检测信号yi（t）进行离散化，并将其表示为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/6.png"><br>where Tsamp is a sampling period.<br><code>其中Tsamp是采样周期。</code></p><p>D. Neural Network Input Data<br><code>D.神经网络输入数据</code><br>In UWA short-time sensing scenarios, inter-carrier interference (ICI) is mainly caused by frequency-adjacent RBs, called out-of-band emission (OOBE). Therefore, the convolution process in convolutional neural network (CNN) can effectively mitigate ICI by capturing features of the signals in adjacent RBs. Furthermore, the convolution process effectively extracts correlation features among adjacent time samples, where animal noise and sinusoidal-based signals have strong correlations among contiguous time samples. As a result, the proposed spectrum sensing scheme employs CNN to mitigate the effects of animal noise and ICI.<br><code>在水声短时间感知场景中，载波间干扰（ICI）主要由频率相邻的RB引起，称为带外发射（OOBE）。因此，卷积神经网络（CNN）中的卷积过程可以通过捕获相邻RB中信号的特征来有效地减轻ICI。此外，卷积过程有效地提取相邻时间样本之间的相关性特征，其中动物噪声和基于正弦的信号在连续时间样本之间具有强相关性。因此，所提出的频谱感测方案采用CNN来减轻动物噪声和ICI的影响。</code><br>The received time-domain and frequency-domain signals utilized in the CNN model are represented as follows:<br><code>CNN模型中使用的接收到的时域和频域信号表示如下：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/8.png"><br>In (5), Ncol and Nrow mean column size and row size of 2D input, respectively. In addition, Re{y} is a function that outputs the real part of y, Im{y} is a function that outputs the imaginary part of y, ˆ Yi[n] is a frequency-domain discrete signal in which ˆyi[n] is transformed by fast Fourier transform (FFT), ∀p ∈ [1, · · · , Ncol], and ∀q ∈ [1, · · · , Nrow]. Finally, all 2D inputs are combined into a single 3D network input with the size of 4 ×Ncol ×Nrow.<br><code>在（5）中，Ncol和Nrow分别表示2D输入的列大小和行大小。另外，Re{y}是输出y的真实的部分的函数，Im{y}是输出y的虚部的函数，ˆ Yi[n] 是通过快速傅立叶变换（FFT）对ˆyi[n]进行变换的频域离散信号，Yp ∈ [1，· · ·，Ncol]，并且Yq ∈ [1，· · ·，Nrow]。最后，将所有2D输入合并为一个大小为4 ×Ncol ×Nrow的3D网络输入。</code><br>E. CNN Output and Loss Model<br><code>E.CNN输出和损失模型</code><br>The goal of the spectrum sensing in the CR protocol is to detect idle RBs, and thus the output of the CNN should indicate the state of RBs. In addition, the proposed scheme detects dolphin sounds, and thus the presence or absence of dolphin sounds should be included in the output. Therefore, we design the label of the output as<br><code>CR协议中的频谱感测的目标是检测空闲RB，因此CNN的输出应该指示RB的状态。此外，所提出的方案检测海豚的声音，因此海豚的声音的存在或不存在应该包括在输出中。因此，我们将输出的标签设计为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/9.png"><br>where NRB is the number of RBs, S^(us)_q + D^(id)_q = 1 for all q, S^(us)_1 = 1 if RB1 is used, and D^(id)_1 = 1 if RB1 is idle.<br><code>其中NRB是RB的数量，对于所有q，S^(us)_q + D^(id)_q = 1，如果使用RB1，则S^(us)_1 = 1，并且如果RB1空闲，则D^(id)_1 = 1。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/10.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/11.png"><br><code>类似地，s（pr）dol + d（ab）dol = 1，如果海豚音存在，s（pr）dol = 1，如果海豚音不存在，d（ab）dol = 1。让我们将网络的输出定义为：对于所有q，都有S（us）q，对于所有q，都有D（id）q，都有S（pr）dol和D（ab）dol。利用交叉熵，成本函数设计如下。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/12.png"></p><p>F. Training Process<br><code>F.训练过程</code><br>Algorithm 1 depicts the training architecture of the proposed scheme. As shown in the algorithm, training is divided into a MATLAB-based module for generating sensing signals and a Python-based CNN learning module for updating network weights. In the MATLAB-based module, Nongoing and Nact are first determined, where Nongoing and Nact mean the number of sensor nodes transmitting a packet and the number of noise nodes generating noise, respectively. To prevent the biased dataset, the values of Nongoing and Nact should be uniformly distributed within the generated data.<br><code>算法1描述了所提出的方案的训练架构。如算法所示，训练分为基于MATLAB的模块，用于生成传感信号，以及基于Python的CNN学习模块，用于更新网络权重。在基于MATLAB的模块中，首先确定Nongoing和Nact，其中Nongoing和Nact分别表示发送数据包的传感器节点的数量和产生噪声的噪声节点的数量。为了防止有偏的数据集，Nongoing和Nact的值应该均匀分布在生成的数据中。</code><br>The MATLAB-based module randomly deploys nodes in the specified underwater space and creates channels between them using the BELLHOP simulator. Then, to generate a sensing signal, Nongoing and Nact number of nodes are activated to transmit a packet or to generate noise.<br><code>基于MATLAB的模块在指定的水下空间随机部署节点，并使用BELLHOP模拟器在它们之间创建通道。然后，为了生成感测信号，Nongoing和Nact数量的节点被激活以发送分组或生成噪声。</code></p><h1 id="IV-NUMERICAL-RESULTS"><a href="#IV-NUMERICAL-RESULTS" class="headerlink" title="IV. NUMERICAL RESULTS"></a>IV. NUMERICAL RESULTS</h1><p><code>四.数值结果</code><br>A. Simulation Setup<br><code>A.仿真设置</code><br>The UWA network and OFDM waveform parameters are listed in Table I, where the OFDM parameters are based on the actual measurements in [15] and [16]. According to [15] and [16], OFDM symbol duration higher than 100 ms and CP duration higher than 20 ms should be used due to the high delay spread of UWA channels. With limited computing power on a real time basis, the proposed scheme uses a customized shallow CNN structure composed of 5 layers, where the depth of the CNN has been intentionally set to the minimum depth that yields converged sensing performance for all considered scenarios. Each layer of the network consists of a convolutional layer and a Rectified Linear Unit (ReLU) activation function, without any pooling layers. The used functions and hyperparameters for the CNN are summarized as follows: Adam optimizer, CosineAnnealingLR scheduler, initial learning rate of 1×10−4, and two kernels with sizes of 5×5 and 3×3. In addition, the CNN input sizes for sensing times of 45 ms and 22.5 ms are defined as 4 × 50 × 20 and 4 × 50 × 10, respectively. In addition, a cross-validation technique is employed to prevent overfitting.<br><code>UWA网络和OFDM波形参数列于表I中，其中OFDM参数基于[15]和[16]中的实际测量。根据[15]和[16]，由于UWA信道的高延迟扩展，应使用高于100 ms的OFDM符号持续时间和高于20 ms的CP持续时间。在真实的时间基础上具有有限的计算能力的情况下，所提出的方案使用由5层组成的定制的浅CNN结构，其中CNN的深度被有意地设置为针对所有考虑的场景产生收敛的感测性能的最小深度。网络的每一层都由卷积层和整流线性单元（ReLU）激活函数组成，没有任何池化层。CNN使用的函数和超参数总结如下：Adam优化器，CosineAnnealingLR调度器，初始学习率为1×10−4，两个大小为5×5和3×3的内核。此外，感知时间为45 ms和22.5 ms的CNN输入大小分别定义为4 × 50 × 20和4 × 50 × 10。此外，交叉验证技术，以防止过度拟合。</code></p><p>B. Discussion of CNN Complexity<br><code>B.CNN复杂度的讨论</code><br>Let us define the dimensions of the input and output of CNN are Cin and Cout, respectively. Then, the number of flops of CNN can be obtained by<br><code>让我们将CNN的输入和输出的维度分别定义为Cin和Cout。然后，CNN的触发次数可以通过以下方式获得：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/13.png"><br>where a × b is a convolutional kernel size, and α × β is an input shape of CNN. Therefore, the computational complexity of CNN is O(FCNN) flops. On the other hand, the dominant computational complexity of the ED-based sensing arises from the FFT operation due to the OFDMA protocol. Hence, the computational complexity of the ED-based sensing is O(Nsam log2(Nsam)), where Nsam is the number of sensing samples.<br><code>其中a × B是卷积核大小，α × β是CNN的输入形状。因此，CNN的计算复杂度是O（FCNN）flops。另一方面，基于ED的感测的主要计算复杂度由归因于OFDMA协议的FFT操作引起。因此，基于ED的感测的计算复杂度是O（Nsam log2（Nsam）），其中Nsam是感测样本的数量。</code><br>In general, the worst case of α × β is similar to Nsam and the remaining terms in FCNN, excluding the term α × β, are larger than log2(Nsam). Thus, the computational complexity of CNN is typically larger than that of the ED-based sensing. Nevertheless, the proposed scheme exhibits significantly lower complexity in comparison to networks such as ResNet-152. Therefore, the proposed scheme is still suitable for use in UWA equipment, such as autonomous underwater vehicles (AUVs).<br><code>一般来说，α × β的最差情况与Nsam相似，FCNN中的其余项（不包括α × β项）大于log 2（Nsam）。因此，CNN的计算复杂度通常大于基于ED的感测的计算复杂度。尽管如此，与ResNet-152等网络相比，该方案的复杂度明显降低。因此，所提出的方案仍然适用于UWA设备，如自主水下航行器（AUV）。</code></p><p>C. Performance Evaluation<br><code>C.绩效评价</code><br>For performance comparison, we have implemented an ED-based sensing scheme that can operate in a short sensing time. In addition, to demonstrate the suitability of the proposed NN structure, a fully connected network (FCN) has been implemented in two versions: 1) a model with similar computational complexity to the CNN scheme, and 2) a model which achieves the converged sensing performance with significantly higher computational complexity than the CNN-based scheme.<br><code>对于性能比较，我们已经实现了一个基于ED的传感方案，可以在很短的传感时间内工作。此外，为了证明所提出的NN结构的适用性，全连接网络（FCN）已经以两个版本实现：1）具有与CNN方案相似的计算复杂度的模型，以及2）实现收敛感测性能的模型，其计算复杂度显著高于基于CNN的方案。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/14.png" alt="Fig. 5. ROC curves of CNN-based, FCN-based, and ED-based schemes for different sensing times with the SNR of 6 dB."><br><code>图五.SNR为6 dB时，基于CNN、基于FCN和基于ED的方案在不同感知时间的ROC曲线。</code><br>Figs. 5 present ROC curves of the spectrum sensing schemes. The ROC curve is a graph that measures classification performance through true positive rate (TPR) and false positive rate (FPR). TPR represents the proportion of actual positive cases that are correctly identified as positive, while FPR represents the proportion of actual negative cases that are incorrectly identified as positive. Here, the positive represents a detection of a spectrum hole.<br><code>图5给出了频谱感知方案的ROC曲线。ROC曲线是通过真阳性率（TPR）和假阳性率（FPR）来测量分类性能的图。TPR代表被正确识别为阳性的实际阳性病例的比例，而FPR代表被错误识别为阳性的实际阴性病例的比例。这里，正表示检测到频谱空洞。</code><br>In Fig. 5a, area under the curve (AUC) of the proposed scheme is 5 % larger than that of the ED-based scheme, and thus the proposed CNN-based scheme is a better sensor than the ED-based scheme. However, as discussed in Section IVB, the CNN-based scheme requires a higher computational complexity than the ED-based scheme, which indicates that there exists a trade-off between a computational complexity and sensing performance. Although the trade-off exists, the FCN-based scheme requiring a high computational complexity exhibits inferior sensing performance compared to the CNN-based scheme with a relatively low computational complexity, as depicted in Fig. 5a. This indicates that the proposed CNN-based scheme is a well-suited model for the UWA-CR spectrum sensing because it effectively captures both animal noise and ICI caused by the short sensing time, leading to superior sensing performance compared to the FCN-based scheme.<br><code>在图5a中，所提出的方案的曲线下面积（AUC）比基于ED的方案的曲线下面积大5%，因此所提出的基于CNN的方案是比基于ED的方案更好的传感器。然而，如第IVB节中所讨论的，基于CNN的方案需要比基于ED的方案更高的计算复杂度，这指示在计算复杂度和感测性能之间存在折衷。尽管存在折衷，但是与具有相对低的计算复杂度的基于CNN的方案相比，需要高计算复杂度的基于FCN的方案表现出较差的感测性能，如图5a所示。这表明所提出的基于CNN的方案是非常适合UWA-CR频谱感测的模型，因为它有效地捕获动物噪声和由短感测时间引起的ICI，从而导致与基于FCN的方案相比具有上级感测性能。</code><br>In Fig. 5b, if the sensing time reduces to 22.5 ms, the interval between frequency samples becomes wider. As a result, the ED-based sensing scheme detects the presence or absence of a signal by using only one frequency sample, and thus it has a very poor ROC curve. In the case of the proposed scheme, the sensing performance degrades if the sensing time is reduced to 22.5 ms. However, since the CNN extracts meaningful information from other frequency samples, the performance degradation is not severe compared to the ED-based sensing scheme.<br><code>在图5b中，如果感测时间减少到22.5ms，则频率样本之间的间隔变得更宽。结果，基于ED的感测方案通过仅使用一个频率样本来检测信号的存在或不存在，并且因此其具有非常差的ROC曲线。在所提出的方案的情况下，如果感测时间减少到22.5 ms，则感测性能降低。然而，由于CNN从其他频率样本中提取有意义的信息，因此与基于ED的感测方案相比，性能降低并不严重。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/15.png"><br><code>表II在固定SNR为10 dB的环境中，建议方案的海豚声检测精度（%）</code><br>Table II shows the accuracy of detecting the dolphin sounds of the proposed scheme, where the accuracy means the ratio of accurately matching the presence or absence of dolphins. In an environment where the SNR is fixed at 10 dB, as the signal-to-interference ratio (SIR) increases, the dolphin’s loudness decreases. Therefore, as the value of SIR increases, the detection accuracy decreases from 97.2 % to 88.4 % with a sensing time of 45 ms. However, the low dolphin’s loudness means that the dolphin is far away, and thus it is more important to have 97 % accuracy at an SIR of 0 dB where the dolphin may be close. In the table, the sensing accuracy varies by approximately 1 % depending on the sensing time. Although the sensing accuracy improves as the sensing time increases, it only converges to about 98 % at the sensing time of 90 ms and SIR of 0 dB due to the error rate in data generation. Therefore, achieving 100 % sensing accuracy is still challenging.<br><code>表II显示了拟议方案检测海豚声音的准确性，其中准确性是指准确匹配海豚存在或不存在的比率。在SNR固定为10 dB的环境中，随着信号干扰比（SIR）的增加，海豚的响度降低。因此，随着SIR值的增加，检测准确度从97.2%降低到88.4%，感知时间为45 ms。然而，海豚的低响度意味着海豚很远，因此在SIR为0 dB时，海豚可能很近，因此更重要的是具有97%的准确度。在该表中，感测精度根据感测时间而变化大约1%。尽管感测精度随着感测时间的增加而提高，但是由于数据生成中的错误率，其仅在90 ms的感测时间和0 dB的SIR处收敛到约98%。因此，实现100%的感测精度仍然具有挑战性。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/sp/16.png"><br>Fig. 6 shows the true negative rate (TNR) values at which the sensing scheme achieves TPR of 0.8, where TNR represents the proportion of actual negative cases that are correctly identified as negative. With the same sensing time, the proposed scheme achieves TPR values of 0.8 at 10 % higher TNR values compared to the ED-based sensing scheme. In addition, the proposed scheme with a sensing time of 45 ms achieves a TPR value of 0.8 at an SNR of 12 dB with a TNR value of 1. That is, 80 % of the signals can be detected without error.<br><code>图6示出了感测方案实现TPR为0.8时的真阴性率（TNR）值，其中TNR表示被正确识别为阴性的实际阴性情况的比例。在相同的感测时间下，与基于ED的感测方案相比，所提出的方案在高10%的TNR值下实现了0.8的TPR值。此外，所提出的方案具有45 ms的感测时间，在SNR为12 dB，TNR值为1时，TPR值为0.8。也就是说，80%的信号可以被无误差地检测。</code></p><h1 id="V-CONCLUSION"><a href="#V-CONCLUSION" class="headerlink" title="V. CONCLUSION"></a>V. CONCLUSION</h1><p>This letter proposed a CNN-based spectrum sensing scheme to reduce the sensing time and to improve sensing accuracy in the OFDMA-based UWA-CR network. To generate a realistic sensing signal for training the CNN, UWA channels were generated using a ray-tracing-based BELLHOP simulator. Then, various noises were added to the received signal. The proposed scheme showed more than 10 % improvement in the TNR performance in all SNR regimes compared to the existing ED-based sensing. In future work, we will apply cooperative sensing to the proposed scheme to improve sensing accuracy. Moreover, we will optimize energy efficiency by conducting a comprehensive system performance analysis to assess power consumption arising from sensing time and computational complexity.<br><code>本文提出了一种基于CNN的频谱感知方案，以减少基于OFDMA的UWA-CR网络中的感知时间，提高感知精度。为了生成用于训练CNN的真实感测信号，使用基于光线跟踪的BELLHOP模拟器生成UWA通道。然后，各种噪声被添加到所接收的信号。与现有的基于ED的感测相比，所提出的方案在所有SNR机制中的TNR性能表现出超过10%的改善。在未来的工作中，我们将应用合作感知的建议方案，以提高传感精度。此外，我们将进行全面的系统性能分析，以评估因感知时间和计算复杂性而产生的功耗，从而优化能源效率。</code></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文翻译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文翻译】Underwater Acoustic Communication Receiver Using Deep Belief Network</title>
      <link href="/2023/11/23/lun-wen-fan-yi-underwater-acoustic-communication-receiver-using-deep-belief-network/"/>
      <url>/2023/11/23/lun-wen-fan-yi-underwater-acoustic-communication-receiver-using-deep-belief-network/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><em><strong>Underwater Acoustic Communication Receiver Using Deep Belief Network</strong></em><br><code>基于深信度网络的水声通信接收机</code></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Underwater environments create a challenging channel for communications.<br><code>水下环境为通信创造了具有挑战性的信道。</code><br>In this paper, we design a novel receiver system by exploring the machine learning technique–Deep Belief Network (DBN)– to combat the signal distortion caused by the Doppler effect and multi-path propagation.<br><code>在本文中，我们设计了一种新的接收机系统，通过探索机器学习技术-深度信念网络（DBN）-来对抗由多普勒效应和多径传播引起的信号失真。</code><br>We evaluate the performance of the proposed receiver system in both simulation experiments and sea trials. Our proposed receiver system comprises of DBN based de-noising and classification of the received signal.<br><code>我们评估所提出的接收机系统在模拟实验和海上试验的性能。我们提出的接收机系统包括基于DBN的去噪和接收信号的分类。</code><br>First, the received signal is segmented into frames before the each of these frames is individually pre-processed using a novel pixelization algorithm.<br><code>首先，接收到的信号被分割成帧，然后使用新颖的像素化算法对这些帧中的每一个进行单独的预处理。</code><br>Then, using the DBN based de-noising algorithm, features are extracted from these frames and used to reconstruct the received signal.<br><code>然后，使用基于DBN的去噪算法，从这些帧中提取特征并用于重建接收信号。</code><br>Finally, DBN based classification of the reconstructed signal occurs.<br><code>最后，发生重构信号的基于DBN的分类。</code><br>Our proposed DBN based receiver system does show better performance in channels influenced by the Doppler effect and multi-path propagation with a performance improvement of 13.2dB at 10−3 Bit Error Rate (BER).<br><code>我们提出的基于DBN的接收机系统在受多普勒效应和多径传播影响的信道中确实表现出更好的性能，在10−3比特误码率（BER）下性能提高了13.2dB。</code><br>Index Terms<br><code>索引词</code><br>Underwater acoustic communications, receiver systems, machine learning, signal processing, Doppler effect.<br><code>水声通信，接收系统，机器学习，信号处理，多普勒效应。</code></p><h1 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h1><p>Underwater Acoustic Communications (UWAC) is a knowledge rich field that has, in the recent years, gained a tremendous amount of interest for its many applications in the field of ocean exploration, defense, and marine commercial industries.<br><code>水下声通信是一个知识丰富的领域，近年来，由于其在海洋勘探、国防和海洋商业领域的许多应用，引起了人们极大的兴趣。</code><br>A few notable applications are underwater exploration [1], underwater mine detection [2], and underwater communications between submarines or underwater nodes [3].<br><code>一些值得注意的应用是水下勘探[1]，水下地雷探测[2]以及潜艇或水下节点之间的水下通信[3]。</code><br>Due to a rapidly growing need for data-heavy underwater systems, the expectations and requirements of the underwater system design has risen up to a point where a growing number of researchers are starting to turn to unconventional methods like machine learning (ML) and deep learning (DL) to combat the challenging underwater environment.<br><code>由于对数据密集型水下系统的需求迅速增长，水下系统设计的期望和要求已经上升到一个点，越来越多的研究人员开始转向机器学习（ML）和深度学习（DL）等非传统方法来应对具有挑战性的水下环境。</code><br>In this paper, we propose a novel receiver system that utilizes the capabilities of Deep Belief Networks (DBNs) to redesign the de-noising and demodulation blocks of the communication system.<br><code>在本文中，我们提出了一种新的接收机系统，利用深度信念网络（DBN）的能力，重新设计的通信系统的去噪和解调块。</code><br>Generally, conventional signal processing algorithms in communications are based on strong mathematical foundations and are designed specifically for a variety of specific channels and system models [4], [5].<br><code>通常，通信中的传统信号处理算法基于强大的数学基础，并且是专门针对各种特定信道和系统模型而设计的[4]，[5]。</code><br>For instance, the Binary Phase-Shift Key (BPSK) modulation was designed for the detection of a constellation symbol in a channel of additive white Guassian noise (AWGN) [6].<br><code>例如，二进制相移键控（BPSK）调制被设计用于检测加性白色高斯噪声（AWGN）信道中的星座符号[6]。</code><br>These signal processing algorithms are constructed on expert knowledge of the tractable channel models, which in turn are established on a simplification of Maxwell’s equations [7].<br><code>这些信号处理算法是基于易处理信道模型的专业知识构建的，而易处理信道模型又是基于麦克斯韦方程的简化而建立的[7]。</code><br>UWAC signals, however, are not electromagnetic in nature [8], [9]. As such, the UWAC channel is widely characterized as one of the most complex channels to model and has yet to develop a palpable or definite model.<br><code>然而，UWAC信号本质上不是电磁的[8]，[9]。因此，UWAC信道被广泛地表征为最复杂的信道之一，并且尚未开发出明显或明确的模型。</code><br>Its high complexity is mostly derived from its fast varying characteristics, such as the Doppler effect and the propagation properties.<br><code>其高复杂度主要来源于其快速变化的特性，如多普勒效应和传播特性。</code><br>Since communications are heavily reliant on the characteristics of sound, it shows a strong correlation with the properties of sound [8].<br><code>由于通信在很大程度上依赖于声音的特性，因此它与声音的属性有很强的相关性[8]。</code><br>By understanding how sounds are influenced during sea trials, one can optimize the efficiency of the communication through adaptation.<br><code>通过了解在海上试验期间声音是如何受到影响的，人们可以通过适应来优化通信效率。</code><br>As sound propagates underwater at a very low speed of approximately 1500 m/s [9], and propagation occurs over multiple paths, it is very common to observe a delay spreading over tens or even hundreds of milliseconds which results in frequency-selective signal distortion.<br><code>由于声音在水下以大约1500m/s的非常低的速度传播[9]，并且传播发生在多个路径上，因此观察到延迟扩展超过数十甚至数百毫秒是非常常见的，这导致频率选择性信号失真。</code><br>This motion also results in an extreme Doppler effect.<br><code>这种运动也会导致极端的多普勒效应。</code><br>Multi-path propagation in the ocean is governed by three effects– (1) sound reflecting off underwater surfaces like bubbles and the seabed, (2) sound refraction in the water due to density change, and (3) energy loss [10].<br><code>海洋中的多径传播受三种效应控制-（1）声音从水下表面（如气泡和海底）反射，（2）由于密度变化引起的水中声折射，以及（3）能量损失[10]。</code><br>These effects will cause an elongation of the path traveled, and thus a time delay.<br><code>这些效应将导致行进路径的延长，从而导致时间延迟。</code><br>The first also creates reverberation, which causes a reflection phase change and a reflection amplitude change.<br><code>第一种也产生混响，这导致反射相位变化和反射幅度变化。</code><br>The second is a consequence of the spatial variability of sound speed, which is dependent on temperature, salinity, and pressure.<br><code>第二个是声速的空间变化的结果，这取决于温度，盐度和压力。</code><br>These factors vary with depth and location.<br><code>这些因素因深度和位置而异。</code><br>The final effect is heavily dependent on the signal frequency, as well as the pH level of the water.<br><code>最终效果在很大程度上取决于信号频率以及水的pH值。</code><br>This dependence is a consequence of absorption, where the signal energy is converted to heat.<br><code>这种依赖性是吸收的结果，其中信号能量被转换为热量。</code><br>In addition to the absorption loss, the signal typically experiences a spreading loss, which increases with distance [8].<br><code>除了吸收损耗之外，信号通常会经历扩展损耗，其随着距离的增加而增加[8]。</code><br>To correct for the intersymbol interference (ISI) caused by the propagation, the works done in [11]–[13] used an adaptive equalizer to flexibly compensate for the changes in the channel.<br><code>为了校正由传播引起的符号间干扰（ISI），[11]-[13]中所做的工作使用自适应均衡器来灵活地补偿信道的变化。</code><br>Another distinguishing property of UWAC is the channel’s time variability – (1) inherent changes in the propagation medium and (2) transmitter/receiver motion.<br><code>UWAC的另一个显著特性是信道的时间可变性-（1）传播介质的固有变化和（2）发射机/接收机运动。</code><br>Inherent changes include long term changes like seasonal temperatures and instantaneous changes caused by shipping routes and moving water surfaces.<br><code>内在的变化包括长期的变化，如季节性温度和航运路线和移动水面引起的瞬时变化。</code><br>These factors result in both a scattering of the signal and a Doppler effect spreading due to the changing path length [10].<br><code>这些因素导致信号的散射和由于路径长度变化而扩展的多普勒效应[10]。</code><br>A combination of these factors creates a complex challenge of modeling a sufficiently accurate channel model.<br><code>这些因素的组合产生了对足够准确的信道模型进行建模的复杂挑战。</code><br>To combat Doppler shifts, many Doppler scale estimation techniques have been proposed as seen in [14], [15].<br><code>为了对抗多普勒频移，已经提出了许多多普勒尺度估计技术，如[14]、[15]中所见。</code><br>In recent years, ML and more specifically DL have gained recognition for their performance in fields known for their high modelling complexity [16], such as image recognition [17], natural language processing [18], and handwriting analysis [19].<br><code>近年来，ML（更具体地说，DL）在以其高建模复杂性而闻名的领域[16]中的表现获得了认可，例如图像识别[17]，自然语言处理[18]和手写分析[19]。</code><br>Currently researchers have begun to explore the applications of ML and DL in the area of communications.<br><code>目前，研究人员已经开始探索ML和DL在通信领域的应用。</code><br>For example, recent research by Wang et al. [20] exploited deep learning to detect signal modulations in underwater channels.<br><code>例如，Wang等人最近的研究[20]利用深度学习来检测水下信道中的信号调制。</code><br>Other studies like [21], [22] used deep neural network (NN)-based auto encoders to demodulate received signals.<br><code>其他研究如[21]，[22]使用基于深度神经网络（NN）的自动编码器来解调接收到的信号。</code><br>As such, we can expect that applying ML techniques to communication blocks in order to provide a promising solution to the complex channel problem will yield significant improvements in decoding the physical layer.<br><code>因此，我们可以预期，将ML技术应用于通信块以提供对复杂信道问题的有希望的解决方案将在解码物理层方面产生显著的改进。</code><br>Works in [23], [24] investigate the use of DL based orthogonal frequency-division multiplexing (OFDM) receiver to recover signals corrupted by the UWAC channel.<br><code>在[23]、[24]中的工作研究了使用基于DL的正交频分复用（OFDM）接收机来恢复被UWAC信道破坏的信号。</code><br>In this paper, we explore a receiver system that fundamentally rethinks the traditional communications system design.<br><code>在本文中，我们探讨了一种接收机系统，从根本上重新思考传统的通信系统设计。</code><br>The receiver system utilizes DBNs, more specifically a greedy layer-wise ML algorithm that is able to automatically learn a new latent representation of the data.<br><code>接收器系统利用DBN，更具体地说，是一种贪婪逐层ML算法，能够自动学习数据的新潜在表示。</code><br>The two main functions of the receiver system are – (1) de-noising the received signal and (2) classifying the signals into their binary representatives.<br><code>接收机系统的两个主要功能是：（1）对接收到的信号进行去噪，以及（2）将信号分类成它们的二进制表示。</code><br>The features extracted by the DBN are considered as the properties of input data and are formed by considering the output layer of the DBN.<br><code>由DBN提取的特征被认为是输入数据的属性，并通过考虑DBN的输出层而形成。</code><br>For the first aspect of the proposed receiver, we train a DBN such that it learns to extract features of the received signal.<br><code>对于所提出的接收器的第一个方面，我们训练DBN，使得它学习提取接收信号的特征。</code><br>The trained DBN distinguishes the features of the segmented pre-processed signal and groups them with the same “clean” framed training data.<br><code>训练的DBN区分分段的预处理信号的特征，并将它们与相同的“干净”帧训练数据分组。</code><br>Furthermore, DBN is capable of reconstructing the input data based on their reduced, learned representation.<br><code>此外，DBN能够基于其简化的学习表示来重建输入数据。</code><br>After the DBN, the classification part of our system uses the features which can be tuned by back-propagation for classification.<br><code>在DBN之后，我们系统的分类部分使用可以通过反向传播进行分类调整的特征。</code><br>The main contributions of this paper are as follows:<br><code>本文的主要贡献如下：</code><br>1)Developed a de-noising DBN model. The trained DBN model distinguishes the features extracted from a segmented pre-processed signal. It then groups these features with the same “clean” framed training data.<br><code>建立了一个去噪DBN模型。训练的DBN模型区分从分段的预处理信号中提取的特征。然后，它将这些特征与相同的“干净”框架训练数据进行分组。</code><br>2)Redesigned the demodulation block using a classifying DBN model that utilizes feature extraction and back-propagation for classification of the received signals.<br><code>使用分类DBN模型重新设计解调模块，该模型利用特征提取和反向传播对接收信号进行分类。</code><br>3)The simulation results show that our proposed DBN based system is able to remain relatively resolute against the different characteristics of the UWAC channel. Furthermore, a sea trial was conducted to verify the performance of the proposed DBN-based receiver system in a real life environment.<br><code>仿真结果表明，我们提出的基于DBN的系统是能够保持相对坚决对UWAC信道的不同特性。此外，进行了海上试验，以验证所提出的基于DBN的接收机系统在真实的生活环境中的性能。</code><br>The remainder of this paper is organized as follows.<br><code>本文的其余部分组织如下。</code><br>Section II describes the communication system, underwater channel, and receiver system models.<br><code>第二节介绍了通信系统、水下信道和接收机系统模型。</code><br>Section III illustrates the proposed DBN based de-noising technique used.<br><code>第三节说明了所提出的基于DBN的去噪技术。</code><br>Section IV provided a description of the proposed DBN based demodulation technique used in the novel receiver system.<br><code>第四节提供了在新颖的接收机系统中使用的所提出的基于DBN的解调技术的描述。</code><br>Section V discussed the results of the proposed receiver system and the sea trial used for validation.<br><code>第五节讨论了拟议接收器系统的结果和用于验证的海上试验。</code><br>Finally, conclusions are drawn in Section VI.<br><code>最后，在第六节中得出结论。</code></p><h1 id="II-SYSTEM-MODEL-OVERVIEW"><a href="#II-SYSTEM-MODEL-OVERVIEW" class="headerlink" title="II. SYSTEM MODEL OVERVIEW"></a>II. SYSTEM MODEL OVERVIEW</h1><p><code>系统模型概述</code><br>In this section, we describe the proposed end-to-end communication system, represented in Fig. 1, comprising of a single transmitter and receiver.<br><code>在本节中，我们描述所提出的端到端通信系统，如图1所示，包括单个发射机和接收机。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/1.png" alt="Fig. 1. End-to-end System Model"><br><code>图一 端到端系统模型</code><br>The following subsections will describe the overall communication system model used to test our proposed system, the derivation of the underwater acoustic channel model, and an overview of the proposed receiver system.<br><code>以下小节将描述用于测试我们提出的系统的整体通信系统模型、水下声学信道模型的推导以及所提出的接收器系统的概述。</code></p><p>A. Communication System Model<br><code>A.通信系统模型</code><br>First, let y(n) be the representation of the convoluted transmitted bits, Y(n), and the binary representation of the modulated target transmitted signal z(t) during the n-th transmission.<br><code>首先，设y（n）是卷积的发送比特的表示，Y（n），以及在第n次发送期间调制的目标发送信号z（t）的二进制表示。</code><br>A series of transmission symbols y(n) are translated into different transmission signal waveforms z(t) via a Phase Shift Key (PSK) modulator as described in [25].<br><code>一系列传输符号y（n）通过相移键控（PSK）调制器被转换成不同的传输信号波形z（t），如[25]所述。</code><br>Second, let x(t) represent the overall transmitted signal consisting of z(t) and a Hyperbolic Frequency Modulated (HFM) pilot [26]. x(t) is then relayed through a channel model to obtain the received signal s(t):<br><code>其次，让x（t）表示由z（t）和双曲调频（HFM）导频组成的总发射信号[26]。x（t）然后通过信道模型被中继以获得接收信号s（t）：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/2.png"><br>where H(t) is the channel equation, · represents the dot product, and no(t) is the Additive White Gaussian Noise (AWGN).<br><code>其中H（t）是信道方程，·表示点积，no（t）是加性白色高斯噪声（AWGN）。</code><br>Finally, we generate a set of training data [zt,yt], t = 1, 2,… ,n,where zt is a training signal, yt is the corresponding BPSK binary label vector consisting of 0s and 1s, and n is the number of the training signals.<br><code>最后，我们生成一组训练数据[zt，yt]，t = 1，2，.，n，其中zt是训练信号，yt是由0和1组成的对应BPSK二进制标记向量，n是训练信号的数目。</code><br>Once detection and removal of the HFM pilot is completed, the desired section of the received signal s(t) acts as the input for the proposed receiver system.<br><code>一旦完成HFM导频的检测和去除，接收信号的期望部分(t)用作所提出的接收机系统的输入。</code><br>The algorithm will build a model from the training data L, such that for a given s’(t), the trained model will be able to predict a reconstructed waveform ˜z(n) and its corresponding label ˜y(n).<br><code>该算法将从训练数据L构建模型，使得对于给定的s'(t)，训练的模型将能够预测重构的波形˜z（n）及其对应的标签˜y（n）。</code><br>Therefore, predicting the received bits, ˜Y(n).<br><code>因此，预测所接收的比特，˜Y(n)</code></p><p>B. Underwater Channel<br><code>B.水下信道</code><br>Underwater acoustic communication channels are regarded by researchers as one of the most complex communication channels to model. Multi-path propagation and Doppler effects are recognized as one of the most challenging factors of the underwater acoustic channel [27]–[29].<br><code>水声通信信道被认为是最复杂的通信信道之一。多径传播和多普勒效应被认为是水声信道最具挑战性的因素之一[27]-[29]。</code><br>The more common techniques to approximately simulate the underwater acoustic channel vary from signal-to-noise ratio (SNR)-based channel models tha channel vary from signal-to-noise ratio (SNR)-based channel models that rely on empirical equations as seen in [8] to models that are based on the assumption of Rayleigh signal fading in [30], [31].<br><code>近似模拟水下声学信道的更常见的技术从基于信噪比（SNR）的信道模型到基于瑞利信号衰落的假设的模型，其中信道从依赖于如[8]中所见的经验方程的基于信噪比（SNR）的信道模型变化[30]、[31]。</code><br>In this subsection, the channel model used for the simulation will be presented, taking into consideration multi-path propagation and Doppler effect.<br><code>在本小节中，将给出用于仿真的信道模型，考虑多径传播和多普勒效应。</code><br>1)Multi-Path Propagation: Multi-path propagation in the ocean is mostly governed by sound reflecting off underwater surfaces like bubbles and the seabed [32]. These effects will cause an elongation of the path traveled, and thus a time delay. The received signal in a mutli-path environment can be generally represented as seen in [33], [34]:<br><code>多路径传播：海洋中的多径传播主要由气泡和海底等水下表面反射的声音控制[32]。这些效应将导致行进路径的延长，从而导致时间延迟。多径环境中的接收信号通常可以如[33]、[34]中所示表示：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/3.png"><br>where A represents the reverberation created by the reflection and scattering. This phenomena results in a reflection phase change and a propagation loss [32]. As such, Eq. 2 can be further expressed as:<br><code>其中A表示由反射和散射产生的混响。这种现象导致反射相位变化和传播损耗[32]。因此，Eq。2可以进一步表示为：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/4.png"><br>where a^a_i is the amplitude variation caused by the reverberation and fading of the channel, ◦ represents the Hadamard product, and a^b_i (θ(t)) is the phase variation and is modeled as seen in [35]:<br><code>其中a^a_i是由信道的混响和衰落引起的幅度变化，θ i表示Hadamard乘积，并且a^b_i (θ(t))是相位变化，并且如[35]中所示建模：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/5.png"><br>where j represents the imaginary number, k is the length of the signal and θ_k is the phase shift corresponding to the change in angle.<br><code>其中j表示虚数，k是信号的长度，θ_k是对应于角度变化的相移。</code><br>2)Doppler Effect: In underwater communications, a combination of the low speed of underwater sound propagation and the relative movement of the transmitter and receiver introduces the Doppler effect [8]. Let v denote the speed of the relative movement of the transmitter and the receiver, and fc denote the carrier frequency of the transmitted signal. The carrier frequency at the receiver is given by:<br><code>多普勒效应：在水下通信中，水下声音传播的低速和发射器和接收器的相对运动的组合引入了多普勒效应[8]。设v表示发射器和接收器的相对运动速度，fc表示发射信号的载波频率。接收器处的载波频率由下式给出：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/6.png"><br>where Δs denotes the sound propagation speed underwater. Note that Δrt is positive in the event that the receiver is moving toward the transmitter, otherwise Δrt is negative.<br><code>其中Δs表示水下的声音传播速度。注意，如果接收器正向发射器移动，则Δrt为正，否则Δrt为负。</code><br>In the time domain, the Doppler effect can be construed as a lengthening or compression of the transmitted waveform [36], [37]. The Doppler effect can be depicted in the time-domain as:<br><code>在时域中，多普勒效应可以被解释为发射波形的延长或压缩[36]，[37]。多普勒效应可以在时域中描述为：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/7.png"><br>where α is the Doppler co-efficient.<br><code>其中α是多普勒系数。</code><br>Taking into consideration the above contributing characteristics, the channel model used in this paper is:<br><code>考虑到上述贡献特性，本文中使用的信道模型为：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/8.png"></p><p>C. Receiver System Model<br><code>C.接收机系统模型</code><br>The receiver model is comprised of two blocks – (1) de-noising and (2) demodulation.<br><code>接收器模型由两个模块组成-（1）去噪和（2）解调。</code><br>In the de-noising component, the input signal s’(t) is first converted and normalized into a pixelized matrix m(t) via the proposed pre-processing method. m(t) is then partitioned into i number of m_i(t) to meet the requirement of the proposed algorithm for feature extraction. The learning features of the training data is used to find the closest match to the features of m_i(t), which is then used as a basis for reconstruction via:<br><code>在去噪组件中，输入信号s'(t)首先通过所提出的预处理方法将m(t)转换并归一化为像素化矩阵m(t)。然后将m(t)划分为i个m_i(t)，以满足所提出的算法对特征提取的要求。训练数据的学习特征用于找到与m_i(t)的特征最接近的匹配，然后将其用作重建的基础，通过：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/9.png"><br>where Ψ(·) is a learning function, W and b represents the weights and bias of the network.<br><code>其中，Ψ(·)是学习函数，W和b表示网络的权重和偏置。</code><br>In the demodulation block, the reconstructed waveform ˜z is classified to a label y˜(n) via:<br><code>在解调块中，通过以下方式将重构波形˜z分类为标签y˜(n) ：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/10.png"><br>where Φ(·) is a learning function.<br><code>其中Φ(·)是学习函数。</code><br>The focus of this paper is then to optimize the learning functions, Ψ(·) and Φ(·), and their corresponding weights and bias.<br><code>然后，本文的重点是优化学习函数，Ψ(·)和 Φ(·)，以及它们相应的权重和偏差。</code></p><h1 id="III-PROPOSED-DBN-BASED-DE-NOISING"><a href="#III-PROPOSED-DBN-BASED-DE-NOISING" class="headerlink" title="III. PROPOSED DBN-BASED DE-NOISING"></a>III. PROPOSED DBN-BASED DE-NOISING</h1><p><code>基于DBN的去噪方法</code><br>In this section, the de-noising algorithm, consisting of both the pre-processing method and the de-noising DBN, is described. An overview of our proposed algorithm is shown in Fig.2.<br><code>在本节中，描述了由预处理方法和去噪DBN组成的去噪算法。我们提出的算法的概述如图2所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/11.png" alt="Fig. 2. Overview of De-noising Block."><br><code>图二 去噪模块概述</code><br>The input is the received signal s(t) and the output is the reconstructed signal ˜z(t).<br><code>输入是接收信号s(t)并且输出是重构的信号˜z(t)。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/12.png"></p><p>A. Pre-Processing<br><code>A.预处理</code><br>The pixelization method is defined as:<br><code>像素化方法定义为：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/13.png"><br>The input signal s’(t) is first normalized into the range of 0 to 1.<br><code>输入信号s'(t)首先归一化到0到1的范围内。</code><br>We then proceed to pixelize the signal to form m(t).<br><code>然后我们继续对信号进行像素化以形成m(t)。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/14.png" alt="a"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/15.png" alt="b"><br>Let Pix be the number of pixels (length wise), which controls the resolution of the pixelization. The implemented pixelization algorithm is shown in Algorithm 1. The input and output of the pixelization is shown in Fig.3a and Fig.3b respectively.<br><code>设Pix为像素数（长度方向），它控制像素化的分辨率。所实现的像素化算法如算法1所示。像素化的输入和输出分别如图3a和图3b所示。</code><br><em><strong>Fig. 3. Visualization of the Pre-processing Block Diagram presents the received signal (input) and the pixelized matrix mi(t) (output). The received signal is pre-processed into 4 matrices to provide the DBN based de-noising algorithm with more features to extract.</strong></em><br><code>图三 预处理框图的可视化呈现了接收信号（输入）和像素化矩阵mi(t)（输出）。接收信号被预处理成4个矩阵，以提供基于DBN的去噪算法，以提取更多的特征。</code><br>Lastly, m(t) is resized to various resolutions, as shown in Fig. 3. This allows for more features to be extracted and used for the reconstruction.<br><code>最后，将m(t)调整为各种分辨率，如图3所示。这允许提取更多的特征并用于重建。</code></p><p>B. De-Noising DBN (Stacked RBMs)<br><code>B 去噪DBN（Stacked RBM）</code><br>DBNs are probabilistic generative algorithms which provide a joint probability distribution over observable data and labels.<br><code>DBN是概率生成算法，其提供可观察数据和标签的联合概率分布。</code><br>Restricted Boltzmann Machines (RBMs) are the building blocks of a DBN. Hence, in this section first we briefly describe RBMs and then we will explore DBN.<br><code>受限玻尔兹曼机（RBM）是DBN的构建块。因此，在本节中，我们首先简要介绍RBM，然后我们将探讨DBN。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/16.png" alt="Fig. 4. Overview of a DBN consisting of stacked RBMs."><br><code>图4 由堆叠RBM组成的DBN的概述。</code><br>Fig.4 illustrates the concept of stacking RBMs to form a DBN.<br><code>图4示出了堆叠RBM以形成DBN的概念。</code><br>A Boltzmann Machine (BM) is a particular form of a Markov Random Field (MRF), where its energy function is linear in its free parameters.<br><code>玻尔兹曼机（BM）是马尔可夫随机场（MRF）的一种特殊形式，其能量函数在其自由参数中是线性的。</code><br>Some of its variables (hidden units) allow the machine to represent complicated distributions internally. However, they are unobserved.<br><code>它的一些变量（隐藏单元）允许机器在内部表示复杂的分布。然而，它们未被观察到。</code></p><ol><li><p>RBMs: The energy function of the joint configuration in Boltzmann machines is given as follows:<br><code>RBM：玻尔兹曼机中关节构型的能量函数如下所示：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/17.png"><br>where the visible nodes v ∈ R correspond to the input and uv is the number of visible nodes, the hidden nodes h ∈ R represent the latent features and uh is the number of hidden nodes, W represents the concurrent weights linking the nodes of the visible to the hidden layer, b and c are the bias terms of the hidden and visible nodes respectively.<br><code>其中可见节点v ∈ R对应于输入，uv是可见节点的数量，隐藏节点h ∈ R表示潜在特征，uh是隐藏节点的数量，W表示将可见节点链接到隐藏层的并发权重，b和c分别是隐藏节点和可见节点的偏置项。</code><br>The free energy can also be expressed in the following form:<br><code>自由能也可以用以下形式表示：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/18.png"><br>Because visible and hidden units are conditionally independent of one-another, the following equations hold true.<br><code>因为可见和隐藏单元是有条件地相互独立的，所以下面的等式成立。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/19.png"><br>When binary units are used, so that vj and hk ∈ 0, 1,and a probabilistic version of the usual neural activation is obtained:<br><code>当使用二进制单位时，使得vj和hk ∈ 0，1，并且获得通常神经激活的概率版本：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/20.png"><br>The free energy of an RBM with binary units becomes<br><code>具有二进制单位的RBM的自由能变为</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/21.png"><br>Since RBMs are energy based algorithms, i.e. they associate a scalar energy to each configuration of the variables of interest, training them corresponds to modifying that energy function so that its shape has desirable properties, such as low energy configurations.<br><code>由于RBM是基于能量的算法，即它们将标量能量与感兴趣的变量的每个配置相关联，因此训练它们对应于修改该能量函数，使得其形状具有期望的属性，例如低能量配置。</code><br>Energy-based probabilistic models define a probability distribution through an energy function, as follows:<br><code>基于能量的概率模型通过能量函数定义概率分布，如下所示：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/22.png"><br>where Q is the partition function that is obtained via:<br><code>其中，Q是通过下式获得的配分函数：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/23.png"><br>To optimize the parameters of the network at each layer k, the following optimization problem shown by Eqn. 18 is minimize via partial differentiation with respects to W, b, c.<br><code>为了优化每个层k处的网络的参数，下面的由等式11所示的优化问题。18通过关于W、B、c偏微分最小化。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/24.png"></p></li><li><p>Stacking RBMs Into a DBN: ADBN is comprised of stacked restricted Boltzmann machines with a fast-learning algorithm that allows the structure to achieve better results with less computational effort.<br><code>将RBM堆叠到DBN中：ADBN由堆叠的受限玻尔兹曼机组成，具有快速学习算法，允许结构以更少的计算工作量获得更好的结果。</code><br>It models the joint distribution between an observed vector x and l hidden layers hk as follows:<br><code>它对观测向量x和l个隐藏层hk之间的联合分布建模如下：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/25.png"><br>where x = h0, P(hk|hk+1)) is a conditional distribution for the visible units conditioned on the hidden units of the RBM at level k,and P(hl−1|hl) is the visible-hidden joint distribution (output).<br><code>其中x = h 0，P(hk|hk+1)) 是以水平k处的RBM的隐藏单元为条件的可见单元的条件分布，并且P(hl−1|hl)是可见-隐藏联合分布（输出）。</code></p></li><li><p>Training: To train a DBN such that it can perform matrix de-noising, the normalized pixel values of the pixelized signal are used as input. By using min-max normalization, mi(t) is transformed into a floating-point number system with a range of 0 and 1.<br><code>训练：为了训练DBN，使其可以执行矩阵去噪，将像素化信号的归一化像素值用作输入。通过使用最小-最大归一化，mi(t)被转换成范围为0和1的浮点数系统。</code><br>The main idea is to train a DBN to be able to associate noisy mi(t) to mi(t) with lower noise or no noise.<br><code>主要思想是训练DBN，使其能够将有噪声的mi(t)与具有较低噪声或无噪声的mi(t)相关联。</code><br>This idea can be implemented by learning the features extracted from the noisy and clean mi(t) contents.<br><code>这个想法可以通过学习从噪声和干净的mi(t)内容中提取的特征来实现。</code><br>These features are then presented in some nodes at the last layer of the network.<br><code>然后，这些特征在网络的最后一层的一些节点中呈现。</code><br>The network is trained with a variety of noisy mi(t) as input and clean mi(t) as the desired output.<br><code>该网络是用各种噪声mi(t) 作为输入和干净mi(t) 作为期望输出来训练的。</code><br>Using a standard basis called relative activity to detect noise nodes, each node is defined as the difference between two values of a particular node which results from feeding the network a clean mi(t) and its corresponding noisy mi(t).<br><code>使用称为相对活动的标准基础来检测噪声节点，每个节点被定义为特定节点的两个值之间的差，这是通过向网络馈送干净的mi(t)及其相应的噪声mi(t)而产生的。</code><br>As a result, if a particular node is a noise node, it should have higher relative activity.<br><code>因此，如果特定节点是噪声节点，则它应该具有较高的相对活动性。</code><br>On the other hand, if it is a clean noiseless node, it should have a lower relative activity.<br><code>另一方面，如果它是一个干净的无噪声节点，它应该有一个较低的相对活动。</code><br>This theory is justified by the fact that the activation of mi(t) nodes should be same for both clean noiseless and its corresponding noisy mi(t).<br>该理论通过以下事实来证明：对于干净的无噪声和其对应的有噪声mi（t），mi(t)节点的激活应该是相同的。<br>By performing the above action for all mi(t) and averaging the values of the last layer’s nodes, the average relative activity of the last layer is computed. The nodes with a higher average relative activity are still viewed as noise nodes.<br><code>通过对所有mi(t)执行上述动作并对最后一层的节点的值求平均，计算最后一层的平均相对活动。具有较高平均相对活性的节点仍然被视为噪声节点。</code><br>Once the noise nodes are discovered, the next step is to lower their activity by selecting the average value of all the noise nodes as their neutral values.<br><code>一旦发现了噪声节点，下一步就是通过选择所有噪声节点的平均值作为它们的中性值来降低它们的活动性。</code><br>As such, the noise nodes are then considered inactive and a clean noiseless mi(t) can be reconstructed.<br><code>因此，噪声节点然后被认为是不活动的，并且可以重建干净的无噪声mi(t)。</code></p></li></ol><p>C. Results of DBN Based De-Noising<br><code>C.基于DBN的去噪结果</code><br>In this subsection, we evaluate the proposed DBN based de-noising technique.<br><code>在本小节中，我们评估了所提出的基于DBN的去噪技术。</code><br>As a baseline for comparison, we used the conventional MLE method devised in [38] and the de-noising auto encoder in [39].<br><code>作为比较的基线，我们使用[38]中设计的传统MLE方法和[39]中的去噪自动编码器。</code><br>To analyze the only performance of the de-noising capability, the system used was uncoded.<br><code>为了分析去噪能力的唯一性能，所使用的系统未编码。</code><br>For the following simulation experiments, the simulated BPSK dataset contains 100,000 transmitted signals periods, in which 50% is used for training, 20% on validation and the remaining 30% on testing.<br><code>对于下面的仿真实验，仿真的BPSK数据集包含100，000个发送信号周期，其中50%用于训练，20%用于验证，剩余的30%用于测试。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/26.png" alt="TABLE II MEAN AND STANDARD DEVIATION OF RANDOM DISTRIBUTION FOR SIMULATED CHANNEL PARAMETERS"><br><code>表二 模拟通道参数随机分布的平均值和标准差</code><br>The dataset was generated using Eq.6 and Table.II.<br><code>使用等式6和表II生成数据集。</code><br>The fc, sampling frequency fs and bit rate Rb of the BPSK signals were set at 2kHz, 40kHz and 1kbits/s.<br><code>BPSK信号的fc、采样频率fs和比特率Rb分别设置为2kHz、40 kHz和1 kbits/s。</code><br>The frequency of random change, fδ, was 2kHz. For consistency, the de-noising auto encoder used as a comparison in this section was trained using the same dataset.<br><code>随机变化频率fδ为2kHz。为了保持一致性，本节中用作比较的去噪自动编码器使用相同的数据集进行训练。</code><br>First, we conducted a simulation experiment to evaluate the proposed DBN based de-noising technique’s ability to remove noise for channels with extremely high noise.<br><code>首先，我们进行了一个模拟实验，以评估所提出的DBN为基础的去噪技术的能力，以消除噪声信道具有极高的噪声。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/27.png" alt="Fig. 5. De-noising BER Accuracy comparison with uncoded BPSK under AWGN channel. For consistency, MLE was used as the demodulation technique for all 3 methods."><br><code>图五 AWGN信道下与未编码BPSK的去噪误码率精度比较。为了保持一致性，MLE被用作所有3种方法的解调技术。</code><br>Fig.5 shows the Bit Error Rate (BER) of the proposed DBN based de-noising technique under the AWGN channel.<br><code>图5示出了所提出的基于DBN的去噪技术在AWGN信道下的误码率（BER）。</code><br>As a baseline for comparison, we have provided the BER of the MLE and de-noising auto encoder to highlight the substantial gains for highly negative Eb/No.<br><code>作为比较的基线，我们提供了MLE和去噪自动编码器的BER，以突出高度负Eb/No.</code><br>A reason for this could be the existence of noise invariable properties in the features extracted by the DBN in the proposed DBN based de-noising technique.<br><code>其原因可能是在所提出的基于DBN的去噪技术中由DBN提取的特征中存在噪声不变性质。</code><br>Evidence of this can be seen by the converging performance of the algorithm to the baseline as the noise level decreases, resulting in a decrease in functionality of the noise invariant property.<br><code>这一点的证据可以通过随着噪声水平降低算法收敛到基线的性能来看出，从而导致噪声不变属性的功能降低。</code><br>At BER of 10^−2, the performance of the proposed DBN based de-noising technique has a significantly smaller gain of 2.4dB for de-noising auto encoder and 2.6dB for the MLE.<br><code>在BER为10−2时，所提出的基于DBN的去噪技术的性能对于去噪自动编码器具有2.4dB的显著较小的增益，对于MLE具有2.6dB的增益。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/28.png" alt="Fig. 6. Visualization of proposed DBN based de-noising algorithm under AWGN channel presents the received signal (input) and the reconstructed signal (output). The reconstructed signal, depicted by the magenta line, is shown in relation to the transmitted signal (ideal output), depicted in green."><br><code>图6 在AWGN信道下，所提出的基于DBN的去噪算法的可视化呈现了接收信号（输入）和重构信号（输出）。由洋红线描绘的重构信号相对于由绿色描绘的传输信号（理想输出）示出。</code><br>Fig.6 shows a visualization of the de-noising outcome.<br><code>图6示出了去噪结果的可视化。</code><br>At Eb/No = −30dB, the received signal is highly distorted by the channel noise. However, the proposed technique is still able to partially predict the waveform shape.<br><code>在Eb/No = −30dB时，接收信号因信道噪声而严重失真。然而，所提出的技术仍然能够部分地预测波形形状。</code><br>At Eb No = −5dB, the waveform can be almost perfectly reconstructed.<br><code>在Eb/No = −5dB时，波形几乎可以完美地重建。</code><br>The second simulation experiment we conducted tested the algorithm’s ability to remain resolute against multi-path propagation.<br><code>第二个仿真实验，我们进行了测试的算法的能力，以保持坚决反对多径传播。</code><br>The simulated channel distorted received signals, utilized as test cases in this experiment, were modelled by Eq. 2.<br><code>本实验中用作测试案例的模拟信道失真接收信号由公式 2 建模。</code><br>The number of multi-paths in the dataset was distributed as 40% 1-path, 30% 2-paths and 30% 3-paths.<br><code>数据集中的多路径数量分布为40%的1路径，30%的2路径和30%的3路径。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/29.png" alt="Fig. 7. De-noising BER Accuracy comparison with uncoded BPSK under Multi-path channel, modelled in Eq.3. For consistency, MLE was used as the demodulation technique for all 3 methods."><br><code>图7 在多径信道下与未编码BPSK的去噪BER精度比较，在等式3中建模。为了保持一致性，MLE被用作所有3种方法的解调技术。</code><br>The results of the experiment are shown in Fig.7.<br><code>实验结果如图7所示。</code><br>With the increasing number of paths, the BER of the proposed DBN based de-noising algorithm achieves considerable gains while remaining relatively stable in comparison to the de-noising auto encoder and MLE.<br><code>随着路径数量的增加，所提出的基于DBN的去噪算法的BER实现了相当大的增益，同时与去噪自动编码器和MLE相比保持相对稳定。</code><br>Finally, to test the influence of the Doppler effect on the proposed DBN based de-noising algorithm, a simulation experiment was conducted using Eq.5 for the channel.<br><code>最后，为了测试多普勒效应对所提出的基于DBN的去噪算法的影响，使用等式5对信道进行仿真实验。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/30.png" alt="Fig. 8. De-noising Demodulation BER Accuracy comparison with uncoded BPSK, modelled in Eq.5. For consistency, MLE was used as the demodulation technique for all 3 methods."><br><code>图8 去噪解调BER精度与未编码BPSK的比较，如公式5所示。为了保持一致性，MLE被用作所有3种方法的解调技术。</code><br>Fig.8 depicts the results under three different scenarios, where the α =0.5, 1, 1.5 resulting in a fc =1kHz, 2kHz, 3kHz.<br><code>图8描述了三种不同情况下的结果，其中α =0.5，1，1.5导致fc =1kHz，2kHz，3kHz。</code><br>The BER of the proposed algorithm for all three scenarios are observed to be similar.<br><code>所提出的算法的BER为所有三种情况下观察到的是相似的。</code><br>Thus implying that for a certain range of α, the algorithm is able to remain relatively rigid to the influences of the Doppler effect.<br><code>因此，这意味着对于α的一定范围，该算法能够对多普勒效应的影响保持相对刚性。</code></p><p>A. Classification DBN<br><code>A.DBN分类</code><br>For the classification DBN, the same general stacked energy based RBM algorithm is used as described in Section IIIB. The input is the reconstructed signal ˜z(t) and the output consists of the respective binary labels ˜y(n) of 0s and 1s.<br><code>对于分类DBN，使用与第IIIB节中所述相同的基于一般堆叠能量的RBM算法。输入是重构信号˜z(t)，输出由0和1的相应二进制标记˜y(n)组成。</code></p><p>B. Determining the Structure of Classification DBN<br><code>B.DBN分类结构的确定</code><br>To determine the classification DBN structure, we investigated the influence of different network structures on the performance of the algorithm in the classification task at Eb/No = 0dB.<br><code>为了确定分类DBN结构，我们研究了不同网络结构对算法在Eb/No = 0dB的分类任务中的性能的影响。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/31.png" alt="TABLE III EFFECTS OF TRAINING EPOCH AND NUMBER OF NODES ON CLASSIFICATION DBN"><br><code>表III 训练时期和节点数对DBN分类的影响</code><br>As shown in Table III, we attempted nine different network structures, which consist of a varying number of hidden units, and trained them for a varying number of epochs.<br><code>如表III所示，我们尝试了九种不同的网络结构，这些结构由不同数量的隐藏单元组成，并对它们进行了不同数量的训练。</code><br>The best classification BER results obtained was using the [1250, 50] structure.<br><code>得到的最佳分类BER结果是使用[1250，50]结构。</code><br>Although the structure [1250, 100] seems to achieve approximately the same results, the time needed for training is significantly larger.<br><code>虽然结构[1250，100]似乎达到了大致相同的结果，但训练所需的时间明显更长。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/32.png" alt="TABLE IV THE FINAL HYPERPARAMETERS USED IN OUR PROPOSED CLASSIFICATION DBN, WHICH CONSISTS OF 2LAYERS OF RDMS"><br><code>表IV 在我们提出的分类DBN中使用的最终超参数，DBN由2层RDMS组成</code><br>To minimize complexity and maximize the performance of the algorithm, we have chosen to use the structure as illustrated in Table IV.<br><code>为了最小化算法的复杂性并最大化算法的性能，我们选择使用如表IV所示的结构。</code></p><p>C. Results of Classification DBN<br><code>C.DBN分类结果</code><br>In this subsection, we evaluate the proposed classification DBN.<br><code>在本小节中，我们评估了所提出的分类DBN。</code><br>As a baseline for comparison, we used the conventional MLE method devised in [38] to illustrate the similar performance of the demodulation techniques.<br><code>作为比较的基线，我们使用[38]中设计的传统MLE方法来说明解调技术的类似性能。</code><br>For the following simulation experiments, the simulated dataset contains 100,000 transmitted signals periods, in which 50% is used for training, 20% for validation, and the remaining 30% for testing.<br><code>对于以下仿真实验，仿真数据集包含100，000个发送信号周期，其中50%用于训练，20%用于验证，剩余30%用于测试。</code><br>The dataset was generated using a AWGN channel model at a range of Eb/No = −10dB to 30dB. For a fair comparison with MLE, the fc of the BPSK signals is set at 2kHz.<br><code>该数据集使用AWGN信道模型在Eb/No = −10dB至30dB范围内生成。为了与MLE进行公平比较，BPSK信号的fc被设置为2kHz。</code><br>Using the MLE as a baseline, this experiment illustrates that the demodulation performance level of the proposed classification DBN is similar to MLE.<br><code>使用MLE作为基线，该实验说明了所提出的分类DBN的解调性能水平与MLE相似。</code><br>The results are shown in Fig. 9. This implies that the classification DBN has learned to extract significant features from the PSK modulation scheme.<br><code>结果如图9所示。这意味着分类DBN已经学会从PSK调制方案中提取重要特征。</code><br>For a truly fair comparison, the proposed algorithm is also compared to Quadrature Phase Shift Keying (QPSK), derived in [25], without much extra training.<br><code>为了进行真正公平的比较，还将所提出的算法与在[25]中导出的正交相移键控（QPSK）进行比较，而无需进行过多的额外训练。</code><br>As seen, at BER 10−3, the algorithm’s performance for QPSK has a BER of 0.67dB less in comparison to MLE.<br><code>如图所示，在BER 10−3时，该算法对QPSK的性能比MLE低0.67dB。</code><br>A more inclusive training dataset for higher-order modulation schemes could increase the performance of the algorithm in this area.<br><code>高阶调制方案的更具包容性的训练数据集可以提高该算法在这一领域的性能。</code></p><h1 id="V-RESULTS-AND-DISCUSSION"><a href="#V-RESULTS-AND-DISCUSSION" class="headerlink" title="V. RESULTS AND DISCUSSION"></a>V. RESULTS AND DISCUSSION</h1><p><code>结果和讨论</code><br>This section will evaluate the proposed receiver as a whole as seen in Fig.1.<br><code>本节将评估所提出的接收器作为一个整体，如图1所示。</code><br>First, the performance of the receiver will analyzed using the simulated underwater model shown in Eq.6.<br><code>首先，将使用等式6中所示的模拟水下模型来分析接收器的性能。</code><br>Then, the conditions of the conducted sea trial will be described.<br><code>然后，将描述所进行的海上试验的条件。</code><br>Finally, the collected sea trial data was used to validate the real application of the proposed receiver system.<br><code>最后，利用采集的海试数据验证了接收机系统的真实的应用。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/33.png" alt="Fig. 10. Transmitted Data Structure."><br><code>图10 传输数据结构。</code><br>The data frame of the testing dataset used in both the simulation experiments and sea trials is shown in Fig.10.<br><code>在模拟实验和海上试验中使用的测试数据集的数据框架如图10所示。</code><br>The pilot consists of a single up-sweep and a down-sweep HFM signal, which is used for detection of the incoming received data signal.<br><code>导频由单个上扫和下扫HFM信号组成，其用于检测输入的接收数据信号。</code><br>The HFM modulated signal has a bit rate of 50 bits/s and a frequency range of 1-4kHz.<br><code>HFM调制信号具有50比特/秒的比特率和1-4kHz的频率范围。</code><br>The data frame includes 416 bits of coded BPSK modulated signals.<br><code>数据帧包括416比特的编码BPSK调制信号。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/34.png" alt="TABLE V SPECIFICATIONS ON THE SIMULATED AND SEA TRIAL DATA SET"><br><code>表V 模拟和海上试验数据集的规格</code><br>The specifications of the data structure is recorded in Table.V.<br><code>数据结构的规范记录在表V中。</code></p><p>A. Simulation Overall Results<br><code>A.模拟总体结果</code><br>In this subsection, we evaluate the overall proposed receiver system.<br><code>在这一小节中，我们评估了整个拟议的接收机系统。</code><br>To assess performance under a underwater environment, we will be employing 5 systems for evaluation – (1) MLE demodulation, (2) de-noising auto encoder with MLE demodulation, (3) the proposed DBN based receiver, (4) DL orthogonal frequency-division multiplexing (OFDM) [23], and (5) SIC DL [24].<br><code>为了评估水下环境下的性能，我们将采用5种系统进行评估-（1）MLE解调，（2）具有MLE解调的去噪自动编码器，（3）提出的基于DBN的接收器，（4）DL正交频分复用（OFDM）[23]，以及（5）SIC DL [24]。</code><br>For the following simulation experiments, the training data and channel model used to train the individual parts of the proposed receiver system were the same as stated in Section IV and V.<br><code>对于下面的仿真实验，用于训练所提出的接收机系统的各个部分的训练数据和信道模型与第IV和V节中所述的相同。</code><br>For equitable contrast, the de-noising auto encoder did not go through any extra training.<br><code>为了公平的对比度，去噪自动编码器没有经过任何额外的训练。</code><br>The simulated BPSK testing dataset contains 10,000 transmitted signals periods<br><code>模拟的BPSK测试数据集包含10，000个发送信号周期</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/35.png" alt="TABLE VI MEAN AND STANDARD DEVIATION OF RANDOM DISTRIBUTION FOR SIMULATED OVERALL CHANNEL PARAMETERS"><br><code>表6 模拟总体通道参数随机分布的平均值和标准差</code><br>The dataset was generated using Eq.6 and the random distributions seen in Table.VI.<br><code>使用公式6和表VI中的随机分布生成数据集。</code><br>The number of multi-paths in the dataset was distributed as 40% 1-path, 30% 2-paths and 30% 3-paths.<br><code>数据集中的多路径数量分布为40%的1路径，30%的2路径和30%的3路径。</code><br>The dataset contains dataset of 60% 1kHz fδ and 40% 2kHz fδ in each multi-path cluster.<br><code>该数据集在每个多径簇中包含60%1kHz fδ和40%2kHz fδ的数据集。</code><br>The increase in fδ is used to further simulate the complex occurrence of the underwater scattering.<br><code>fδ的增加用于进一步模拟水下散射的复杂发生。</code><br>To fairly evaluate the performance of the proposed receiver with the two systems mentioned above, the fc of the BPSK signals is set at 2kHz.<br><code>为了公平地评估所提出的接收机与上述两种系统的性能，BPSK信号的fc被设置为2kHz。</code><br>In a previous investigation seen in [40], we discovered that the feature extraction ability of the DBN has created a characteristic that is invariant to the influences of the Doppler effect.<br><code>在[40]中看到的以前的研究中，我们发现DBN的特征提取能力已经创建了一个对多普勒效应的影响不变的特征。</code><br>Therefore, we assume that even though the classification DBN was only trained on fc =2kHz, the performance of the proposed classification DBN will not be significantly degraded by the range of fc used in the testing dataset used.<br><code>因此，我们假设即使分类DBN仅在fc =2kHz上训练，所提出的分类DBN的性能也不会因所使用的测试数据集中使用的fc范围而显著降低。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/36.png" alt="Fig. 11. BER Accuracy comparison with coded BPSK and OFDM-BPSK under simulated underwater conditions, modelled by Eq.6."><br><code>图11 在模拟水下条件下，与编码BPSK和OFDM-BPSK的BER精度比较，由等式6建模。</code><br>Fig.11 depicts the performance of the five systems with regards to the above described testing scenario.<br><code>图11描绘了关于上述测试场景的五个系统的性能。</code><br>The proposed receiver (consisting of both DBN De-noise and DBN Classification) is seen to outperform the other algorithms over a large range of Eb No for both uncoded BPSK and OFDM-BPSK.<br><code>所提出的接收机（包括DBN去噪和DBN分类）被认为是优于其他算法在大范围的Eb No的未编码的BPSK和OFDM-BPSK。</code><br>This implies that the proposed receiver system is able to remain invariant to changes in instantaneous amplitude, phase and frequencies, such that the shown coding gain can be achieved.<br><code>这意味着所提出的接收机系统能够对瞬时幅度、相位和频率的变化保持不变，使得可以实现所示的编码增益。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/37.png" alt="TABLE VII ALGORITHMIC COMPUTATIONAL COMPLEXITY COMPARISON"><br><code>表VII 算法推理复杂性比较</code><br>Table.VII compares the computational complexity of the five algorithms, where n represents input size n for each function.<br><code>表VII比较了五种算法的计算复杂度，其中n表示每个函数的输入大小n。</code><br>The results show that our proposed system requires a large amount of training time in comparison to the auto encoder and MLE.<br><code>结果表明，我们提出的系统需要大量的训练时间相比，自动编码器和MLE。</code><br>However, shown in Fig.11, our proposed algorithm outperformed the auto encoder and MLE by 7.8dB and 12dB at Eb/N0 =5 and Eb/N0 =0 respectively for the BPSK modulated system.<br><code>然而，如图11所示，对于BPSK调制系统，我们提出的算法在Eb/N0 =5和Eb/N0 =0时分别优于自动编码器和MLE 7.8dB和12dB。</code></p><p>B. Sea Trial Set-up<br><code>B.海上试航设置</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/38.png" alt="Fig.12a Sea Trial End-to-End System Diagram"><br><code>图12a 海试端到端系统图</code><br>The communication system used in the underwater acoustic sea trial is depicted in Fig.12a.<br><code>图12a描述了水声海上试验中使用的通信系统。</code><br>Before transmission, the desired transmitted signal x(t) is converted from digital values to analog sensor signals using a National Instruments-Data Acquisition (NI-DAQ) hardware unit. The signal is then amplified before being transmitted.<br><code>在传输之前，使用National Instruments-Data Acquisition（NI-DAQ）硬件单元将期望的传输信号x(t)从数字值转换为模拟传感器信号。然后，信号在传输之前被放大。</code><br>At the receiver end, the signal is first received by the hydrophone and amplified by ISO-TECH IPS-3303.<br><code>在接收器端，信号首先由水听器接收并由ISO-TECH IPS-3303放大。</code><br>The corresponding NI-DAQ will translate the analog sensor signal to digital values for the proposed communication system.<br><code>相应的NI-DAQ将模拟传感器信号转换为数字值，用于拟议的通信系统。</code><br>In March 2019, a sea trial was conducted in the waters near Selat Pauh, Singapore, where the bottom is muddy with the deepest depth of approximately 25m.<br><code>2019年3月，在新加坡宝海峡附近的沃茨进行了海试，该水域底部泥泞，最深处约25米。</code><br>The waters is considered to be relatively stationary with occasional disturbance from the large vessels traveling to the port.<br><code>该沃茨被认为是相对静止的，偶尔会有大型船只驶入港口。</code><br>In this trial, the distance and depth of the transmitter and receiver was kept at about 300m and 9m respectively, with a variation of 50m and 1m due to the changing currents.<br><code>在这次试验中，发射器和接收器的距离和深度分别保持在300米和9米左右，由于水流的变化，变化了50米和1米。</code><br>The carrier frequency of the BPSK modulated signal was varied at 1kHz intervals for different trials.<br><code>对于不同的试验，BPSK调制信号的载波频率以1kHz的间隔变化。</code><br>The trials were conducted at fc =1kHz, fc =2kHz, fc =3kHz and fc =4kHz.<br><code>试验在fc =1kHz、fc =2kHz、fc =3kHz和fc =4kHz下进行。</code><br>Due to the limitations of the hardware used in the trial, the sampling rate was set at 40 kHz.<br><code>由于试验中使用的硬件的限制，采样率设置为40 kHz。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/39.png" alt="TABLE VIII SPECIFICATIONS ON THE SEA TRIAL"><br><code>表八 试航规格</code><br>The specifications of the sea trial is recorded in Table.VIII.<br><code>海试的技术要求见表VIII。</code></p><p>C. Sea Trial Results<br><code>C.海上试验结果</code><br>In this subsection, the collected data from the sea trial described in Section V-B was used to validate the real application of the proposed receiver system.<br><code>在本小节中，使用第V-B节中描述的海上试验中收集的数据来验证拟议接收器系统的真实的应用。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/un/39.png" alt="TABLE IX SEA TRIAL DATA RESULTS AND ACCURACY COMPARISON BETWEEN MLE WITH DOPPLER SYNCHRONIZATION AND PROPOSED RECEIVER SYSTEM"><br><code>表九 带多普勒同步的MLE和建议的接收器系统之间的SEA试验数据结果和精度比较</code><br>The results of which are shown in Table.IX.<br><code>其结果见表九。</code><br>The estimated average α was calculated using the up and down sweep of the HFM pilot signal and the SNR was estimated using MATLAB.<br><code>使用HFM导频信号的上下扫描计算估计的平均α，并且使用MATLAB估计SNR。</code><br>For this evaluation, we collected data for 10 trials.<br><code>在本次评估中，我们收集了10项试验的数据。</code><br>Trial 1-4 were conducted on Day 1 and the results obtained from the sea trial were significantly better than seen in Fig.11 with the most significant on Exp.3 with an coded BER of 0.0093 in comparison to BER of 0.045.<br><code>试验1-4在第1天进行，从海上试验中获得的结果明显好于图11中所示，其中实验3最显著，编码BER为0.0093，而BER为0.045。</code><br>This implies that during Day 1, the complexity of the channel was significantly lower than that simulated in the above trial.<br><code>这意味着在第1天期间，通道的复杂性显著低于上述试验中模拟的复杂性。</code><br>The data collected from Trials 5 and 6 on Day 2 had a much lower performance significance with an improvement of 0.08.<br><code>在第2天从试验5和6收集的数据具有低得多的性能显著性，改善为0.08。</code><br>On Day 3, while carrying out Exp. 7-10, we experienced heavy rain, which resulted in a more complex dataset.<br><code>在第3天，当执行Exp.7-10，我们经历了大雨，这导致了一个更复杂的数据集。</code><br>As such, the BER seen from the sea trials conducted on Day 3 shows similar performance to the simulated results.<br><code>因此，从第3天进行的海上试验中看到的BER显示出与模拟结果相似的性能。</code><br>Overall, our proposed receiver system is able to keep a significant performance improvement from the 10^−1 BER of the MLE with Doppler sync. to a 10^−2 BER.<br><code>总的来说，我们提出的接收机系统能够保持显着的性能改善，从10^-1误码率的MLE与多普勒同步。10^-2 BER</code></p><h1 id="VI-CONCLUSION-AND-OUTLOOK"><a href="#VI-CONCLUSION-AND-OUTLOOK" class="headerlink" title="VI. CONCLUSION AND OUTLOOK"></a>VI. CONCLUSION AND OUTLOOK</h1><p><code>结论与展望</code><br>In this paper, we have proposed a novel receiver system that uses DBNs to redesign the de-noising and demodulation techniques for underwater acoustic communications.<br><code>在本文中，我们提出了一种新的接收机系统，使用DBNs重新设计的去噪和解调技术的水声通信。</code><br>Our approach has also provided an interesting and important pathway for the application of machine learning techniques to underwater communications systems.<br><code>我们的方法也为机器学习技术在水下通信系统中的应用提供了一条有趣而重要的途径。</code><br>Firstly, although the performance of the receiver system matches performance of traditional systems, without significant improvement, in the AWGN channels, it does show better performance in the more realistically simulated underwater channels influenced by Doppler and multi-path.<br><code>首先，虽然该接收机系统的性能与传统系统的性能相匹配，但在AWGN信道中没有显着改善，但在更真实地模拟的受多普勒和多径影响的水下信道中，它确实表现出更好的性能。</code><br>A comparison with the traditional MLE and the promising de-noising auto encoder was completed in various underwater scenarios.<br><code>与传统的MLE和有前途的去噪自动编码器在各种水下场景的比较完成。</code><br>These simulated experiments revealed extremely competitive BER performances with a performance improvement of 13.2dB at 10^−3 BER.<br><code>这些模拟实验显示了极具竞争力的BER性能，在10^−3 BER时性能提高了13.2dB。</code><br>Therefore, demonstrating the powerful potential for machine learning to be used in more complex underwater acoustic channels.<br><code>因此，展示了机器学习在更复杂的水声信道中的强大潜力。</code><br>As a further investigation, we will increase the complexity by accommodating different mixtures of noise like rayleigh noise and exponential noise.<br><code>作为进一步的研究，我们将增加复杂性，通过容纳不同的混合噪声，如瑞利噪声和指数噪声。</code><br>As an additional step, we collected real life data, through a sea trial, to analyze the performance of the proposed receiver in a real scenario.<br><code>作为附加步骤，我们通过海上试验收集了真实的寿命数据，以分析所提出的接收器在真实的场景中的性能。</code><br>The results of which were promising with a substantial improvement from a coded 10^−1 BER using the traditional MLE method with Doppler synchronization to a coded 10^−2 BER with the proposed receiver.<br><code>其结果是有希望的，从使用具有多普勒同步的传统MLE方法的编码10^-1 BER到使用所提出的接收器的编码10^-2 BER有了实质性的改善。</code><br>This implies the real possibility of designing machine learning based underwater acoustic communication systems.<br><code>这意味着设计基于机器学习的水声通信系统的真实的可能性。</code><br>Finally, the strength of using DBNs to design our proposed receiver is denoted by its seemingly learned ability to comprehend and classify differing sets of received signals.<br><code>最后，使用DBN来设计我们提出的接收器的强度由其看似学习的能力来理解和分类不同的接收信号集来表示。</code><br>Despite the varying parameters– frequency, amplitude, phase and time frames between each random shift– of the scenarios we have chosen to examine the receivers under, our proposed receiver has remained relatively invariant with the largest variation of 5.2dB at an uncoded 10^−4 BER between the presence of 1-path and 3-paths.<br><code>尽管我们选择在不同的场景下检查接收器的参数-频率，幅度，相位和每个随机偏移之间的时间帧-，我们提出的接收器保持相对不变，在1-路径和3-路径之间的未编码10^-4 BER下的最大变化为5.2dB。</code><br>This phenomena suggests that the DBNs have successfully extracted meaningful features from the signals that could potentially be unchanged to the fluctuations of the underwater channel.<br><code>这种现象表明，DBN已经成功地从信号中提取了有意义的特征，这些特征可能对水下信道的波动保持不变。</code></p><h1 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h1><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>本文介绍了一种新的水下通信接收机系统，采用深度信念网络（DBN）技术应对多普勒效应和多径传播引起的信号失真。使用新的像素化算法将信号切分成帧，然后用基于DBN的去噪算法提取特征重建接收信号，最后对重构信号进行分类。<br>实验结果显示，该系统在受多普勒效应和多径传播影响的信道中性能优越，10−3比特误码率（BER）下性能提高了13.2dB。</p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>水声通信在海洋等领域具有许多应用，数据密集型水下系统需求增长。但水下环境为通信带来挑战，主要有两个方面：<br>(1)声波在水下传播速度很低，并在多个路径上传播，因此存在延迟，导致信号失真。<br>(2)声在水下表面反射、密度变化引起折射、能量损失引起时间延时。</p><p>研究学者已经探索机器学习和深度学习在通信领域的应用。例如，Y. Wang, “Modulation classification of underwater communication with deep learning network,” 利用深度学习来检测水下信道中的信号调制。其他研究如V. Q. Dang and Y. Pei, “A study on feature extraction of handwriting data using kernel method-based autoencoder,”使用基于深度神经网络的自编码器来调解接收信号。因此，我们可以预期，将ML技术和DL应用于通信块，将在物理层解码方面产生显著的改进。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文翻译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【笔记】利用Python进行数据分析</title>
      <link href="/2023/11/13/bi-ji-li-yong-python-jin-xing-shu-ju-fen-xi/"/>
      <url>/2023/11/13/bi-ji-li-yong-python-jin-xing-shu-ju-fen-xi/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h1 id="1、准备工作"><a href="#1、准备工作" class="headerlink" title="1、准备工作"></a>1、准备工作</h1><h2 id="重要的Python库"><a href="#重要的Python库" class="headerlink" title="重要的Python库"></a>重要的Python库</h2><ul><li>pandas </li><li>matplotlib </li><li>IPython和Jupyter </li><li>SciPy </li><li>scikit-learn </li><li>statsmodels</li></ul><p>NumPy NumPy（Numerical Python的简称）是Python科学计算的基础包。本书大部分内容都基于NumPy以及构建于其上的库。它提供了以下功能（不限于此）：</p><ul><li>快速高效的多维数组对象ndarray。</li><li>用于对数组执行元素级计算以及直接对数组执行数学运算的函数。</li><li>用于读写硬盘上基于数组的数据集的工具。</li><li>线性代数运算、傅里叶变换，以及随机数生成。</li></ul><p>NumPy在数据分析方面的主要作用</p><ul><li>为Python提供快速的数组处理能力</li><li>作为在算法和库之间传递数据的容器。对于数值型数据，NumPy数组在存储和处理数据时要比内置的Python数据结构高效得多。</li><li>由低级语言（比如C和Fortran）编写的库可以直接操作NumPy数组中的数据，无需进行任何数据复制工作。</li></ul><p>pandas提供了快速便捷处理结构化数据的大量数据结构和函数。<br>本书用得最多的pandas对象是DataFrame，它是一个面向列（column-oriented）的二维表结构，另一个是Series，一个一维的标签化数组对象。</p><p>Python社区已经广泛采取了一些常用模块的命名惯例：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import pandas as pd</span><br><span class="line">import seaborn as sns</span><br><span class="line">import statsmodels as sm</span><br></pre></td></tr></tbody></table></figure></body></html>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【学习路线图】深度学习从入门到入土</title>
      <link href="/2023/11/11/xue-xi-lu-xian-tu-shen-du-xue-xi-cong-ru-men-dao-ru-tu/"/>
      <url>/2023/11/11/xue-xi-lu-xian-tu-shen-du-xue-xi-cong-ru-men-dao-ru-tu/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><em><strong>参考视频</strong></em>：</p><p><a href="https://www.bilibili.com/video/BV1yg411K72z/?vd_source=2f15c73d6c41bdc70cb28cbb3b6f68cf">三个月从零入门深度学习，保姆级学习路线图</a><br><a href="https://www.bilibili.com/video/BV1sd4y1y7Dp/?vd_source=2f15c73d6c41bdc70cb28cbb3b6f68cf">多篇SCI学长教你如何快速入门深度学习！</a><br><a href="https://www.bilibili.com/video/BV16341177c1/?spm_id_from=333.999.0.0&amp;vd_source=2f15c73d6c41bdc70cb28cbb3b6f68cf">AI学习路线分享|做完这些，你已经成为机器学习方面的专家</a></p><h1 id="1、基础部分"><a href="#1、基础部分" class="headerlink" title="1、基础部分"></a>1、基础部分</h1><h3 id="1-1-数学"><a href="#1-1-数学" class="headerlink" title="1-1 数学"></a>1-1 数学</h3><p>高数：导数/微分/积分、梯度、泰勒展开公式<br>线代：向量、矩阵、运算、范数、特征向量和特征值、泰勒展开公式<br>概率论：条件概率、期望</p><p><em><strong>推荐书籍</strong></em>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">《白话机器学习的数学》</span><br><span class="line">《机器学习的数学》</span><br><span class="line">《Mathematics for Machine Learning》</span><br></pre></td></tr></tbody></table></figure><h3 id="1-2-python"><a href="#1-2-python" class="headerlink" title="1-2 python"></a>1-2 python</h3><p>1）<a href="https://www.liaoxuefeng.com/wiki/1016959663602400">廖雪峰官网免费教程</a><br>从第一讲看到常用的第三方模块<br>2）<a href="https://www.runoob.com/python/python-tutorial.html">Python菜鸟教程 </a></p><blockquote><p>1、Python基础知识<br>2、Python函数<br>3、Python面向对象编程</p></blockquote><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Numpy：Numpy数据类型、Numpy常用函数</span><br><span class="line">Pandas：Pandas的简洁、Pandas数组属性、Pandas数据处理、Pandas函数</span><br></pre></td></tr></tbody></table></figure><p>3）<a href="https://www.bilibili.com/video/BV1c4411e77t/?spm_id_from=333.999.0.0">【Python教程】《零基础入门学习Python》小甲鱼</a></p><blockquote><p>时间充足可以看下面视频<br><a href="https://www.bilibili.com/video/BV1v4411B7Zv/?vd_source=2f15c73d6c41bdc70cb28cbb3b6f68cf">(强推)Python面向对象编程五步曲-从零到就业【上】</a><br><a href="https://www.bilibili.com/video/BV1jE411871a/?spm_id_from=333.788.recommend_more_video.0&amp;vd_source=2f15c73d6c41bdc70cb28cbb3b6f68cf">(强推)Python面向对象编程五步曲-从零到就业【中】</a><br><a href="https://www.bilibili.com/video/BV1ag4y1q7ib/?spm_id_from=333.788.recommend_more_video.0&amp;vd_source=2f15c73d6c41bdc70cb28cbb3b6f68cf">(强推)Python面向对象编程五步曲-从零到就业【下】</a></p></blockquote><p>4）python实战<br><a href="https://github.com/iamseancheney/python_for_data_analysis_2nd_chinese_version">《利用Python进行数据分析》</a><br><em><strong>《Python编程从入门到实践第3版》</strong></em></p><h1 id="2、机器学习理论入门"><a href="#2、机器学习理论入门" class="headerlink" title="2、机器学习理论入门"></a>2、机器学习理论入门</h1><p>1）李航老师<em><strong>《统计学习》</strong></em></p><ul><li><p><em><strong>三个准则</strong></em></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作为入门选手，不要每章都看</span><br><span class="line">不要从零造轮子去实现算法，太浪费时间</span><br><span class="line">必须能手推公式</span><br></pre></td></tr></tbody></table></figure></li><li><p><em><strong>必看目录</strong></em></p><blockquote><ol><li>统计学习概论</li><li>感知机</li><li>朴素贝叶斯</li><li>决策树</li><li>逻辑回归和最大熵</li><li>提升树 </li><li><a href="https://zhuanlan.zhihu.com/p/290964953">Xgboost完全详细解读（原理+代码）</a></li></ol></blockquote></li></ul><p>2）公式推导：<a href="https://www.cnblogs.com/pinard/category/894692.html?page=4">刘建平博客</a><br>代码实现：<a href="https://github.com/ljpzzz/machinelearning">刘建平github</a><br><a href="https://www.bilibili.com/video/BV1No4y1o7ac/?spm_id_from=333.999.0.0&amp;vd_source=2f15c73d6c41bdc70cb28cbb3b6f68cf">【合集】十分钟 机器学习 系列视频 《统计学习方法》</a><br><a href="https://www.bilibili.com/video/BV1aE411o7qd/?spm_id_from=333.999.0.0">【机器学习】【白板推导系列】</a><br>3）<a href="https://www.bilibili.com/video/BV1Pa411X76s/?spm_id_from=333.999.0.0&amp;vd_source=2f15c73d6c41bdc70cb28cbb3b6f68cf">(强推|双字)2022吴恩达机器学习Deeplearning.ai课程</a><br><em><strong>《吴恩达机器学习手册》</strong></em><br>4）<a href="https://www.bilibili.com/video/BV1Wv411h7kN/?spm_id_from=333.999.0.0&amp;vd_source=2f15c73d6c41bdc70cb28cbb3b6f68cf">(强推)李宏毅2021/2022春机器学习课程</a><br>5）<a href="https://www.bilibili.com/video/BV1qf4y1x7kB/?spm_id_from=333.999.0.0&amp;vd_source=2f15c73d6c41bdc70cb28cbb3b6f68cf">(强推)浙江大学-机器学习</a></p><h1 id="3-机器学习实战入门"><a href="#3-机器学习实战入门" class="headerlink" title="3.机器学习实战入门"></a>3.机器学习实战入门</h1><p><em><strong>《阿里云天池大赛赛题解析（机器学习篇）》</strong></em></p><p><a href="https://tianchi.aliyun.com/specials/promotion/bookcode">开源代码</a><br>四个任务：</p><blockquote><ol><li>工业蒸汽预测</li><li>天猫用户重复购买预测</li><li>O2O优惠券预测</li><li>阿里云安全恶意程序检测</li></ol></blockquote><p>怎么看:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">有四个任务是吧，你挑其中的一个或者两个，不需要都看，没必要；</span><br><span class="line"></span><br><span class="line">怎么确定把这一个或者两个任务吃透呢？</span><br><span class="line"></span><br><span class="line">七个步骤：赛题理解、数据探索、特征工程、模型训练、模型验证、特征优化、模型融合7个步骤</span><br><span class="line"></span><br><span class="line">四个任务中挑一个或者两个，在一周，七天的话，三天看一个，七天看两个，或者七天你就看一个，</span><br><span class="line">比如第一个，把它吃透就够了；</span><br></pre></td></tr></tbody></table></figure><h1 id="4-深度学习理论入门"><a href="#4-深度学习理论入门" class="headerlink" title="4.深度学习理论入门"></a>4.深度学习理论入门</h1><p>1）李沐<em><strong>《动手学深度学习》</strong></em></p><ul><li><a href="https://space.bilibili.com/1567748478/video?tid=0&amp;pn=6&amp;keyword=&amp;order=pubdate">B站链接</a></li><li><a href="https://courses.d2l.ai/zh-v2/">课程主页</a></li><li><a href="https://zh.d2l.ai/">教材</a></li><li><a href="https://discuss.d2l.ai/c/16">课程论坛讨论</a></li><li><a href="https://discuss.pytorch.org/">Pytorch论坛</a></li></ul><p>2）<em><strong>计算机视觉（CV）</strong></em>*</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">图像理解：分类、检测、分割、追踪</span><br><span class="line">图像生成：GAN模型</span><br></pre></td></tr></tbody></table></figure><p>课程链接：<a href="https://www.bilibili.com/video/BV1nJ411z7fe/?from=search&amp;seid=7594775480695125527&amp;spm_id_from=333.337.0.0">cs231n</a><br><em><strong>要求：</strong></em></p><blockquote><ol><li><p>一共33讲，不要都看；作为入门来说，主要是学习p1-p22;</p></li><li><p><a href="https://github.com/rishabh-16/cs231n-2019-assignments">完成作业：1，2，第三个不用看</a><br>不要自己从零开始做这个作业，直接看代码怎么实现的，理解代码</p></li><li><p>学到什么程度？<br>反向传播梯度回传，损失函数，优化算法，多层感知机，卷积神经网络，普通的循环神经网络，<br>以及一些dropout和BN掌握住</p></li></ol></blockquote><p>3）<em><strong>自然语言处理（NLP）</strong></em></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">文本分类、文本匹配、序列标注、文本生成</span><br></pre></td></tr></tbody></table></figure><p>课程链接：<a href="https://www.bilibili.com/video/BV1pt411h7aT?from=search&amp;seid=11365156277928976769&amp;spm_id_from=333.337.0.0">cs224n</a><br><em><strong>要求</strong></em></p><blockquote><ol><li><p>一共18讲，看P1-P5和P8，P9,P11；</p></li><li><p><a href="https://github.com/Luvata/CS224N-2019/tree/master/CS224N-2019/">完成作业</a>：重点看a1，a2，a4，a5；其实a5这个不做的话，也没问题，把前面给的这个三个一定自己走一遍</p></li><li><p>学到什么程度</p></li></ol><p> 通过看这个视频你要能够达到什么地步呢？</p><p> 其实这个视频和cs231n在基础部分是重叠的，对于基础部分，大家可以都看，两者兼学会更好</p><p> 必须熟悉的掌握：反向传播，词向量，RNN，GRU，Lstm，Seq2Seq以及attention机制；初步了解卷积神经网络；</p></blockquote><p>4）必看的一本书：<br><a href="https://nndl.github.io/nndl-book.pdf">邱锡鹏《神经网络与深度学习》</a></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">举个例子，比如说看完网络优化和正则化了视频讲解没有搞定；找到这本书对应的章节，</span><br><span class="line">比如第七章看一遍，邱老师是怎么讲解的；</span><br></pre></td></tr></tbody></table></figure><p>5）PyTorch框架学习<br><a href="https://www.bilibili.com/video/BV1hE411t7RN/?spm_id_from=333.337.search-card.all.click">小土堆</a></p><p><a href="https://www.bilibili.com/video/BV1Y7411d7Ys/?spm_id_from=333.337.search-card.all.click">刘二大人</a>：<a href="https://blog.csdn.net/bit452/category_10569531.html">源代码</a></p><p><a href="https://github.com/yunjey/pytorch-tutorial">一个入门学习的仓库</a>：写的非常好，浅显易懂，没有一个多余的代码</p><p><a href="https://github.com/lucidrains">深度学习模型</a></p><blockquote><p>1.环境配置:确保你的开发环境满足项目的依赖要求，有些项目可能需要特定的Python版本、特定的库或特定的配置。<br>2.数据集:不要局限于配置数据集，这意味着你可以使用任何形状和大小的随机数作为输入， 但请注意，某些模型或算法可能对输入数据的形状和类型有特定的要求。<br>3.一行一行感受:这是一种很好的学习方法，可以帮助你理解代码的逻辑和流程。你可以逐行运行代码， 观察每一步的结果，这样可以帮助你更好地理解代码是如何工作的。<br>4.错误和异常:在运行代码时，可能会遇到错误或异常，不要害怕这些，它们是学习过程中的一部分，当你遇到问题时，尝试理解为什么会出错，并查找解决方案。<br>5.文档和注释:查看项目的文档和注释，这可以帮助你更好地理解代码的功能和工作原理。<br>6.测试:尝试修改输入或代码，看看结果如何变化，这样可以帮助你更好地理解代码的健壮性和可变性。</p></blockquote><p><a href="https://nn.labml.ai/">逐行代码解读</a></p><p><em><strong>《PyTorch函数手册》</strong></em><br>6）Tensorflow<br><a href="https://www.bilibili.com/video/BV1B7411L7Qt/?spm_id_from=333.337.search-card.all.click&amp;vd_source=2f15c73d6c41bdc70cb28cbb3b6f68cf">【北京大学】Tensorflow2.0</a></p><h1 id="5-深度学习实战入门"><a href="#5-深度学习实战入门" class="headerlink" title="5.深度学习实战入门"></a>5.深度学习实战入门</h1><blockquote><p>要记住我们学习深度学习是为了实战</p></blockquote><p>两个学习曲线非常平滑的项目：<br>一个是新闻分类项目，一个是街景字符识别，也就是图片分类项目，有的人可能会认为这两个项目非常简单，但是我认为千万不要小瞧这两个项目，扎扎实实做完这两个项目，对你的帮助绝对比你想象的要大<br>1）<em><strong>自然语言处理-新闻文本分类</strong></em></p><blockquote><p>1.<a href="https://tianchi.aliyun.com/notebook/118252">赛题解析</a><br>2.<a href="https://tianchi.aliyun.com/notebook/118253">分析数据</a><br>3.<a href="https://tianchi.aliyun.com/notebook/118254">基于机器学习的文本分类任务</a><br>4.不同深度学习模型：</p><blockquote><p>1、<a href="https://tianchi.aliyun.com/notebook/118255">fastext</a><br>2、<a href="https://tianchi.aliyun.com/notebook/118268">Word2vec</a><br>3、<a href="https://tianchi.aliyun.com/notebook/118258">CNN 做文本分类</a><br>4、<a href="https://tianchi.aliyun.com/notebook/118259">RNN做文本分类</a><br>5、<a href="https://tianchi.aliyun.com/notebook/118260">BERT做文本分类</a>-这个可以先不看，等你入了深度学习的门，认为自己想搞NLP这个方向了，你再去看相关的论文</p></blockquote></blockquote><p>2）<em><strong>计算机视觉-街景符号识别</strong></em></p><blockquote><p>1.<a href="https://tianchi.aliyun.com/notebook/108659">赛题理解</a><br>2.<a href="https://tianchi.aliyun.com/notebook/108150">数据读取与数据扩增</a><br>3.<a href="https://tianchi.aliyun.com/notebook/108711">构建模型</a><br>4.<a href="https://tianchi.aliyun.com/notebook/108780">模型的训练与验证</a><br>5.<a href="https://tianchi.aliyun.com/notebook/108656">模型的集成</a></p></blockquote><p><em><strong>要求</strong></em></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">必须弄清楚每行代码</span><br><span class="line">尽情调参</span><br></pre></td></tr></tbody></table></figure><h1 id="6-面试题"><a href="#6-面试题" class="headerlink" title="6.面试题"></a>6.面试题</h1><p><em><strong>《百面机器学习》</strong></em></p></body></html>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【笔记】The Craft of Research</title>
      <link href="/2023/11/09/bi-ji-the-craft-of-research/"/>
      <url>/2023/11/09/bi-ji-the-craft-of-research/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h1 id="1-跟读者建立联系"><a href="#1-跟读者建立联系" class="headerlink" title="1.跟读者建立联系"></a>1.跟读者建立联系</h1><p><a href="https://www.bilibili.com/video/BV1hY411T7vy/?spm_id_from=333.999.0.0&amp;vd_source=2f15c73d6c41bdc70cb28cbb3b6f68cf">跟读者建立联系【研究的艺术·一】</a></p><h2 id="两件事情"><a href="#两件事情" class="headerlink" title="两件事情"></a>两件事情</h2><p>语文或英语成绩好不代表你的论文写作就很好。</p><p>如果是30岁左右在国内接受教育的话（跟老师一样背景的话）很有可能你在国内教育的时候，没有接受过系统的专业论文的写作；</p><p>如果你在20岁的话，可能在语文和英语课里面写的作文跟 论文所要求的那一种格式不一样；</p><p>研究做了一些年，甚至发表过一些顶级的会议和期刊的情况下，很有可能论文写作没有你想象的那么好（可能是能够符合发表的标准；写东西比较快）要写得很好 很优美 很清晰 是需要大量的练习的，这还有很多的提升空间的。 </p><h2 id="好的写作能带来的好处"><a href="#好的写作能带来的好处" class="headerlink" title="好的写作能带来的好处"></a>好的写作能带来的好处</h2><p>让你的思考更加的深入，更加的有条理；<br>让更多的人愿意读你的论文，之后论文的影响力；<br>参考书名：《The Craft of Research, Fourth Edition (Chicago Guides to Writing, Editing, and Publishing)》</p><p>这本书不仅对论文写作有用，对商业的写作也是有用的（总结报告、项目计划等）；<br>如果是英语写作的话，强烈建议读它的英语版本的；<br>当然中文版也是可以去读的（大部分东西是跨语言的）；<br>过一遍可能是不够的，需要再去读几遍。</p><h2 id="The-Craft-of-Research-的介绍"><a href="#The-Craft-of-Research-的介绍" class="headerlink" title="The Craft of Research 的介绍"></a>The Craft of Research 的介绍</h2><p>这本书是整个芝加哥 关于写作系列丛书之一；<br>这些丛书中，这几本也是非常有名的：<br>《The Chicago Guide to Grammar, Usage, and Punctuation》：关于语法使用和标点符号的写作指导，是非常基本的 也是权威的一本关于英语写作的书；<br>《A Manual for Writers of Research Papers, Theses, and Dissertations》：关于写研究论文的比较基础的书；<br>这本书是教怎么样去讲这个故事，并将这个故事写下来；<br>书的版本不同可能是在找文献这一块不同，可能看前面的版本也问题不大；<br>在作者上面均是大学英语系的，在写作上面应该是比较权威的（用词与写作方法上），但是在举的例子上，比较偏文学和自然科学（写作的想法是相通的，适用范围广）；</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>第一部分，提出了一个核心的观点：在写作的时候，永远要知道你的读者是谁，他们想要知道什么；</li><li>第二部分，关于是 怎么样去问问题，怎么样去找答案；</li><li>第三部分，是讲 怎么样讲一个故事（怎么样提一个论点，怎么样安排论据来支撑你的论点）；</li><li>第四部分，是讲 怎么样把这个故事给你写下来（关于写作主要在这个部分）；</li></ul><p>开始讲第一部分——研究、研究者和读者：</p><p>本书的受众很广：可以是 正准备做第一个研究项目的本科生，也可以是比较年轻的研究者，本课的受众就缩小了范围（可以是研究项目已经做到一半了或者快要做完了，开始准备想怎么写论文；或已经做过了几个研究写过了几篇论文但想提升论文写作的），所以跳过了前言部分（当然也是可以去读一下的） </p><p>1.1 比较偏背景的知识，其中介绍了研究是什么东西<br>（研究就是要去收集信息来回答一个疑问，这个回答完之后呢能解决某一个问题（比较偏社会和自然学科这一块））<br>对于技术类来说，收集信息可能是提出一个新的解决方案，然后去做一些实验来证实他的有效性；方法是解决某一个问题（不会把question 和 problem 在两个东西分开），但是这里为了使得我们去想这件事。</p><p>1.2 为什么要去写一篇文章：</p><ul><li>写文章自己会记得；</li><li>写的时候会帮助你理解事情；</li><li>写作可以用来测试我们的想法；</li></ul><p>所以说，写作使我们，记得更加精确，理解更好以及评估我们的想法是不是客观的。 </p><p>1.3 为什么要用一个正式的论文格式：<br>研究就是跟 在同一个研究领域的同行之间进行交流，而论文的格式可认为是通讯中的协议，为了方便个体之间相互快速的理解研究的内容。 </p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文写作 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文翻译】A Transformer-Based Deep Learning Network for Underwater Acoustic Target Recognition</title>
      <link href="/2023/11/04/lun-wen-fan-yi-a-transformer-based-deep-learning-network-for-underwater-acoustic-target-recognition/"/>
      <url>/2023/11/04/lun-wen-fan-yi-a-transformer-based-deep-learning-network-for-underwater-acoustic-target-recognition/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Underwater acoustic target recognition (UATR) is usually difficult due to the complex and multipath underwater environment.<br><code>由于水下环境的复杂性和多径性，水声目标识别一直是一个难点。</code><br>Currently, deep-learning(DL)-based UATR methods have proved their effectiveness and have outperformed the traditional methods by using powerful convolution neural networks (CNNs) to extract discriminative features on acoustic spectrograms.<br><code>目前，基于深度学习（DL）的UATR方法已经证明了它们的有效性，并且通过使用强大的卷积神经网络（CNN）来提取声谱图上的区分特征，其性能优于传统方法。</code><br>However, CNNs always fail to capture the global information implicated in the spectrogram due to the use of a small kernel and thus encounter the performance bottleneck.<br><code>然而，由于使用小的内核，CNN总是无法捕获频谱图中包含的全局信息，从而遇到性能瓶颈。</code><br>To this end, we propose the UATR-transformer based on a convolution-free architecture, referred to as the transformer, which can perceive both the global and local information from acoustic spectrograms, and thus improve the accuracy.<br><code>为此，我们提出了基于无卷积架构的UATR-transformer，称为Transformer，它可以从声谱图中感知全局和局部信息，从而提高准确性。</code><br>Experiments on two real-world data demonstrate that our proposed model has achieved comparative results to the state of art CNNs and thus can be applied to certain cases in UATR.<br><code>在两个真实数据上的实验表明，我们提出的模型与最先进的CNN取得了比较结果，因此可以应用于UATR中的某些情况。</code><br><em><strong>Index Terms</strong></em>—Acoustic spectrogram, convolution-free, transformer, underwater acoustic target recognition (UATR).<br><code>索引词——声谱图，无卷积，Transformer，水声目标识别。</code></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1><p>Underwater acoustic target recognition (UATR) is one of the major functions in a sonar system.<br><code>水声目标识别是声纳系统的主要功能之一。</code><br>It has attracted much attention due to its wide applications in the detection and classification of underwater targets, measurement of the impact of ship radiated noise, and the design of abandoned ships [1].<br><code>由于其在水下目标的检测和分类、舰船辐射噪声影响的测量以及废弃舰船的设计等方面的广泛应用，引起了人们的广泛关注[1]。</code><br>However, because of the complicated ship noise radiation itself and various experimental interference, it is challenged in the case of the complex marine environment and the fact that underwater acoustic signals can be easily affected by various physical variables.<br><code>然而，由于船舶噪声辐射本身的复杂性和各种实验干扰，在复杂的海洋环境和水声信号易受各种物理量影响的情况下，对船舶噪声辐射的研究面临着挑战。</code><br>Currently, the issue of UATR can be commonly solved by time–frequency (T–F) analysis, for example, the low frequency analysis and recording (LOFAR) [2], detection of envelope modulation on noise (DEMON) analysis [3], and the Mel-frequency cepstral coefficient (MFCC) [4] inspired by human auditory perception.<br><code>目前，UATR的问题通常可以通过时频（T-F）分析来解决，例如，低频分析和记录（LOFAR）[2]，噪声包络调制检测（DEMON）分析[3]以及受人类听觉感知启发的Mel频率倒谱系数（MFCC）[4]。</code><br>On the other hand, to automatically classify the underwater acoustic signals with efficiency, deep-learning (DL)-based UATR has become a major trend of development.<br><code>另一方面，为了有效地自动分类水声信号，基于深度学习（DL）的UATR已成为发展的主要趋势。</code><br>One of the major challenges is that the underwater acoustic spectrogram, unlike common images or audio spectrograms, lacks sufficient texture features due to multivariate influences in the marine environment such as ambient noise interference, multipath interference, and transmission loss.<br><code>其中一个主要的挑战是，水下声谱图，不像普通的图像或音频声谱图，缺乏足够的纹理特征，由于在海洋环境中的多变量影响，如环境噪声干扰，多径干扰，和传输损耗。</code><br>As such, it merely provides a certain correlation between the time and frequency axes, which implies that a powerful DL model with strong robustness is needed.<br><code>因此，它仅提供时间轴和频率轴之间的某种相关性，这意味着需要具有强鲁棒性的强大DL模型。</code><br>Owing to the convolution neural networks (CNNs) and their variations, DL-based UATR has made certain results.<br><code>由于卷积神经网络（CNN）及其变体，基于DL的UATR取得了一定的成果。</code><br>For example, Hu et al. [5] studied a novel DL model by introducing depthwise (DW) separable and time-dilated convolution for passive UATR and has proved its effectiveness compared with traditional UATR methods.<br><code>例如，Hu等人[5]通过引入用于被动UATR的dependency（DW）可分离和时间扩张卷积来研究一种新的DL模型，并与传统UATR方法相比证明了其有效性。</code><br>Zhang et al. [6] combined the recurrent neural network (RNN) and CNN to design a modulation recognition model for underwater acoustic signals.<br><code>Zhang等人[6]将递归神经网络（RNN）和CNN相结合，设计了一种水声信号的调制识别模型。</code><br>Cao et al. [7] studied an end-to-end clas53 sification model that combines the CNN with second-order pooling to capture the temporal correlations from the T–F spectrogram of the radiated acoustic signals.<br><code>Cao等人[7]研究了一种端到端分类模型，该模型将CNN与二阶池化相结合，以从辐射声信号的T-F频谱图中捕获时间相关性。</code><br>In another work [1], a hybrid CNN and RNN model is also proposed to extract multidimensional T–F features from spectrograms, which can effectively achieve robust feature extraction and UATR.<br><code>在另一项工作[1]中，还提出了一种混合CNN和RNN模型来从谱图中提取多维T-F特征，该模型可以有效地实现鲁棒的特征提取和UATR。</code><br>In all, they research leverage convolution mechanism to continuously enrich the manual or automatic features by sliding a kernel window on acoustic data and expect to improve the recognition performance by various designs of CNN structure.<br><code>总之，他们研究利用卷积机制，通过在声学数据上滑动内核窗口来不断丰富手动或自动特征，并期望通过CNN结构的各种设计来提高识别性能。</code><br>Nevertheless, for CNN-based UATR methods, only parts of the acoustic spectrogram that are close enough to fit within the kernel size can interact with each other in a convolution layer.<br><code>然而，对于基于CNN的UATR方法，只有足够接近以适合核大小的声谱图的部分可以在卷积层中相互作用。</code><br>While for the items that are further apart, the global information between time and frequency segments is rarely considered.<br><code>而对于距离较远的项目，很少考虑时间段和频率段之间的全局信息。</code><br>In this case, the existing CNN-based UATR methods encounter performance bottlenecks and therefore may not achieve satisfactory results.<br><code>在这种情况下，现有的基于CNN的UATR方法遇到性能瓶颈，因此可能无法获得令人满意的结果。</code><br>To this end, we propose the UATR-transformer model for UATR based on transformer [8], which is known as a convolution-free architecture.<br><code>为此，我们基于transformer [8]提出了UATR的UATR-transformer模型，这被称为无卷积架构。</code><br>Currently, transformer seems to achieve state-of-the-art results and has shown its great potential in various scientific fields.<br><code>目前，Transformer已经取得了很好的研究成果，并在各个科学领域显示出了巨大的潜力。</code><br>Particularly, its great success in audio processing further enhances the feasibility for underwater acoustic research, such as the audio spectrogram transformer (AST) [9] inspired by data efficient image transformer (DeiT) [10], and the hierarchical token-semantic audio transformer (HTS-AT) [11] inspired by Swin transformer [12].<br><code>特别是，其在音频处理方面的巨大成功进一步增强了水下声学研究的可行性，例如受数据高效图像Transformer（DeiT）[10]启发的音频频谱图Transformer（AST）[9]，以及受Swin Transformer [12]启发的分层令牌语义音频Transformer（HTS-AT）[11]。</code><br>Compared with the CNNs for UATR, the transformer architecture is believed to perceive both global and local information from the acoustic spectrogram.<br><code>与用于UATR的CNN相比，Transformer架构被认为从声谱图中感知全局和局部信息。</code><br>And the experimental results based on two real-world data show that our proposed model exceeds CNN architecture with excellent generalization, which has proved its effectiveness in introducing transformer into UATR and thus can be applied to certain cases.<br><code>基于两个真实数据的实验结果表明，该模型优于CNN结构，具有良好的泛化能力，证明了该模型在UATR中引入Transformer的有效性，并可应用于特定情况。</code><br>To sum up, this letter mainly includes the following contributions<br><code>概括起来，这封信主要包括以下几点贡献</code></p><ol><li>First of all, we take advantage of the transformer as a backbone instead of the CNN. To the best of our knowledge, researchers have introduced the attention  mechanism knowledge, researchers have introduced the attention mechanism into UATR to analyze the inner workings and behavior of DL models [13], however, this is the first time that the ent knowledge, researchers have introduced the attention mechanism into UATR to analyze the inner workings and behavior of DL models [13], however, this is the first time that the entire transformer owing to multihead self-attention (MHSA) [8] module has been introduced into the field of UATR. which is certainly meaningful since the powerful transformer is in the stage of rapid development and is considered to be a strong competitor to the CNN and RNN due to its strong representation ability.<br><code>首先，我们利用Transformer作为主干，而不是CNN。据我们所知，研究人员已经引入了注意力机制知识，研究人员将注意力机制引入UATR来分析DL模型的内部工作和行为[13]，然而，这是第一次将注意力机制引入UATR来分析DL模型的内部工作和行为[13]，然而，这是第一次将由于多头自注意（MHSA）[8]模块而产生的整个Transformer引入UATR领域。这当然是有意义的，因为强大的Transformer正处于快速发展阶段，并且由于其强大的表示能力而被认为是CNN和RNN的强有力竞争者。</code></li><li>Moreover, we develop a novel progressive token embedding strategy for underwater acoustic spectrograms. As such, we expect that our model can extract high-level features from acoustic spectrogram as it not only takes global information into account via MHSA, but also implements hierarchical local aggregation (partial convolution) within each T–F token. Besides, we adopt the T–F Tokens Pooling classifier instead of the [CLS] token used in traditional transformers, which enables our model to capture the relationship between all aggregated T–F tokens.<br><code>此外，我们开发了一种新的渐进式token嵌入策略的水声频谱。因此，我们期望我们的模型可以从声谱图中提取高级特征，因为它不仅通过MHSA考虑全局信息，而且还在每个T-F token内实现分层局部聚合（部分卷积）。此外，我们采用T-F tokens池分类器代替传统transformer中使用的[CLS] token，这使得我们的模型能够捕获所有聚合T-F tokens之间的关系。</code></li></ol><h1 id="2-PROPOSED-METHOD"><a href="#2-PROPOSED-METHOD" class="headerlink" title="2.PROPOSED METHOD"></a>2.PROPOSED METHOD</h1><p><code>建议的方法</code><br>For UATR, this research studies an efficient acoustic Mel-spectrogram transformer with T–F tokens aggregating and the tokens pooling classifier, referred to as the UATR-transformer, to recognize two real-world underwater acoustic signal data.<br><code>针对UATR，本文研究了一种有效的基于T-F tokens聚集的声学Mel-谱图Transformer和tokens池分类器，简称UATR-Transformer，并对两组真实的水声信号数据进行了识别。</code><br>Specifically, our work is fundamentally inspired by the Tokens-To-Token Vision Transformer (T2T-ViT) introduced for image classification [14] and is leveraged with a 120 novel classification strategy.<br><code>具体来说，我们的工作从根本上受到了为图像分类引入的Tokens-To-Token Vision Transformer（T2T-ViT）的启发[14]，并利用了120种新颖的分类策略。</code><br>The overall workflow can be seen in Fig. 1.<br><code>总体工作流程如图1所示。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/at/1.png" alt="Figure 1 Illustration of the overall workflow (inspired by vision transformer [8]). Qh, Kh,and Vh, respectively, means the query matrix, the key matrix, and the value matrix."><br><code>整体工作流程的图示（灵感来自Vision Transformer [8]）。Qh、Kh和Vh分别表示查询矩阵、键矩阵和值矩阵。</code><br>The Mel-fbank feature consisting of half-overlapped triangular filters on center frequency fmel can be written as follows:<br><code>由中心频率fmel上的半重叠三角形滤波器组成的Mel-fbank特征可以写为：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/at/2.png"><br>As the frequency f (Hz) increases, the bandwidth of the filter gradually increases, which makes it more conducive to extracting low-frequency features of underwater target signals.<br><code>随着频率f（Hz）的增大，滤波器的带宽逐渐增大，更有利于提取水下目标信号的低频特征。</code><br>To this end, the Mel-fbank is chosen as the input representation for the UATR transformer.<br><code>为此，选择Mel-fbank作为UATR Transformer的输入表示。</code><br>As shown on the left of Fig. 1, the input acoustic waveform of t seconds is first converted into a sequence of t × f log Mel-fbank features computed every 10 ms with a 25-ms Hamming window, as the input representation for our proposed model, where f means the number of filter banks.<br><code>如图1左侧所示，t秒的输入声波波形首先被转换为一系列t × f log Mel-fbank特征，每10 ms计算一次，汉明窗口为25 ms，作为我们提出的模型的输入表示，其中f表示滤波器组的数量。</code><br>As a similar work in audio processing, tokenization in AST is to divide the spectrogram into 16 × 16 patches with an overlap of 6 in both time and frequency dimensions, which will be flattened into high-dimensional embedding vectors using a linear projection layer.<br><code>与音频处理中的类似工作一样，AST中的标记化是将频谱图划分为16 × 16块，在时间和频率维度上都有6个重叠，这些块将使用线性投影层平坦化为高维嵌入向量。</code><br>As such, the model cannot well capture the partial relationship between neighboring tokens by directly splitting spectrogram to tokens with fixed lengths.<br><code>因此，该模型不能很好地捕捉相邻标记之间的部分关系，通过直接分裂频谱图与固定长度的标记。</code><br>We are then motivated to employ a novel tokenization approach to circumvent the limit mentioned above and to apply it to underwater acoustic spectrograms.<br><code>然后，我们有动机采用一种新的标记化方法来规避上述限制，并将其应用于水下声谱图。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/at/3.png" alt="Figure 2 Overall structure of T–F tokens aggregating module."><br><code>T–F tokens聚合模块的总体结构。</code><br>As illustrated in Fig. 2, we use a progressive tokenization module that integrates neighboring T–F tokens which are split from spectrogram into one smaller token on both time and frequency dimensions, referred to as the T–F tokens aggregating block, which encodes the local structure for surrounding T–F tokens while concurrently reducing the token length and therefore improve the efficiency.<br><code>如图2所示，我们使用渐进式tokens模块，该模块将从频谱图分裂的相邻T-F tokens在时间和频率维度上集成为一个较小的token，称为T-F tokens聚合块，其对周围T-F tokens的局部结构进行编码，同时减少tokens长度，从而提高效率。</code><br>To be more explicit, the T–F tokens aggregating module consists of three main steps: T–F Split, token attention calculation, and T–F Reconstruction.<br><code>更明确地说，T-F tokens聚合模块包括三个主要步骤：T-F分割，标记注意力计算和T-F重建。</code><br>Specifically, the T1 ×F1 input spectrogram I0 is first split into an overlapping sequence of CK^2 × L pixels by sliding a kernel window with a size of [K, S, P],of which C is the number of the channels<br><code>具体地，首先通过滑动大小为[K，S，P]的核窗口将T1 ×F1输入谱图I0分割成CK ^2 × L像素的重叠序列，其中C是通道的数目</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/at/4.png"><br>At the dth iteration, Td and Fd are the time and frequency axes, Kd, Sd,and Pd are the kernel size, the stride, and the padding of the kernel window, respectively.<br><code>在第d次迭代时，Td和Fd是时间轴和频率轴，Kd、Sd和Pd分别是内核窗口的内核大小、步幅和填充。</code><br>To capture the spatial structure, the relation between each token is measured by attention calculation in an MHSA layer with a number of heads H = 1 (will be discussed later), in this case, information between surrounding tokens is aggregated together, and then the output is flattened into the C × T2 × F2 spectrogram as the reconstruction process.<br><code>为了捕捉空间结构，在具有头部数量H = 1的MHSA层中通过注意力计算来测量每个标记之间的关系（将在后面讨论），在这种情况下，周围标记之间的信息被聚集在一起，然后输出被平坦化为C × T2 × F2谱图作为重建过程。</code><br>These steps are repeated multiple times to get the final T–F tokens of proper size.<br><code>这些步骤重复多次，以获得适当大小的最终T-F tokens。</code><br>Particularly, this module can be defined as the operation of reducing the number of T–F tokens on a spatial level, and simultaneously establishing a strong correlation between them.<br><code>特别地，该模块可以被定义为在空间水平上减少T-F tokens的数量，并且同时在它们之间建立强相关性的操作。</code><br>By using a linear projection layer, finally, each aggregated T–F token is mapped into a 192-D feature vector.<br><code>通过使用线性投影层，最后，每个聚合的T-F令牌被映射到192-D特征向量。</code><br>In our experiment, we use the setup of kernel size (K, S, P) = (7, 4, 2), (3, 2, 1) and (3, 2, 1), which results in T/16×F/16 tokens with an embedding dimension of 192 since the stride is 4, 2 and 2.<br><code>在我们的实验中，我们使用内核大小（K，S，P）=（7，4，2），（3，2，1）和（3，2，1）的设置，这导致T/16×F/16令牌的嵌入维数为192，因为步幅为4，2和2。</code><br>Importantly, in order to capture the spatial structure of the spectrogram, a learnable positional embedding is attached to each T–F token, which is initialed as follows:<br><code>重要的是，为了捕获频谱图的空间结构，每个T-F标记都附加了一个可学习的位置嵌入，初始化如下：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/at/5.png"><br>where pos means the order of the T–F token, i ∈ [1, DEmbedding/2],and DEmbedding is the embedding dimension.<br><code>其中pos表示T-F令牌的阶数，i ∈ [1，DEmbedding/2]，DEmbedding是嵌入维数。</code><br>As a result, the original Mel-spectrogram is encoded by a series of T–F tokens with order information as input fed into N transformer encoders.<br><code>结果，原始Mel频谱图由一系列T-F令牌编码，其中顺序信息作为输入馈送到N个Transformer编码器中。</code><br>In particular, a transformer is made up of several encoder and decoder blocks.<br><code>特别地，Transformer由若干编码器和解码器块组成。</code><br>Since our model is designed for UATR which is considered a classification task, we only exploit the encoder blocks, intuitively.<br><code>由于我们的模型是为UATR设计的，UATR被认为是一个分类任务，因此我们只直观地利用编码器块。</code><br>Our model exploits N transformer encoders with the MHSA mechanism, which calculates the attention weight between each T–F token.<br><code>我们的模型利用N个Transformer编码器和MHSA机制，计算每个T-F令牌之间的注意力权重。</code><br>Specifically, in the first transformer encoder, the input feature after tokenization can be denoted as X0 of size P × DEmbedding and P = T/16 × F/16.<br><code>具体地，在第一Transformer编码器中，标记化之后的输入特征可以表示为大小为P × DEmbedding并且P = T/16 × F/16的X 0。</code><br>Then the MHSA layer in N transformer encoder projects Xn−1 with Qh, Kh,and Vh, referred to as the query matrix, the key matrix, and the value matrix, respectively,<br><code>然后，N Transformer编码器中的MHSA层用Qh、Kh和Vh投影Xn-1，Qh、Kh和Vh分别被称为查询矩阵、键矩阵和值矩阵，</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/at/6.png"><br>In which, W^Q h , W^K h ,and W^V h are learnable projection matrixes, H is the number of heads, h ∈[1, H] represents which head to be calculated, dimension of the attention weight Dattn = DEmbedding/H.<br><code>其中，W^Q h、W^K h和W^V h是可学习的投影矩阵，H是头部的数量，h ∈[1，H]表示要计算哪个头部，注意力权重的维数Dattn = DEmbedding/H。</code><br>After MHSA and multilayer perceptron (MLP) in N transformer encoders, we have an output of size P × DEmbedding with both local and global information between time and frequency axes.<br><code>在N个Transformer编码器中的MHSA和多层感知器（MLP）之后，我们具有大小为P × DEmbedding的输出，其在时间和频率轴之间具有局部和全局信息。</code><br>To classify labels in most transformer models, an extra token named [CLS] token that gathers classification information is appended at the beginning of the sequence and is directly fed into an MLP head.<br><code>为了对大多数Transformer模型中的标签进行分类，在序列的开头附加一个名为[CLS]的标记，用于收集分类信息，并将其直接送入MLP头。</code><br>However, in the area of acoustic processing, models may not well capture the start and the end of sample data when the embedding dimension is large.<br><code>然而，在声学处理领域，当嵌入维数较大时，模型可能无法很好地捕获样本数据的开始和结束。</code><br>To this end, we use the tokens pooling layer to predict the label, which is expected to improve the performance of UATR because it examines the final prediction by grouping all T–F tokens.<br><code>为此，我们使用tokens池层来预测标签，这有望提高UATR的性能，因为它通过对所有T-F令牌进行分组来检查最终预测。</code><br>Specifically, in the final layer output, each T–F token contains information about its corresponding time frames and frequency bins, and our model takes these latent tokens after attention calculation as an activation map, which is expected to jointly consider the relationship between the frequency bins and the time frames contained in each token so that we can integrate the meanings of each T–F token rather than using extra tokens.<br><code>具体来说，在最终的层输出中，每个T-F标记包含关于其对应的时间帧和频率箱的信息，并且我们的模型将注意力计算后的这些潜在标记作为激活图，期望共同考虑每个标记中包含的频率箱和时间帧之间的关系，以便我们可以整合每个T-F标记的含义，而不是使用额外的标记。</code><br>In particular, the tokens pooling classifier after the final encoder has a kernel size (3, F/16), stride (1, 1) with a padding size (1, 0) to integrate all frequency bins and map the 192-D features into the number of classes C.<br><code>具体地，在最终编码器之后的令牌池化分类器具有核大小（3，F/16）、步幅（1，1）和填充大小（1，0），以整合所有频率仓并将192-D特征映射到多个类别C中。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/at/7.png" alt="表1 DETAILED DESCRIPTION OF UATR-TRANSFORMER ARCHITECTURE"><br><code>UATR-变压器体系结构的详细描述</code><br>The detailed description of the UATR transformer architecture can be seen in Table I, in which B means the batch size, C is the number of classes, Kc, Sc,and Pc are the kernel size, stride, and padding of the convolution block, respectively.<br><code>UATR Transformer架构的详细描述可以在表I中看到，其中B表示批量大小，C是类的数量，Kc、Sc和Pc分别是卷积块的内核大小、步幅和填充。</code></p><h1 id="3-DATASET-DESCRIPTION"><a href="#3-DATASET-DESCRIPTION" class="headerlink" title="3.DATASET DESCRIPTION"></a>3.DATASET DESCRIPTION</h1><p><code>数据集描述</code><br>We conduct experiments on two publicly available datasets: First, the Shipsear database of underwater noise produced by various vessels [15]; this dataset is recorded in a shallow water environment, in which case it contains both the natural and anthropogenic environment noise in real conditions.<br><code>我们在两个公开的数据集上进行实验：首先，由各种船只产生的水下噪声的Shipsear数据库[15];该数据集在浅水环境中记录，在这种情况下，它包含真实的条件下的自然和人为环境噪声。</code><br>In addition, a larger dataset refers to the Deepship database [16], which consists of 47 h and 4 min real-world underwater recordings produced by more than 250 types of ships belonging to four classes (no background noise provided).<br><code>此外，更大的数据集是指Deepship数据库[16]，其中包括47小时和4分钟的真实水下记录，由属于四个类别的250多种船舶产生（没有提供背景噪音）。</code><br>For both datasets, sample data is first resampled at a sampling frequency of 16 kHz.<br><code>对于这两个数据集，首先以16 kHz的采样频率对样本数据进行重新采样。</code><br>To jointly take feature size, computer resources, and classification accuracy into account, all audio files are sliced into 5-s segments with the proper size, and randomly shuffled before training, of which 70% of them are selected for training and the remaining 30% for testing.<br><code>为了综合考虑特征大小、计算机资源和分类准确性，所有音频文件都被切成大小合适的5秒段，并在训练前随机洗牌，其中70%用于训练，其余30%用于测试。</code><br>Details of the two datasets can be seen in Table II.<br><code>两个数据集的详细信息见表II。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/at/8.png" alt="表2 DATASET PARTITIONS"><br><code>数据集分区</code></p><h1 id="4-EXPERIMENTAL-RESULTS-AND-DISCUSSION"><a href="#4-EXPERIMENTAL-RESULTS-AND-DISCUSSION" class="headerlink" title="4.EXPERIMENTAL RESULTS AND DISCUSSION"></a>4.EXPERIMENTAL RESULTS AND DISCUSSION</h1><p><code>实验结果和讨论</code><br>All experiments are conducted using Pytorch 1.8.0 with Python version 3.8 and verified by a computer with GPU of Nvidia GeForce RTX 3090 and Core i9-10900K CPU.<br><code>所有实验都使用Pytorch 1.8.0和Python 3.8版本进行，并在具有Nvidia GeForce RTX 3090 GPU和Core i9- 10900 K CPU的计算机上进行验证。</code><br>For data augmentation, we employ the T–F mask optimization by the implementation of torchaudio.transforms.FrequencyMasking and TimeMasking, of which the frequency mask is 48 and the time mask is 192 in our experiments.<br><code>对于数据增强，我们采用了T-F掩码优化，通过实现torchaudio.transforms.FrequencyMasking和TimeMasking，其中频率掩码为48，时间掩码为192。</code><br>And we normalize the input Mel-spectrogram before being applied to the model so that the dataset mean and standard deviation are 0 and 0.5, respectively.<br><code>在应用于模型之前，我们对输入的Mel谱图进行归一化，使数据集的平均值和标准差分别为0和0.5。</code><br>Moreover, as a classification task, the cross-entropy (CE) loss LCE is used as the loss function, which is shown as follows:<br><code>此外，作为分类任务，使用交叉熵（CE）损失LCE作为损失函数，其如下所示：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/at/9.png"><br>where x[class] and x[ j ] are the one-hot label of sample x and the probability of class j output, respectively, j = 1, 2, … , C,and C is the number of classes.<br><code>其中x[class]和x[ j ]分别是样本x的独热标签和类j输出的概率，j = 1，2，.，C，C是类的数量。</code><br>In our experiment, layer normalization is applied before each layer and residual modules to reduce the distribution variation between channels, which is proved more effective than batch normalization for the transformer architecture.<br><code>在我们的实验中，层归一化应用于每个层和剩余模块之前，以减少通道之间的分布变化，这被证明比批量归一化更有效的Transformer架构。</code><br>For the Shipsear and Deepship experiments, we use a fixed initial learning rate of 1e−4and 2.5e−4, and cut it with a decay rate of 0.5 and 0.6 after 50 epochs, respectively.<br><code>对于Shipsear和Deepship实验，我们使用固定的初始学习率1e− 4和2.5e−4，并在50个epoch后分别将其削减为0.5和0.6的衰减率。</code><br>Moreover, the batch size is 16 and 32 for Shipsear and Deepship, respectively, and the number of epochs is both 80, other hyperparameters are the same for the two datasets. We mention that these hyperparameters are determined by repeated experiments.<br><code>此外，Shipsear和Deepship的批量大小分别为16和32，epoch数均为80，其他超参数对于两个数据集是相同的。我们提到，这些超参数是通过重复实验确定的。</code><br>The network parameters are updated with the Adam optimizer.<br><code>使用Adam优化器更新网络参数。</code><br>The loss and accuracy curve in Fig. 3 depicts that the UATR transformer was not overfit.<br><code>图3中的损耗和精度曲线显示UATR Transformer不是过拟合。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/at/10.png" alt="Fig.3 Variation of accuracy, training, and validation loss over epochs. Experiment on the (a) Shipsear dataset and (b) Deepship dataset."><br><code>准确性、训练和验证损失随时间的变化。在（a）Shipsear数据集和（B）Deepship数据集上进行实验。</code><br>In the first experiment, the effect of different transformer structures is comprehensively analyzed, in which AST represents the AST with traditional [CLS] token and MLP classifier, AST-TP means the AST with tokens pooling classifier, UATR-transformer-MLP represents the UATR transformer with traditional [CLS] token and MLP classifier, and the UATR transformer means the designed UATR transformer with tokens pooling classifier.<br><code>在第一个实验中，全面分析了不同Transformer结构的效果，其中AST表示具有传统[CLS]令牌和MLP分类器的AST，AST-TP表示具有令牌池分类器的AST，UATR-Transformer-MLP表示具有传统[CLS]令牌和MLP分类器的UATR Transformer，UATR Transformer表示设计的具有令牌池分类器的UATR Transformer。</code><br>We report the average overall accuracy (OA) and Kappa value for five-times experiments, of which OA_se and Kappa_se are corresponding assessment metrics for the Shipsear dataset, while OA_ds and Kappa_ds for the Deepship dataset.<br><code>我们报告了五次实验的平均总体准确度（OA）和Kappa值，其中OA_se和Kappa_se是Shipsear数据集的相应评估指标，而OA_ds和Kappa_ds是Deepship数据集的相应评估指标。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/at/11.png" alt="表III PERFORMANCE COMPARISON USING DIFFERENT TRANSFORMERS"><br><code>不同变压器的性能比较</code><br>As illustrated in Table III, we can clearly see that the UATR transformer achieves the best recognition performance on both datasets.<br><code>如表III所示，我们可以清楚地看到，UATR Transformer在两个数据集上都实现了最佳识别性能。</code><br>It is probable that using hierarchically split tokens via T–F token aggregating makes the features more discriminative.<br><code>通过T-F令牌聚合使用分层拆分的令牌可能使特征更具区分性。</code><br>Meanwhile, owing to the integrated information from the frequency domain, the tokens pooling classifier has more advantages than [CLS] token and MLP in traditional transformer for UATR.<br><code>同时，由于结合了频域信息，令牌池分类器比传统Transformer中的[CLS]令牌和MLP具有更多的优势。</code><br>In the second experiment, the performance of the model is evaluated on both accuracy and complexity by comparison with other DL methods.<br><code>在第二个实验中，通过与其他DL方法的比较，从准确度和复杂度两个方面评估了该模型的性能。</code><br>We set up the UATR-transformer model that has comparable trainable parameters with EfficientNet-b0 (CNN) [17], CRNN (CNN + LSTM) [1], and MbNet-V2 (DW separable CNN) [18], of which the main module among them has been widely used for UATR [5], [6], [19].<br><code>我们建立了UATR转换器模型，该模型具有与EfficientNet-b 0（CNN）[17]，CRNN（CNN + LSTM）[1]和MbNet-V2（DW可分离CNN）[18]相当的可训练参数，其中主要模块已广泛用于UATR [5]，[6]，[19]。</code><br>We mention that networks are modified to accept the input of 1-D Mel-fbank features and trained from scratch as the UATR transformer with the same data augmentation strategy.<br><code>我们提到，网络被修改为接受1-D Mel-fbank特征的输入，并从头开始训练为具有相同数据增强策略的UATR Transformer。</code><br>To show the real-time performance of the prediction model, besides OA and Kappa, we report the number of trainable para- meters (No.params), the average prediction time (Avg.time) with the standard deviation, and the frames per second (FPS) on ShipsEar to evaluate the computational complexity shown in Table IV.<br><code>为了显示预测模型的实时性能，除了OA和Kappa之外，我们还报告了ShipsEar上的可训练帕拉数量（参数数量）、平均预测时间（平均时间）和标准差以及每秒帧数（FPS），以评估表IV中所示的计算复杂度。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/at/12.png" alt="表IV METHOD COMPARISONWITH CNNSON TWO DATASETS"><br><code>两种数据集上CNN方法的比较</code><br>Regarding the recognition accuracy, the results obtained by EfficientNet-b0 and CRNN are close, and each has its advantages on either dataset.<br><code>关于识别精度，EfficientNet-b 0和CRNN获得的结果接近，并且在任何一个数据集上都有各自的优势。</code><br>Owing to the combination of DW and pointwise (PW) components for extracting feature maps, MbNet-V2 achieved better performance compared to EfficientNet-b0 and CRNN that make use of conventional convolution operations, while the UATR transformer achieved satisfactory recognition accuracy on both datasets in the case of the same hyperparameter setup, of which the OA was 96.9% and 95.3%, Kappa value of 0.959 and 0.937, respectively, which has proved its advanced robustness and generalization ability.<br><code>由于DW和逐点（PW）分量的组合用于提取特征图，MbNet-V2与使用传统卷积运算的EfficientNet-b 0和CRNN相比具有更好的性能，而UATR Transformer在相同超参数设置的情况下在两个数据集上都获得了令人满意的识别准确率，其中OA为96.9%和95.3%，Kappa值分别为0.959和0.937，证明了其先进的鲁棒性和泛化能力。</code><br>In addition, for all models, the performance was slightly worse on the DeepShip dataset, probably because the DeepShip dataset was a larger and more complex dataset, which makes it more difficult to recognize. Regarding computational complexity, the UATR transformer predicts acoustic targets more quickly than other CNN models due to the progressive token embedding strategy designed to reduce redundancy.<br><code>此外，对于所有模型，DeepShip数据集的性能略差，可能是因为DeepShip数据集更大，更复杂，这使得它更难以识别。关于计算复杂性，UATR Transformer比其他CNN模型更快地预测声学目标，这是由于旨在减少冗余的渐进令牌嵌入策略。</code><br>EfficientNet-b0, which contains the largest parameters, reports the worst prediction speed with an Avg.time of 13.4 ms due to its large spatial size and wide network width.<br><code>包含最大参数的EfficientNet-b 0报告了最差的预测速度，平均时间为13.4 ms，这是由于其空间大小和网络宽度较大。</code><br>MbNet-V2 takes a shorter prediction time due to the use of DW and PW, which makes it having a relatively low number of parameters and operational cost compared to the conventional convolution operation.<br><code>由于使用DW和PW，MbNet-V2需要更短的预测时间，这使得它与传统卷积运算相比具有相对较低的参数数量和操作成本。</code><br>In summary, the comparative results show that the recognition accuracy of the UATR transformer is significantly higher than that of the other representative models with relatively small model parameters and the best processing speed.<br><code>综上所述，对比结果表明，UATR Transformer的识别准确率明显高于模型参数相对较小、处理速度最好的其他代表性模型。</code><br>As major parameters for the UATR transformer, we further analyze the parameter sensitivity of embedding dimension DEmbedding, the number of heads H, and the number of transformer encoders N based on the Shipsear dataset for efficiency.<br><code>作为UATR Transformer的主要参数，我们基于Shipsear数据集进一步分析了嵌入维数DEmbedding、头数H和Transformer编码器数N的参数敏感性，以提高效率。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/at/13.png" alt="Fig.4 Effect of parameters DEmbedding, H, N on the OA based on Shipsear. (a) Different embedding dimension DEmbedding. (b) Different number of heads H. (c) Different number of encoders N."><br><code>参数DEmbedding、H、N对基于Shipsear的OA的影响。(a)不同的嵌入维数DEmbedding。(b)不同的头数H。(c)不同数量的编码器N。</code><br>As can be seen in Fig. 4(a), when DEmbedding &gt; 192, the performance is slightly worse, this is probably because that model cannot aggregate the information between too long-distance tokens, which may also bring redundancy and thus decreases the recognition accuracy.<br><code>如图4（a）所示，当DEmbedding &gt; 192时，性能稍差，这可能是因为该模型不能聚合太长距离的令牌之间的信息，这也可能带来冗余，从而降低识别准确率。</code><br>While for small DEmbedding, some important information may be discarded so that spatial features are insufficiently captured, in which case the performance significantly drops.<br><code>而对于小的DEmbedding，一些重要的信息可能会被丢弃，使得空间特征不能被充分捕获，在这种情况下，性能显著下降。</code><br>With regard to the effect of the number of heads H shown in Fig. 4(b), larger heads H have no significant effect on recognition accuracy, which indicates that small H is also useful to perceive spatial information between each token.<br><code>关于图4（b）中所示的头部H的数量的影响，较大的头部H对识别准确性没有显著影响，这表明小的H对于感知每个标记之间的空间信息也是有用的。</code><br>Moreover, based on the results plotted in Fig. 4(c), when the number of encoders N &lt; 9, the performance will be improved as N increases.<br><code>此外，基于图4（c）中绘制的结果，当编码器的数量N &lt; 9时，性能将随着N的增加而改善。</code><br>It is probable that the deeper network enhances the representation ability and thus improves layer-by-layer feature learning performance. While for N &gt; 9, the model may be overfitted to a certain extent, and therefore the performance decreases on Shipsear.<br><code>更深的网络可能增强了表示能力，从而提高了逐层特征学习性能。而当N &gt; 9时，模型可能会出现一定程度的过拟合，从而导致模型在Shipsear上的性能下降。</code><br>In all, embedding dimension DEmbedding parameter is more sensitive to the UATR transformer than H and N.<br><code>总的来说，嵌入维数DEmbedding参数对UATR Transformer的影响比H和N更敏感。</code></p><h1 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5.Conclusion"></a>5.Conclusion</h1><p>In this letter, an UATR method based on the UATR transformer is proposed.<br><code>本文提出了一种基于UATR Transformer的UATR方法。</code><br>In the UATR transformer, input representation such as 1-D Mel-fbank features is first split into T–F tokens with a proper embedding dimension.<br><code>在UATR Transformer中，输入表示（例如1-D Mel-fbank特征）首先被分割成具有适当嵌入维度的T-F tokens。</code><br>Then neighboring T–F tokens are aggregated into smaller tokens on both time and frequency dimensions to reduce the token size and redundancy.<br><code>然后，相邻的T-F tokens聚合成更小的令牌上的时间和频率维度，以减少令牌的大小和冗余。</code><br>In this case, the UATR transformer can capture both global and local information on spectrograms and thus improve the UATR performance.<br><code>在这种情况下，UATR Transformer可以捕获频谱图上的全局和局部信息，从而提高UATR性能。</code><br>In all, the UATR transformer shows superior performance with relatively few model parameters compared with CNNs, which have proved the effectiveness of introducing the transformer into UATR.<br><code>总之，UATR Transformer与CNN相比，在模型参数相对较少的情况下表现出上级的性能，这证明了将Transformer引入UATR的有效性。</code><br>For future research studies, with the various designs of the transformer, its great potential in UATR is believed to be revealed.<br><code>对于未来的研究，随着Transformer设计的多样化，其在UATR中的巨大潜力将被进一步挖掘。</code></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文翻译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【笔记】水声通信（周胜利）</title>
      <link href="/2023/11/03/bi-ji-shui-sheng-tong-xin-zhou-sheng-li/"/>
      <url>/2023/11/03/bi-ji-shui-sheng-tong-xin-zhou-sheng-li/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h1 id="序"><a href="#序" class="headerlink" title="序"></a>序</h1><p>水声通信是时变，频变、空变的随机多径信道，水声通信具有窄宽带和高噪声的特性。</p><p>MilicaStojanovic提出的二阶锁相环信道跟踪与自适应反馈均衡技术相结合的单载波通信体制，是水声通信从低速率发展到高速率水声通信的一个里程碑。</p><p>目前，高速水声通信的研究主要在以下三个方面：<br>a）对单载波体制下的时间反转技术、频域均衡等技术的研究<br>b）对多载波调制下的相关接收处理技术的研究<br>c）在多输入多输出技术架构下，对包括分别与单载波调制和多载波调制相结合的相关接收处理算法的研究</p><p>正交频分复用（OFDM）技术是一种多载波调制的传输技术。它具有天然的高速率和抗多路径干扰的优势，是中近程高速水声通信的主流方案之一。</p><h1 id="第一章-引言"><a href="#第一章-引言" class="headerlink" title="第一章 引言"></a>第一章 引言</h1><h3 id="水声通信媒介"><a href="#水声通信媒介" class="headerlink" title="水声通信媒介"></a>水声通信媒介</h3><p>为了在水面漂浮系统和水下宝贵的资产之间建立通信，使用了四种不同的通信<br>媒介。</p><ul><li><em><strong>电缆</strong></em>。可以提供鲁棒的通信性能。但是部署和维修费用非常高。这就促使使用无线数据传输。</li><li><em><strong>声波</strong></em>。对于水下无线通信系统，声波因其在水下环境中具有相对低的吸收损失而作为主要通信载体来使用。但是，声波传播速度低并且频带非常有限。</li><li><em><strong>电磁(EM)波</strong></em>。在无线电频段使用电磁波比使用声波有个优点，最主要的是<br>速度快和工作频率高(从而带来较高的带宽)。在水下通信中使用电磁波的主要限制是由于海水的导电性使得电磁波的衰减很大。</li><li><em><strong>光波</strong></em>。很明显，使用光波通信在数据速率上有很大的优势。但是光通信在水<br>下也有几个缺点。首先,光信号很快在水中被吸收;第二，由悬浮粒子和浮游生物带来的光散射非常大。第三,在水的上半部分环境光的高照度会对光通信的使用产生另一个负面影响。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/shui/1-1.png"><br>声波在海水中传播良好并能到达相当远的距离。这证明了在许多水下无线通信中使用声波的合理性。</li></ul><h3 id="水声通信的特点"><a href="#水声通信的特点" class="headerlink" title="水声通信的特点"></a>水声通信的特点</h3><p>通过海水的极低声传播速度是声波区别于电磁波传播的一个重要因素。水中的声速取决于<code>水温</code>、<code>盐度</code>和<code>压力</code>。<br>水下声速随着水温、盐度和深度的增长而增大。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/shui/1-2.png"></p><p>在深海中，典型声速剖面是深度的函数，根据深度，声速剖面可分为四层。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/shui/1-3.png"></p><ul><li><em><strong>表面层</strong></em>。表面层通常有几十米水深。由于风的混合影响，该层的温度和盐度者都趋于均匀，这使得<code>声速为常量</code>。表面层也称为混合层。</li><li><em><strong>季节性</strong></em>和<em><strong>恒定温跃层</strong></em>。在温跃层，随着水深的增加,水温在降低，如图1.1所示。在一个负梯这两层中，压力和盐度的增加无法补偿温度降低的影响。因此，在深度上有一个负梯度的声速剖面。在季节性温跃层，负梯度随着季节变化，而在恒定温跃层则软交少有季节性的变化。<br>深等温层。水温几乎是保持在4℃左右的常数。声速也因此主要由水压决定，这导致随深度变化的声速剖面为正梯度。</li><li><em><strong>深等温层</strong></em>。水温几乎是保持在4℃左右的常数。声速也因此主要由水压决定，这导致随深度变化的声速剖面为正梯度。</li></ul><p>根据Snell定律，当声速在介质中发生变化时，声线会沿着传播速度较低的方向弯曲。在浅海中，声速通常是常量，声信号会沿直线传播。然而，在深海中，声速剖面会引起声传播路径的变化。在温跃层和深等温层之间的特定水深处有一个声道轴，此处的声速最小。当声信号在声道轴上传播时，声射线会向下弯曲；当声信号在温跃层和深等温层之间传播时，声射线会向上弯曲。这种传播现象被称为深海声道，对应的传播被称为SOFAR（水底测音装置）传播。SOFAR传播的一个有趣现象是可以通过较短的传播时间传播较远的距离。由于声速的不一致性引起的折射，在声场中存在阴影区和会聚区。阴影区表示直达声路径无法到达的区域，而会聚区表示一簇声路径集中穿透的区域。<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/shui/1-4.png"></p><h3 id="传播损失"><a href="#传播损失" class="headerlink" title="传播损失"></a>传播损失</h3><p>声波在水中传播的过程中,有三种主要的能量损失机理:(i)<code>吸收损失</code>,(ii)<code>几何扩展</code>，(iii)<code>散射损失</code>。</p><p>在传播过程中，声波能量可能转换成其他形式并被媒介吸收。吸收的能量损失由材料的缺陷所直接控制，这是因为经过材料的物理波传播的类型不同。对于电磁波而言,这个缺陷是海水的电导性。对于声波，材料缺陷是无伸缩性的，它将声波能量转换成热能</p></body></html>]]></content>
      
      
      
        <tags>
            
            <tag> 水声通信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【笔记】零基础入门学习Python（小甲鱼）</title>
      <link href="/2023/11/02/bi-ji-ling-ji-chu-ru-men-xue-xi-python-xiao-jia-yu/"/>
      <url>/2023/11/02/bi-ji-ling-ji-chu-ru-men-xue-xi-python-xiao-jia-yu/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><a href="https://fishc.com.cn/forum.php">鱼C论坛</a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文翻译】Attention is all you need</title>
      <link href="/2023/11/01/lun-wen-fan-yi-attention-is-all-you-need/"/>
      <url>/2023/11/01/lun-wen-fan-yi-attention-is-all-you-need/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.<br><code>显性序列转导模型基于包括编码器和解码器的复杂递归或卷积神经网络。</code><br>The best-performing models also connect the encoder and decoder through an attention mechanism.<br><code>性能最好的模型还通过注意力机制连接编码器和解码器。</code><br>We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.<br><code>我们提出了一个新的简单的网络架构，Transformer，完全基于注意力机制，完全免除了递归和卷积。</code><br>Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.<br><code>在两个机器翻译任务上的实验表明，这些模型在质量上是上级的，同时具有更好的并行性，并且需要更少的训练时间。</code><br>Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.<br><code>我们的模型在WMT 2014英语到德语翻译任务中达到了28.4 BLEU，比现有的最佳结果（包括集成）提高了2 BLEU以上。</code><br>On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.<br><code>在WMT 2014英语到法语的翻译任务中，我们的模型在8个GPU上训练了3.5天后，建立了一个新的单模型最先进的BLEU得分为41.0，这是文献中最佳模型训练成本的一小部分。</code></p><h1 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7.Conclusion"></a>7.Conclusion</h1><p>In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.<br><code>在这项工作中，我们提出了Transformer，这是第一个完全基于注意力的序列转换模型，用多头自注意力取代了编码器-解码器架构中最常用的递归层。</code><br>For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.<br><code>对于翻译任务，Transformer的训练速度明显快于基于递归或卷积层的架构。</code><br>On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.<br><code>在WMT 2014英语到德语和WMT 2014英语到法语的翻译任务中，我们都达到了最先进的水平。</code><br>In the former task our best model outperforms even all previously reported ensembles.<br><code>在前一个任务中，我们最好的模型甚至优于所有以前报道的合奏。</code><br>We are excited about the future of attention-based models and plan to apply them to other tasks.<br><code>我们对基于注意力的模型的未来感到兴奋，并计划将其应用于其他任务。</code><br>We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.<br><code>我们计划将Transformer扩展到涉及文本以外的输入和输出方式的问题，并研究本地的有限注意力机制，以有效地处理大型输入和输出，如图像，音频和视频。减少世代的连续性是我们的另一个研究目标。</code><br>The code we used to train and evaluate our models is available at <a href="https://github.com/">https://github.com/</a> tensorflow/tensor2tensor.<br><code>我们用来训练和评估模型的代码可以在https://github.com/ tensorflow/tensor 2 tensor上找到。</code><br>Acknowledgements<br><code>致谢</code><br>We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.<br><code>我们感谢Nal Kalchbrenner和Stephan Gouws富有成效的评论、更正和启发。</code></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1><p><code>导言</code><br>Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5].<br><code>递归神经网络，特别是长短期记忆[12]和门控递归神经网络[7]，已经被牢固地确立为序列建模和转导问题（如语言建模和机器翻译）中的最先进方法[29，2，5]。</code><br>Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].<br><code>此后，许多努力继续推动递归语言模型和编码器-解码器架构的边界[31，21，13]。</code><br>Recurrent models typically factor computation along the symbol positions of the input and output sequences.<br><code>递归模型通常沿输入和输出序列的符号位置沿着因子计算。</code><br>Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t.<br><code>将位置与计算时间中的步骤对齐，它们生成隐藏状态ht的序列，作为前一个隐藏状态ht-1和位置t的输入的函数。</code><br>This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.<br><code>这种固有的顺序性质排除了训练示例中的并行化，这在较长的序列长度下变得至关重要，因为内存约束限制了示例之间的并行化。</code><br>Recent work has achieved significant improvements in computational efficiency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter.<br><code>最近的工作通过因式分解技巧[18]和条件计算[26]实现了计算效率的显着提高，同时还提高了后者的模型性能。</code><br>The fundamental constraint of sequential computation, however, remains.<br><code>然而，顺序计算的基本约束仍然存在。</code><br>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network.<br><code>注意力机制已经成为各种任务中引人注目的序列建模和转导模型的组成部分，允许在不考虑它们在输入或输出序列中的距离的情况下对依赖关系进行建模[2，16]。然而，除了少数情况外，在所有情况下[22]，这种注意力机制都与循环网络结合使用。</code><br>In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.<br><code>然而，除了少数情况外，在所有情况下，这种注意力机制都与循环网络结合使用。</code><br>In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.<br><code>在这项工作中，我们提出了Transformer，一个模型架构避免复发，而是完全依赖于注意力机制，以绘制输入和输出之间的全局依赖关系。</code><br>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.<br><code>Transformer支持更高的并行化，在8个P100 GPU上训练12小时后，翻译质量就能达到最新水平。</code></p><h1 id="2-Background"><a href="#2-Background" class="headerlink" title="2.Background"></a>2.Background</h1><p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.<br><code>减少顺序计算的目标也构成了扩展神经GPU [20]，ByteNet [15]和ConvS2S [8]的基础，所有这些都使用卷积神经网络作为基本构建块，并行计算所有输入和输出位置的隐藏表示。</code><br>In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.<br><code>在这些模型中，将来自两个任意输入或输出位置的信号关联起来所需的操作数量随着位置之间的距离而增长，对于ConvS2S是线性的，而对于ByteNet是线性的。</code><br>This makes it more difficult to learn dependencies between distant positions [11].<br><code>这使得学习远距离位置之间的依赖关系变得更加困难[11]。</code><br>In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.<br><code>在Transformer中，这被减少到恒定数量的操作，尽管由于平均注意力加权位置而降低了有效分辨率，这是我们用3.2节中描述的多头注意力抵消的效果。</code><br>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.<br><code>自我注意力（英语：Self-attention），有时也被称为内部注意力（intra-attention），是一种将单个序列的不同位置联系起来以计算序列的表示的注意力机制。</code><br>Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].<br><code>自我注意已成功用于各种任务，包括阅读理解，抽象概括，文本蕴涵和学习任务独立的句子表征[4，22，23，19]。</code><br>End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28].<br><code>端到端记忆网络基于循环注意机制而不是序列对齐的循环，并且已被证明在简单语言问题回答和语言建模任务中表现良好[28]。</code><br>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.<br><code>然而，据我们所知，Transformer是第一个完全依靠自我注意力来计算其输入和输出的表示而不使用序列对齐的RNN或卷积的转换模型。</code><br>In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].<br><code>在下面的章节中，我们将描述Transformer，激发自我注意力，并讨论它相对于[14，15]和[8]等模型的优势。</code></p><h1 id="3-Model-Architecture"><a href="#3-Model-Architecture" class="headerlink" title="3.Model Architecture"></a>3.Model Architecture</h1><p><code>模型架构</code><br>Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].<br><code>大多数竞争性神经序列转导模型具有编码器-解码器结构[5，2，29]。</code><br>Here, the encoder maps an input sequence of symbol representations (x1, …, xn) to a sequence of continuous representations z = (z1, …, zn).<br><code>这里，编码器将符号表示（x1，.，xn）转换为连续表示序列z =（z1，...，Zn）。</code><br>Given z, the decoder then generates an output sequence (y1, …, ym) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next.<br><code>给定z，解码器然后生成输出序列（y1，...，ym）的符号一次一个元素。在每一步，模型都是自回归的[9]，在生成下一个符号时，消耗先前生成的符号作为额外的输入。</code><br>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.<br><code>Transformer遵循这种整体架构，编码器和解码器均使用堆叠的自注意和逐点全连接层，分别如图1的左半部分和右半部分所示。</code></p><h3 id="3-1Encoder-and-Decoder-Stacks"><a href="#3-1Encoder-and-Decoder-Stacks" class="headerlink" title="3.1Encoder and Decoder Stacks"></a>3.1Encoder and Decoder Stacks</h3><p><code>编码器和解码器堆栈</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/attention/1.png" alt="Figure 1: The Transformer - model architecture."><br><code>图1：Transformer模型架构。</code><br>Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.<br><code>编码器：编码器由N = 6个相同层的堆栈组成。每层有两个子层。第一种是多头自注意机制，第二种是简单的位置全连接前馈网络。我们在两个子层中的每一个周围使用残差连接[10]，然后进行层归一化[1]。也就是说，每个子层的输出是LayerNorm（x + Sublayer（x）），其中Sublayer（x）是子层本身实现的函数。为了促进这些残余连接，模型中的所有子层以及嵌入层产生维度dmodel = 512的输出。</code><br>Decoder: The decoder is also composed of a stack ofN = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.<br><code>解码器：解码器也是由一个堆栈的N = 6相同的层。除了每个编码器层中的两个子层之外，解码器还插入第三子层，该第三子层对编码器堆栈的输出执行多头注意。与编码器类似，我们在每个子层周围使用残差连接，然后进行层归一化。我们还修改了解码器堆栈中的自关注子层，以防止位置关注后续位置。这种掩蔽，结合输出嵌入偏移一个位置的事实，确保了位置i的预测只能依赖于小于i的位置处的已知输出。</code></p><h3 id="3-2Attention"><a href="#3-2Attention" class="headerlink" title="3.2Attention"></a>3.2Attention</h3><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.<br><code>注意力函数可以被描述为将查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。</code><br>The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.<br><code>输出被计算为值的加权和，其中分配给每个值的权重由查询与对应键的兼容性函数计算。</code></p><h5 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h5><p><code>标度点积注意力</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/attention/2.png" alt="Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel."><br><code>图2：（左）标度点积注意力。（右）多头注意力由几个平行运行的注意力层组成。</code><br>We call our particular attention “Scaled Dot-Product Attention” (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv.<br><code>我们将这种特殊的注意力称为“标度点积注意力”（图2）。输入由维度dk的查询和键以及维度dv的值组成。</code><br>We compute the dot products of the query with all keys, divide each by √ dk, and apply a softmax function to obtain the weights on the values.<br><code>我们计算查询与所有键的点积，将每个键除以√dk，并应用softmax函数来获得值的权重。</code><br>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.<br><code>在实践中，我们同时计算一组查询的注意力函数，这些查询被打包到一个矩阵Q中。</code><br>The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:<br><code>键和值也被打包到矩阵K和V中。我们计算输出矩阵为：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/attention/3.png"><br>The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention.<br><code>两种最常用的注意力函数是加法注意力[2]和点积（乘法）注意力。</code><br>Dot-product attention is identical to our algorithm, except for the scaling factor of 1/√ dk .<br><code>点积注意力与我们的算法相同，除了比例因子为1/√ dk。</code><br>Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.<br><code>加法注意力使用具有单个隐藏层的前馈网络计算兼容性函数。</code><br>While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.<br><code>虽然两者在理论复杂度上相似，但点积注意力在实践中要快得多，空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。</code><br>While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by 1/√ dk .<br><code>虽然对于较小的dk值，这两种机制的表现相似，但在dk值较大的情况下，加法注意力优于点积注意力[3]。我们怀疑，对于较大的dk值，点积的大小会变大，从而将softmax函数推入梯度极小的区域。为了抵消这种影响，我们将点积缩放1/√ dk。</code></p><h5 id="3-2-2Multi-Head-Attention"><a href="#3-2-2Multi-Head-Attention" class="headerlink" title="3.2.2Multi-Head Attention"></a>3.2.2Multi-Head Attention</h5><p><code>多头注意</code><br>Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively.<br><code>我们发现，与使用dmodel维的键、值和查询来执行单个注意力函数不同的是，使用不同的学习线性投影将查询、键和值分别线性投影h次到dk、dk和dv维是有益的。</code><br>On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.<br><code>在这些查询、键和值的每个投影版本上，我们并行执行注意力函数，产生dv-dimensional输出值。</code><br>These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.<br><code>这些数据被连接起来并再次投影，从而得到最终值，如图2所示。</code><br>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.<br><code>多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。对于一个单一的注意力头，平均化抑制了这一点。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/attention/4.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/attention/5.png"><br><code>其中投影是参数矩阵WQi ∈ Rdmodel×dk，WKi ∈ Rdmodel×dk，WVi ∈ Rdmodel×dv和WO ∈ Rdv ×dmodel。在这项工作中，我们采用h = 8个平行的注意层，或头部。对于每一个，我们使用dk = dv = dmodel/h = 64。由于每个头的维数降低，总的计算成本是类似的单头注意与全维。</code></p><h5 id="3-2-3Applications-of-Attention-in-our-Model"><a href="#3-2-3Applications-of-Attention-in-our-Model" class="headerlink" title="3.2.3Applications of Attention in our Model"></a>3.2.3Applications of Attention in our Model</h5><p><code>注意力在模型中的应用</code><br>The Transformer uses multi-head attention in three different ways:<br><code>Transformer以三种不同的方式使用多头注意力：</code></p><ul><li>In “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].</li><li><code>在“编码器-解码器注意”层中，查询来自先前的解码器层，并且存储器键和值来自编码器的输出。这使得解码器中的每个位置都可以覆盖输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意力机制，如[31，2，8]。</code></li><li>The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.</li><li><code>编码器包含自我注意层。在自关注层中，所有的键、值和查询都来自同一个地方，在这种情况下，是编码器中前一层的输出。编码器中的每个位置可以涉及编码器的前一层中的所有位置。</code></li><li>Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.</li><li><code>类似地，解码器中的自关注层允许解码器中的每个位置关注解码器中的直到并且包括该位置的所有位置。我们需要防止解码器中的冗余信息流，以保持自回归特性。我们通过屏蔽（设置为−∞）softmax输入中对应于非法连接的所有值来实现这一点。参见图2。</code></li></ul><h3 id="3-3Position-wise-Feed-Forward-Networks"><a href="#3-3Position-wise-Feed-Forward-Networks" class="headerlink" title="3.3Position-wise Feed-Forward Networks"></a>3.3Position-wise Feed-Forward Networks</h3><p><code>位置前馈网络</code><br>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.<br><code>除了注意力子层之外，我们的编码器和解码器中的每一层都包含一个完全连接的前馈网络，该网络单独且相同地应用于每个位置。这包括两个线性变换，中间有一个ReLU激活。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/attention/6.png"><br>While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.<br><code>虽然不同位置的线性变换是相同的，但它们在层与层之间使用不同的参数。另一种描述这种情况的方式是两个卷积，内核大小为1。输入和输出的维度为dmodel = 512，内层的维度为dff = 2048。</code></p><h3 id="3-4Embeddings-and-Softmax"><a href="#3-4Embeddings-and-Softmax" class="headerlink" title="3.4Embeddings and Softmax"></a>3.4Embeddings and Softmax</h3><p><code>嵌入式和Softmax</code><br>Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.<br><code>与其他序列转换模型类似，我们使用学习的嵌入将输入令牌和输出令牌转换为维度dmodel的向量。</code><br>We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.<br><code>我们还使用常用的学习线性变换和softmax函数将解码器输出转换为预测的下一个令牌概率。</code><br>In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [24].<br><code>在我们的模型中，我们在两个嵌入层和pre-softmax线性变换之间共享相同的权重矩阵，类似于[24]。</code><br>In the embedding layers, we multiply those weights by √ dmodel.<br><code>在嵌入层中，我们将这些权重乘以√ dmodel.</code></p><h3 id="3-5Positional-Encoding"><a href="#3-5Positional-Encoding" class="headerlink" title="3.5Positional Encoding"></a>3.5Positional Encoding</h3><p><code>位置编码</code><br>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.<br><code>由于我们的模型不包含递归和卷积，为了让模型利用序列的顺序，我们必须注入一些关于序列中令牌的相对或绝对位置的信息。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/attention/7.png"><br><em><strong>Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.</strong></em><br>To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [8].<br><code>为此，我们在编码器和解码器堆栈的底部向输入嵌入添加“位置编码”。位置编码与嵌入具有相同的维度dmodel，因此两者可以相加。有许多位置编码的选择，学习和固定[8]。</code><br>In this work, we use sine and cosine functions of different frequencies:<br><code>在这项工作中，我们使用不同频率的正弦和余弦函数：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/attention/8.png"><br><em><strong>where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.</strong></em><br><code>其中pos是位置，i是尺寸。即，位置编码的每个维度对应于正弦曲线。波长形成从2π到10000 · 2π的几何级数。我们之所以选择这个函数，是因为我们假设它可以让模型很容易地学会通过相对位置来参与，因为对于任何固定的偏移量k，PEpos+k都可以表示为PEpos的线性函数。</code><br>We also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.<br><code>我们还尝试使用学习的位置嵌入[8]，发现两个版本产生了几乎相同的结果（见表3行（E））。我们选择正弦版本，因为它可以允许模型外推到比训练期间遇到的更长的序列长度。</code></p><h1 id="4-Why-Self-Attention"><a href="#4-Why-Self-Attention" class="headerlink" title="4.Why Self-Attention"></a>4.Why Self-Attention</h1><p><code>为什么自我关注</code><br>In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1, …, xn) to another sequence of equal length (z1, …, zn), with xi, zi ∈ Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.<br><code>在本节中，我们将自注意层的各个方面与通常用于映射一个可变长度的符号表示序列（x1，...，xn）到另一个相等长度的序列（z1，...，zn），其中xi，zi ∈ Rd，诸如典型序列转换编码器或解码器中的隐藏层。为了激发我们对自我注意力的使用，我们认为有三个必要条件。</code><br>One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.<br><code>一个是每层的总计算复杂度。另一个是可以并行化的计算量，这是通过所需的最小顺序操作数来衡量的。</code><br>The third is the path length between long-range dependencies in the network.<br><code>第三个是网络中长距离依赖之间的路径长度。</code><br>Learning long-range dependencies is a key challenge in many sequence transduction tasks.<br><code>学习长程依赖性是许多序列转导任务中的关键挑战。</code><br>One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.<br><code>影响学习这种依赖关系的能力的一个关键因素是前向和后向信号必须在网络中穿过的路径的长度。</code><br>The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11].<br><code>输入和输出序列中任意位置组合之间的路径越短，就越容易学习长程依赖性[11]。</code><br>Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.<br><code>因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。</code><br>As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.<br><code>如表1所示，自注意层用恒定数量的顺序执行操作连接所有位置，而递归层需要O（n）顺序操作。</code><br>In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations.<br><code>就计算复杂度而言，当序列长度n小于表示维度d时，自注意层比递归层更快，这是最常见的情况，最先进的机器翻译模型使用的句子表示，例如词段[31]和字节对[25]表示。</code><br>To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position.<br><code>为了提高涉及非常长的序列的任务的计算性能，自我注意力可以被限制为仅考虑以相应的输出位置为中心的输入序列中的大小为r的邻域。</code><br>This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.<br><code>这将使最大路径长度增加到O（n/r）。我们计划在未来的工作中进一步研究这种方法。</code><br>A single convolutional layer with kernel width k &lt; n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths between any two positions in the network.<br><code>具有内核宽度k &lt; n的单个卷积层不连接所有输入和输出位置对。这样做需要一个O（n/k）卷积层的堆栈（在连续内核的情况下），或者O（logk（n））（在扩张卷积的情况下）[15]，增加了网络中任何两个位置之间的最长路径的长度。</code><br>Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.<br><code>卷积层通常比递归层昂贵k倍。然而，可分离卷积[6]大大降低了复杂度，为O（k · n · d + n · d2）。然而，即使k = n，可分离卷积的复杂性也等于自注意层和逐点前馈层的组合，这是我们在模型中采用的方法。</code><br>As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.<br><code>作为附带的好处，自我注意力可以产生更多可解释的模型。我们从我们的模型中检查注意力分布，并在附录中展示和讨论示例。不仅个别注意头清楚地学会执行不同的任务，许多人似乎表现出与句子的句法和语义结构相关的行为。</code></p><h1 id="5-Training"><a href="#5-Training" class="headerlink" title="5.Training"></a>5.Training</h1><p>This section describes the training regime for our models.<br><code>本节描述了我们模型的训练机制。</code></p><h3 id="5-1Training-Data-and-Batching"><a href="#5-1Training-Data-and-Batching" class="headerlink" title="5.1Training Data and Batching"></a>5.1Training Data and Batching</h3><p><code>训练数据和批处理</code><br>We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.<br><code>我们在标准的WMT 2014英语-德语数据集上进行了训练，该数据集由大约450万个句子对组成。</code><br>Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens.<br><code>句子使用字节对编码[3]进行编码，它具有大约37000个标记的共享源目标词汇表。</code><br>For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [31].<br><code>对于英语-法语，我们使用了更大的WMT 2014英语-法语数据集，其中包含3600万个句子，并将标记拆分为32000个单词。</code><br>Sentence pairs were batched together by approximate sequence length.<br><code>句子对按近似序列长度分批在一起。</code><br>Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.<br><code>每个训练批次包含一组句子对，其中包含大约25000个源标记和25000个目标标记。</code></p><h3 id="5-2Hardware-and-Schedule"><a href="#5-2Hardware-and-Schedule" class="headerlink" title="5.2Hardware and Schedule"></a>5.2Hardware and Schedule</h3><p><code>硬件和时间表</code><br>We trained our models on one machine with 8 NVIDIA P100 GPUs. 7<br><code>我们在一台配备8个NVIDIA P100 GPU的机器上训练我们的模型。</code><br>For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.<br><code>对于我们使用本文中描述的超参数的基础模型，每个训练步骤大约需要0.4秒。</code><br>We trained the base models for a total of 100,000 steps or 12 hours.<br><code>我们对基础模型进行了总共10万步或12小时的训练。</code><br>For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).<br><code>对于我们的大模型（在表3的底部行描述），步进时间为1.0秒。大模型被训练了30万步（3.5天）。</code></p><h3 id="5-3Optimizer"><a href="#5-3Optimizer" class="headerlink" title="5.3Optimizer"></a>5.3Optimizer</h3><p><code>优化程序</code><br>We used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and  = 10−9. We varied the learning rate over the course of training, according to the formula:<br><code>我们使用Adam优化器[17]，其中β1 = 0.9，β2 = 0.98，β 2 = 10−9。我们根据以下公式在训练过程中改变学习率：</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/attention/9.png"><br>This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.<br><code>这对应于对于第一warmup_steps训练步骤线性地增加学习速率，并且此后与步骤数的平方根倒数成比例地减小学习速率。我们使用warmup_steps = 4000。</code></p><h3 id="5-4Regularization"><a href="#5-4Regularization" class="headerlink" title="5.4Regularization"></a>5.4Regularization</h3><p><code>正则化</code><br>We employ three types of regularization during training:<br><code>我们在训练过程中使用三种类型的正则化：</code><br><em><strong>Residual Dropout</strong></em><br>We apply dropout [27] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.<br><em><strong>残留脱落</strong></em><br><code>我们将dropout [27]应用于每个子层的输出，然后将其添加到子层输入并归一化。此外，我们将dropout应用于编码器和解码器堆栈中的嵌入和位置编码的总和。对于基础模型，我们使用Pdrop = 0.1的速率。</code></p><p>Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.<br><code>Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/attention/10.png"><br><em><strong>Label Smoothing</strong></em><br>During training, we employed label smoothing of value ls = 0.1 [30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.<br><em><strong>标签平滑</strong></em><br><code>在训练过程中，我们使用了值为0.1的标签平滑[30]。这会伤害困惑，因为模型会变得更加不确定，但会提高准确性和BLEU分数。</code></p><h1 id="6-Results"><a href="#6-Results" class="headerlink" title="6.Results"></a>6.Results</h1><h3 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h3><p><code>机器翻译</code><br>On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4.<br><code>在WMT 2014英语到德语的翻译任务中，大Transformer模型（表2中的Transformer（大））比之前报道的最佳模型（包括集成）的BLEU高出2.0以上，建立了28.4的新的最先进的BLEU分数。</code><br>The configuration of this model is listed in the bottom line of Table 3.<br><code>该型号的配置列于表3的底部行。</code><br>Training took 3.5 days on 8 P100 GPUs.<br><code>在8个P100 GPU上进行了3.5天的训练。</code><br>Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.<br><code>甚至我们的基础模型也超过了所有以前发布的模型和集合，而训练成本只是任何竞争模型的一小部分。</code><br>On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model.<br><code>在WMT 2014英语到法语的翻译任务中，我们的大模型达到了41.0的BLEU分数，超过了之前发布的所有单个模型，而训练成本不到之前最先进模型的1/4。</code><br>The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.<br><code>针对英语到法语训练的Transformer（大）模型使用了丢弃率Pdrop = 0.1，而不是0.3。</code><br>For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [31]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [31].<br><code>对于基本模型，我们使用通过平均最后5个检查点获得的单个模型，这些检查点以10分钟的间隔写入。对于大型模型，我们平均了最后20个检查点。我们使用波束搜索，波束大小为4，长度惩罚α = 0.6 [31]。这些超参数是在开发集上进行实验后选择的。我们将推理过程中的最大输出长度设置为输入长度+ 50，但尽可能提前终止[31]。</code><br>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU.<br><code>表2总结了我们的结果，并将我们的翻译质量和培训成本与文献中的其他模型架构进行了比较。我们通过乘以训练时间、使用的GPU数量以及每个GPU的持续单精度浮点容量估计值来估计用于训练模型的浮点运算数量。</code></p><h3 id="Model-Variations"><a href="#Model-Variations" class="headerlink" title="Model Variations"></a>Model Variations</h3><p><code>型号变化</code><br>To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.<br><code>为了评估Transformer不同组件的重要性，我们以不同的方式改变了我们的基础模型，在开发集newst2013上测量了英语到德语翻译的性能变化。我们使用了前一节中描述的波束搜索，但没有检查点平均。我们在表3中给出了这些结果。</code><br>In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.<br><code>在表3的行（A）中，我们改变了注意力头部的数量以及注意力键和值维度，保持计算量不变，如3.2.2节所述。虽然单头注意力比最佳设置差0.9 BLEU，但质量也会随着过多的头而下降。</code></p><p>Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.<br><code>表3：Transformer架构的变化。未列出的值与基本模型的值相同。所有指标都是在英语到德语翻译开发集，newstest2013。根据我们的字节对编码，列出的困惑是每个单词的，不应该与每个单词的困惑进行比较。</code><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/attention/11.png"><br>In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical results to the base model.<br><code>在表3行（B）中，我们观察到减小注意力键大小dk会损害模型质量。这表明确定兼容性并不容易，并且比点积更复杂的兼容性函数可能是有益的。我们在行（C）和（D）中进一步观察到，正如预期的那样，更大的模型更好，并且dropout非常有助于避免过度拟合。在行（E）中，我们用学习的位置嵌入[8]替换了正弦位置编码，并观察到与基本模型几乎相同的结果。</code></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文翻译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【笔记】AI论文精读（李沐）</title>
      <link href="/2023/10/31/bi-ji-ai-lun-wen-jing-du-li-mu/"/>
      <url>/2023/10/31/bi-ji-ai-lun-wen-jing-du-li-mu/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h1 id="0-如何读论文"><a href="#0-如何读论文" class="headerlink" title="0.如何读论文"></a>0.如何读论文</h1><h3 id="论文结构"><a href="#论文结构" class="headerlink" title="论文结构"></a>论文结构</h3><p>1.title<br>2.abstract<br>3.introduction<br>4.method<br>5.experiments<br>6.conclusion</p><h3 id="论文阅读方法"><a href="#论文阅读方法" class="headerlink" title="论文阅读方法"></a>论文阅读方法</h3><ul><li><p>第一遍：<code>标题</code>、<code>摘要</code>、<code>结论</code>。可以看一看方法和实验部分重要的图和表。这样可以花费十几分钟时间了解到论文是否适合你的研究方向。</p></li><li><p>第二遍：确定论文值得读之后，可以快速的把整个论文过一遍，不需要知道所有的细节，需要了解重要的图和表，知道每一个部分在干什么，圈出相关文献。觉得文章太难，可以读引用的文献。</p></li><li><p>第三遍：提出什么问题，用什么方法来解决这个问题。实验是怎么做的。知道每一句在做什么，可以脑补一下整个过程。</p></li></ul><h1 id="1-深度学习奠基作之一：AlexNet"><a href="#1-深度学习奠基作之一：AlexNet" class="headerlink" title="1.深度学习奠基作之一：AlexNet"></a>1.深度学习奠基作之一：AlexNet</h1><h1 id="2-撑起计算机视觉半边天的ResNet"><a href="#2-撑起计算机视觉半边天的ResNet" class="headerlink" title="2.撑起计算机视觉半边天的ResNet"></a>2.撑起计算机视觉半边天的ResNet</h1><p><em><strong>Deep Residual Learning for Image Recognition</strong></em></p><h1 id="3-Transformer论文逐段精读"><a href="#3-Transformer论文逐段精读" class="headerlink" title="3.Transformer论文逐段精读"></a>3.Transformer论文逐段精读</h1><p><a href="https://yangqinghui66.github.io/2023/11/01/lun-wen-jing-du-attention-is-all-you-need/">【论文精读】Attention-is-all-you-need</a><br><a href="https://yangqinghui66.github.io/2023/12/04/lun-wen-zhu-duan-jing-du-transformer-lun-wen-zhu-duan-jing-du/">【论文逐段精读】Swin Transformer论文逐段精读</a></p><h1 id="4-零基础多图详解图神经网络（GNN-GCN）"><a href="#4-零基础多图详解图神经网络（GNN-GCN）" class="headerlink" title="4.零基础多图详解图神经网络（GNN/GCN）"></a>4.零基础多图详解图神经网络（GNN/GCN）</h1><h1 id="5-GAN论文逐段精读"><a href="#5-GAN论文逐段精读" class="headerlink" title="5.GAN论文逐段精读"></a>5.GAN论文逐段精读</h1><h1 id="6-BERT论文逐段精读"><a href="#6-BERT论文逐段精读" class="headerlink" title="6.BERT论文逐段精读"></a>6.BERT论文逐段精读</h1><h1 id="7-ViT论文逐段精读"><a href="#7-ViT论文逐段精读" class="headerlink" title="7.ViT论文逐段精读"></a>7.ViT论文逐段精读</h1><h1 id="8-MAE论文逐段精读"><a href="#8-MAE论文逐段精读" class="headerlink" title="8.MAE论文逐段精读"></a>8.MAE论文逐段精读</h1><h1 id="9-MoCo论文逐段精读"><a href="#9-MoCo论文逐段精读" class="headerlink" title="9.MoCo论文逐段精读"></a>9.MoCo论文逐段精读</h1><h1 id="10-Deepmind用机器学习指导数学直觉论文逐段精读"><a href="#10-Deepmind用机器学习指导数学直觉论文逐段精读" class="headerlink" title="10.Deepmind用机器学习指导数学直觉论文逐段精读"></a>10.Deepmind用机器学习指导数学直觉论文逐段精读</h1><h1 id="11-Swin-Transformer论文逐段精读"><a href="#11-Swin-Transformer论文逐段精读" class="headerlink" title="11.Swin Transformer论文逐段精读"></a>11.Swin Transformer论文逐段精读</h1><h1 id="如何找研究想法-1"><a href="#如何找研究想法-1" class="headerlink" title="如何找研究想法 1"></a>如何找研究想法 1</h1></body></html>]]></content>
      
      
      <categories>
          
          <category> 论文精读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【笔记】十分钟 机器学习 系列视频 《统计学习方法》（简博士）</title>
      <link href="/2023/10/28/bi-ji-shi-fen-zhong-ji-qi-xue-xi-xi-lie-shi-pin-tong-ji-xue-xi-fang-fa-jian-bo-shi/"/>
      <url>/2023/10/28/bi-ji-shi-fen-zhong-ji-qi-xue-xi-xi-lie-shi-pin-tong-ji-xue-xi-fang-fa-jian-bo-shi/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h1 id="第一节-统计学习的定义与分类"><a href="#第一节-统计学习的定义与分类" class="headerlink" title="第一节 统计学习的定义与分类"></a>第一节 统计学习的定义与分类</h1><h3 id="统计学习的应用"><a href="#统计学习的应用" class="headerlink" title="统计学习的应用"></a>统计学习的应用</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/1-1.png"></p><h3 id="统计学习的概念"><a href="#统计学习的概念" class="headerlink" title="统计学习的概念"></a>统计学习的概念</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/1-3.png"><br><em><strong>主要特点</strong></em><br>（1）以计算机及网络为平台，是建立在计算机及网络上的<br>（2）以数据为研究对象，是数据驱动的学科<br>（3）目的是对数据进行预测与分析<br>（4）以方法为中心，统计学习方法构建模型并应用模型进行预测与分析<br>（5）是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独自的理论体系与方法论。<br><em><strong>三要素</strong></em><br>统计学习方法包括模型的假设空间、模型选择的准则以及模型学习的算法。称其为统计学习方法的三要素，简称为<code>模型</code>(model)、<code>策略</code>(strategy)和<code>算法</code>(algoriithm)。</p><h3 id="统计学习方法的步骤"><a href="#统计学习方法的步骤" class="headerlink" title="统计学习方法的步骤"></a>统计学习方法的步骤</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/1-4.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/1-5.png"></p><h3 id="统计学习的分类"><a href="#统计学习的分类" class="headerlink" title="统计学习的分类"></a>统计学习的分类</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/1-6.png"></p><h1 id="第二节统计学习的基本分类"><a href="#第二节统计学习的基本分类" class="headerlink" title="第二节统计学习的基本分类"></a>第二节统计学习的基本分类</h1><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-1.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-2.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-3.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-4.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-5.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-6.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-7.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-8.png"></p><h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-9.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-10.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-11.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-12.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-13.png"></p><h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/2-14.png"></p><h1 id="第三节统计学习的三要素"><a href="#第三节统计学习的三要素" class="headerlink" title="第三节统计学习的三要素"></a>第三节统计学习的三要素</h1><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/3-1-0.png"></p><h3 id="监督学习：模型"><a href="#监督学习：模型" class="headerlink" title="监督学习：模型"></a>监督学习：模型</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/3-2.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/3-3.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/3-4.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/3-5.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/3-6.png"></p><h3 id="监督学习：策略"><a href="#监督学习：策略" class="headerlink" title="监督学习：策略"></a>监督学习：策略</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/3-7.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/3-8.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/3-9.png"></p><h3 id="监督学习：算法"><a href="#监督学习：算法" class="headerlink" title="监督学习：算法"></a>监督学习：算法</h3><ul><li>算法：如何求解最优模型的问题</li><li>若优化问题存在显式解析解，算法简易</li><li>通常不存在解析解，需要数值计算方法，比如梯度下降法</li></ul><h3 id="无监督学习：三要素"><a href="#无监督学习：三要素" class="headerlink" title="无监督学习：三要素"></a>无监督学习：三要素</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/3-10.png"></p><h1 id="第四节模型的评估和选择"><a href="#第四节模型的评估和选择" class="headerlink" title="第四节模型的评估和选择"></a>第四节模型的评估和选择</h1><h3 id="训练误差与测试误差"><a href="#训练误差与测试误差" class="headerlink" title="训练误差与测试误差"></a>训练误差与测试误差</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/4-1.png"><br><em><strong>训练误差</strong></em><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/4-2.png"><br><em><strong>测试误差</strong></em><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/4-3.png"><br><em><strong>误差率与准确率</strong></em><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/4-4.png"></p><h1 id="过拟合与模型选择"><a href="#过拟合与模型选择" class="headerlink" title="过拟合与模型选择"></a>过拟合与模型选择</h1><p>*** 多项式拟合案例***<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/4-5.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/4-6.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/4-7png"></p><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/4-8.png"></p><h3 id="预测误差与模型复杂度"><a href="#预测误差与模型复杂度" class="headerlink" title="预测误差与模型复杂度"></a>预测误差与模型复杂度</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/4-9.png"></p><h1 id="第五章正则化与交叉验证"><a href="#第五章正则化与交叉验证" class="headerlink" title="第五章正则化与交叉验证"></a>第五章正则化与交叉验证</h1><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/5-1.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/5-2.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/5-3.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/5-4.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/5-5.png"></p><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>数据充足情况下：<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/5-6.png"></p><ul><li>训练集：用以训练模型</li><li>验证集：用以选择模型</li><li>测试集：用以最终对学习方法的评估</li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【笔记】PyTorch深度学习快速入门教程（小土堆）</title>
      <link href="/2023/10/11/bi-ji-pytorch-shen-du-xue-xi-kuai-su-ru-men-jiao-cheng-xiao-tu-dui/"/>
      <url>/2023/10/11/bi-ji-pytorch-shen-du-xue-xi-kuai-su-ru-men-jiao-cheng-xiao-tu-dui/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h1 id="1-PyTorch环境的配置及安装"><a href="#1-PyTorch环境的配置及安装" class="headerlink" title="1.PyTorch环境的配置及安装"></a>1.PyTorch环境的配置及安装</h1><p>安装anaconda<br>安装包：</p><p>打开Anaconda Prompt<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/xiaotudui/1-0.png" alt="1-0.png"><br>配置环境<br>conda create -n pytorch python=3.7<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/xiaotudui/1-1.png" alt="1-1.png"><br>激活环境<br>conda activate pytorch</p><p>jupyter notebook</p><p>安装PyTorch<br><a href="https://pytorch.org/">PyTorch官网</a><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/xiaotudui/1-2.png" alt="1-2.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/xiaotudui//1-3.png" alt="1-3.png"></p><p>无显卡：conda install pytorch torchvision torchaudio cpuonly -c pytorch</p><p>import torch</p><p>torch.cuda.is_available()</p><p>conda install nb_conda</p><h1 id="2-PyTorch编辑器的选择、安装及配置"><a href="#2-PyTorch编辑器的选择、安装及配置" class="headerlink" title="2.PyTorch编辑器的选择、安装及配置"></a>2.PyTorch编辑器的选择、安装及配置</h1><p>PyCharm安装<br><a href="https://www.jetbrains.com/pycharm/">PyCharm官网</a></p><h1 id="5-PyTorch加载数据初认识"><a href="#5-PyTorch加载数据初认识" class="headerlink" title="5.PyTorch加载数据初认识"></a>5.PyTorch加载数据初认识</h1><p>激活D盘“myenv”的虚拟环境<br>conda activate D:\myenv</p><p>jupyter notebook</p><p>from torch.utils.data import Dataset</p><p>help(Dataset)</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 课程笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【笔记】动手学深度学习v2（全73讲）李沐</title>
      <link href="/2023/09/30/bi-ji-dong-shou-xue-shen-du-xue-xi-v2-quan-73-jiang-li-mu/"/>
      <url>/2023/09/30/bi-ji-dong-shou-xue-shen-du-xue-xi-v2-quan-73-jiang-li-mu/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h1 id="00预告"><a href="#00预告" class="headerlink" title="00预告"></a>00预告</h1><p>学习深度学习关键是动手</p><ul><li>深度学习是人工智能最热的领域</li><li>核心是神经网络</li><li>神经网络是一门语言</li><li>应该像学习Python/C++一样学习深度学习<br><a href="https://zh.d2l.ai/">《动手深度学习》书籍链接</a><br><a href="https://courses.d2l.ai/zh-v2/">课程网站</a></li></ul><h1 id="01课程安排"><a href="#01课程安排" class="headerlink" title="01课程安排"></a>01课程安排</h1><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ul><li>介绍深度学习经典和最新模型<ul><li>LeNet, ResNet, LSTM, BERT, …</li></ul></li><li>机器学习<ul><li>损失函数、目标函数、 过拟合、 优化</li></ul></li><li>实践<ul><li>使用Pytorch实现介绍的知识点</li><li>在真实数据上体验算法效果</li></ul></li></ul><h3 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h3><ul><li>深度学习基础——线性神经网络，多层感知机</li><li>卷积神经网络——LeNet, AlexNet, VGG, Inception, ResNet</li><li>循环神经网络——RNN, GRU, LSTM, seq2seq</li><li>注意力机制——Attention, Transformer</li><li>优化算法——SGD, Momentum, Adam</li><li>高性能计算——并行，多GPU，分布式</li><li>计算机视觉——目标检测，语义分割</li><li>自然语言处理——词嵌入，BERT</li></ul><h3 id="你将学到什么"><a href="#你将学到什么" class="headerlink" title="你将学到什么"></a>你将学到什么</h3><ul><li>What<ul><li>深度学习里有那些技术</li></ul></li><li>How<ul><li>如何实现和调参</li></ul></li><li>Why<ul><li>背后的原因（直觉、数学）</li></ul></li></ul><h3 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h3><ul><li><a href="https://courses.d2l.ai/zh-v2/">课程主页</a></li><li><a href="https://zh.d2l.ai/">教材</a></li><li><a href="https://discuss.d2l.ai/c/16">课程论坛讨论</a></li><li><a href="https://discuss.pytorch.org/">Pytorch论坛</a></li></ul><h1 id="02深度学习介绍"><a href="#02深度学习介绍" class="headerlink" title="02深度学习介绍"></a>02深度学习介绍</h1><h1 id="03安装"><a href="#03安装" class="headerlink" title="03安装"></a>03安装</h1><p>pip install d2l -i <a href="http://pypi.douban.com/simple">http://pypi.douban.com/simple</a> –trusted-host pypi.douban.com –user</p><h1 id="05线性代数"><a href="#05线性代数" class="headerlink" title="05线性代数"></a>05线性代数</h1><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>标量由只有一个元素的张量表示</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([3.0])</span><br><span class="line">y = torch.tensor([2.0])</span><br><span class="line"></span><br><span class="line">x + y, x * y, x / y, x**y</span><br></pre></td></tr></tbody></table></figure><p>你可以将向量视为标量值组成的列表</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(4)</span><br><span class="line">x</span><br></pre></td></tr></tbody></table></figure><p>通过张量的索引来访问任一元素</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[3]</span><br></pre></td></tr></tbody></table></figure><h1 id="06矩阵计算"><a href="#06矩阵计算" class="headerlink" title="06矩阵计算"></a>06矩阵计算</h1><h1 id="07-自动求导"><a href="#07-自动求导" class="headerlink" title="07 自动求导"></a>07 自动求导</h1><h1 id="08线性回归-基础优化算法"><a href="#08线性回归-基础优化算法" class="headerlink" title="08线性回归 + 基础优化算法"></a>08线性回归 + 基础优化算法</h1><p><em><strong>线性回归</strong></em></p><h3 id="一个简化模型"><a href="#一个简化模型" class="headerlink" title="一个简化模型"></a>一个简化模型</h3><p>![](<a href="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-1">https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-1</a> .png)</p><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-2.png"></p><h3 id="线性模型可以看做是单层神经网络"><a href="#线性模型可以看做是单层神经网络" class="headerlink" title="线性模型可以看做是单层神经网络"></a>线性模型可以看做是单层神经网络</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-3.png"></p><h3 id="神经网络源于神经科学"><a href="#神经网络源于神经科学" class="headerlink" title="神经网络源于神经科学"></a>神经网络源于神经科学</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-4.png"></p><h3 id="衡量预估质量"><a href="#衡量预估质量" class="headerlink" title="衡量预估质量"></a>衡量预估质量</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-5.png"></p><h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-6.png"></p><h3 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-7.png"></p><h3 id="显示解"><a href="#显示解" class="headerlink" title="显示解"></a>显示解</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-8.png"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-9.png"><br><em><strong>基础优化方法</strong></em></p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-10.png"></p><h3 id="选择学习率"><a href="#选择学习率" class="headerlink" title="选择学习率"></a>选择学习率</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-11.png"></p><h3 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-12.png"></p><h3 id="选择批量大小"><a href="#选择批量大小" class="headerlink" title="选择批量大小"></a>选择批量大小</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-13.png"></p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/8-14.png"></p><h3 id="线性回归的从零开始实现"><a href="#线性回归的从零开始实现" class="headerlink" title="线性回归的从零开始实现"></a>线性回归的从零开始实现</h3><p>我们将从零开始实现整个方法，包括数据流水线、模型、损失函数和小批量随机梯度下降优化器</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import math</span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">from d2l import torch as d2l</span><br></pre></td></tr></tbody></table></figure><h1 id="09-Softmax-回归-损失函数-图片分类数据集"><a href="#09-Softmax-回归-损失函数-图片分类数据集" class="headerlink" title="09 Softmax 回归 + 损失函数 + 图片分类数据集"></a>09 Softmax 回归 + 损失函数 + 图片分类数据集</h1><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-1.png"><br>Softmax 回归是机器学习中非常经典、重要的模型，虽然它名字里带着回归，但它其实是个分类问题</p><h3 id="回归-vs-分类"><a href="#回归-vs-分类" class="headerlink" title="回归 vs 分类"></a>回归 vs 分类</h3><ul><li>回归估计一个连续值</li><li>分类预测一个离散类别<br>MNIST：手写数字识别（10类）<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-2.png"><br>ImgeNet：自然物体分类（1000类）<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-3.png"></li></ul><h3 id="Kaggle上的分类问题"><a href="#Kaggle上的分类问题" class="headerlink" title="Kaggle上的分类问题"></a>Kaggle上的分类问题</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-4.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-5.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-6.png"></p><h3 id="从回归到多类分类"><a href="#从回归到多类分类" class="headerlink" title="从回归到多类分类"></a>从回归到多类分类</h3><p><code>回归</code></p><ul><li>单连续数值输出</li><li>自然区间R</li><li>跟真实值的区别作为损失<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-7.png"><br><code>分类</code></li><li>通常多个输出</li><li>输出i是预测为第i类的置信度<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-8.png"></li></ul><h3 id="从回归到多类分类——均方损失"><a href="#从回归到多类分类——均方损失" class="headerlink" title="从回归到多类分类——均方损失"></a>从回归到多类分类——均方损失</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-9.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-10.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-11.png"></p><h3 id="Softmax和交叉熵损失"><a href="#Softmax和交叉熵损失" class="headerlink" title="Softmax和交叉熵损失"></a>Softmax和交叉熵损失</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-12.png"></p><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><ul><li>Softmax回归是一个多类分类模型</li><li>使用Softmax操作得到每个类的预测置信度</li><li>使用交叉熵来衡量预测和标号的区别</li></ul><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>用来衡量预测值和真实值之间的区别<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-13.png"></p><ol><li><p>均方损失函数<br> <img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-14.png"><br> 蓝色的线——当y=0时，变换预测值后y’的函数<br> 绿色的线——似然函数<br> 橙色的线——损失函数的梯度</p></li><li><p>绝对值损失函数</p></li></ol><h3 id="softmax回归的从零开始实现"><a href="#softmax回归的从零开始实现" class="headerlink" title="softmax回归的从零开始实现"></a>softmax回归的从零开始实现</h3><p>就像我们从零开始实现线性回归一样， 你应该知道实现softmax的细节</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from IPython import display</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">batch_size = 256 #每次随机读取256张图片</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">#返回训练集和测试集的迭代器</span><br></pre></td></tr></tbody></table></figure><p>将展平每个图像，把它们看作长度为784的向量。 因为我们的数据集有10个类别，所以网络输出维度为 10</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#图片的长和宽都为28，28*28=784</span><br><span class="line">#对于softmax函数而言，输入必须是一个向量，需要把整个图片拉长，拉成一个向量</span><br><span class="line">num_inputs = 784</span><br><span class="line">num_outputs = 10</span><br><span class="line"></span><br><span class="line">#定义权重w，初始成一个高斯随机分布的值，均值为0，方差为0.01，形状是行数等于输入的个数、列数等于输出的个数，计算梯度requires_grad=True</span><br><span class="line">W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=True) #对于每一个输出都需要一个偏移</span><br></pre></td></tr></tbody></table></figure><p>给定一个矩阵<code>X</code>，我们可以对所有元素求和</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#定义一个2*3的矩阵</span><br><span class="line">X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])</span><br><span class="line">#如果按照维度=0来求和的话</span><br><span class="line">X.sum(0, keepdim=True), X.sum(1, keepdim=True)</span><br></pre></td></tr></tbody></table></figure><p>实现softmax<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/9-15.png"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def softmax(X):</span><br><span class="line">    X_exp = torch.exp(X) #对每个元素做指数计算</span><br><span class="line">    partition = X_exp.sum(1, keepdim=True) #按照维度为1来求和</span><br><span class="line">    return X_exp / partition #广播机制：X_exp的每一行除以partition对应行的那个数</span><br></pre></td></tr></tbody></table></figure><p>我们将每个元素变成一个非负数。此外，依据概率原理，每行总和为1</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.normal(0, 1, (2, 5)) #创建一个随机的均值为0，方差为1的两行五列的矩阵X</span><br><span class="line">X_prob = softmax(X) #将X放入softmax函数</span><br><span class="line">X_prob, X_prob.sum(1)</span><br></pre></td></tr></tbody></table></figure><p>实现softmax回归模型</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def net(X):</span><br><span class="line">    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)</span><br></pre></td></tr></tbody></table></figure><p>创建一个数据y_hat，其中包含2个样本在3个类别的预测概率， 使用y作为y_hat中概率的索引</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = torch.tensor([0, 2])</span><br><span class="line">y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])</span><br><span class="line">y_hat[[0, 1], y]</span><br></pre></td></tr></tbody></table></figure><h1 id="10多层感知机"><a href="#10多层感知机" class="headerlink" title="10多层感知机"></a>10多层感知机</h1><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-1.png"></p><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><ul><li><p>给定输入x，权重w，和偏移b，感知机输出：<br>（其中x，w为向量，b为标量）<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-2.png"><br>输入w和权重x做内积，加上偏移b<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-3.png"></p></li><li><p>二分类：-1或1</p><ul><li>VS 回归输出实数</li><li>VS Softmax回归输出概率</li></ul></li></ul><h3 id="训练感知机"><a href="#训练感知机" class="headerlink" title="训练感知机"></a>训练感知机</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-4.png"><br><code>预测和真实值如果准确就是同号，预测错误就是异号</code><br>等价于使用批量大小为1的梯度下降，并使用如下的损失函数：<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-5.png"></p><h3 id="收敛定理"><a href="#收敛定理" class="headerlink" title="收敛定理"></a>收敛定理</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-6.png"></p><h3 id="XOR问题（Minsky-Papert，1969）"><a href="#XOR问题（Minsky-Papert，1969）" class="headerlink" title="XOR问题（Minsky &amp; Papert，1969）"></a>XOR问题（Minsky &amp; Papert，1969）</h3><p>感知机不能拟合XOR函数，它智能产生线性分割面<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-7.png"></p><h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><ul><li>感知机是一个二分类模型，是最早的AI模型之一</li><li>它的求解算法等价于使用批量大小为1的梯度下降</li><li>它不能拟合XOR函数，导致的第一次AI寒冬</li></ul><h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-8.png"></p><h3 id="学习XOR"><a href="#学习XOR" class="headerlink" title="学习XOR"></a>学习XOR</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-9.png"></p><h3 id="单隐藏层"><a href="#单隐藏层" class="headerlink" title="单隐藏层"></a>单隐藏层</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-10.png"></p><h3 id="单隐藏层——单分类"><a href="#单隐藏层——单分类" class="headerlink" title="单隐藏层——单分类"></a>单隐藏层——单分类</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-11.png"></p><h3 id="Sigmoid激活函数"><a href="#Sigmoid激活函数" class="headerlink" title="Sigmoid激活函数"></a>Sigmoid激活函数</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-12.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-13.png"></p><h3 id="Tanh激活函数"><a href="#Tanh激活函数" class="headerlink" title="Tanh激活函数"></a>Tanh激活函数</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-14.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-15.png"></p><h3 id="ReLU激活函数"><a href="#ReLU激活函数" class="headerlink" title="ReLU激活函数"></a>ReLU激活函数</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-16.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-17.png"></p><h3 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-18.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-19.png"></p><h3 id="多隐藏层"><a href="#多隐藏层" class="headerlink" title="多隐藏层"></a>多隐藏层</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-20.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-21.png"></p><h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h3><ul><li>多层感知机使用隐藏层和激活函数来得到非线性模型</li><li>常用激活函数是Sigmoid，Tanh，ReLU</li><li>使用Softmax来处理多类分类</li><li>超参数为隐藏层数，和各个隐藏层大小</li></ul><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><p>多层感知机的从零开始实现</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">batch_size = 256</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></tbody></table></figure><p>实现一个具有单隐藏层的多层感知机，它包含256个隐藏单元</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = 784, 10, 256</span><br><span class="line"></span><br><span class="line">W1 = nn.Parameter(</span><br><span class="line">    torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))</span><br><span class="line">W2 = nn.Parameter(</span><br><span class="line">    torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br></pre></td></tr></tbody></table></figure><p>实现ReLU激活函数</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def relu(X):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    return torch.max(X, a)</span><br></pre></td></tr></tbody></table></figure><p>实现我们的模型</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def net(X):</span><br><span class="line">    X = X.reshape((-1, num_inputs))</span><br><span class="line">    H = relu(X @ W1 + b1)</span><br><span class="line">    return (H @ W2 + b2)</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></tbody></table></figure><p>多层感知机的训练过程与softmax回归的训练过程完全相同</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = 10, 0.1</span><br><span class="line">updater = torch.optim.SGD(params, lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-22.png"></p><p>在一些测试数据上应用这个模型</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.predict_ch3(net, test_iter)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/10-23.png"></p><h1 id="11模型选择-过拟合和欠拟合"><a href="#11模型选择-过拟合和欠拟合" class="headerlink" title="11模型选择 + 过拟合和欠拟合"></a>11模型选择 + 过拟合和欠拟合</h1><h2 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a>训练误差和泛化误差</h2><ul><li>训练误差：模型在训练数据上的误差</li><li>泛化误差：模型在新数据上的误差</li><li>例子：根据摸考成绩来预测未来考试分数<ul><li>在过去的考试中表现很好(训练误差)不代表未来考试一定会好(泛化误差)</li><li>学生A通过背书在摸考中拿到很好成绩</li><li>学生B知道答案后面的原因</li></ul></li></ul><h3 id="验证数据集和测试数据集"><a href="#验证数据集和测试数据集" class="headerlink" title="验证数据集和测试数据集"></a>验证数据集和测试数据集</h3><ul><li>验证数据集：一个用来评估模型好坏的数据集<ul><li>例如拿出50%的训练数据</li><li>不要跟训练数据混在一起（常犯错误)</li></ul></li><li>测试数据集：只用一次的数据集。例如<ul><li>未来的考试</li><li>我出价的房子的实际成交价</li><li>用在Kaggle私有排行榜中的数据集</li></ul></li></ul><h3 id="K-则交叉验证"><a href="#K-则交叉验证" class="headerlink" title="K-则交叉验证"></a>K-则交叉验证</h3><ul><li>在没有足够多数据时使用（(这是常态)</li><li>算法：<ul><li>将训练数据分割成K块-</li><li>For i= 1,…,K<ul><li>使用第i块作为验证数据集，其余的作为训练数据集</li></ul></li><li>报告K个验证集误差的平均</li></ul></li><li>常用：K=5或10</li></ul><h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h2><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-1.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-2.png"></p><h3 id="模型容量"><a href="#模型容量" class="headerlink" title="模型容量"></a>模型容量</h3><ul><li>拟合各种函数的能力</li><li>低容量的模型难以拟合训练数据</li><li>高容量的模型可以记住所有的训练数据</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-3.png"></p><h3 id="模型容量的影响"><a href="#模型容量的影响" class="headerlink" title="模型容量的影响"></a>模型容量的影响</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-4.png"></p><h3 id="估计模型容量"><a href="#估计模型容量" class="headerlink" title="估计模型容量"></a>估计模型容量</h3><ul><li>难以在不同的种类算法之间比较<ul><li>例如树模型和神经网络</li></ul></li><li>给定一个模型种类,将有两个主要因素<ul><li>参数的个数</li><li>参数值的选择范围<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-5.png"></li></ul></li></ul><h3 id="VC维"><a href="#VC维" class="headerlink" title="VC维"></a>VC维</h3><ul><li>统计学习理论的一个核心思想</li><li>对于一个分类模型，VC等于一个最大的数据集的大小，不管如何给定标号，都存在一个模型来对它进行完美分类<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-6.png"></li></ul><h3 id="线性分类器的VC维"><a href="#线性分类器的VC维" class="headerlink" title="线性分类器的VC维"></a>线性分类器的VC维</h3><ul><li>2维输入的感知机，VC维=3<ul><li>能够分类任何三个点，但不是4个(xor)</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-7.png"></p><ul><li>支持N维输入的感知机的VC维是N+1</li><li>—些多层感知机的VC维O(N $log_2$ N)</li></ul><h3 id="VC维的用处"><a href="#VC维的用处" class="headerlink" title="VC维的用处"></a>VC维的用处</h3><ul><li>提供为什么一个模型好的理论依据<ul><li>它可以衡量训练误差和泛化误差之间的间隔</li></ul></li><li>但深度学习中很少使用<ul><li>衡量不是很准确</li><li>计算深度学习模型的VC维很困难</li></ul></li></ul><h3 id="数据复杂度"><a href="#数据复杂度" class="headerlink" title="数据复杂度"></a>数据复杂度</h3><ul><li>多个重要因素<ul><li>样本个数</li><li>个样本的元素个数</li><li>时间、空间结构多样性</li></ul></li></ul><h3 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a>总结</h3><ul><li>模型容量需要匹配数据复杂度，否则可能导致欠拟合和过拟合</li><li>统计机器学习提供数学工具来衡量模型复杂度</li><li>实际中一般靠观察训练误差和验证误差</li></ul><h2 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h2><p>模型选择、欠拟合和过拟合<br>通过多项式拟合来交互地探索这些概念</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br></pre></td></tr></tbody></table></figure><p>使用以下三阶多项式来生成训练和测试数据的标签：<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-8.png"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">max_degree = 20</span><br><span class="line">n_train, n_test = 100, 100</span><br><span class="line">true_w = np.zeros(max_degree)</span><br><span class="line">true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])</span><br><span class="line"></span><br><span class="line">features = np.random.normal(size=(n_train + n_test, 1))</span><br><span class="line">np.random.shuffle(features)</span><br><span class="line">poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))</span><br><span class="line">for i in range(max_degree):</span><br><span class="line">    poly_features[:, i] /= math.gamma(i + 1)</span><br><span class="line">labels = np.dot(poly_features, true_w)</span><br><span class="line">labels += np.random.normal(scale=0.1, size=labels.shape)</span><br></pre></td></tr></tbody></table></figure><p>看一下前2个样本</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">true_w, features, poly_features, labels = [</span><br><span class="line">    torch.tensor(x, dtype=torch.float32)</span><br><span class="line">    for x in [true_w, features, poly_features, labels]]</span><br><span class="line"></span><br><span class="line">features[:2], poly_features[:2, :], labels[:2]</span><br></pre></td></tr></tbody></table></figure><p>实现一个函数来评估模型在给定数据集上的损失</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def evaluate_loss(net, data_iter, loss):  </span><br><span class="line">    """评估给定数据集上模型的损失。"""</span><br><span class="line">    metric = d2l.Accumulator(2)</span><br><span class="line">    for X, y in data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.sum(), l.numel())</span><br><span class="line">    return metric[0] / metric[1]</span><br></pre></td></tr></tbody></table></figure><p>定义训练函数</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def train(train_features, test_features, train_labels, test_labels,</span><br><span class="line">          num_epochs=400):</span><br><span class="line">    loss = nn.MSELoss()</span><br><span class="line">    input_shape = train_features.shape[-1]</span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))</span><br><span class="line">    batch_size = min(10, train_labels.shape[0])</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels.reshape(-1, 1)),</span><br><span class="line">                                batch_size)</span><br><span class="line">    test_iter = d2l.load_array((test_features, test_labels.reshape(-1, 1)),</span><br><span class="line">                               batch_size, is_train=False)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=0.01)</span><br><span class="line">    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',</span><br><span class="line">                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],</span><br><span class="line">                            legend=['train', 'test'])</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class="line">        if epoch == 0 or (epoch + 1) % 20 == 0:</span><br><span class="line">            animator.add(epoch + 1, (evaluate_loss(</span><br><span class="line">                net, train_iter, loss), evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    print('weight:', net[0].weight.data.numpy())</span><br></pre></td></tr></tbody></table></figure><p>三阶多项式函数拟合(正态)</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train(poly_features[:n_train, :4], poly_features[n_train:, :4],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-9.png"><br>线性函数拟合(欠拟合)</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train(poly_features[:n_train, :2], poly_features[n_train:, :2],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-10.png"><br>高阶多项式函数拟合(过拟合)</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class="line">      labels[:n_train], labels[n_train:], num_epochs=1500)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/11-11.png"></p><h1 id="12权重衰退"><a href="#12权重衰退" class="headerlink" title="12权重衰退"></a>12权重衰退</h1><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-1.png"></p><h3 id="使用均方范数作为硬性限制"><a href="#使用均方范数作为硬性限制" class="headerlink" title="使用均方范数作为硬性限制"></a>使用均方范数作为硬性限制</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-2.png"></p><h3 id="使用均方范数作为柔性限制"><a href="#使用均方范数作为柔性限制" class="headerlink" title="使用均方范数作为柔性限制"></a>使用均方范数作为柔性限制</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-3.png"></p><h3 id="演示对最优解的影响"><a href="#演示对最优解的影响" class="headerlink" title="演示对最优解的影响"></a>演示对最优解的影响</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-4.png"></p><h3 id="参数更新法则"><a href="#参数更新法则" class="headerlink" title="参数更新法则"></a>参数更新法则</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-5.png"></p><h3 id="总结-6"><a href="#总结-6" class="headerlink" title="总结"></a>总结</h3><ul><li>权重衰退通过L2正则项使得模型参数不会过大，从而控制模型复杂度</li><li>正则项权重是控制模型复杂度的超参数</li></ul><h3 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h3><p>权重衰减是最广泛使用的正则化的技术之一</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br></pre></td></tr></tbody></table></figure><p>像以前一样生成一些数据<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-6.png"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5</span><br><span class="line">true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05</span><br><span class="line">train_data = d2l.synthetic_data(true_w, true_b, n_train)</span><br><span class="line">train_iter = d2l.load_array(train_data, batch_size)</span><br><span class="line">test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class="line">test_iter = d2l.load_array(test_data, batch_size, is_train=False)</span><br></pre></td></tr></tbody></table></figure><p>初始化模型参数</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def init_params():</span><br><span class="line">    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)</span><br><span class="line">    b = torch.zeros(1, requires_grad=True)</span><br><span class="line">    return [w, b]</span><br></pre></td></tr></tbody></table></figure><p>定义L2范数惩罚</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def l2_penalty(w):</span><br><span class="line">    return torch.sum(w.pow(2)) / 2</span><br></pre></td></tr></tbody></table></figure><p>定义训练代码实现</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def train(lambd):</span><br><span class="line">    w, b = init_params()</span><br><span class="line">    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss</span><br><span class="line">    num_epochs, lr = 100, 0.003</span><br><span class="line">    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',</span><br><span class="line">                            xlim=[5, num_epochs], legend=['train', 'test'])</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        for X, y in train_iter:</span><br><span class="line">            with torch.enable_grad():</span><br><span class="line">                l = loss(net(X), y) + lambd * l2_penalty(w)</span><br><span class="line">            l.sum().backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        if (epoch + 1) % 5 == 0:</span><br><span class="line">            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    print('w的L2范数是：', torch.norm(w).item())</span><br></pre></td></tr></tbody></table></figure><p>忽略正则化直接训练</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(lambd=0)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-7.png"><br>使用权重衰减</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(lambd=3)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-8.png"><br>简洁实现</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def train_concise(wd):</span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, 1))</span><br><span class="line">    for param in net.parameters():</span><br><span class="line">        param.data.normal_()</span><br><span class="line">    loss = nn.MSELoss()</span><br><span class="line">    num_epochs, lr = 100, 0.003</span><br><span class="line">    trainer = torch.optim.SGD([{</span><br><span class="line">        "params": net[0].weight,</span><br><span class="line">        'weight_decay': wd}, {</span><br><span class="line">            "params": net[0].bias}], lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',</span><br><span class="line">                            xlim=[5, num_epochs], legend=['train', 'test'])</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        for X, y in train_iter:</span><br><span class="line">            with torch.enable_grad():</span><br><span class="line">                trainer.zero_grad()</span><br><span class="line">                l = loss(net(X), y)</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        if (epoch + 1) % 5 == 0:</span><br><span class="line">            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    print('w的L2范数：', net[0].weight.norm().item())</span><br></pre></td></tr></tbody></table></figure><p>这些图看起来和我们从零开始实现权重衰减时的图相同</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_concise(0)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-9.png"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_concise(3)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/12-10.png"></p><h1 id="13丢弃法"><a href="#13丢弃法" class="headerlink" title="13丢弃法"></a>13丢弃法</h1><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><ul><li>一个好的模型需要对输入数据的扰动鲁棒<ul><li>使用有噪音的数据等价于Tikhonov正则</li><li>丢弃法：在层之间加入噪音</li></ul></li></ul><h3 id="无偏差的加入噪音"><a href="#无偏差的加入噪音" class="headerlink" title="无偏差的加入噪音"></a>无偏差的加入噪音</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/13-1.png"></p><h3 id="使用丢弃法"><a href="#使用丢弃法" class="headerlink" title="使用丢弃法"></a>使用丢弃法</h3><p>通常将丢弃法作用在隐藏全连接层的输出上<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/13-2.png"></p><h3 id="推理中的丢弃法"><a href="#推理中的丢弃法" class="headerlink" title="推理中的丢弃法"></a>推理中的丢弃法</h3><ul><li>正则项只在训练中使用：他们影响模型参数的更新</li><li>在推理过程中，丢弃法直接返回输入h = dropout(h)<ul><li>这样也能保证确定性的输出</li></ul></li></ul><h3 id="总结-7"><a href="#总结-7" class="headerlink" title="总结"></a>总结</h3><ul><li>丢弃法将一些输出项随机置0来控制模型复杂度</li><li>常作用在多层感知机的隐藏层输出上</li><li>丢弃概率是控制模型复杂度的超参数</li></ul><h3 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h3><p>我们实现<code>dropout_layer</code>函数，该函数以<code>dropout</code>的概率丢弃张量输入<code>X</code>中的元素</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">def dropout_layer(X, dropout):</span><br><span class="line">    assert 0 &lt;= dropout &lt;= 1</span><br><span class="line">    if dropout == 1:</span><br><span class="line">        return torch.zeros_like(X)</span><br><span class="line">    if dropout == 0:</span><br><span class="line">        return X</span><br><span class="line">    mask = (torch.Tensor(X.shape).uniform_(0, 1) &gt; dropout).float()</span><br><span class="line">    return mask * X / (1.0 - dropout)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>测试<code>dropout_layer</code>函数</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(16, dtype=torch.float32).reshape((2, 8))</span><br><span class="line">print(X)</span><br><span class="line">print(dropout_layer(X, 0.))</span><br><span class="line">print(dropout_layer(X, 0.5))</span><br><span class="line">print(dropout_layer(X, 1.))</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/13-3.png"><br>定义具有两个隐藏层的多层感知机，每个隐藏层包含256个单元</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256</span><br><span class="line"></span><br><span class="line">dropout1, dropout2 = 0.2, 0.5</span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span><br><span class="line">                 is_training=True):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.num_inputs = num_inputs</span><br><span class="line">        self.training = is_training</span><br><span class="line">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class="line">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class="line">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    def forward(self, X):</span><br><span class="line">        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))</span><br><span class="line">        if self.training == True:</span><br><span class="line">            H1 = dropout_layer(H1, dropout1)</span><br><span class="line">        H2 = self.relu(self.lin2(H1))</span><br><span class="line">        if self.training == True:</span><br><span class="line">            H2 = dropout_layer(H2, dropout2)</span><br><span class="line">        out = self.lin3(H2)</span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br></pre></td></tr></tbody></table></figure><p>训练和测试</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr, batch_size = 10, 0.5, 256</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/13-4.png"><br>简洁实现</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),</span><br><span class="line">                    nn.Dropout(dropout1), nn.Linear(256, 256), nn.ReLU(),</span><br><span class="line">                    nn.Dropout(dropout2), nn.Linear(256, 10))</span><br><span class="line"></span><br><span class="line">def init_weights(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=0.01)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights);</span><br></pre></td></tr></tbody></table></figure><p>对模型进行训练和测试</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></tbody></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/13-5.png"></p><h1 id="14数值稳定性-模型初始化和激活函数"><a href="#14数值稳定性-模型初始化和激活函数" class="headerlink" title="14数值稳定性 + 模型初始化和激活函数"></a>14数值稳定性 + 模型初始化和激活函数</h1><h1 id="19卷积层"><a href="#19卷积层" class="headerlink" title="19卷积层"></a>19卷积层</h1><p><em><strong>分类猫和狗</strong></em></p><ul><li>使用一个还不错的相机采集图片（12M像素）</li><li>RGB图片有36M元素</li><li>使用100大小的单隐藏层MLP，模型有3.6B元素<ul><li>远多于世界上所有猫和狗总算（900M狗，600M猫）</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-1.png"><br><em><strong>回顾：单隐藏层MLP</strong></em><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-2.png"></p><h3 id="两个原则"><a href="#两个原则" class="headerlink" title="两个原则"></a>两个原则</h3><ul><li>平移不变性</li><li>局部性</li></ul><h3 id="重新考察全连接层"><a href="#重新考察全连接层" class="headerlink" title="重新考察全连接层"></a>重新考察全连接层</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-3.png"></p><h3 id="原则1—平移不变性"><a href="#原则1—平移不变性" class="headerlink" title="原则1—平移不变性"></a>原则1—平移不变性</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-4.png"></p><h3 id="原则2—局部性5"><a href="#原则2—局部性5" class="headerlink" title="原则2—局部性5"></a>原则2—局部性5</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-5.png"><br><em><strong>总结</strong></em><br>对全连接层使用平移不变性和局部性得到卷积层<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-6.png"></p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-7.png"></p><h3 id="二维交叉相关"><a href="#二维交叉相关" class="headerlink" title="二维交叉相关"></a>二维交叉相关</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-8.png"></p><h3 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-9.png"><br>例子<br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-10.png"></p><h3 id="交叉相关vs卷积"><a href="#交叉相关vs卷积" class="headerlink" title="交叉相关vs卷积"></a>交叉相关vs卷积</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-11.png"></p><h3 id="一维和三维交叉相关"><a href="#一维和三维交叉相关" class="headerlink" title="一维和三维交叉相关"></a>一维和三维交叉相关</h3><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/19-12.png"><br><em><strong>总结</strong></em></p><ul><li>卷积层将输入和核矩阵进行交叉相关，加上偏移后得到输出</li><li>核矩阵和偏移是可学习的参数</li><li>核矩阵的大小是超参数</li></ul><h1 id="68-Transformer"><a href="#68-Transformer" class="headerlink" title="68 Transformer"></a>68 Transformer</h1><h3 id="Transformer架构"><a href="#Transformer架构" class="headerlink" title="Transformer架构"></a>Transformer架构</h3><ul><li>基于编码器-解码器架构来处理序列对</li><li>跟使用注意力的seq2seq不同，Transformer是<em><strong>纯基于注意力</strong></em></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/68-1.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/68-2.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/68-3.png"></p><h3 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h3><ul><li>对同一Key，value，query，希望抽取不同的信息<ul><li>例如短距离关系和长距离关系</li></ul></li><li>多头注意力使用h个独立的注意力池化<ul><li>合并各个头（head）输出得到最终输出</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/68-4.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/68-5.png"></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 课程笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【leetcode】704、二分查找</title>
      <link href="/2023/09/26/leetcode-704-er-fen-cha-zhao/"/>
      <url>/2023/09/26/leetcode-704-er-fen-cha-zhao/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>给定一个 <code>n</code> 个元素有序的（升序）整型数组 <code>nums</code> 和一个目标值 <code>target</code> ，写一个函数搜索 <code>nums</code> 中的 <code>target</code>，如果目标值存在返回下标，否则返回 <code>-1</code>。</p><p><strong>示例 1:</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入: nums = [-1,0,3,5,9,12], target = 9</span><br><span class="line">输出: 4</span><br><span class="line">解释: 9 出现在 nums 中并且下标为 4</span><br></pre></td></tr></tbody></table></figure><p><strong>示例 2:</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入: nums = [-1,0,3,5,9,12], target = 2</span><br><span class="line">输出: -1</span><br><span class="line">解释: 2 不存在 nums 中因此返回 -1</span><br></pre></td></tr></tbody></table></figure><p><strong>提示：</strong></p><ol><li>你可以假设 <code>nums</code> 中的所有元素是不重复的。</li><li><code>n</code> 将在 <code>[1, 10000]</code>之间。</li><li><code>nums</code> 的每个元素都将在 <code>[-9999, 9999]</code>之间。</li></ol></body></html>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【leetcode】1、两数之和</title>
      <link href="/2023/09/26/leetcode-1-liang-shu-zhi-he/"/>
      <url>/2023/09/26/leetcode-1-liang-shu-zhi-he/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>给定一个整数数组 <code>nums</code> 和一个整数目标值 <code>target</code>，请你在该数组中找出 <strong>和为目标值</strong> <em><code>target</code></em> 的那 <strong>两个</strong> 整数，并返回它们的数组下标。</p><p>你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。</p><p>你可以按任意顺序返回答案。</p><p><strong>示例 1：</strong></p><hr><p>输入：nums = [2,7,11,15], target = 9<br>输出：[0,1]<br>解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。</p><hr><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：nums = [3,2,4], target = 6</span><br><span class="line">输出：[1,2]</span><br></pre></td></tr></tbody></table></figure><p><strong>示例 3：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：nums = [3,3], target = 6</span><br><span class="line">输出：[0,1]</span><br></pre></td></tr></tbody></table></figure><p><strong>提示：</strong></p><ul><li><code>2 &lt;= nums.length &lt;= 104</code></li><li><code>-109 &lt;= nums[i] &lt;= 109</code></li><li><code>-109 &lt;= target &lt;= 109</code></li><li><strong>只会存在一个有效答案</strong></li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>markdown语法</title>
      <link href="/2023/09/23/markdown-yu-fa/"/>
      <url>/2023/09/23/markdown-yu-fa/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>参考文章：</p><p><a href="https://blog.csdn.net/weixin_43863919/article/details/124648510">【超用心整理】Markdown常用语法介绍，看这一个就够了_markdown语法_海星啊的博客-CSDN博客</a></p><p><a href="https://www.jianshu.com/p/191d1e21f7ed/">Markdown基本语法 - 简书 (jianshu.com)</a></p><p><a href="https://www.jianshu.com/p/82e730892d42">Markdown的常用语法(个人总结) - 简书 (jianshu.com)</a></p><p><a href="https://blog.csdn.net/VistorsYan/article/details/109138602">markdown语法最全汇总_VistorsYan的博客-CSDN博客</a></p><p><a href="https://www.runoob.com/markdown/md-tutorial.html">Markdown 教程 | 菜鸟教程 (runoob.com)</a></p><h1 id="一、标题"><a href="#一、标题" class="headerlink" title="一、标题"></a>一、标题</h1><ol><li>使用 # 号标记<br>使用 # 号可表示 1-6 级标题，一级标题对应一个 # 号，二级标题对应两个 # 号，以此类推。</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 一级标题</span><br><span class="line">## 二级标题</span><br><span class="line">### 三级标题</span><br><span class="line">#### 四级标题</span><br><span class="line">##### 五级标题</span><br><span class="line">###### 六级标题</span><br></pre></td></tr></tbody></table></figure><p>效果：</p><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><ol start="2"><li>使用 = 和 - 标记一级和二级标题</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这是一个一级标题</span><br><span class="line">===</span><br><span class="line">这是一个二级标题</span><br><span class="line">---</span><br></pre></td></tr></tbody></table></figure><h1 id="这是一个一级标题"><a href="#这是一个一级标题" class="headerlink" title="这是一个一级标题"></a>这是一个一级标题</h1><h2 id="这是一个二级标题"><a href="#这是一个二级标题" class="headerlink" title="这是一个二级标题"></a>这是一个二级标题</h2><h1 id="二、字体"><a href="#二、字体" class="headerlink" title="二、字体"></a>二、字体</h1><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">*这是加粗的文字*</span><br><span class="line">**这是倾斜的文字**</span><br><span class="line">***这是斜体加粗的文字***</span><br><span class="line">~~这是加删除线的文字~~</span><br><span class="line">[^这是脚注]</span><br></pre></td></tr></tbody></table></figure><p>效果：</p><p><em>这是加粗的文字</em><br><strong>这是倾斜的文字</strong><br><em><strong>这是斜体加粗的文字</strong></em><br><del>这是加删除线的文字</del><br>[^这是脚注]</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;这是引用内容</span><br><span class="line">&gt;&gt;这是引用内容</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;这是引用内容</span><br></pre></td></tr></tbody></table></figure><blockquote><p>这是引用内容</p><blockquote><p>这是引用内容</p><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><p>这是引用内容</p></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><h1 id="三、列表"><a href="#三、列表" class="headerlink" title="三、列表"></a>三、列表</h1><p>无序列表使用星号(*)、加号(+)或是减号(-)作为列表标记，这些标记后面要添加一个空格</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">* 第一项 </span><br><span class="line">* 第二项 </span><br><span class="line">* 第三项 </span><br><span class="line"></span><br><span class="line">+ 第一项 </span><br><span class="line">+ 第二项 </span><br><span class="line">+ 第三项 </span><br><span class="line"></span><br><span class="line">- 第一项 </span><br><span class="line">- 第二项 </span><br><span class="line">- 第三项 </span><br></pre></td></tr></tbody></table></figure><p>效果：</p><ul><li>第一项 </li><li>第二项 </li><li>第三项</li></ul><ul><li>第一项 </li><li>第二项 </li><li>第三项</li></ul><ul><li>第一项 </li><li>第二项 </li><li>第三项</li></ul><p>有序列表使用数字并加上 <strong>.</strong> 号来表示</p><p><a name="helloworld">Hello World</a></p></body></html>]]></content>
      
      
      
        <tags>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>搭建个人博客</title>
      <link href="/2023/09/23/da-jian-ge-ren-bo-ke/"/>
      <url>/2023/09/23/da-jian-ge-ren-bo-ke/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p><a href="https://cdn.jsdelivr.net/gh/user/repo@version/file">https://cdn.jsdelivr.net/gh/user/repo@version/file</a><br><a href="https://cdn.jsdelivr.net/gh/pgzxc/CDN@1.0/blog-image/dog.png">https://cdn.jsdelivr.net/gh/pgzxc/CDN@1.0/blog-image/dog.png</a></p><h2 id="jsdelivr图片的使用"><a href="#jsdelivr图片的使用" class="headerlink" title="jsdelivr图片的使用"></a>jsdelivr图片的使用</h2><h3 id="一-直接使用仓库的图片"><a href="#一-直接使用仓库的图片" class="headerlink" title="一 直接使用仓库的图片"></a>一 直接使用仓库的图片</h3><p>![][1]</p><h3 id="二-使用发布版本的图片"><a href="#二-使用发布版本的图片" class="headerlink" title="二 使用发布版本的图片"></a>二 使用发布版本的图片</h3><p>![][2]<br>​<br>[1]:<a href="https://cdn.jsdelivr.net/gh/pgzxc/CDN/blog-image/dog.png">https://cdn.jsdelivr.net/gh/pgzxc/CDN/blog-image/dog.png</a><br>[2]:<a href="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN@1.0/blog-img/limu/4-1.png">https://cdn.jsdelivr.net/gh/YangQingHui66/CDN@1.0/blog-img/limu/4-1.png</a><br>————————————————<br>版权声明：本文为CSDN博主「PGzxc」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/Calvin_zhou/article/details/108733518">https://blog.csdn.net/Calvin_zhou/article/details/108733518</a><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN@1.0/blog-img/limu/4-1.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN@1.0/blog-img/science/1-1.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/jian/1-1.png"><br><img src="https://cdn.jsdelivr.net/gh/YangQingHui66/CDN/blog-img/limu/4-1.png"></p><h1 id="hexo发生error：spawn-failed错误的解决方法"><a href="#hexo发生error：spawn-failed错误的解决方法" class="headerlink" title="hexo发生error：spawn failed错误的解决方法"></a>hexo发生error：spawn failed错误的解决方法</h1><h2 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h2><ol><li>删除<code>.deploy_git</code>文件夹;</li><li>输入<code>git config --global core.autocrlf false</code></li><li>然后，依次执行：<br><code>hexo clean</code><br><code>hexo g</code><br><code>hexo d</code></li></ol></body></html>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
